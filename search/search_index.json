{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Zero to Mastery TensorFlow for Deep Learning Book","text":"<p>This is the online book version of the Zero to Mastery Deep Learning with TensorFlow course.</p> <p>This course will teach you foundations of deep learning and TensorFlow.</p> <p>The course is video based. However, the videos are based on the contents of this online book.</p> <p>For full code and resources see the course GitHub.</p>"},{"location":"#important-links","title":"Important links","text":"<ul> <li>\ud83c\udfa5 Watch the first 14-hours of the course on YouTube (notebooks 00, 01, 02)</li> <li>\ud83d\udcbb Sign up to the full course on the Zero to Mastery Academy (videos for notebooks 03-10)</li> <li>\ud83e\udd14 Got questions about the course? Check out the livestream Q&amp;A for the course launch</li> <li>\u2753 Stuck? Ask a question on the GitHub Discussions page or see the FAQ </li> </ul>"},{"location":"#course-materials","title":"Course materials","text":"<p>The following table represents contents of the book (each notebook is a chapter) with extra links to slides, exercises and extra-curriculum.</p> Number Notebook Exercises &amp; Extra-curriculum Slides 00 TensorFlow Fundamentals Go to exercises &amp; extra-curriculum Go to slides 01 TensorFlow Regression Go to exercises &amp; extra-curriculum Go to slides 02 TensorFlow Classification Go to exercises &amp; extra-curriculum Go to slides 03 TensorFlow Computer Vision Go to exercises &amp; extra-curriculum Go to slides 04 Transfer Learning Part 1: Feature extraction Go to exercises &amp; extra-curriculum Go to slides 05 Transfer Learning Part 2: Fine-tuning Go to exercises &amp; extra-curriculum Go to slides 06 Transfer Learning Part 3: Scaling up Go to exercises &amp; extra-curriculum Go to slides 07 Milestone Project 1: Food Vision \ud83c\udf54\ud83d\udc41, Template (your challenge) Go to exercises &amp; extra-curriculum Go to slides 08 TensorFlow NLP Fundamentals Go to exercises &amp; extra-curriculum Go to slides 09 Milestone Project 2: SkimLit \ud83d\udcc4\ud83d\udd25 Go to exercises &amp; extra-curriculum Go to slides 10 TensorFlow Time Series Fundamentals &amp; Milestone Project 3: BitPredict \ud83d\udcb0\ud83d\udcc8 Go to exercises &amp; extra-curriculum Go to slides 11 Preparing to Pass the TensorFlow Developer Certification Exam (archive) Go to exercises &amp; extra-curriculum Go to slides"},{"location":"#course-structure","title":"Course structure","text":"<p>This course is code first. The goal is to get you writing deep learning code as soon as possible.</p> <p>It is taught with the following mantra:</p> <pre><code>Code -&gt; Concept -&gt; Code -&gt; Concept -&gt; Code -&gt; Concept\n</code></pre> <p>This means we write code first then step through the concepts behind it.</p> <p>If you've got 6-months experience writing Python code and a willingness to learn (most important), you'll be able to do the course.</p>"},{"location":"#should-you-do-this-course","title":"Should you do this course?","text":"<p>Do you have 1+ years experience with deep learning and writing TensorFlow code?</p> <p>If yes, no you shouldn't, use your skills to build something. </p> <p>If no, move onto the next question.</p> <p>Have you done at least one beginner machine learning course and would like to learn about deep learning/TensorFlow?</p> <p>If yes, this course is for you.</p> <p>If no, go and do a beginner machine learning course and if you decide you want to learn TensorFlow, this page will still be here.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>What do I need to know to go through this course?</p> <ul> <li>6+ months writing Python code. Can you write a Python function which accepts and uses parameters? That\u2019s good enough. If you don\u2019t know what that means, spend another month or two writing Python code and then come back here.</li> <li>At least one beginner machine learning course. Are you familiar with the idea of training, validation and test sets? Do you know what supervised learning is? Have you used pandas, NumPy or Matplotlib before? If no to any of these, I\u2019d going through at least one machine learning course which teaches these first and then coming back. </li> <li>Comfortable using Google Colab/Jupyter Notebooks. This course uses Google Colab throughout. If you have never used Google Colab before, it works very similar to Jupyter Notebooks with a few extra features. If you\u2019re not familiar with Google Colab notebooks, I\u2019d suggest going through the Introduction to Google Colab notebook.</li> <li>Plug: The Zero to Mastery beginner-friendly machine learning course (I also teach this) teaches all of the above (and this course, the one you're reading about now, is designed as a follow on).</li> </ul>"},{"location":"#how-to-use-this-book","title":"How to use this book","text":"<p>All of the materials are taught code-first. The chapters are Jupyter Notebooks (also Google Colab notebooks) which can be run interactively.</p> <p>You can read all of the materials but they'll be best learned if you practice writing the code yourself.</p> <p> To start running a notebook interactively, click the \"Open in Colab\" button at the top of each chapter. </p>"},{"location":"#who-made-this-book","title":"Who made this book?","text":"<p>I did, ah, me, Daniel, Daniel Bourke. I'm a machine learning engineer who makes YouTube videos and writes stories, pop philosophy and machine learning coding tutorials (like the ones contained in this book).</p> <p>Sometimes documentation and other resources can be a bit hard to read for certain things. So I've done my best to make this a book (and a video course, I mean, that's where this book came from) I'd like to have read when I was getting into the exciting world of deep learning.</p>"},{"location":"#extensions","title":"Extensions","text":"<p>Enjoyed this book/course?</p> <p>I'd also recommend the following:</p> <ul> <li>Neural Networks and Deep Learning Book by Michael Nielsen - If the Zero to Mastery TensorFlow for Deep Learning book is top down, this book is bottom up. A fantastic resource to sandwich your knowledge. </li> <li>Deeplearning.AI specializations - This course focuses on code-first, the deeplearning.ai specializations will teach you what's going on behind the code.</li> <li>Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow Book (especially the 2nd half) - Many of the materials in this course were inspired by and guided by the pages of this beautiful text book.</li> <li>Full Stack Deep Learning - Learn how to turn your models into machine learning-powered applications.</li> <li>Made with ML MLOps materials - Similar to Full Stack Deep Learning but comprised into many small lessons around all the pieces of the puzzle (data collection, labelling, deployment and more) required to build a full-stack machine learning-powered application.</li> <li>fast.ai Curriculum - One of the best (and free) AI/deep learning courses online. Enough said.</li> <li>\"How does a beginner data scientist like me gain experience?\" by Daniel Bourke - Read this on how to get experience for a job after studying online/at unveristy (start the job before you have it).</li> </ul> <p>Get ready to dream in tensors!</p> <p>Onward.</p>"},{"location":"00_tensorflow_fundamentals/","title":"00. Getting started with TensorFlow: A guide to the fundamentals","text":"In\u00a0[1]: Copied! <pre># Create timestamp\nimport datetime\n\nprint(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\")\n</pre> # Create timestamp import datetime  print(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\") <pre>Notebook last run (end-to-end): 2023-04-25 05:22:53.455288\n</pre> In\u00a0[2]: Copied! <pre># Import TensorFlow\nimport tensorflow as tf\nprint(tf.__version__) # find the version number (should be 2.x+)\n</pre> # Import TensorFlow import tensorflow as tf print(tf.__version__) # find the version number (should be 2.x+) <pre>2.12.0\n</pre> In\u00a0[3]: Copied! <pre># Create a scalar (rank 0 tensor)\nscalar = tf.constant(7)\nscalar\n</pre> # Create a scalar (rank 0 tensor) scalar = tf.constant(7) scalar Out[3]: <pre>&lt;tf.Tensor: shape=(), dtype=int32, numpy=7&gt;</pre> <p>A scalar is known as a rank 0 tensor. Because it has no dimensions (it's just a number).</p> <p>\ud83d\udd11 Note: For now, you don't need to know too much about the different ranks of tensors (but we will see more on this later). The important point is knowing tensors can have an unlimited range of dimensions (the exact amount will depend on what data you're representing).</p> In\u00a0[4]: Copied! <pre># Check the number of dimensions of a tensor (ndim stands for number of dimensions)\nscalar.ndim\n</pre> # Check the number of dimensions of a tensor (ndim stands for number of dimensions) scalar.ndim Out[4]: <pre>0</pre> In\u00a0[5]: Copied! <pre># Create a vector (more than 0 dimensions)\nvector = tf.constant([10, 10])\nvector\n</pre> # Create a vector (more than 0 dimensions) vector = tf.constant([10, 10]) vector Out[5]: <pre>&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([10, 10], dtype=int32)&gt;</pre> In\u00a0[6]: Copied! <pre># Check the number of dimensions of our vector tensor\nvector.ndim\n</pre> # Check the number of dimensions of our vector tensor vector.ndim Out[6]: <pre>1</pre> In\u00a0[7]: Copied! <pre># Create a matrix (more than 1 dimension)\nmatrix = tf.constant([[10, 7],\n                      [7, 10]])\nmatrix\n</pre> # Create a matrix (more than 1 dimension) matrix = tf.constant([[10, 7],                       [7, 10]]) matrix Out[7]: <pre>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[10,  7],\n       [ 7, 10]], dtype=int32)&gt;</pre> In\u00a0[8]: Copied! <pre>matrix.ndim\n</pre> matrix.ndim Out[8]: <pre>2</pre> <p>By default, TensorFlow creates tensors with either an <code>int32</code> or <code>float32</code> datatype.</p> <p>This is known as 32-bit precision (the higher the number, the more precise the number, the more space it takes up on your computer).</p> In\u00a0[9]: Copied! <pre># Create another matrix and define the datatype\nanother_matrix = tf.constant([[10., 7.],\n                              [3., 2.],\n                              [8., 9.]], dtype=tf.float16) # specify the datatype with 'dtype'\nanother_matrix\n</pre> # Create another matrix and define the datatype another_matrix = tf.constant([[10., 7.],                               [3., 2.],                               [8., 9.]], dtype=tf.float16) # specify the datatype with 'dtype' another_matrix Out[9]: <pre>&lt;tf.Tensor: shape=(3, 2), dtype=float16, numpy=\narray([[10.,  7.],\n       [ 3.,  2.],\n       [ 8.,  9.]], dtype=float16)&gt;</pre> In\u00a0[10]: Copied! <pre># Even though another_matrix contains more numbers, its dimensions stay the same\nanother_matrix.ndim\n</pre> # Even though another_matrix contains more numbers, its dimensions stay the same another_matrix.ndim Out[10]: <pre>2</pre> In\u00a0[11]: Copied! <pre># How about a tensor? (more than 2 dimensions, although, all of the above items are also technically tensors)\ntensor = tf.constant([[[1, 2, 3],\n                       [4, 5, 6]],\n                      [[7, 8, 9],\n                       [10, 11, 12]],\n                      [[13, 14, 15],\n                       [16, 17, 18]]])\ntensor\n</pre> # How about a tensor? (more than 2 dimensions, although, all of the above items are also technically tensors) tensor = tf.constant([[[1, 2, 3],                        [4, 5, 6]],                       [[7, 8, 9],                        [10, 11, 12]],                       [[13, 14, 15],                        [16, 17, 18]]]) tensor Out[11]: <pre>&lt;tf.Tensor: shape=(3, 2, 3), dtype=int32, numpy=\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]],\n\n       [[13, 14, 15],\n        [16, 17, 18]]], dtype=int32)&gt;</pre> In\u00a0[12]: Copied! <pre>tensor.ndim\n</pre> tensor.ndim Out[12]: <pre>3</pre> <p>This is known as a rank 3 tensor (3-dimensions), however a tensor can have an arbitrary (unlimited) amount of dimensions.</p> <p>For example, you might turn a series of images into tensors with shape (224, 224, 3, 32), where:</p> <ul> <li>224, 224 (the first 2 dimensions) are the height and width of the images in pixels.</li> <li>3 is the number of colour channels of the image (red, green blue).</li> <li>32 is the batch size (the number of images a neural network sees at any one time).</li> </ul> <p>All of the above variables we've created are actually tensors. But you may also hear them referred to as their different names (the ones we gave them):</p> <ul> <li>scalar: a single number.</li> <li>vector: a number with direction (e.g. wind speed with direction).</li> <li>matrix: a 2-dimensional array of numbers.</li> <li>tensor: an n-dimensional arrary of numbers (where n can be any number, a 0-dimension tensor is a scalar, a 1-dimension tensor is a vector).</li> </ul> <p>To add to the confusion, the terms matrix and tensor are often used interchangably.</p> <p>Going forward since we're using TensorFlow, everything we refer to and use will be tensors.</p> <p>For more on the mathematical difference between scalars, vectors and matrices see the visual algebra post by Math is Fun.</p> <p></p> In\u00a0[13]: Copied! <pre># Create the same tensor with tf.Variable() and tf.constant()\nchangeable_tensor = tf.Variable([10, 7])\nunchangeable_tensor = tf.constant([10, 7])\nchangeable_tensor, unchangeable_tensor\n</pre> # Create the same tensor with tf.Variable() and tf.constant() changeable_tensor = tf.Variable([10, 7]) unchangeable_tensor = tf.constant([10, 7]) changeable_tensor, unchangeable_tensor Out[13]: <pre>(&lt;tf.Variable 'Variable:0' shape=(2,) dtype=int32, numpy=array([10,  7], dtype=int32)&gt;,\n &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([10,  7], dtype=int32)&gt;)</pre> <p>Now let's try to change one of the elements of the changable tensor.</p> In\u00a0[14]: Copied! <pre># Will error (requires the .assign() method)\nchangeable_tensor[0] = 7\nchangeable_tensor\n</pre> # Will error (requires the .assign() method) changeable_tensor[0] = 7 changeable_tensor <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-14-daecfbad2415&gt; in &lt;cell line: 2&gt;()\n      1 # Will error (requires the .assign() method)\n----&gt; 2 changeable_tensor[0] = 7\n      3 changeable_tensor\n\nTypeError: 'ResourceVariable' object does not support item assignment</pre> <p>To change an element of a <code>tf.Variable()</code> tensor requires the <code>assign()</code> method.</p> In\u00a0[15]: Copied! <pre># Won't error\nchangeable_tensor[0].assign(7)\nchangeable_tensor\n</pre> # Won't error changeable_tensor[0].assign(7) changeable_tensor Out[15]: <pre>&lt;tf.Variable 'Variable:0' shape=(2,) dtype=int32, numpy=array([7, 7], dtype=int32)&gt;</pre> <p>Now let's try to change a value in a <code>tf.constant()</code> tensor.</p> In\u00a0[16]: Copied! <pre># Will error (can't change tf.constant())\nunchangeable_tensor[0].assign(7)\nunchangleable_tensor\n</pre> # Will error (can't change tf.constant()) unchangeable_tensor[0].assign(7) unchangleable_tensor <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-16-3947b974feb9&gt; in &lt;cell line: 2&gt;()\n      1 # Will error (can't change tf.constant())\n----&gt; 2 unchangeable_tensor[0].assign(7)\n      3 unchangleable_tensor\n\n/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py in __getattr__(self, name)\n    441         np_config.enable_numpy_behavior()\n    442       \"\"\")\n--&gt; 443     self.__getattribute__(name)\n    444 \n    445   @staticmethod\n\nAttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'assign'</pre> <p>Which one should you use? <code>tf.constant()</code> or <code>tf.Variable()</code>?</p> <p>It will depend on what your problem requires. However, most of the time, TensorFlow will automatically choose for you (when loading data or modelling data).</p> In\u00a0[17]: Copied! <pre># Create two random (but the same) tensors\nrandom_1 = tf.random.Generator.from_seed(42) # set the seed for reproducibility\nrandom_1 = random_1.normal(shape=(3, 2)) # create tensor from a normal distribution \nrandom_2 = tf.random.Generator.from_seed(42)\nrandom_2 = random_2.normal(shape=(3, 2))\n\n# Are they equal?\nrandom_1, random_2, random_1 == random_2\n</pre> # Create two random (but the same) tensors random_1 = tf.random.Generator.from_seed(42) # set the seed for reproducibility random_1 = random_1.normal(shape=(3, 2)) # create tensor from a normal distribution  random_2 = tf.random.Generator.from_seed(42) random_2 = random_2.normal(shape=(3, 2))  # Are they equal? random_1, random_2, random_1 == random_2 Out[17]: <pre>(&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n array([[-0.7565803 , -0.06854702],\n        [ 0.07595026, -1.2573844 ],\n        [-0.23193765, -1.8107855 ]], dtype=float32)&gt;,\n &lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n array([[-0.7565803 , -0.06854702],\n        [ 0.07595026, -1.2573844 ],\n        [-0.23193765, -1.8107855 ]], dtype=float32)&gt;,\n &lt;tf.Tensor: shape=(3, 2), dtype=bool, numpy=\n array([[ True,  True],\n        [ True,  True],\n        [ True,  True]])&gt;)</pre> <p>The random tensors we've made are actually pseudorandom numbers (they appear as random, but really aren't).</p> <p>If we set a seed we'll get the same random numbers (if you've ever used NumPy, this is similar to <code>np.random.seed(42)</code>).</p> <p>Setting the seed says, \"hey, create some random numbers, but flavour them with X\" (X is the seed).</p> <p>What do you think will happen when we change the seed?</p> In\u00a0[18]: Copied! <pre># Create two random (and different) tensors\nrandom_3 = tf.random.Generator.from_seed(42)\nrandom_3 = random_3.normal(shape=(3, 2))\nrandom_4 = tf.random.Generator.from_seed(11)\nrandom_4 = random_4.normal(shape=(3, 2))\n\n# Check the tensors and see if they are equal\nrandom_3, random_4, random_1 == random_3, random_3 == random_4\n</pre> # Create two random (and different) tensors random_3 = tf.random.Generator.from_seed(42) random_3 = random_3.normal(shape=(3, 2)) random_4 = tf.random.Generator.from_seed(11) random_4 = random_4.normal(shape=(3, 2))  # Check the tensors and see if they are equal random_3, random_4, random_1 == random_3, random_3 == random_4 Out[18]: <pre>(&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n array([[-0.7565803 , -0.06854702],\n        [ 0.07595026, -1.2573844 ],\n        [-0.23193765, -1.8107855 ]], dtype=float32)&gt;,\n &lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n array([[ 0.2730574 , -0.29925638],\n        [-0.3652325 ,  0.61883307],\n        [-1.0130816 ,  0.2829171 ]], dtype=float32)&gt;,\n &lt;tf.Tensor: shape=(3, 2), dtype=bool, numpy=\n array([[ True,  True],\n        [ True,  True],\n        [ True,  True]])&gt;,\n &lt;tf.Tensor: shape=(3, 2), dtype=bool, numpy=\n array([[False, False],\n        [False, False],\n        [False, False]])&gt;)</pre> <p>What if you wanted to shuffle the order of a tensor?</p> <p>Wait, why would you want to do that?</p> <p>Let's say you working with 15,000 images of cats and dogs and the first 10,000 images of were of cats and the next 5,000 were of dogs. This order could effect how a neural network learns (it may overfit by learning the order of the data), instead, it might be a good idea to move your data around.</p> In\u00a0[19]: Copied! <pre># Shuffle a tensor (valuable for when you want to shuffle your data)\nnot_shuffled = tf.constant([[10, 7],\n                            [3, 4],\n                            [2, 5]])\n# Gets different results each time\ntf.random.shuffle(not_shuffled)\n</pre> # Shuffle a tensor (valuable for when you want to shuffle your data) not_shuffled = tf.constant([[10, 7],                             [3, 4],                             [2, 5]]) # Gets different results each time tf.random.shuffle(not_shuffled) Out[19]: <pre>&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=\narray([[ 2,  5],\n       [ 3,  4],\n       [10,  7]], dtype=int32)&gt;</pre> In\u00a0[20]: Copied! <pre># Shuffle in the same order every time using the seed parameter (won't acutally be the same)\ntf.random.shuffle(not_shuffled, seed=42)\n</pre> # Shuffle in the same order every time using the seed parameter (won't acutally be the same) tf.random.shuffle(not_shuffled, seed=42) Out[20]: <pre>&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=\narray([[ 2,  5],\n       [ 3,  4],\n       [10,  7]], dtype=int32)&gt;</pre> <p>Wait... why didn't the numbers come out the same?</p> <p>It's due to rule #4 of the <code>tf.random.set_seed()</code> documentation.</p> <p>\"4. If both the global and the operation seed are set: Both seeds are used in conjunction to determine the random sequence.\"</p> <p><code>tf.random.set_seed(42)</code> sets the global seed, and the <code>seed</code> parameter in <code>tf.random.shuffle(seed=42)</code> sets the operation seed.</p> <p>Because, \"Operations that rely on a random seed actually derive it from two seeds: the global and operation-level seeds. This sets the global seed.\"</p> In\u00a0[21]: Copied! <pre># Shuffle in the same order every time\n\n# Set the global random seed\ntf.random.set_seed(42)\n\n# Set the operation random seed\ntf.random.shuffle(not_shuffled, seed=42)\n</pre> # Shuffle in the same order every time  # Set the global random seed tf.random.set_seed(42)  # Set the operation random seed tf.random.shuffle(not_shuffled, seed=42) Out[21]: <pre>&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=\narray([[10,  7],\n       [ 3,  4],\n       [ 2,  5]], dtype=int32)&gt;</pre> In\u00a0[22]: Copied! <pre># Set the global random seed\ntf.random.set_seed(42) # if you comment this out you'll get different results\n\n# Set the operation random seed\ntf.random.shuffle(not_shuffled)\n</pre> # Set the global random seed tf.random.set_seed(42) # if you comment this out you'll get different results  # Set the operation random seed tf.random.shuffle(not_shuffled) Out[22]: <pre>&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=\narray([[ 3,  4],\n       [ 2,  5],\n       [10,  7]], dtype=int32)&gt;</pre> In\u00a0[23]: Copied! <pre># Make a tensor of all ones\ntf.ones(shape=(3, 2))\n</pre> # Make a tensor of all ones tf.ones(shape=(3, 2)) Out[23]: <pre>&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=\narray([[1., 1.],\n       [1., 1.],\n       [1., 1.]], dtype=float32)&gt;</pre> In\u00a0[24]: Copied! <pre># Make a tensor of all zeros\ntf.zeros(shape=(3, 2))\n</pre> # Make a tensor of all zeros tf.zeros(shape=(3, 2)) Out[24]: <pre>&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=\narray([[0., 0.],\n       [0., 0.],\n       [0., 0.]], dtype=float32)&gt;</pre> <p>You can also turn NumPy arrays in into tensors.</p> <p>Remember, the main difference between tensors and NumPy arrays is that tensors can be run on GPUs.</p> <p>\ud83d\udd11 Note: A matrix or tensor is typically represented by a capital letter (e.g. <code>X</code> or <code>A</code>) where as a vector is typically represented by a lowercase letter (e.g. <code>y</code> or <code>b</code>).</p> In\u00a0[25]: Copied! <pre>import numpy as np\nnumpy_A = np.arange(1, 25, dtype=np.int32) # create a NumPy array between 1 and 25\nA = tf.constant(numpy_A,  \n                shape=[2, 4, 3]) # note: the shape total (2*4*3) has to match the number of elements in the array\nnumpy_A, A\n</pre> import numpy as np numpy_A = np.arange(1, 25, dtype=np.int32) # create a NumPy array between 1 and 25 A = tf.constant(numpy_A,                   shape=[2, 4, 3]) # note: the shape total (2*4*3) has to match the number of elements in the array numpy_A, A Out[25]: <pre>(array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n        18, 19, 20, 21, 22, 23, 24], dtype=int32),\n &lt;tf.Tensor: shape=(2, 4, 3), dtype=int32, numpy=\n array([[[ 1,  2,  3],\n         [ 4,  5,  6],\n         [ 7,  8,  9],\n         [10, 11, 12]],\n \n        [[13, 14, 15],\n         [16, 17, 18],\n         [19, 20, 21],\n         [22, 23, 24]]], dtype=int32)&gt;)</pre> In\u00a0[26]: Copied! <pre># Create a rank 4 tensor (4 dimensions)\nrank_4_tensor = tf.zeros([2, 3, 4, 5])\nrank_4_tensor\n</pre> # Create a rank 4 tensor (4 dimensions) rank_4_tensor = tf.zeros([2, 3, 4, 5]) rank_4_tensor Out[26]: <pre>&lt;tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=\narray([[[[0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]]],\n\n\n       [[[0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]],\n\n        [[0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0.]]]], dtype=float32)&gt;</pre> In\u00a0[27]: Copied! <pre>rank_4_tensor.shape, rank_4_tensor.ndim, tf.size(rank_4_tensor)\n</pre> rank_4_tensor.shape, rank_4_tensor.ndim, tf.size(rank_4_tensor) Out[27]: <pre>(TensorShape([2, 3, 4, 5]), 4, &lt;tf.Tensor: shape=(), dtype=int32, numpy=120&gt;)</pre> In\u00a0[28]: Copied! <pre># Get various attributes of tensor\nprint(\"Datatype of every element:\", rank_4_tensor.dtype)\nprint(\"Number of dimensions (rank):\", rank_4_tensor.ndim)\nprint(\"Shape of tensor:\", rank_4_tensor.shape)\nprint(\"Elements along axis 0 of tensor:\", rank_4_tensor.shape[0])\nprint(\"Elements along last axis of tensor:\", rank_4_tensor.shape[-1])\nprint(\"Total number of elements (2*3*4*5):\", tf.size(rank_4_tensor).numpy()) # .numpy() converts to NumPy array\n</pre> # Get various attributes of tensor print(\"Datatype of every element:\", rank_4_tensor.dtype) print(\"Number of dimensions (rank):\", rank_4_tensor.ndim) print(\"Shape of tensor:\", rank_4_tensor.shape) print(\"Elements along axis 0 of tensor:\", rank_4_tensor.shape[0]) print(\"Elements along last axis of tensor:\", rank_4_tensor.shape[-1]) print(\"Total number of elements (2*3*4*5):\", tf.size(rank_4_tensor).numpy()) # .numpy() converts to NumPy array <pre>Datatype of every element: &lt;dtype: 'float32'&gt;\nNumber of dimensions (rank): 4\nShape of tensor: (2, 3, 4, 5)\nElements along axis 0 of tensor: 2\nElements along last axis of tensor: 5\nTotal number of elements (2*3*4*5): 120\n</pre> <p>You can also index tensors just like Python lists.</p> In\u00a0[29]: Copied! <pre># Get the first 2 items of each dimension\nrank_4_tensor[:2, :2, :2, :2]\n</pre> # Get the first 2 items of each dimension rank_4_tensor[:2, :2, :2, :2] Out[29]: <pre>&lt;tf.Tensor: shape=(2, 2, 2, 2), dtype=float32, numpy=\narray([[[[0., 0.],\n         [0., 0.]],\n\n        [[0., 0.],\n         [0., 0.]]],\n\n\n       [[[0., 0.],\n         [0., 0.]],\n\n        [[0., 0.],\n         [0., 0.]]]], dtype=float32)&gt;</pre> In\u00a0[30]: Copied! <pre># Get the dimension from each index except for the final one\nrank_4_tensor[:1, :1, :1, :]\n</pre> # Get the dimension from each index except for the final one rank_4_tensor[:1, :1, :1, :] Out[30]: <pre>&lt;tf.Tensor: shape=(1, 1, 1, 5), dtype=float32, numpy=array([[[[0., 0., 0., 0., 0.]]]], dtype=float32)&gt;</pre> In\u00a0[31]: Copied! <pre># Create a rank 2 tensor (2 dimensions)\nrank_2_tensor = tf.constant([[10, 7],\n                             [3, 4]])\n\n# Get the last item of each row\nrank_2_tensor[:, -1]\n</pre> # Create a rank 2 tensor (2 dimensions) rank_2_tensor = tf.constant([[10, 7],                              [3, 4]])  # Get the last item of each row rank_2_tensor[:, -1] Out[31]: <pre>&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([7, 4], dtype=int32)&gt;</pre> <p>You can also add dimensions to your tensor whilst keeping the same information present using <code>tf.newaxis</code>.</p> In\u00a0[32]: Copied! <pre># Add an extra dimension (to the end)\nrank_3_tensor = rank_2_tensor[..., tf.newaxis] # in Python \"...\" means \"all dimensions prior to\"\nrank_2_tensor, rank_3_tensor # shape (2, 2), shape (2, 2, 1)\n</pre> # Add an extra dimension (to the end) rank_3_tensor = rank_2_tensor[..., tf.newaxis] # in Python \"...\" means \"all dimensions prior to\" rank_2_tensor, rank_3_tensor # shape (2, 2), shape (2, 2, 1) Out[32]: <pre>(&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n array([[10,  7],\n        [ 3,  4]], dtype=int32)&gt;,\n &lt;tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy=\n array([[[10],\n         [ 7]],\n \n        [[ 3],\n         [ 4]]], dtype=int32)&gt;)</pre> <p>You can achieve the same using <code>tf.expand_dims()</code>.</p> In\u00a0[33]: Copied! <pre>tf.expand_dims(rank_2_tensor, axis=-1) # \"-1\" means last axis\n</pre> tf.expand_dims(rank_2_tensor, axis=-1) # \"-1\" means last axis Out[33]: <pre>&lt;tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy=\narray([[[10],\n        [ 7]],\n\n       [[ 3],\n        [ 4]]], dtype=int32)&gt;</pre> In\u00a0[34]: Copied! <pre># You can add values to a tensor using the addition operator\ntensor = tf.constant([[10, 7], [3, 4]])\ntensor + 10\n</pre> # You can add values to a tensor using the addition operator tensor = tf.constant([[10, 7], [3, 4]]) tensor + 10 Out[34]: <pre>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[20, 17],\n       [13, 14]], dtype=int32)&gt;</pre> <p>Since we used <code>tf.constant()</code>, the original tensor is unchanged (the addition gets done on a copy).</p> In\u00a0[35]: Copied! <pre># Original tensor unchanged\ntensor\n</pre> # Original tensor unchanged tensor Out[35]: <pre>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[10,  7],\n       [ 3,  4]], dtype=int32)&gt;</pre> <p>Other operators also work.</p> In\u00a0[36]: Copied! <pre># Multiplication (known as element-wise multiplication)\ntensor * 10\n</pre> # Multiplication (known as element-wise multiplication) tensor * 10 Out[36]: <pre>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[100,  70],\n       [ 30,  40]], dtype=int32)&gt;</pre> In\u00a0[37]: Copied! <pre># Subtraction\ntensor - 10\n</pre> # Subtraction tensor - 10 Out[37]: <pre>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[ 0, -3],\n       [-7, -6]], dtype=int32)&gt;</pre> <p>You can also use the equivalent TensorFlow function. Using the TensorFlow function (where possible) has the advantage of being sped up later down the line when running as part of a TensorFlow graph.</p> In\u00a0[38]: Copied! <pre># Use the tensorflow function equivalent of the '*' (multiply) operator\ntf.multiply(tensor, 10)\n</pre> # Use the tensorflow function equivalent of the '*' (multiply) operator tf.multiply(tensor, 10) Out[38]: <pre>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[100,  70],\n       [ 30,  40]], dtype=int32)&gt;</pre> In\u00a0[39]: Copied! <pre># The original tensor is still unchanged\ntensor\n</pre> # The original tensor is still unchanged tensor Out[39]: <pre>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[10,  7],\n       [ 3,  4]], dtype=int32)&gt;</pre> In\u00a0[40]: Copied! <pre># Matrix multiplication in TensorFlow\nprint(tensor)\ntf.matmul(tensor, tensor)\n</pre> # Matrix multiplication in TensorFlow print(tensor) tf.matmul(tensor, tensor) <pre>tf.Tensor(\n[[10  7]\n [ 3  4]], shape=(2, 2), dtype=int32)\n</pre> Out[40]: <pre>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[121,  98],\n       [ 42,  37]], dtype=int32)&gt;</pre> In\u00a0[41]: Copied! <pre># Matrix multiplication with Python operator '@'\ntensor @ tensor\n</pre> # Matrix multiplication with Python operator '@' tensor @ tensor Out[41]: <pre>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[121,  98],\n       [ 42,  37]], dtype=int32)&gt;</pre> <p>Both of these examples work because our <code>tensor</code> variable is of shape (2, 2).</p> <p>What if we created some tensors which had mismatched shapes?</p> In\u00a0[42]: Copied! <pre># Create (3, 2) tensor\nX = tf.constant([[1, 2],\n                 [3, 4],\n                 [5, 6]])\n\n# Create another (3, 2) tensor\nY = tf.constant([[7, 8],\n                 [9, 10],\n                 [11, 12]])\nX, Y\n</pre> # Create (3, 2) tensor X = tf.constant([[1, 2],                  [3, 4],                  [5, 6]])  # Create another (3, 2) tensor Y = tf.constant([[7, 8],                  [9, 10],                  [11, 12]]) X, Y Out[42]: <pre>(&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n array([[1, 2],\n        [3, 4],\n        [5, 6]], dtype=int32)&gt;,\n &lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n array([[ 7,  8],\n        [ 9, 10],\n        [11, 12]], dtype=int32)&gt;)</pre> In\u00a0[43]: Copied! <pre># Try to matrix multiply them (will error)\nX @ Y\n</pre> # Try to matrix multiply them (will error) X @ Y <pre>\n---------------------------------------------------------------------------\nInvalidArgumentError                      Traceback (most recent call last)\n&lt;ipython-input-43-62e1e4702ffd&gt; in &lt;cell line: 2&gt;()\n      1 # Try to matrix multiply them (will error)\n----&gt; 2 X @ Y\n\n/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs)\n    151     except Exception as e:\n    152       filtered_tb = _process_traceback_frames(e.__traceback__)\n--&gt; 153       raise e.with_traceback(filtered_tb) from None\n    154     finally:\n    155       del filtered_tb\n\n/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)\n   7260 def raise_from_not_ok_status(e, name):\n   7261   e.message += (\" name: \" + name if name is not None else \"\")\n-&gt; 7262   raise core._status_to_exception(e) from None  # pylint: disable=protected-access\n   7263 \n   7264 \n\nInvalidArgumentError: {{function_node __wrapped__MatMul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Matrix size-incompatible: In[0]: [3,2], In[1]: [3,2] [Op:MatMul]</pre> <p>Trying to matrix multiply two tensors with the shape <code>(3, 2)</code> errors because the inner dimensions don't match.</p> <p>We need to either:</p> <ul> <li>Reshape X to <code>(2, 3)</code> so it's <code>(2, 3) @ (3, 2)</code>.</li> <li>Reshape Y to <code>(3, 2)</code> so it's <code>(3, 2) @ (2, 3)</code>.</li> </ul> <p>We can do this with either:</p> <ul> <li><code>tf.reshape()</code> - allows us to reshape a tensor into a defined shape.</li> <li><code>tf.transpose()</code> - switches the dimensions of a given tensor.</li> </ul> <p></p> <p>Let's try <code>tf.reshape()</code> first.</p> In\u00a0[44]: Copied! <pre># Example of reshape (3, 2) -&gt; (2, 3)\ntf.reshape(Y, shape=(2, 3))\n</pre> # Example of reshape (3, 2) -&gt; (2, 3) tf.reshape(Y, shape=(2, 3)) Out[44]: <pre>&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[ 7,  8,  9],\n       [10, 11, 12]], dtype=int32)&gt;</pre> In\u00a0[45]: Copied! <pre># Try matrix multiplication with reshaped Y\nX @ tf.reshape(Y, shape=(2, 3))\n</pre> # Try matrix multiplication with reshaped Y X @ tf.reshape(Y, shape=(2, 3)) Out[45]: <pre>&lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy=\narray([[ 27,  30,  33],\n       [ 61,  68,  75],\n       [ 95, 106, 117]], dtype=int32)&gt;</pre> <p>It worked, let's try the same with a reshaped <code>X</code>, except this time we'll use <code>tf.transpose()</code> and <code>tf.matmul()</code>.</p> In\u00a0[46]: Copied! <pre># Example of transpose (3, 2) -&gt; (2, 3)\ntf.transpose(X)\n</pre> # Example of transpose (3, 2) -&gt; (2, 3) tf.transpose(X) Out[46]: <pre>&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[1, 3, 5],\n       [2, 4, 6]], dtype=int32)&gt;</pre> In\u00a0[47]: Copied! <pre># Try matrix multiplication \ntf.matmul(tf.transpose(X), Y)\n</pre> # Try matrix multiplication  tf.matmul(tf.transpose(X), Y) Out[47]: <pre>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[ 89,  98],\n       [116, 128]], dtype=int32)&gt;</pre> In\u00a0[48]: Copied! <pre># You can achieve the same result with parameters\ntf.matmul(a=X, b=Y, transpose_a=True, transpose_b=False)\n</pre> # You can achieve the same result with parameters tf.matmul(a=X, b=Y, transpose_a=True, transpose_b=False) Out[48]: <pre>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[ 89,  98],\n       [116, 128]], dtype=int32)&gt;</pre> <p>Notice the difference in the resulting shapes when tranposing <code>X</code> or reshaping <code>Y</code>.</p> <p>This is because of the 2nd rule mentioned above:</p> <ul> <li><code>(3, 2) @ (2, 3)</code> -&gt; <code>(3, 3)</code> done with <code>X @ tf.reshape(Y, shape=(2, 3))</code></li> <li><code>(2, 3) @ (3, 2)</code> -&gt; <code>(2, 2)</code> done with <code>tf.matmul(tf.transpose(X), Y)</code></li> </ul> <p>This kind of data manipulation is a reminder: you'll spend a lot of your time in machine learning and working with neural networks reshaping data (in the form of tensors) to prepare it to be used with various operations (such as feeding it to a model).</p> In\u00a0[49]: Copied! <pre># Perform the dot product on X and Y (requires X to be transposed)\ntf.tensordot(tf.transpose(X), Y, axes=1)\n</pre> # Perform the dot product on X and Y (requires X to be transposed) tf.tensordot(tf.transpose(X), Y, axes=1) Out[49]: <pre>&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[ 89,  98],\n       [116, 128]], dtype=int32)&gt;</pre> <p>You might notice that although using both <code>reshape</code> and <code>tranpose</code> work, you get different results when using each.</p> <p>Let's see an example, first with <code>tf.transpose()</code> then with <code>tf.reshape()</code>.</p> In\u00a0[50]: Copied! <pre># Perform matrix multiplication between X and Y (transposed)\ntf.matmul(X, tf.transpose(Y))\n</pre> # Perform matrix multiplication between X and Y (transposed) tf.matmul(X, tf.transpose(Y)) Out[50]: <pre>&lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy=\narray([[ 23,  29,  35],\n       [ 53,  67,  81],\n       [ 83, 105, 127]], dtype=int32)&gt;</pre> In\u00a0[51]: Copied! <pre># Perform matrix multiplication between X and Y (reshaped)\ntf.matmul(X, tf.reshape(Y, (2, 3)))\n</pre> # Perform matrix multiplication between X and Y (reshaped) tf.matmul(X, tf.reshape(Y, (2, 3))) Out[51]: <pre>&lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy=\narray([[ 27,  30,  33],\n       [ 61,  68,  75],\n       [ 95, 106, 117]], dtype=int32)&gt;</pre> <p>Hmm... they result in different values.</p> <p>Which is strange because when dealing with <code>Y</code> (a <code>(3x2)</code> matrix), reshaping to <code>(2, 3)</code> and tranposing it result in the same shape.</p> In\u00a0[52]: Copied! <pre># Check shapes of Y, reshaped Y and tranposed Y\nY.shape, tf.reshape(Y, (2, 3)).shape, tf.transpose(Y).shape\n</pre> # Check shapes of Y, reshaped Y and tranposed Y Y.shape, tf.reshape(Y, (2, 3)).shape, tf.transpose(Y).shape Out[52]: <pre>(TensorShape([3, 2]), TensorShape([2, 3]), TensorShape([2, 3]))</pre> <p>But calling <code>tf.reshape()</code> and <code>tf.transpose()</code> on <code>Y</code> don't necessarily result in the same values.</p> In\u00a0[53]: Copied! <pre># Check values of Y, reshape Y and tranposed Y\nprint(\"Normal Y:\")\nprint(Y, \"\\n\") # \"\\n\" for newline\n\nprint(\"Y reshaped to (2, 3):\")\nprint(tf.reshape(Y, (2, 3)), \"\\n\")\n\nprint(\"Y transposed:\")\nprint(tf.transpose(Y))\n</pre> # Check values of Y, reshape Y and tranposed Y print(\"Normal Y:\") print(Y, \"\\n\") # \"\\n\" for newline  print(\"Y reshaped to (2, 3):\") print(tf.reshape(Y, (2, 3)), \"\\n\")  print(\"Y transposed:\") print(tf.transpose(Y)) <pre>Normal Y:\ntf.Tensor(\n[[ 7  8]\n [ 9 10]\n [11 12]], shape=(3, 2), dtype=int32) \n\nY reshaped to (2, 3):\ntf.Tensor(\n[[ 7  8  9]\n [10 11 12]], shape=(2, 3), dtype=int32) \n\nY transposed:\ntf.Tensor(\n[[ 7  9 11]\n [ 8 10 12]], shape=(2, 3), dtype=int32)\n</pre> <p>As you can see, the outputs of <code>tf.reshape()</code> and <code>tf.transpose()</code> when called on <code>Y</code>, even though they have the same shape, are different.</p> <p>This can be explained by the default behaviour of each method:</p> <ul> <li><code>tf.reshape()</code> - change the shape of the given tensor (first) and then insert values in order they appear (in our case, 7, 8, 9, 10, 11, 12).</li> <li><code>tf.transpose()</code> - swap the order of the axes, by default the last axis becomes the first, however the order can be changed using the <code>perm</code> parameter.</li> </ul> <p>So which should you use?</p> <p>Again, most of the time these operations (when they need to be run, such as during the training a neural network, will be implemented for you).</p> <p>But generally, whenever performing a matrix multiplication and the shapes of two matrices don't line up, you will transpose (not reshape) one of them in order to line them up.</p> In\u00a0[54]: Copied! <pre># Create a new tensor with default datatype (float32)\nB = tf.constant([1.7, 7.4])\n\n# Create a new tensor with default datatype (int32)\nC = tf.constant([1, 7])\nB, C\n</pre> # Create a new tensor with default datatype (float32) B = tf.constant([1.7, 7.4])  # Create a new tensor with default datatype (int32) C = tf.constant([1, 7]) B, C Out[54]: <pre>(&lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.7, 7.4], dtype=float32)&gt;,\n &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 7], dtype=int32)&gt;)</pre> In\u00a0[55]: Copied! <pre># Change from float32 to float16 (reduced precision)\nB = tf.cast(B, dtype=tf.float16)\nB\n</pre> # Change from float32 to float16 (reduced precision) B = tf.cast(B, dtype=tf.float16) B Out[55]: <pre>&lt;tf.Tensor: shape=(2,), dtype=float16, numpy=array([1.7, 7.4], dtype=float16)&gt;</pre> In\u00a0[56]: Copied! <pre># Change from int32 to float32\nC = tf.cast(C, dtype=tf.float32)\nC\n</pre> # Change from int32 to float32 C = tf.cast(C, dtype=tf.float32) C Out[56]: <pre>&lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 7.], dtype=float32)&gt;</pre> In\u00a0[57]: Copied! <pre># Create tensor with negative values\nD = tf.constant([-7, -10])\nD\n</pre> # Create tensor with negative values D = tf.constant([-7, -10]) D Out[57]: <pre>&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([ -7, -10], dtype=int32)&gt;</pre> In\u00a0[58]: Copied! <pre># Get the absolute values\ntf.abs(D)\n</pre> # Get the absolute values tf.abs(D) Out[58]: <pre>&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([ 7, 10], dtype=int32)&gt;</pre> In\u00a0[59]: Copied! <pre># Create a tensor with 50 random values between 0 and 100\nE = tf.constant(np.random.randint(low=0, high=100, size=50))\nE\n</pre> # Create a tensor with 50 random values between 0 and 100 E = tf.constant(np.random.randint(low=0, high=100, size=50)) E Out[59]: <pre>&lt;tf.Tensor: shape=(50,), dtype=int64, numpy=\narray([75, 89, 76, 55, 36, 10, 93, 90, 28, 41, 21,  6, 43, 69, 47, 16, 33,\n       98, 30, 27, 20, 51,  8, 49, 82, 35, 12, 47, 59, 14, 41, 75, 71,  7,\n       48, 67, 90, 73, 71,  7,  7, 51, 81, 20, 13, 90, 47, 33, 17, 51])&gt;</pre> In\u00a0[60]: Copied! <pre># Find the minimum\ntf.reduce_min(E)\n</pre> # Find the minimum tf.reduce_min(E) Out[60]: <pre>&lt;tf.Tensor: shape=(), dtype=int64, numpy=6&gt;</pre> In\u00a0[61]: Copied! <pre># Find the maximum\ntf.reduce_max(E)\n</pre> # Find the maximum tf.reduce_max(E) Out[61]: <pre>&lt;tf.Tensor: shape=(), dtype=int64, numpy=98&gt;</pre> In\u00a0[62]: Copied! <pre># Find the mean\ntf.reduce_mean(E)\n</pre> # Find the mean tf.reduce_mean(E) Out[62]: <pre>&lt;tf.Tensor: shape=(), dtype=int64, numpy=46&gt;</pre> In\u00a0[63]: Copied! <pre># Find the sum\ntf.reduce_sum(E)\n</pre> # Find the sum tf.reduce_sum(E) Out[63]: <pre>&lt;tf.Tensor: shape=(), dtype=int64, numpy=2320&gt;</pre> <p>You can also find the standard deviation (<code>tf.reduce_std()</code>) and variance (<code>tf.reduce_variance()</code>) of elements in a tensor using similar methods.</p> In\u00a0[64]: Copied! <pre># Create a tensor with 50 values between 0 and 1\nF = tf.constant(np.random.random(50))\nF\n</pre> # Create a tensor with 50 values between 0 and 1 F = tf.constant(np.random.random(50)) F Out[64]: <pre>&lt;tf.Tensor: shape=(50,), dtype=float64, numpy=\narray([0.08892525, 0.94484011, 0.72484292, 0.11100388, 0.23637676,\n       0.40758941, 0.78697704, 0.51382876, 0.67817528, 0.33801469,\n       0.06847686, 0.43940259, 0.05384978, 0.90974966, 0.17755086,\n       0.55687455, 0.22395101, 0.61040988, 0.19115218, 0.43669498,\n       0.95362449, 0.65974345, 0.98141608, 0.72890794, 0.31333329,\n       0.95735583, 0.80562309, 0.08673455, 0.5237697 , 0.9006758 ,\n       0.07103048, 0.88667591, 0.70505817, 0.79932324, 0.28416341,\n       0.50271115, 0.2614137 , 0.22194647, 0.96336433, 0.88853101,\n       0.23221737, 0.92196873, 0.84103254, 0.01333408, 0.24513585,\n       0.74766312, 0.8508123 , 0.94218343, 0.90917265, 0.78489794])&gt;</pre> In\u00a0[65]: Copied! <pre># Find the maximum element position of F\ntf.argmax(F)\n</pre> # Find the maximum element position of F tf.argmax(F) Out[65]: <pre>&lt;tf.Tensor: shape=(), dtype=int64, numpy=22&gt;</pre> In\u00a0[66]: Copied! <pre># Find the minimum element position of F\ntf.argmin(F)\n</pre> # Find the minimum element position of F tf.argmin(F) Out[66]: <pre>&lt;tf.Tensor: shape=(), dtype=int64, numpy=43&gt;</pre> In\u00a0[67]: Copied! <pre># Find the maximum element position of F\nprint(f\"The maximum value of F is at position: {tf.argmax(F).numpy()}\") \nprint(f\"The maximum value of F is: {tf.reduce_max(F).numpy()}\") \nprint(f\"Using tf.argmax() to index F, the maximum value of F is: {F[tf.argmax(F)].numpy()}\")\nprint(f\"Are the two max values the same (they should be)? {F[tf.argmax(F)].numpy() == tf.reduce_max(F).numpy()}\")\n</pre> # Find the maximum element position of F print(f\"The maximum value of F is at position: {tf.argmax(F).numpy()}\")  print(f\"The maximum value of F is: {tf.reduce_max(F).numpy()}\")  print(f\"Using tf.argmax() to index F, the maximum value of F is: {F[tf.argmax(F)].numpy()}\") print(f\"Are the two max values the same (they should be)? {F[tf.argmax(F)].numpy() == tf.reduce_max(F).numpy()}\") <pre>The maximum value of F is at position: 22\nThe maximum value of F is: 0.9814160834311638\nUsing tf.argmax() to index F, the maximum value of F is: 0.9814160834311638\nAre the two max values the same (they should be)? True\n</pre> In\u00a0[68]: Copied! <pre># Create a rank 5 (5 dimensions) tensor of 50 numbers between 0 and 100\nG = tf.constant(np.random.randint(0, 100, 50), shape=(1, 1, 1, 1, 50))\nG.shape, G.ndim\n</pre> # Create a rank 5 (5 dimensions) tensor of 50 numbers between 0 and 100 G = tf.constant(np.random.randint(0, 100, 50), shape=(1, 1, 1, 1, 50)) G.shape, G.ndim Out[68]: <pre>(TensorShape([1, 1, 1, 1, 50]), 5)</pre> In\u00a0[69]: Copied! <pre># Squeeze tensor G (remove all 1 dimensions)\nG_squeezed = tf.squeeze(G)\nG_squeezed.shape, G_squeezed.ndim\n</pre> # Squeeze tensor G (remove all 1 dimensions) G_squeezed = tf.squeeze(G) G_squeezed.shape, G_squeezed.ndim Out[69]: <pre>(TensorShape([50]), 1)</pre> In\u00a0[70]: Copied! <pre># Create a list of indices\nsome_list = [0, 1, 2, 3]\n\n# One hot encode them\ntf.one_hot(some_list, depth=4)\n</pre> # Create a list of indices some_list = [0, 1, 2, 3]  # One hot encode them tf.one_hot(some_list, depth=4) Out[70]: <pre>&lt;tf.Tensor: shape=(4, 4), dtype=float32, numpy=\narray([[1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.]], dtype=float32)&gt;</pre> <p>You can also specify values for <code>on_value</code> and <code>off_value</code> instead of the default <code>0</code> and <code>1</code>.</p> In\u00a0[71]: Copied! <pre># Specify custom values for on and off encoding\ntf.one_hot(some_list, depth=4, on_value=\"We're live!\", off_value=\"Offline\")\n</pre> # Specify custom values for on and off encoding tf.one_hot(some_list, depth=4, on_value=\"We're live!\", off_value=\"Offline\") Out[71]: <pre>&lt;tf.Tensor: shape=(4, 4), dtype=string, numpy=\narray([[b\"We're live!\", b'Offline', b'Offline', b'Offline'],\n       [b'Offline', b\"We're live!\", b'Offline', b'Offline'],\n       [b'Offline', b'Offline', b\"We're live!\", b'Offline'],\n       [b'Offline', b'Offline', b'Offline', b\"We're live!\"]], dtype=object)&gt;</pre> In\u00a0[72]: Copied! <pre># Create a new tensor\nH = tf.constant(np.arange(1, 10))\nH\n</pre> # Create a new tensor H = tf.constant(np.arange(1, 10)) H Out[72]: <pre>&lt;tf.Tensor: shape=(9,), dtype=int64, numpy=array([1, 2, 3, 4, 5, 6, 7, 8, 9])&gt;</pre> In\u00a0[73]: Copied! <pre># Square it\ntf.square(H)\n</pre> # Square it tf.square(H) Out[73]: <pre>&lt;tf.Tensor: shape=(9,), dtype=int64, numpy=array([ 1,  4,  9, 16, 25, 36, 49, 64, 81])&gt;</pre> In\u00a0[75]: Copied! <pre># Find the squareroot (will error), needs to be non-integer\ntf.sqrt(H)\n</pre> # Find the squareroot (will error), needs to be non-integer tf.sqrt(H) <pre>\n---------------------------------------------------------------------------\nInvalidArgumentError                      Traceback (most recent call last)\n&lt;ipython-input-75-d7db039da8bb&gt; in &lt;cell line: 2&gt;()\n      1 # Find the squareroot (will error), needs to be non-integer\n----&gt; 2 tf.sqrt(H)\n\n/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs)\n    151     except Exception as e:\n    152       filtered_tb = _process_traceback_frames(e.__traceback__)\n--&gt; 153       raise e.with_traceback(filtered_tb) from None\n    154     finally:\n    155       del filtered_tb\n\n/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)\n   7260 def raise_from_not_ok_status(e, name):\n   7261   e.message += (\" name: \" + name if name is not None else \"\")\n-&gt; 7262   raise core._status_to_exception(e) from None  # pylint: disable=protected-access\n   7263 \n   7264 \n\nInvalidArgumentError: Value for attr 'T' of int64 is not in the list of allowed values: bfloat16, half, float, double, complex64, complex128\n\t; NodeDef: {{node Sqrt}}; Op&lt;name=Sqrt; signature=x:T -&gt; y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]&gt; [Op:Sqrt]</pre> In\u00a0[76]: Copied! <pre># Change H to float32\nH = tf.cast(H, dtype=tf.float32)\nH\n</pre> # Change H to float32 H = tf.cast(H, dtype=tf.float32) H Out[76]: <pre>&lt;tf.Tensor: shape=(9,), dtype=float32, numpy=array([1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=float32)&gt;</pre> In\u00a0[77]: Copied! <pre># Find the square root\ntf.sqrt(H)\n</pre> # Find the square root tf.sqrt(H) Out[77]: <pre>&lt;tf.Tensor: shape=(9,), dtype=float32, numpy=\narray([1.       , 1.4142135, 1.7320508, 2.       , 2.2360678, 2.4494896,\n       2.6457512, 2.828427 , 3.       ], dtype=float32)&gt;</pre> In\u00a0[78]: Copied! <pre># Find the log (input also needs to be float)\ntf.math.log(H)\n</pre> # Find the log (input also needs to be float) tf.math.log(H) Out[78]: <pre>&lt;tf.Tensor: shape=(9,), dtype=float32, numpy=\narray([0.       , 0.6931472, 1.0986123, 1.3862944, 1.609438 , 1.7917595,\n       1.9459102, 2.0794415, 2.1972246], dtype=float32)&gt;</pre> In\u00a0[79]: Copied! <pre># Create a variable tensor\nI = tf.Variable(np.arange(0, 5))\nI\n</pre> # Create a variable tensor I = tf.Variable(np.arange(0, 5)) I Out[79]: <pre>&lt;tf.Variable 'Variable:0' shape=(5,) dtype=int64, numpy=array([0, 1, 2, 3, 4])&gt;</pre> In\u00a0[80]: Copied! <pre># Assign the final value a new value of 50\nI.assign([0, 1, 2, 3, 50])\n</pre> # Assign the final value a new value of 50 I.assign([0, 1, 2, 3, 50]) Out[80]: <pre>&lt;tf.Variable 'UnreadVariable' shape=(5,) dtype=int64, numpy=array([ 0,  1,  2,  3, 50])&gt;</pre> In\u00a0[81]: Copied! <pre># The change happens in place (the last value is now 50, not 4)\nI\n</pre> # The change happens in place (the last value is now 50, not 4) I Out[81]: <pre>&lt;tf.Variable 'Variable:0' shape=(5,) dtype=int64, numpy=array([ 0,  1,  2,  3, 50])&gt;</pre> In\u00a0[82]: Copied! <pre># Add 10 to every element in I\nI.assign_add([10, 10, 10, 10, 10])\n</pre> # Add 10 to every element in I I.assign_add([10, 10, 10, 10, 10]) Out[82]: <pre>&lt;tf.Variable 'UnreadVariable' shape=(5,) dtype=int64, numpy=array([10, 11, 12, 13, 60])&gt;</pre> In\u00a0[83]: Copied! <pre># Again, the change happens in place\nI\n</pre> # Again, the change happens in place I Out[83]: <pre>&lt;tf.Variable 'Variable:0' shape=(5,) dtype=int64, numpy=array([10, 11, 12, 13, 60])&gt;</pre> In\u00a0[84]: Copied! <pre># Create a tensor from a NumPy array\nJ = tf.constant(np.array([3., 7., 10.]))\nJ\n</pre> # Create a tensor from a NumPy array J = tf.constant(np.array([3., 7., 10.])) J Out[84]: <pre>&lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([ 3.,  7., 10.])&gt;</pre> In\u00a0[85]: Copied! <pre># Convert tensor J to NumPy with np.array()\nnp.array(J), type(np.array(J))\n</pre> # Convert tensor J to NumPy with np.array() np.array(J), type(np.array(J)) Out[85]: <pre>(array([ 3.,  7., 10.]), numpy.ndarray)</pre> In\u00a0[86]: Copied! <pre># Convert tensor J to NumPy with .numpy()\nJ.numpy(), type(J.numpy())\n</pre> # Convert tensor J to NumPy with .numpy() J.numpy(), type(J.numpy()) Out[86]: <pre>(array([ 3.,  7., 10.]), numpy.ndarray)</pre> <p>By default tensors have <code>dtype=float32</code>, where as NumPy arrays have <code>dtype=float64</code>.</p> <p>This is because neural networks (which are usually built with TensorFlow) can generally work very well with less precision (32-bit rather than 64-bit).</p> In\u00a0[87]: Copied! <pre># Create a tensor from NumPy and from an array\nnumpy_J = tf.constant(np.array([3., 7., 10.])) # will be float64 (due to NumPy)\ntensor_J = tf.constant([3., 7., 10.]) # will be float32 (due to being TensorFlow default)\nnumpy_J.dtype, tensor_J.dtype\n</pre> # Create a tensor from NumPy and from an array numpy_J = tf.constant(np.array([3., 7., 10.])) # will be float64 (due to NumPy) tensor_J = tf.constant([3., 7., 10.]) # will be float32 (due to being TensorFlow default) numpy_J.dtype, tensor_J.dtype Out[87]: <pre>(tf.float64, tf.float32)</pre> In\u00a0[88]: Copied! <pre># Create a simple function\ndef function(x, y):\n  return x ** 2 + y\n\nx = tf.constant(np.arange(0, 10))\ny = tf.constant(np.arange(10, 20))\nfunction(x, y)\n</pre> # Create a simple function def function(x, y):   return x ** 2 + y  x = tf.constant(np.arange(0, 10)) y = tf.constant(np.arange(10, 20)) function(x, y) Out[88]: <pre>&lt;tf.Tensor: shape=(10,), dtype=int64, numpy=array([ 10,  12,  16,  22,  30,  40,  52,  66,  82, 100])&gt;</pre> In\u00a0[89]: Copied! <pre># Create the same function and decorate it with tf.function\n@tf.function\ndef tf_function(x, y):\n  return x ** 2 + y\n\ntf_function(x, y)\n</pre> # Create the same function and decorate it with tf.function @tf.function def tf_function(x, y):   return x ** 2 + y  tf_function(x, y) Out[89]: <pre>&lt;tf.Tensor: shape=(10,), dtype=int64, numpy=array([ 10,  12,  16,  22,  30,  40,  52,  66,  82, 100])&gt;</pre> <p>If you noticed no difference between the above two functions (the decorated one and the non-decorated one) you'd be right.</p> <p>Much of the difference happens behind the scenes. One of the main ones being potential code speed-ups where possible.</p> In\u00a0[90]: Copied! <pre>print(tf.config.list_physical_devices('GPU'))\n</pre> print(tf.config.list_physical_devices('GPU')) <pre>[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n</pre> <p>If the above outputs an empty array (or nothing), it means you don't have access to a GPU (or at least TensorFlow can't find it).</p> <p>If you're running in Google Colab, you can access a GPU by going to Runtime -&gt; Change Runtime Type -&gt; Select GPU (note: after doing this your notebook will restart and any variables you've saved will be lost).</p> <p>Once you've changed your runtime type, run the cell below.</p> In\u00a0[91]: Copied! <pre>import tensorflow as tf\nprint(tf.config.list_physical_devices('GPU'))\n</pre> import tensorflow as tf print(tf.config.list_physical_devices('GPU')) <pre>[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n</pre> <p>If you've got access to a GPU, the cell above should output something like:</p> <p><code>[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]</code></p> <p>You can also find information about your GPU using <code>!nvidia-smi</code>.</p> In\u00a0[92]: Copied! <pre>!nvidia-smi\n</pre> !nvidia-smi <pre>Tue Apr 25 05:24:15 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   32C    P0    51W / 400W |    693MiB / 40960MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\n</pre> <p>\ud83d\udd11 Note: If you have access to a GPU, TensorFlow will automatically use it whenever possible.</p>"},{"location":"00_tensorflow_fundamentals/#00-getting-started-with-tensorflow-a-guide-to-the-fundamentals","title":"00. Getting started with TensorFlow: A guide to the fundamentals\u00b6","text":""},{"location":"00_tensorflow_fundamentals/#what-is-tensorflow","title":"What is TensorFlow?\u00b6","text":"<p>TensorFlow is an open-source end-to-end machine learning library for preprocessing data, modelling data and serving models (getting them into the hands of others).</p>"},{"location":"00_tensorflow_fundamentals/#why-use-tensorflow","title":"Why use TensorFlow?\u00b6","text":"<p>Rather than building machine learning and deep learning models from scratch, it's more likely you'll use a library such as TensorFlow. This is because it contains many of the most common machine learning functions you'll want to use.</p>"},{"location":"00_tensorflow_fundamentals/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>TensorFlow is vast. But the main premise is simple: turn data into numbers (tensors) and build machine learning algorithms to find patterns in them.</p> <p>In this notebook we cover some of the most fundamental TensorFlow operations, more specificially:</p> <ul> <li>Introduction to tensors (creating tensors)</li> <li>Getting information from tensors (tensor attributes)</li> <li>Manipulating tensors (tensor operations)</li> <li>Tensors and NumPy</li> <li>Using @tf.function (a way to speed up your regular Python functions)</li> <li>Using GPUs with TensorFlow</li> <li>Exercises to try</li> </ul> <p>Things to note:</p> <ul> <li>Many of the conventions here will happen automatically behind the scenes (when you build a model) but it's worth knowing so if you see any of these things, you know what's happening.</li> <li>For any TensorFlow function you see, it's important to be able to check it out in the documentation, for example, going to the Python API docs for all functions and searching for what you need: https://www.tensorflow.org/api_docs/python/ (don't worry if this seems overwhelming at first, with enough practice, you'll get used to navigating the documentaiton).</li> </ul>"},{"location":"00_tensorflow_fundamentals/#introduction-to-tensors","title":"Introduction to Tensors\u00b6","text":"<p>If you've ever used NumPy, tensors are kind of like NumPy arrays (we'll see more on this later).</p> <p>For the sake of this notebook and going forward, you can think of a tensor as a multi-dimensional numerical representation (also referred to as n-dimensional, where n can be any number) of something. Where something can be almost anything you can imagine:</p> <ul> <li>It could be numbers themselves (using tensors to represent the price of houses).</li> <li>It could be an image (using tensors to represent the pixels of an image).</li> <li>It could be text (using tensors to represent words).</li> <li>Or it could be some other form of information (or data) you want to represent with numbers.</li> </ul> <p>The main difference between tensors and NumPy arrays (also an n-dimensional array of numbers) is that tensors can be used on GPUs (graphical processing units) and TPUs (tensor processing units).</p> <p>The benefit of being able to run on GPUs and TPUs is faster computation, this means, if we wanted to find patterns in the numerical representations of our data, we can generally find them faster using GPUs and TPUs.</p> <p>Okay, we've been talking enough about tensors, let's see them.</p> <p>The first thing we'll do is import TensorFlow under the common alias <code>tf</code>.</p>"},{"location":"00_tensorflow_fundamentals/#creating-tensors-with-tfconstant","title":"Creating Tensors with <code>tf.constant()</code>\u00b6","text":"<p>As mentioned before, in general, you usually won't create tensors yourself. This is because TensorFlow has modules built-in (such as <code>tf.io</code> and <code>tf.data</code>) which are able to read your data sources and automatically convert them to tensors and then later on, neural network models will process these for us.</p> <p>But for now, because we're getting familar with tensors themselves and how to manipulate them, we'll see how we can create them ourselves.</p> <p>We'll begin by using <code>tf.constant()</code>.</p>"},{"location":"00_tensorflow_fundamentals/#creating-tensors-with-tfvariable","title":"Creating Tensors with <code>tf.Variable()</code>\u00b6","text":"<p>You can also (although you likely rarely will, because often, when working with data, tensors are created for you automatically) create tensors using <code>tf.Variable()</code>.</p> <p>The difference between <code>tf.Variable()</code> and <code>tf.constant()</code> is tensors created with <code>tf.constant()</code> are immutable (can't be changed, can only be used to create a new tensor), where as, tensors created with <code>tf.Variable()</code> are mutable (can be changed).</p>"},{"location":"00_tensorflow_fundamentals/#creating-random-tensors","title":"Creating random tensors\u00b6","text":"<p>Random tensors are tensors of some abitrary size which contain random numbers.</p> <p>Why would you want to create random tensors?</p> <p>This is what neural networks use to intialize their weights (patterns) that they're trying to learn in the data.</p> <p>For example, the process of a neural network learning often involves taking a random n-dimensional array of numbers and refining them until they represent some kind of pattern (a compressed way to represent the original data).</p> <p>How a network learns A network learns by starting with random patterns (1) then going through demonstrative examples of data (2) whilst trying to update its random patterns to represent the examples (3).</p> <p>We can create random tensors by using the <code>tf.random.Generator</code> class.</p>"},{"location":"00_tensorflow_fundamentals/#other-ways-to-make-tensors","title":"Other ways to make tensors\u00b6","text":"<p>Though you might rarely use these (remember, many tensor operations are done behind the scenes for you), you can use <code>tf.ones()</code> to create a tensor of all ones and <code>tf.zeros()</code> to create a tensor of all zeros.</p>"},{"location":"00_tensorflow_fundamentals/#getting-information-from-tensors-shape-rank-size","title":"Getting information from tensors (shape, rank, size)\u00b6","text":"<p>There will be times when you'll want to get different pieces of information from your tensors, in particuluar, you should know the following tensor vocabulary:</p> <ul> <li>Shape: The length (number of elements) of each of the dimensions of a tensor.</li> <li>Rank: The number of tensor dimensions. A scalar has rank 0, a vector has rank 1, a matrix is rank 2, a tensor has rank n.</li> <li>Axis or Dimension: A particular dimension of a tensor.</li> <li>Size: The total number of items in the tensor.</li> </ul> <p>You'll use these especially when you're trying to line up the shapes of your data to the shapes of your model. For example, making sure the shape of your image tensors are the same shape as your models input layer.</p> <p>We've already seen one of these before using the <code>ndim</code> attribute. Let's see the rest.</p>"},{"location":"00_tensorflow_fundamentals/#manipulating-tensors-tensor-operations","title":"Manipulating tensors (tensor operations)\u00b6","text":"<p>Finding patterns in tensors (numberical representation of data) requires manipulating them.</p> <p>Again, when building models in TensorFlow, much of this pattern discovery is done for you.</p>"},{"location":"00_tensorflow_fundamentals/#basic-operations","title":"Basic operations\u00b6","text":"<p>You can perform many of the basic mathematical operations directly on tensors using Pyhton operators such as, <code>+</code>, <code>-</code>, <code>*</code>.</p>"},{"location":"00_tensorflow_fundamentals/#matrix-mutliplication","title":"Matrix mutliplication\u00b6","text":"<p>One of the most common operations in machine learning algorithms is matrix multiplication.</p> <p>TensorFlow implements this matrix multiplication functionality in the <code>tf.matmul()</code> method.</p> <p>The main two rules for matrix multiplication to remember are:</p> <ol> <li>The inner dimensions must match:</li> </ol> <ul> <li><code>(3, 5) @ (3, 5)</code> won't work</li> <li><code>(5, 3) @ (3, 5)</code> will work</li> <li><code>(3, 5) @ (5, 3)</code> will work</li> </ul> <ol> <li>The resulting matrix has the shape of the outer dimensions:</li> </ol> <ul> <li><code>(5, 3) @ (3, 5)</code> -&gt; <code>(5, 5)</code></li> <li><code>(3, 5) @ (5, 3)</code> -&gt; <code>(3, 3)</code></li> </ul> <p>\ud83d\udd11 Note: '<code>@</code>' in Python is the symbol for matrix multiplication.</p>"},{"location":"00_tensorflow_fundamentals/#the-dot-product","title":"The dot product\u00b6","text":"<p>Multiplying matrices by eachother is also referred to as the dot product.</p> <p>You can perform the <code>tf.matmul()</code> operation using <code>tf.tensordot()</code>.</p>"},{"location":"00_tensorflow_fundamentals/#matrix-multiplication-tidbits","title":"Matrix multiplication tidbits\u00b6","text":"<ul> <li>If we transposed <code>Y</code>, it would be represented as $\\mathbf{Y}^\\mathsf{T}$ (note the capital T for tranpose).</li> <li>Get an illustrative view of matrix multiplication by Math is Fun.</li> <li>Try a hands-on demo of matrix multiplcation: http://matrixmultiplication.xyz/ (shown below).</li> </ul>"},{"location":"00_tensorflow_fundamentals/#changing-the-datatype-of-a-tensor","title":"Changing the datatype of a tensor\u00b6","text":"<p>Sometimes you'll want to alter the default datatype of your tensor.</p> <p>This is common when you want to compute using less precision (e.g. 16-bit floating point numbers vs. 32-bit floating point numbers).</p> <p>Computing with less precision is useful on devices with less computing capacity such as mobile devices (because the less bits, the less space the computations require).</p> <p>You can change the datatype of a tensor using <code>tf.cast()</code>.</p>"},{"location":"00_tensorflow_fundamentals/#getting-the-absolute-value","title":"Getting the absolute value\u00b6","text":"<p>Sometimes you'll want the absolute values (all values are positive) of elements in your tensors.</p> <p>To do so, you can use <code>tf.abs()</code>.</p>"},{"location":"00_tensorflow_fundamentals/#finding-the-min-max-mean-sum-aggregation","title":"Finding the min, max, mean, sum (aggregation)\u00b6","text":"<p>You can quickly aggregate (perform a calculation on a whole tensor) tensors to find things like the minimum value, maximum value, mean and sum of all the elements.</p> <p>To do so, aggregation methods typically have the syntax <code>reduce()_[action]</code>, such as:</p> <ul> <li><code>tf.reduce_min()</code> - find the minimum value in a tensor.</li> <li><code>tf.reduce_max()</code> - find the maximum value in a tensor (helpful for when you want to find the highest prediction probability).</li> <li><code>tf.reduce_mean()</code> - find the mean of all elements in a tensor.</li> <li><code>tf.reduce_sum()</code> - find the sum of all elements in a tensor.</li> <li>Note: typically, each of these is under the <code>math</code> module, e.g. <code>tf.math.reduce_min()</code> but you can use the alias <code>tf.reduce_min()</code>.</li> </ul> <p>Let's see them in action.</p>"},{"location":"00_tensorflow_fundamentals/#finding-the-positional-maximum-and-minimum","title":"Finding the positional maximum and minimum\u00b6","text":"<p>How about finding the position a tensor where the maximum value occurs?</p> <p>This is helpful when you want to line up your labels (say <code>['Green', 'Blue', 'Red']</code>) with your prediction probabilities tensor (e.g. <code>[0.98, 0.01, 0.01]</code>).</p> <p>In this case, the predicted label (the one with the highest prediction probability) would be <code>'Green'</code>.</p> <p>You can do the same for the minimum (if required) with the following:</p> <ul> <li><code>tf.argmax()</code> - find the position of the maximum element in a given tensor.</li> <li><code>tf.argmin()</code> - find the position of the minimum element in a given tensor.</li> </ul>"},{"location":"00_tensorflow_fundamentals/#squeezing-a-tensor-removing-all-single-dimensions","title":"Squeezing a tensor (removing all single dimensions)\u00b6","text":"<p>If you need to remove single-dimensions from a tensor (dimensions with size 1), you can use <code>tf.squeeze()</code>.</p> <ul> <li><code>tf.squeeze()</code> - remove all dimensions of 1 from a tensor.</li> </ul>"},{"location":"00_tensorflow_fundamentals/#one-hot-encoding","title":"One-hot encoding\u00b6","text":"<p>If you have a tensor of indicies and would like to one-hot encode it, you can use <code>tf.one_hot()</code>.</p> <p>You should also specify the <code>depth</code> parameter (the level which you want to one-hot encode to).</p>"},{"location":"00_tensorflow_fundamentals/#squaring-log-square-root","title":"Squaring, log, square root\u00b6","text":"<p>Many other common mathematical operations you'd like to perform at some stage, probably exist.</p> <p>Let's take a look at:</p> <ul> <li><code>tf.square()</code> - get the square of every value in a tensor.</li> <li><code>tf.sqrt()</code> - get the squareroot of every value in a tensor (note: the elements need to be floats or this will error).</li> <li><code>tf.math.log()</code> - get the natural log of every value in a tensor (elements need to floats).</li> </ul>"},{"location":"00_tensorflow_fundamentals/#manipulating-tfvariable-tensors","title":"Manipulating <code>tf.Variable</code> tensors\u00b6","text":"<p>Tensors created with <code>tf.Variable()</code> can be changed in place using methods such as:</p> <ul> <li><code>.assign()</code> - assign a different value to a particular index of a variable tensor.</li> <li><code>.add_assign()</code> - add to an existing value and reassign it at a particular index of a variable tensor.</li> </ul>"},{"location":"00_tensorflow_fundamentals/#tensors-and-numpy","title":"Tensors and NumPy\u00b6","text":"<p>We've seen some examples of tensors interact with NumPy arrays, such as, using NumPy arrays to create tensors.</p> <p>Tensors can also be converted to NumPy arrays using:</p> <ul> <li><code>np.array()</code> - pass a tensor to convert to an ndarray (NumPy's main datatype).</li> <li><code>tensor.numpy()</code> - call on a tensor to convert to an ndarray.</li> </ul> <p>Doing this is helpful as it makes tensors iterable as well as allows us to use any of NumPy's methods on them.</p>"},{"location":"00_tensorflow_fundamentals/#using-tffunction","title":"Using <code>@tf.function</code>\u00b6","text":"<p>In your TensorFlow adventures, you might come across Python functions which have the decorator <code>@tf.function</code>.</p> <p>If you aren't sure what Python decorators do, read RealPython's guide on them.</p> <p>But in short, decorators modify a function in one way or another.</p> <p>In the <code>@tf.function</code> decorator case, it turns a Python function into a callable TensorFlow graph. Which is a fancy way of saying, if you've written your own Python function, and you decorate it with <code>@tf.function</code>, when you export your code (to potentially run on another device), TensorFlow will attempt to convert it into a fast(er) version of itself (by making it part of a computation graph).</p> <p>For more on this, read the Better performnace with tf.function guide.</p>"},{"location":"00_tensorflow_fundamentals/#finding-access-to-gpus","title":"Finding access to GPUs\u00b6","text":"<p>We've mentioned GPUs plenty of times throughout this notebook.</p> <p>So how do you check if you've got one available?</p> <p>You can check if you've got access to a GPU using <code>tf.config.list_physical_devices()</code>.</p>"},{"location":"00_tensorflow_fundamentals/#exercises","title":"\ud83d\udee0 Exercises\u00b6","text":"<ol> <li>Create a vector, scalar, matrix and tensor with values of your choosing using <code>tf.constant()</code>.</li> <li>Find the shape, rank and size of the tensors you created in 1.</li> <li>Create two tensors containing random values between 0 and 1 with shape <code>[5, 300]</code>.</li> <li>Multiply the two tensors you created in 3 using matrix multiplication.</li> <li>Multiply the two tensors you created in 3 using dot product.</li> <li>Create a tensor with random values between 0 and 1 with shape <code>[224, 224, 3]</code>.</li> <li>Find the min and max values of the tensor you created in 6.</li> <li>Created a tensor with random values of shape <code>[1, 224, 224, 3]</code> then squeeze it to change the shape to <code>[224, 224, 3]</code>.</li> <li>Create a tensor with shape <code>[10]</code> using your own choice of values, then find the index which has the maximum value.</li> <li>One-hot encode the tensor you created in 9.</li> </ol>"},{"location":"00_tensorflow_fundamentals/#extra-curriculum","title":"\ud83d\udcd6 Extra-curriculum\u00b6","text":"<ul> <li>Read through the list of TensorFlow Python APIs, pick one we haven't gone through in this notebook, reverse engineer it (write out the documentation code for yourself) and figure out what it does.</li> <li>Try to create a series of tensor functions to calculate your most recent grocery bill (it's okay if you don't use the names of the items, just the price in numerical form).<ul> <li>How would you calculate your grocery bill for the month and for the year using tensors?</li> </ul> </li> <li>Go through the TensorFlow 2.x quick start for beginners tutorial (be sure to type out all of the code yourself, even if you don't understand it).<ul> <li>Are there any functions we used in here that match what's used in there? Which are the same? Which haven't you seen before?</li> </ul> </li> <li>Watch the video \"What's a tensor?\" - a great visual introduction to many of the concepts we've covered in this notebook.</li> </ul>"},{"location":"01_neural_network_regression_in_tensorflow/","title":"01. Neural Network Regression with TensorFlow","text":"In\u00a0[1]: Copied! <pre>import tensorflow as tf\nprint(tf.__version__) # check the version (should be 2.x+)\n\nimport datetime\nprint(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\")\n</pre> import tensorflow as tf print(tf.__version__) # check the version (should be 2.x+)  import datetime print(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\") <pre>2.12.0\nNotebook last run (end-to-end): 2023-05-07 23:10:01.302908\n</pre> In\u00a0[2]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Create features\nX = np.array([-7.0, -4.0, -1.0, 2.0, 5.0, 8.0, 11.0, 14.0])\n\n# Create labels\ny = np.array([3.0, 6.0, 9.0, 12.0, 15.0, 18.0, 21.0, 24.0])\n\n# Visualize it\nplt.scatter(X, y);\n</pre> import numpy as np import matplotlib.pyplot as plt  # Create features X = np.array([-7.0, -4.0, -1.0, 2.0, 5.0, 8.0, 11.0, 14.0])  # Create labels y = np.array([3.0, 6.0, 9.0, 12.0, 15.0, 18.0, 21.0, 24.0])  # Visualize it plt.scatter(X, y); <p>Before we do any modelling, can you calculate the pattern between <code>X</code> and <code>y</code>?</p> <p>For example, say I asked you, based on this data what the <code>y</code> value would be if <code>X</code> was 17.0?</p> <p>Or how about if <code>X</code> was -10.0?</p> <p>This kind of pattern discovery is the essence of what we'll be building neural networks to do for us.</p> In\u00a0[3]: Copied! <pre># Example input and output shapes of a regression model\nhouse_info = tf.constant([\"bedroom\", \"bathroom\", \"garage\"])\nhouse_price = tf.constant([939700])\nhouse_info, house_price\n</pre> # Example input and output shapes of a regression model house_info = tf.constant([\"bedroom\", \"bathroom\", \"garage\"]) house_price = tf.constant([939700]) house_info, house_price Out[3]: <pre>(&lt;tf.Tensor: shape=(3,), dtype=string, numpy=array([b'bedroom', b'bathroom', b'garage'], dtype=object)&gt;,\n &lt;tf.Tensor: shape=(1,), dtype=int32, numpy=array([939700], dtype=int32)&gt;)</pre> In\u00a0[4]: Copied! <pre>house_info.shape\n</pre> house_info.shape Out[4]: <pre>TensorShape([3])</pre> In\u00a0[5]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Create features (using tensors)\nX = tf.constant([-7.0, -4.0, -1.0, 2.0, 5.0, 8.0, 11.0, 14.0])\n\n# Create labels (using tensors)\ny = tf.constant([3.0, 6.0, 9.0, 12.0, 15.0, 18.0, 21.0, 24.0])\n\n# Visualize it\nplt.scatter(X, y);\n</pre> import numpy as np import matplotlib.pyplot as plt  # Create features (using tensors) X = tf.constant([-7.0, -4.0, -1.0, 2.0, 5.0, 8.0, 11.0, 14.0])  # Create labels (using tensors) y = tf.constant([3.0, 6.0, 9.0, 12.0, 15.0, 18.0, 21.0, 24.0])  # Visualize it plt.scatter(X, y); <p>Our goal here will be to use <code>X</code> to predict <code>y</code>.</p> <p>So our input will be <code>X</code> and our output will be <code>y</code>.</p> <p>Knowing this, what do you think our input and output shapes will be?</p> <p>Let's take a look.</p> In\u00a0[6]: Copied! <pre># Take a single example of X\ninput_shape = X[0].shape \n\n# Take a single example of y\noutput_shape = y[0].shape\n\ninput_shape, output_shape # these are both scalars (no shape)\n</pre> # Take a single example of X input_shape = X[0].shape   # Take a single example of y output_shape = y[0].shape  input_shape, output_shape # these are both scalars (no shape) Out[6]: <pre>(TensorShape([]), TensorShape([]))</pre> <p>Huh?</p> <p>From this it seems our inputs and outputs have no shape?</p> <p>How could that be?</p> <p>It's because no matter what kind of data we pass to our model, it's always going to take as input and return as output some kind of tensor.</p> <p>But in our case because of our dataset (only 2 small lists of numbers), we're looking at a special kind of tensor, more specifically a rank 0 tensor or a scalar.</p> In\u00a0[7]: Copied! <pre># Let's take a look at the single examples invidually\nX[0], y[0]\n</pre> # Let's take a look at the single examples invidually X[0], y[0] Out[7]: <pre>(&lt;tf.Tensor: shape=(), dtype=float32, numpy=-7.0&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.0&gt;)</pre> <p>In our case, we're trying to build a model to predict the pattern between <code>X[0]</code> equalling <code>-7.0</code> and <code>y[0]</code> equalling <code>3.0</code>.</p> <p>So now we get our answer, we're trying to use 1 <code>X</code> value to predict 1 <code>y</code> value.</p> <p>You might be thinking, \"this seems pretty complicated for just predicting a straight line...\".</p> <p>And you'd be right.</p> <p>But the concepts we're covering here, the concepts of input and output shapes to a model are fundamental.</p> <p>In fact, they're probably two of the things you'll spend the most time on when you work with neural networks: making sure your input and outputs are in the correct shape.</p> <p>If it doesn't make sense now, we'll see plenty more examples later on (soon you'll notice the input and output shapes can be almost anything you can imagine).</p> <p> If you were working on building a machine learning algorithm for predicting housing prices, your inputs may be number of bedrooms, number of bathrooms and number of garages, giving you an input shape of 3 (3 different features). And since you're trying to predict the price of the house, your output shape would be 1.</p> In\u00a0[8]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create a model using the Sequential API\nmodel = tf.keras.Sequential([\n  tf.keras.layers.Dense(1)\n])\n\n# Compile the model\nmodel.compile(loss=tf.keras.losses.mae, # mae is short for mean absolute error\n              optimizer=tf.keras.optimizers.SGD(), # SGD is short for stochastic gradient descent\n              metrics=[\"mae\"])\n\n# Fit the model\n# model.fit(X, y, epochs=5) # this will break with TensorFlow 2.7.0+\nmodel.fit(tf.expand_dims(X, axis=-1), y, epochs=5)\n</pre> # Set random seed tf.random.set_seed(42)  # Create a model using the Sequential API model = tf.keras.Sequential([   tf.keras.layers.Dense(1) ])  # Compile the model model.compile(loss=tf.keras.losses.mae, # mae is short for mean absolute error               optimizer=tf.keras.optimizers.SGD(), # SGD is short for stochastic gradient descent               metrics=[\"mae\"])  # Fit the model # model.fit(X, y, epochs=5) # this will break with TensorFlow 2.7.0+ model.fit(tf.expand_dims(X, axis=-1), y, epochs=5) <pre>Epoch 1/5\n1/1 [==============================] - 6s 6s/step - loss: 19.2976 - mae: 19.2976\nEpoch 2/5\n1/1 [==============================] - 0s 18ms/step - loss: 19.0164 - mae: 19.0164\nEpoch 3/5\n1/1 [==============================] - 0s 12ms/step - loss: 18.7351 - mae: 18.7351\nEpoch 4/5\n1/1 [==============================] - 0s 13ms/step - loss: 18.4539 - mae: 18.4539\nEpoch 5/5\n1/1 [==============================] - 0s 13ms/step - loss: 18.1726 - mae: 18.1726\n</pre> Out[8]: <pre>&lt;keras.callbacks.History at 0x7f00663d2680&gt;</pre> <p>Boom!</p> <p>We've just trained a model to figure out the patterns between <code>X</code> and <code>y</code>.</p> <p>How do you think it went?</p> In\u00a0[9]: Copied! <pre># Check out X and y\nX, y\n</pre> # Check out X and y X, y Out[9]: <pre>(&lt;tf.Tensor: shape=(8,), dtype=float32, numpy=array([-7., -4., -1.,  2.,  5.,  8., 11., 14.], dtype=float32)&gt;,\n &lt;tf.Tensor: shape=(8,), dtype=float32, numpy=array([ 3.,  6.,  9., 12., 15., 18., 21., 24.], dtype=float32)&gt;)</pre> <p>What do you think the outcome should be if we passed our model an <code>X</code> value of 17.0?</p> In\u00a0[10]: Copied! <pre># Make a prediction with the model\nmodel.predict([17.0])\n</pre> # Make a prediction with the model model.predict([17.0]) <pre>1/1 [==============================] - 0s 115ms/step\n</pre> Out[10]: <pre>array([[-16.701845]], dtype=float32)</pre> <p>It doesn't go very well... it should've output something close to 27.0.</p> <p>\ud83e\udd14 Question: What's Keras? I thought we were working with TensorFlow but every time we write TensorFlow code, <code>keras</code> comes after <code>tf</code> (e.g. <code>tf.keras.layers.Dense()</code>)?</p> <p>Before TensorFlow 2.0+, Keras was an API designed to be able to build deep learning models with ease. Since TensorFlow 2.0+, its functionality has been tightly integrated within the TensorFlow library.</p> In\u00a0[11]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create a model (same as above)\nmodel = tf.keras.Sequential([\n  tf.keras.layers.Dense(1)\n])\n\n# Compile model (same as above)\nmodel.compile(loss=tf.keras.losses.mae,\n              optimizer=tf.keras.optimizers.SGD(),\n              metrics=[\"mae\"])\n\n# Fit model (this time we'll train for longer)\nmodel.fit(tf.expand_dims(X, axis=-1), y, epochs=100) # train for 100 epochs not 10\n</pre> # Set random seed tf.random.set_seed(42)  # Create a model (same as above) model = tf.keras.Sequential([   tf.keras.layers.Dense(1) ])  # Compile model (same as above) model.compile(loss=tf.keras.losses.mae,               optimizer=tf.keras.optimizers.SGD(),               metrics=[\"mae\"])  # Fit model (this time we'll train for longer) model.fit(tf.expand_dims(X, axis=-1), y, epochs=100) # train for 100 epochs not 10 <pre>Epoch 1/100\n1/1 [==============================] - 1s 604ms/step - loss: 12.9936 - mae: 12.9936\nEpoch 2/100\n1/1 [==============================] - 0s 14ms/step - loss: 12.8611 - mae: 12.8611\nEpoch 3/100\n1/1 [==============================] - 0s 16ms/step - loss: 12.7286 - mae: 12.7286\nEpoch 4/100\n1/1 [==============================] - 0s 16ms/step - loss: 12.5961 - mae: 12.5961\nEpoch 5/100\n1/1 [==============================] - 0s 15ms/step - loss: 12.4636 - mae: 12.4636\nEpoch 6/100\n1/1 [==============================] - 0s 15ms/step - loss: 12.3311 - mae: 12.3311\nEpoch 7/100\n1/1 [==============================] - 0s 15ms/step - loss: 12.1986 - mae: 12.1986\nEpoch 8/100\n1/1 [==============================] - 0s 16ms/step - loss: 12.0661 - mae: 12.0661\nEpoch 9/100\n1/1 [==============================] - 0s 13ms/step - loss: 11.9336 - mae: 11.9336\nEpoch 10/100\n1/1 [==============================] - 0s 14ms/step - loss: 11.8011 - mae: 11.8011\nEpoch 11/100\n1/1 [==============================] - 0s 12ms/step - loss: 11.6686 - mae: 11.6686\nEpoch 12/100\n1/1 [==============================] - 0s 13ms/step - loss: 11.5361 - mae: 11.5361\nEpoch 13/100\n1/1 [==============================] - 0s 13ms/step - loss: 11.4036 - mae: 11.4036\nEpoch 14/100\n1/1 [==============================] - 0s 13ms/step - loss: 11.2711 - mae: 11.2711\nEpoch 15/100\n1/1 [==============================] - 0s 13ms/step - loss: 11.1386 - mae: 11.1386\nEpoch 16/100\n1/1 [==============================] - 0s 12ms/step - loss: 11.0061 - mae: 11.0061\nEpoch 17/100\n1/1 [==============================] - 0s 12ms/step - loss: 10.8736 - mae: 10.8736\nEpoch 18/100\n1/1 [==============================] - 0s 12ms/step - loss: 10.7411 - mae: 10.7411\nEpoch 19/100\n1/1 [==============================] - 0s 12ms/step - loss: 10.6086 - mae: 10.6086\nEpoch 20/100\n1/1 [==============================] - 0s 12ms/step - loss: 10.4761 - mae: 10.4761\nEpoch 21/100\n1/1 [==============================] - 0s 12ms/step - loss: 10.3436 - mae: 10.3436\nEpoch 22/100\n1/1 [==============================] - 0s 12ms/step - loss: 10.2111 - mae: 10.2111\nEpoch 23/100\n1/1 [==============================] - 0s 18ms/step - loss: 10.0786 - mae: 10.0786\nEpoch 24/100\n1/1 [==============================] - 0s 10ms/step - loss: 9.9461 - mae: 9.9461\nEpoch 25/100\n1/1 [==============================] - 0s 9ms/step - loss: 9.8136 - mae: 9.8136\nEpoch 26/100\n1/1 [==============================] - 0s 10ms/step - loss: 9.6811 - mae: 9.6811\nEpoch 27/100\n1/1 [==============================] - 0s 10ms/step - loss: 9.5486 - mae: 9.5486\nEpoch 28/100\n1/1 [==============================] - 0s 10ms/step - loss: 9.4161 - mae: 9.4161\nEpoch 29/100\n1/1 [==============================] - 0s 11ms/step - loss: 9.2836 - mae: 9.2836\nEpoch 30/100\n1/1 [==============================] - 0s 11ms/step - loss: 9.1511 - mae: 9.1511\nEpoch 31/100\n1/1 [==============================] - 0s 9ms/step - loss: 9.0186 - mae: 9.0186\nEpoch 32/100\n1/1 [==============================] - 0s 9ms/step - loss: 8.8861 - mae: 8.8861\nEpoch 33/100\n1/1 [==============================] - 0s 10ms/step - loss: 8.7536 - mae: 8.7536\nEpoch 34/100\n1/1 [==============================] - 0s 10ms/step - loss: 8.6211 - mae: 8.6211\nEpoch 35/100\n1/1 [==============================] - 0s 12ms/step - loss: 8.4886 - mae: 8.4886\nEpoch 36/100\n1/1 [==============================] - 0s 10ms/step - loss: 8.3561 - mae: 8.3561\nEpoch 37/100\n1/1 [==============================] - 0s 9ms/step - loss: 8.2236 - mae: 8.2236\nEpoch 38/100\n1/1 [==============================] - 0s 8ms/step - loss: 8.0911 - mae: 8.0911\nEpoch 39/100\n1/1 [==============================] - 0s 10ms/step - loss: 7.9586 - mae: 7.9586\nEpoch 40/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.8261 - mae: 7.8261\nEpoch 41/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.6936 - mae: 7.6936\nEpoch 42/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.5611 - mae: 7.5611\nEpoch 43/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.4286 - mae: 7.4286\nEpoch 44/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.2961 - mae: 7.2961\nEpoch 45/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.1700 - mae: 7.1700\nEpoch 46/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.1644 - mae: 7.1644\nEpoch 47/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.1588 - mae: 7.1588\nEpoch 48/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.1531 - mae: 7.1531\nEpoch 49/100\n1/1 [==============================] - 0s 10ms/step - loss: 7.1475 - mae: 7.1475\nEpoch 50/100\n1/1 [==============================] - 0s 10ms/step - loss: 7.1419 - mae: 7.1419\nEpoch 51/100\n1/1 [==============================] - 0s 8ms/step - loss: 7.1363 - mae: 7.1363\nEpoch 52/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.1306 - mae: 7.1306\nEpoch 53/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.1250 - mae: 7.1250\nEpoch 54/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.1194 - mae: 7.1194\nEpoch 55/100\n1/1 [==============================] - 0s 11ms/step - loss: 7.1138 - mae: 7.1138\nEpoch 56/100\n1/1 [==============================] - 0s 10ms/step - loss: 7.1081 - mae: 7.1081\nEpoch 57/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.1025 - mae: 7.1025\nEpoch 58/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.0969 - mae: 7.0969\nEpoch 59/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.0913 - mae: 7.0913\nEpoch 60/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.0856 - mae: 7.0856\nEpoch 61/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.0800 - mae: 7.0800\nEpoch 62/100\n1/1 [==============================] - 0s 8ms/step - loss: 7.0744 - mae: 7.0744\nEpoch 63/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.0688 - mae: 7.0688\nEpoch 64/100\n1/1 [==============================] - 0s 8ms/step - loss: 7.0631 - mae: 7.0631\nEpoch 65/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.0575 - mae: 7.0575\nEpoch 66/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.0519 - mae: 7.0519\nEpoch 67/100\n1/1 [==============================] - 0s 10ms/step - loss: 7.0463 - mae: 7.0463\nEpoch 68/100\n1/1 [==============================] - 0s 11ms/step - loss: 7.0406 - mae: 7.0406\nEpoch 69/100\n1/1 [==============================] - 0s 10ms/step - loss: 7.0350 - mae: 7.0350\nEpoch 70/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.0294 - mae: 7.0294\nEpoch 71/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.0238 - mae: 7.0238\nEpoch 72/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.0181 - mae: 7.0181\nEpoch 73/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.0125 - mae: 7.0125\nEpoch 74/100\n1/1 [==============================] - 0s 9ms/step - loss: 7.0069 - mae: 7.0069\nEpoch 75/100\n1/1 [==============================] - 0s 10ms/step - loss: 7.0013 - mae: 7.0013\nEpoch 76/100\n1/1 [==============================] - 0s 10ms/step - loss: 6.9956 - mae: 6.9956\nEpoch 77/100\n1/1 [==============================] - 0s 10ms/step - loss: 6.9900 - mae: 6.9900\nEpoch 78/100\n1/1 [==============================] - 0s 10ms/step - loss: 6.9844 - mae: 6.9844\nEpoch 79/100\n1/1 [==============================] - 0s 9ms/step - loss: 6.9788 - mae: 6.9788\nEpoch 80/100\n1/1 [==============================] - 0s 9ms/step - loss: 6.9731 - mae: 6.9731\nEpoch 81/100\n1/1 [==============================] - 0s 10ms/step - loss: 6.9675 - mae: 6.9675\nEpoch 82/100\n1/1 [==============================] - 0s 9ms/step - loss: 6.9619 - mae: 6.9619\nEpoch 83/100\n1/1 [==============================] - 0s 9ms/step - loss: 6.9563 - mae: 6.9563\nEpoch 84/100\n1/1 [==============================] - 0s 8ms/step - loss: 6.9506 - mae: 6.9506\nEpoch 85/100\n1/1 [==============================] - 0s 9ms/step - loss: 6.9450 - mae: 6.9450\nEpoch 86/100\n1/1 [==============================] - 0s 9ms/step - loss: 6.9394 - mae: 6.9394\nEpoch 87/100\n1/1 [==============================] - 0s 9ms/step - loss: 6.9338 - mae: 6.9338\nEpoch 88/100\n1/1 [==============================] - 0s 9ms/step - loss: 6.9281 - mae: 6.9281\nEpoch 89/100\n1/1 [==============================] - 0s 10ms/step - loss: 6.9225 - mae: 6.9225\nEpoch 90/100\n1/1 [==============================] - 0s 9ms/step - loss: 6.9169 - mae: 6.9169\nEpoch 91/100\n1/1 [==============================] - 0s 9ms/step - loss: 6.9113 - mae: 6.9113\nEpoch 92/100\n1/1 [==============================] - 0s 10ms/step - loss: 6.9056 - mae: 6.9056\nEpoch 93/100\n1/1 [==============================] - 0s 8ms/step - loss: 6.9000 - mae: 6.9000\nEpoch 94/100\n1/1 [==============================] - 0s 11ms/step - loss: 6.8944 - mae: 6.8944\nEpoch 95/100\n1/1 [==============================] - 0s 9ms/step - loss: 6.8888 - mae: 6.8888\nEpoch 96/100\n1/1 [==============================] - 0s 9ms/step - loss: 6.8831 - mae: 6.8831\nEpoch 97/100\n1/1 [==============================] - 0s 10ms/step - loss: 6.8775 - mae: 6.8775\nEpoch 98/100\n1/1 [==============================] - 0s 15ms/step - loss: 6.8719 - mae: 6.8719\nEpoch 99/100\n1/1 [==============================] - 0s 10ms/step - loss: 6.8663 - mae: 6.8663\nEpoch 100/100\n1/1 [==============================] - 0s 9ms/step - loss: 6.8606 - mae: 6.8606\n</pre> Out[11]: <pre>&lt;keras.callbacks.History at 0x7f0065eabcd0&gt;</pre> <p>You might've noticed the loss value decrease from before (and keep decreasing as the number of epochs gets higher).</p> <p>What do you think this means for when we make a prediction with our model?</p> <p>How about we try predict on 17.0 again?</p> In\u00a0[12]: Copied! <pre># Remind ourselves of what X and y are\nX, y\n</pre> # Remind ourselves of what X and y are X, y Out[12]: <pre>(&lt;tf.Tensor: shape=(8,), dtype=float32, numpy=array([-7., -4., -1.,  2.,  5.,  8., 11., 14.], dtype=float32)&gt;,\n &lt;tf.Tensor: shape=(8,), dtype=float32, numpy=array([ 3.,  6.,  9., 12., 15., 18., 21., 24.], dtype=float32)&gt;)</pre> In\u00a0[13]: Copied! <pre># Try and predict what y would be if X was 17.0\nmodel.predict([17.0]) # the right answer is 27.0 (y = X + 10)\n</pre> # Try and predict what y would be if X was 17.0 model.predict([17.0]) # the right answer is 27.0 (y = X + 10) <pre>1/1 [==============================] - 0s 61ms/step\n</pre> Out[13]: <pre>array([[29.499516]], dtype=float32)</pre> <p>Much better!</p> <p>We got closer this time. But we could still be better.</p> <p>Now we've trained a model, how could we evaluate it?</p> In\u00a0[14]: Copied! <pre># Make a bigger dataset\nX = np.arange(-100, 100, 4)\nX\n</pre> # Make a bigger dataset X = np.arange(-100, 100, 4) X Out[14]: <pre>array([-100,  -96,  -92,  -88,  -84,  -80,  -76,  -72,  -68,  -64,  -60,\n        -56,  -52,  -48,  -44,  -40,  -36,  -32,  -28,  -24,  -20,  -16,\n        -12,   -8,   -4,    0,    4,    8,   12,   16,   20,   24,   28,\n         32,   36,   40,   44,   48,   52,   56,   60,   64,   68,   72,\n         76,   80,   84,   88,   92,   96])</pre> In\u00a0[15]: Copied! <pre># Make labels for the dataset (adhering to the same pattern as before)\ny = np.arange(-90, 110, 4)\ny\n</pre> # Make labels for the dataset (adhering to the same pattern as before) y = np.arange(-90, 110, 4) y Out[15]: <pre>array([-90, -86, -82, -78, -74, -70, -66, -62, -58, -54, -50, -46, -42,\n       -38, -34, -30, -26, -22, -18, -14, -10,  -6,  -2,   2,   6,  10,\n        14,  18,  22,  26,  30,  34,  38,  42,  46,  50,  54,  58,  62,\n        66,  70,  74,  78,  82,  86,  90,  94,  98, 102, 106])</pre> <p>Since $y=X+10$, we could make the labels like so:</p> In\u00a0[16]: Copied! <pre># Same result as above\ny = X + 10\ny\n</pre> # Same result as above y = X + 10 y Out[16]: <pre>array([-90, -86, -82, -78, -74, -70, -66, -62, -58, -54, -50, -46, -42,\n       -38, -34, -30, -26, -22, -18, -14, -10,  -6,  -2,   2,   6,  10,\n        14,  18,  22,  26,  30,  34,  38,  42,  46,  50,  54,  58,  62,\n        66,  70,  74,  78,  82,  86,  90,  94,  98, 102, 106])</pre> In\u00a0[17]: Copied! <pre># Check how many samples we have\nlen(X)\n</pre> # Check how many samples we have len(X) Out[17]: <pre>50</pre> In\u00a0[18]: Copied! <pre># Split data into train and test sets\nX_train = X[:40] # first 40 examples (80% of data)\ny_train = y[:40]\n\nX_test = X[40:] # last 10 examples (20% of data)\ny_test = y[40:]\n\nlen(X_train), len(X_test)\n</pre> # Split data into train and test sets X_train = X[:40] # first 40 examples (80% of data) y_train = y[:40]  X_test = X[40:] # last 10 examples (20% of data) y_test = y[40:]  len(X_train), len(X_test) Out[18]: <pre>(40, 10)</pre> In\u00a0[19]: Copied! <pre>plt.figure(figsize=(10, 7))\n# Plot training data in blue\nplt.scatter(X_train, y_train, c='b', label='Training data')\n# Plot test data in green\nplt.scatter(X_test, y_test, c='g', label='Testing data')\n# Show the legend\nplt.legend();\n</pre> plt.figure(figsize=(10, 7)) # Plot training data in blue plt.scatter(X_train, y_train, c='b', label='Training data') # Plot test data in green plt.scatter(X_test, y_test, c='g', label='Testing data') # Show the legend plt.legend(); <p>Beautiful! Any time you can visualize your data, your model, your anything, it's a good idea.</p> <p>With this graph in mind, what we'll be trying to do is build a model which learns the pattern in the blue dots (<code>X_train</code>) to draw the green dots (<code>X_test</code>).</p> <p>Time to build a model. We'll make the exact same one from before (the one we trained for longer).</p> In\u00a0[20]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create a model (same as above)\nmodel = tf.keras.Sequential([\n  tf.keras.layers.Dense(1)\n])\n\n# Compile model (same as above)\nmodel.compile(loss=tf.keras.losses.mae,\n              optimizer=tf.keras.optimizers.SGD(),\n              metrics=[\"mae\"])\n\n# Fit model (same as above)\n#model.fit(X_train, y_train, epochs=100) # commented out on purpose (not fitting it just yet)\n</pre> # Set random seed tf.random.set_seed(42)  # Create a model (same as above) model = tf.keras.Sequential([   tf.keras.layers.Dense(1) ])  # Compile model (same as above) model.compile(loss=tf.keras.losses.mae,               optimizer=tf.keras.optimizers.SGD(),               metrics=[\"mae\"])  # Fit model (same as above) #model.fit(X_train, y_train, epochs=100) # commented out on purpose (not fitting it just yet) In\u00a0[22]: Copied! <pre># Doesn't work (model not fit/built)\nmodel.summary()\n</pre> # Doesn't work (model not fit/built) model.summary() <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-22-7d09d31d4e66&gt; in &lt;cell line: 2&gt;()\n      1 # Doesn't work (model not fit/built)\n----&gt; 2 model.summary()\n\n/usr/local/lib/python3.10/dist-packages/keras/engine/training.py in summary(self, line_length, positions, print_fn, expand_nested, show_trainable, layer_range)\n   3227         \"\"\"\n   3228         if not self.built:\n-&gt; 3229             raise ValueError(\n   3230                 \"This model has not yet been built. \"\n   3231                 \"Build the model first by calling `build()` or by calling \"\n\nValueError: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.</pre> <p>Ahh, the cell above errors because we haven't fit or built our model.</p> <p>We also haven't told it what input shape it should be expecting.</p> <p>Remember above, how we discussed the input shape was just one number?</p> <p>We can let our model know the input shape of our data using the <code>input_shape</code> parameter to the first layer (usually if <code>input_shape</code> isn't defined, Keras tries to figure it out automatically).</p> In\u00a0[23]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create a model (same as above)\nmodel = tf.keras.Sequential([\n  tf.keras.layers.Dense(1, input_shape=[1]) # define the input_shape to our model\n])\n\n# Compile model (same as above)\nmodel.compile(loss=tf.keras.losses.mae,\n              optimizer=tf.keras.optimizers.SGD(),\n              metrics=[\"mae\"])\n</pre> # Set random seed tf.random.set_seed(42)  # Create a model (same as above) model = tf.keras.Sequential([   tf.keras.layers.Dense(1, input_shape=[1]) # define the input_shape to our model ])  # Compile model (same as above) model.compile(loss=tf.keras.losses.mae,               optimizer=tf.keras.optimizers.SGD(),               metrics=[\"mae\"]) In\u00a0[24]: Copied! <pre># This will work after specifying the input shape\nmodel.summary()\n</pre> # This will work after specifying the input shape model.summary() <pre>Model: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_3 (Dense)             (None, 1)                 2         \n                                                                 \n=================================================================\nTotal params: 2\nTrainable params: 2\nNon-trainable params: 0\n_________________________________________________________________\n</pre> <p>Calling <code>summary()</code> on our model shows us the layers it contains, the output shape and the number of parameters.</p> <ul> <li>Total params - total number of parameters in the model.</li> <li>Trainable parameters - these are the parameters (patterns) the model can update as it trains.</li> <li>Non-trainable parameters - these parameters aren't updated during training (this is typical when you bring in the already learned patterns from other models during transfer learning).</li> </ul> <p>\ud83d\udcd6 Resource: For a more in-depth overview of the trainable parameters within a layer, check out MIT's introduction to deep learning video.</p> <p>\ud83d\udee0 Exercise: Try playing around with the number of hidden units in the <code>Dense</code> layer (e.g. <code>Dense(2)</code>, <code>Dense(3)</code>). How does this change the Total/Trainable params? Investigate what's causing the change.</p> <p>For now, all you need to think about these parameters is that their learnable patterns in the data.</p> <p>Let's fit our model to the training data.</p> In\u00a0[25]: Copied! <pre># Fit the model to the training data\nmodel.fit(X_train, y_train, epochs=100, verbose=0) # verbose controls how much gets output\n</pre> # Fit the model to the training data model.fit(X_train, y_train, epochs=100, verbose=0) # verbose controls how much gets output Out[25]: <pre>&lt;keras.callbacks.History at 0x7f0065dd80d0&gt;</pre> In\u00a0[26]: Copied! <pre># Check the model summary\nmodel.summary()\n</pre> # Check the model summary model.summary() <pre>Model: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_3 (Dense)             (None, 1)                 2         \n                                                                 \n=================================================================\nTotal params: 2\nTrainable params: 2\nNon-trainable params: 0\n_________________________________________________________________\n</pre> <p>Alongside summary, you can also view a 2D plot of the model using <code>plot_model()</code>.</p> In\u00a0[27]: Copied! <pre>from tensorflow.keras.utils import plot_model\n\nplot_model(model, show_shapes=True)\n</pre> from tensorflow.keras.utils import plot_model  plot_model(model, show_shapes=True) Out[27]: <p>In our case, the model we used only has an input and an output but visualizing more complicated models can be very helpful for debugging.</p> In\u00a0[28]: Copied! <pre># Make predictions\ny_preds = model.predict(X_test)\n</pre> # Make predictions y_preds = model.predict(X_test) <pre>1/1 [==============================] - 0s 45ms/step\n</pre> In\u00a0[29]: Copied! <pre># View the predictions\ny_preds\n</pre> # View the predictions y_preds Out[29]: <pre>array([[44.544697],\n       [47.427135],\n       [50.309574],\n       [53.192013],\n       [56.07445 ],\n       [58.95689 ],\n       [61.839325],\n       [64.72176 ],\n       [67.60421 ],\n       [70.48664 ]], dtype=float32)</pre> <p>Okay, we get a list of numbers but how do these compare to the ground truth labels?</p> <p>Let's build a plotting function to find out.</p> <p>\ud83d\udd11 Note: If you think you're going to be visualizing something a lot, it's a good idea to functionize it so you can use it later.</p> In\u00a0[30]: Copied! <pre>def plot_predictions(train_data=X_train, \n                     train_labels=y_train, \n                     test_data=X_test, \n                     test_labels=y_test, \n                     predictions=y_preds):\n  \"\"\"\n  Plots training data, test data and compares predictions.\n  \"\"\"\n  plt.figure(figsize=(10, 7))\n  # Plot training data in blue\n  plt.scatter(train_data, train_labels, c=\"b\", label=\"Training data\")\n  # Plot test data in green\n  plt.scatter(test_data, test_labels, c=\"g\", label=\"Testing data\")\n  # Plot the predictions in red (predictions were made on the test data)\n  plt.scatter(test_data, predictions, c=\"r\", label=\"Predictions\")\n  # Show the legend\n  plt.legend();\n</pre> def plot_predictions(train_data=X_train,                       train_labels=y_train,                       test_data=X_test,                       test_labels=y_test,                       predictions=y_preds):   \"\"\"   Plots training data, test data and compares predictions.   \"\"\"   plt.figure(figsize=(10, 7))   # Plot training data in blue   plt.scatter(train_data, train_labels, c=\"b\", label=\"Training data\")   # Plot test data in green   plt.scatter(test_data, test_labels, c=\"g\", label=\"Testing data\")   # Plot the predictions in red (predictions were made on the test data)   plt.scatter(test_data, predictions, c=\"r\", label=\"Predictions\")   # Show the legend   plt.legend(); In\u00a0[31]: Copied! <pre>plot_predictions(train_data=X_train,\n                 train_labels=y_train,\n                 test_data=X_test,\n                 test_labels=y_test,\n                 predictions=y_preds)\n</pre> plot_predictions(train_data=X_train,                  train_labels=y_train,                  test_data=X_test,                  test_labels=y_test,                  predictions=y_preds) <p>From the plot we can see our predictions aren't totally outlandish but they definitely aren't anything special either.</p> In\u00a0[32]: Copied! <pre># Evaluate the model on the test set\nmodel.evaluate(X_test, y_test)\n</pre> # Evaluate the model on the test set model.evaluate(X_test, y_test) <pre>1/1 [==============================] - 0s 132ms/step - loss: 30.4843 - mae: 30.4843\n</pre> Out[32]: <pre>[30.484329223632812, 30.484329223632812]</pre> <p>In our case, since we used MAE for the loss function as well as MAE for the metrics, <code>model.evaulate()</code> returns them both.</p> <p>TensorFlow also has built in functions for MSE and MAE.</p> <p>For many evaluation functions, the premise is the same: compare predictions to the ground truth labels.</p> In\u00a0[33]: Copied! <pre># Calculate the mean absolute error\nmae = tf.metrics.mean_absolute_error(y_true=y_test, \n                                     y_pred=y_preds)\nmae\n</pre> # Calculate the mean absolute error mae = tf.metrics.mean_absolute_error(y_true=y_test,                                       y_pred=y_preds) mae Out[33]: <pre>&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=\narray([43.455303, 40.572865, 37.690426, 34.807987, 31.925549, 29.04311 ,\n       26.160675, 23.278236, 20.39579 , 17.610687], dtype=float32)&gt;</pre> <p>Huh? That's strange, MAE should be a single output.</p> <p>Instead, we get 10 values.</p> <p>This is because our <code>y_test</code> and <code>y_preds</code> tensors are different shapes.</p> In\u00a0[34]: Copied! <pre># Check the test label tensor values\ny_test\n</pre> # Check the test label tensor values y_test Out[34]: <pre>array([ 70,  74,  78,  82,  86,  90,  94,  98, 102, 106])</pre> In\u00a0[35]: Copied! <pre># Check the predictions tensor values (notice the extra square brackets)\ny_preds\n</pre> # Check the predictions tensor values (notice the extra square brackets) y_preds Out[35]: <pre>array([[44.544697],\n       [47.427135],\n       [50.309574],\n       [53.192013],\n       [56.07445 ],\n       [58.95689 ],\n       [61.839325],\n       [64.72176 ],\n       [67.60421 ],\n       [70.48664 ]], dtype=float32)</pre> In\u00a0[36]: Copied! <pre># Check the tensor shapes\ny_test.shape, y_preds.shape\n</pre> # Check the tensor shapes y_test.shape, y_preds.shape Out[36]: <pre>((10,), (10, 1))</pre> <p>Remember how we discussed dealing with different input and output shapes is one the most common issues you'll come across, this is one of those times.</p> <p>But not to worry.</p> <p>We can fix it using <code>squeeze()</code>, it'll remove the the <code>1</code> dimension from our <code>y_preds</code> tensor, making it the same shape as <code>y_test</code>.</p> <p>\ud83d\udd11 Note: If you're comparing two tensors, it's important to make sure they're the right shape(s) (you won't always have to manipulate the shapes, but always be on the look out, many errors are the result of mismatched tensors, especially mismatched input and output shapes).</p> In\u00a0[37]: Copied! <pre># Shape before squeeze()\ny_preds.shape\n</pre> # Shape before squeeze() y_preds.shape Out[37]: <pre>(10, 1)</pre> In\u00a0[38]: Copied! <pre># Shape after squeeze()\ny_preds.squeeze().shape\n</pre> # Shape after squeeze() y_preds.squeeze().shape Out[38]: <pre>(10,)</pre> In\u00a0[39]: Copied! <pre># What do they look like?\ny_test, y_preds.squeeze()\n</pre> # What do they look like? y_test, y_preds.squeeze() Out[39]: <pre>(array([ 70,  74,  78,  82,  86,  90,  94,  98, 102, 106]),\n array([44.544697, 47.427135, 50.309574, 53.192013, 56.07445 , 58.95689 ,\n        61.839325, 64.72176 , 67.60421 , 70.48664 ], dtype=float32))</pre> <p>Okay, now we know how to make our <code>y_test</code> and <code>y_preds</code> tenors the same shape, let's use our evaluation metrics.</p> In\u00a0[40]: Copied! <pre># Calcuate the MAE\nmae = tf.metrics.mean_absolute_error(y_true=y_test, \n                                     y_pred=y_preds.squeeze()) # use squeeze() to make same shape\nmae\n</pre> # Calcuate the MAE mae = tf.metrics.mean_absolute_error(y_true=y_test,                                       y_pred=y_preds.squeeze()) # use squeeze() to make same shape mae Out[40]: <pre>&lt;tf.Tensor: shape=(), dtype=float32, numpy=30.48433&gt;</pre> In\u00a0[41]: Copied! <pre># Calculate the MSE\nmse = tf.metrics.mean_squared_error(y_true=y_test,\n                                    y_pred=y_preds.squeeze())\nmse\n</pre> # Calculate the MSE mse = tf.metrics.mean_squared_error(y_true=y_test,                                     y_pred=y_preds.squeeze()) mse Out[41]: <pre>&lt;tf.Tensor: shape=(), dtype=float32, numpy=939.59827&gt;</pre> <p>We can also calculate the MAE using pure TensorFlow functions.</p> In\u00a0[42]: Copied! <pre># Returns the same as tf.metrics.mean_absolute_error()\ntf.reduce_mean(tf.abs(y_test-y_preds.squeeze()))\n</pre> # Returns the same as tf.metrics.mean_absolute_error() tf.reduce_mean(tf.abs(y_test-y_preds.squeeze())) Out[42]: <pre>&lt;tf.Tensor: shape=(), dtype=float64, numpy=30.484329986572266&gt;</pre> <p>Again, it's a good idea to functionize anything you think you might use over again (or find yourself using over and over again).</p> <p>Let's make functions for our evaluation metrics.</p> In\u00a0[43]: Copied! <pre>def mae(y_test, y_pred):\n  \"\"\"\n  Calculuates mean absolute error between y_test and y_preds.\n  \"\"\"\n  return tf.metrics.mean_absolute_error(y_test,\n                                        y_pred)\n  \ndef mse(y_test, y_pred):\n  \"\"\"\n  Calculates mean squared error between y_test and y_preds.\n  \"\"\"\n  return tf.metrics.mean_squared_error(y_test,\n                                       y_pred)\n</pre> def mae(y_test, y_pred):   \"\"\"   Calculuates mean absolute error between y_test and y_preds.   \"\"\"   return tf.metrics.mean_absolute_error(y_test,                                         y_pred)    def mse(y_test, y_pred):   \"\"\"   Calculates mean squared error between y_test and y_preds.   \"\"\"   return tf.metrics.mean_squared_error(y_test,                                        y_pred) In\u00a0[44]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Replicate original model\nmodel_1 = tf.keras.Sequential([\n  tf.keras.layers.Dense(1)\n])\n\n# Compile the model\nmodel_1.compile(loss=tf.keras.losses.mae,\n                optimizer=tf.keras.optimizers.SGD(),\n                metrics=['mae'])\n\n# Fit the model\nmodel_1.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=100)\n</pre> # Set random seed tf.random.set_seed(42)  # Replicate original model model_1 = tf.keras.Sequential([   tf.keras.layers.Dense(1) ])  # Compile the model model_1.compile(loss=tf.keras.losses.mae,                 optimizer=tf.keras.optimizers.SGD(),                 metrics=['mae'])  # Fit the model model_1.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=100) <pre>Epoch 1/100\n2/2 [==============================] - 0s 19ms/step - loss: 30.0988 - mae: 30.0988\nEpoch 2/100\n2/2 [==============================] - 0s 8ms/step - loss: 8.4388 - mae: 8.4388\nEpoch 3/100\n2/2 [==============================] - 0s 6ms/step - loss: 10.5960 - mae: 10.5960\nEpoch 4/100\n2/2 [==============================] - 0s 6ms/step - loss: 13.1312 - mae: 13.1312\nEpoch 5/100\n2/2 [==============================] - 0s 7ms/step - loss: 12.1970 - mae: 12.1970\nEpoch 6/100\n2/2 [==============================] - 0s 6ms/step - loss: 9.4357 - mae: 9.4357\nEpoch 7/100\n2/2 [==============================] - 0s 7ms/step - loss: 8.5754 - mae: 8.5754\nEpoch 8/100\n2/2 [==============================] - 0s 7ms/step - loss: 9.0484 - mae: 9.0484\nEpoch 9/100\n2/2 [==============================] - 0s 6ms/step - loss: 18.7568 - mae: 18.7568\nEpoch 10/100\n2/2 [==============================] - 0s 7ms/step - loss: 10.1199 - mae: 10.1199\nEpoch 11/100\n2/2 [==============================] - 0s 7ms/step - loss: 8.4027 - mae: 8.4027\nEpoch 12/100\n2/2 [==============================] - 0s 6ms/step - loss: 10.6692 - mae: 10.6692\nEpoch 13/100\n2/2 [==============================] - 0s 7ms/step - loss: 9.8023 - mae: 9.8023\nEpoch 14/100\n2/2 [==============================] - 0s 6ms/step - loss: 16.0152 - mae: 16.0152\nEpoch 15/100\n2/2 [==============================] - 0s 8ms/step - loss: 11.4080 - mae: 11.4080\nEpoch 16/100\n2/2 [==============================] - 0s 7ms/step - loss: 8.5437 - mae: 8.5437\nEpoch 17/100\n2/2 [==============================] - 0s 7ms/step - loss: 13.6405 - mae: 13.6405\nEpoch 18/100\n2/2 [==============================] - 0s 7ms/step - loss: 11.4690 - mae: 11.4690\nEpoch 19/100\n2/2 [==============================] - 0s 7ms/step - loss: 17.9154 - mae: 17.9154\nEpoch 20/100\n2/2 [==============================] - 0s 6ms/step - loss: 15.0500 - mae: 15.0500\nEpoch 21/100\n2/2 [==============================] - 0s 7ms/step - loss: 11.0231 - mae: 11.0231\nEpoch 22/100\n2/2 [==============================] - 0s 7ms/step - loss: 8.1580 - mae: 8.1580\nEpoch 23/100\n2/2 [==============================] - 0s 7ms/step - loss: 9.5191 - mae: 9.5191\nEpoch 24/100\n2/2 [==============================] - 0s 6ms/step - loss: 7.6655 - mae: 7.6655\nEpoch 25/100\n2/2 [==============================] - 0s 6ms/step - loss: 13.1908 - mae: 13.1908\nEpoch 26/100\n2/2 [==============================] - 0s 8ms/step - loss: 16.4217 - mae: 16.4217\nEpoch 27/100\n2/2 [==============================] - 0s 7ms/step - loss: 13.1672 - mae: 13.1672\nEpoch 28/100\n2/2 [==============================] - 0s 13ms/step - loss: 14.2568 - mae: 14.2568\nEpoch 29/100\n2/2 [==============================] - 0s 7ms/step - loss: 10.0703 - mae: 10.0703\nEpoch 30/100\n2/2 [==============================] - 0s 6ms/step - loss: 16.3398 - mae: 16.3398\nEpoch 31/100\n2/2 [==============================] - 0s 7ms/step - loss: 23.6485 - mae: 23.6485\nEpoch 32/100\n2/2 [==============================] - 0s 6ms/step - loss: 7.6261 - mae: 7.6261\nEpoch 33/100\n2/2 [==============================] - 0s 6ms/step - loss: 9.3267 - mae: 9.3267\nEpoch 34/100\n2/2 [==============================] - 0s 6ms/step - loss: 13.7369 - mae: 13.7369\nEpoch 35/100\n2/2 [==============================] - 0s 7ms/step - loss: 11.1301 - mae: 11.1301\nEpoch 36/100\n2/2 [==============================] - 0s 7ms/step - loss: 13.3224 - mae: 13.3224\nEpoch 37/100\n2/2 [==============================] - 0s 7ms/step - loss: 9.4813 - mae: 9.4813\nEpoch 38/100\n2/2 [==============================] - 0s 7ms/step - loss: 10.1438 - mae: 10.1438\nEpoch 39/100\n2/2 [==============================] - 0s 7ms/step - loss: 10.1818 - mae: 10.1818\nEpoch 40/100\n2/2 [==============================] - 0s 7ms/step - loss: 10.9152 - mae: 10.9152\nEpoch 41/100\n2/2 [==============================] - 0s 7ms/step - loss: 7.9088 - mae: 7.9088\nEpoch 42/100\n2/2 [==============================] - 0s 7ms/step - loss: 10.0967 - mae: 10.0967\nEpoch 43/100\n2/2 [==============================] - 0s 7ms/step - loss: 8.7052 - mae: 8.7052\nEpoch 44/100\n2/2 [==============================] - 0s 7ms/step - loss: 12.2103 - mae: 12.2103\nEpoch 45/100\n2/2 [==============================] - 0s 7ms/step - loss: 13.7978 - mae: 13.7978\nEpoch 46/100\n2/2 [==============================] - 0s 7ms/step - loss: 8.4711 - mae: 8.4711\nEpoch 47/100\n2/2 [==============================] - 0s 7ms/step - loss: 9.1381 - mae: 9.1381\nEpoch 48/100\n2/2 [==============================] - 0s 7ms/step - loss: 10.6247 - mae: 10.6247\nEpoch 49/100\n2/2 [==============================] - 0s 7ms/step - loss: 7.7550 - mae: 7.7550\nEpoch 50/100\n2/2 [==============================] - 0s 6ms/step - loss: 9.5461 - mae: 9.5461\nEpoch 51/100\n2/2 [==============================] - 0s 7ms/step - loss: 9.1627 - mae: 9.1627\nEpoch 52/100\n2/2 [==============================] - 0s 7ms/step - loss: 16.3674 - mae: 16.3674\nEpoch 53/100\n2/2 [==============================] - 0s 7ms/step - loss: 14.1315 - mae: 14.1315\nEpoch 54/100\n2/2 [==============================] - 0s 7ms/step - loss: 21.1233 - mae: 21.1233\nEpoch 55/100\n2/2 [==============================] - 0s 7ms/step - loss: 16.4010 - mae: 16.4010\nEpoch 56/100\n2/2 [==============================] - 0s 6ms/step - loss: 9.9366 - mae: 9.9366\nEpoch 57/100\n2/2 [==============================] - 0s 7ms/step - loss: 9.6189 - mae: 9.6189\nEpoch 58/100\n2/2 [==============================] - 0s 6ms/step - loss: 8.9519 - mae: 8.9519\nEpoch 59/100\n2/2 [==============================] - 0s 7ms/step - loss: 10.1374 - mae: 10.1374\nEpoch 60/100\n2/2 [==============================] - 0s 6ms/step - loss: 8.4572 - mae: 8.4572\nEpoch 61/100\n2/2 [==============================] - 0s 8ms/step - loss: 9.3126 - mae: 9.3126\nEpoch 62/100\n2/2 [==============================] - 0s 7ms/step - loss: 7.0772 - mae: 7.0772\nEpoch 63/100\n2/2 [==============================] - 0s 7ms/step - loss: 8.6268 - mae: 8.6268\nEpoch 64/100\n2/2 [==============================] - 0s 7ms/step - loss: 9.2062 - mae: 9.2062\nEpoch 65/100\n2/2 [==============================] - 0s 7ms/step - loss: 10.4393 - mae: 10.4393\nEpoch 66/100\n2/2 [==============================] - 0s 7ms/step - loss: 15.7070 - mae: 15.7070\nEpoch 67/100\n2/2 [==============================] - 0s 7ms/step - loss: 10.0863 - mae: 10.0863\nEpoch 68/100\n2/2 [==============================] - 0s 7ms/step - loss: 9.0451 - mae: 9.0451\nEpoch 69/100\n2/2 [==============================] - 0s 7ms/step - loss: 12.5786 - mae: 12.5786\nEpoch 70/100\n2/2 [==============================] - 0s 9ms/step - loss: 8.9553 - mae: 8.9553\nEpoch 71/100\n2/2 [==============================] - 0s 7ms/step - loss: 9.9295 - mae: 9.9295\nEpoch 72/100\n2/2 [==============================] - 0s 8ms/step - loss: 9.9702 - mae: 9.9702\nEpoch 73/100\n2/2 [==============================] - 0s 8ms/step - loss: 12.4305 - mae: 12.4305\nEpoch 74/100\n2/2 [==============================] - 0s 7ms/step - loss: 10.5927 - mae: 10.5927\nEpoch 75/100\n2/2 [==============================] - 0s 7ms/step - loss: 9.6281 - mae: 9.6281\nEpoch 76/100\n2/2 [==============================] - 0s 7ms/step - loss: 11.0915 - mae: 11.0915\nEpoch 77/100\n2/2 [==============================] - 0s 7ms/step - loss: 8.2769 - mae: 8.2769\nEpoch 78/100\n2/2 [==============================] - 0s 7ms/step - loss: 8.9623 - mae: 8.9623\nEpoch 79/100\n2/2 [==============================] - 0s 7ms/step - loss: 19.8117 - mae: 19.8117\nEpoch 80/100\n2/2 [==============================] - 0s 6ms/step - loss: 17.8036 - mae: 17.8036\nEpoch 81/100\n2/2 [==============================] - 0s 7ms/step - loss: 7.0915 - mae: 7.0915\nEpoch 82/100\n2/2 [==============================] - 0s 7ms/step - loss: 10.4045 - mae: 10.4045\nEpoch 83/100\n2/2 [==============================] - 0s 7ms/step - loss: 9.8260 - mae: 9.8260\nEpoch 84/100\n2/2 [==============================] - 0s 7ms/step - loss: 7.9471 - mae: 7.9471\nEpoch 85/100\n2/2 [==============================] - 0s 7ms/step - loss: 9.4590 - mae: 9.4590\nEpoch 86/100\n2/2 [==============================] - 0s 7ms/step - loss: 9.5023 - mae: 9.5023\nEpoch 87/100\n2/2 [==============================] - 0s 7ms/step - loss: 11.4498 - mae: 11.4498\nEpoch 88/100\n2/2 [==============================] - 0s 7ms/step - loss: 9.9486 - mae: 9.9486\nEpoch 89/100\n2/2 [==============================] - 0s 7ms/step - loss: 7.2579 - mae: 7.2579\nEpoch 90/100\n2/2 [==============================] - 0s 6ms/step - loss: 12.7103 - mae: 12.7103\nEpoch 91/100\n2/2 [==============================] - 0s 7ms/step - loss: 7.3206 - mae: 7.3206\nEpoch 92/100\n2/2 [==============================] - 0s 12ms/step - loss: 7.6849 - mae: 7.6849\nEpoch 93/100\n2/2 [==============================] - 0s 8ms/step - loss: 7.1229 - mae: 7.1229\nEpoch 94/100\n2/2 [==============================] - 0s 7ms/step - loss: 12.5571 - mae: 12.5571\nEpoch 95/100\n2/2 [==============================] - 0s 6ms/step - loss: 9.9330 - mae: 9.9330\nEpoch 96/100\n2/2 [==============================] - 0s 7ms/step - loss: 9.1413 - mae: 9.1413\nEpoch 97/100\n2/2 [==============================] - 0s 7ms/step - loss: 12.0806 - mae: 12.0806\nEpoch 98/100\n2/2 [==============================] - 0s 7ms/step - loss: 9.0788 - mae: 9.0788\nEpoch 99/100\n2/2 [==============================] - 0s 8ms/step - loss: 8.4999 - mae: 8.4999\nEpoch 100/100\n2/2 [==============================] - 0s 7ms/step - loss: 14.4517 - mae: 14.4517\n</pre> Out[44]: <pre>&lt;keras.callbacks.History at 0x7f0065a91180&gt;</pre> In\u00a0[45]: Copied! <pre># Make and plot predictions for model_1\ny_preds_1 = model_1.predict(X_test)\nplot_predictions(predictions=y_preds_1)\n</pre> # Make and plot predictions for model_1 y_preds_1 = model_1.predict(X_test) plot_predictions(predictions=y_preds_1) <pre>1/1 [==============================] - 0s 45ms/step\n</pre> In\u00a0[46]: Copied! <pre># Calculate model_1 metrics\nmae_1 = mae(y_test, y_preds_1.squeeze()).numpy()\nmse_1 = mse(y_test, y_preds_1.squeeze()).numpy()\nmae_1, mse_1\n</pre> # Calculate model_1 metrics mae_1 = mae(y_test, y_preds_1.squeeze()).numpy() mse_1 = mse(y_test, y_preds_1.squeeze()).numpy() mae_1, mse_1 Out[46]: <pre>(30.638134, 949.13086)</pre> <p>Build <code>model_2</code></p> <p>This time we'll add an extra dense layer (so now our model will have 2 layers) whilst keeping everything else the same.</p> In\u00a0[47]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Replicate model_1 and add an extra layer\nmodel_2 = tf.keras.Sequential([\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Dense(1) # add a second layer\n])\n\n# Compile the model\nmodel_2.compile(loss=tf.keras.losses.mae,\n                optimizer=tf.keras.optimizers.SGD(),\n                metrics=['mae'])\n\n# Fit the model\nmodel_2.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=100, verbose=0) # set verbose to 0 for less output\n</pre> # Set random seed tf.random.set_seed(42)  # Replicate model_1 and add an extra layer model_2 = tf.keras.Sequential([   tf.keras.layers.Dense(1),   tf.keras.layers.Dense(1) # add a second layer ])  # Compile the model model_2.compile(loss=tf.keras.losses.mae,                 optimizer=tf.keras.optimizers.SGD(),                 metrics=['mae'])  # Fit the model model_2.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=100, verbose=0) # set verbose to 0 for less output Out[47]: <pre>&lt;keras.callbacks.History at 0x7f00643ba560&gt;</pre> In\u00a0[48]: Copied! <pre># Make and plot predictions for model_2\ny_preds_2 = model_2.predict(X_test)\nplot_predictions(predictions=y_preds_2)\n</pre> # Make and plot predictions for model_2 y_preds_2 = model_2.predict(X_test) plot_predictions(predictions=y_preds_2) <pre>WARNING:tensorflow:5 out of the last 5 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7f006436f880&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n</pre> <pre>1/1 [==============================] - 0s 51ms/step\n</pre> <p>Woah, that's looking better already! And all it took was an extra layer.</p> In\u00a0[49]: Copied! <pre># Calculate model_2 metrics\nmae_2 = mae(y_test, y_preds_2.squeeze()).numpy()\nmse_2 = mse(y_test, y_preds_2.squeeze()).numpy()\nmae_2, mse_2\n</pre> # Calculate model_2 metrics mae_2 = mae(y_test, y_preds_2.squeeze()).numpy() mse_2 = mse(y_test, y_preds_2.squeeze()).numpy() mae_2, mse_2 Out[49]: <pre>(10.610324, 120.35542)</pre> <p>Build <code>model_3</code></p> <p>For our 3rd model, we'll keep everything the same as <code>model_2</code> except this time we'll train for longer (500 epochs instead of 100).</p> <p>This will give our model more of a chance to learn the patterns in the data.</p> In\u00a0[50]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Replicate model_2\nmodel_3 = tf.keras.Sequential([\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Dense(1)\n])\n\n# Compile the model\nmodel_3.compile(loss=tf.keras.losses.mae,\n                optimizer=tf.keras.optimizers.SGD(),\n                metrics=['mae'])\n\n# Fit the model (this time for 500 epochs, not 100)\nmodel_3.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=500, verbose=0) # set verbose to 0 for less output\n</pre> # Set random seed tf.random.set_seed(42)  # Replicate model_2 model_3 = tf.keras.Sequential([   tf.keras.layers.Dense(1),   tf.keras.layers.Dense(1) ])  # Compile the model model_3.compile(loss=tf.keras.losses.mae,                 optimizer=tf.keras.optimizers.SGD(),                 metrics=['mae'])  # Fit the model (this time for 500 epochs, not 100) model_3.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=500, verbose=0) # set verbose to 0 for less output Out[50]: <pre>&lt;keras.callbacks.History at 0x7f0065a5c8e0&gt;</pre> In\u00a0[51]: Copied! <pre># Make and plot predictions for model_3\ny_preds_3 = model_3.predict(X_test)\nplot_predictions(predictions=y_preds_3)\n</pre> # Make and plot predictions for model_3 y_preds_3 = model_3.predict(X_test) plot_predictions(predictions=y_preds_3) <pre>WARNING:tensorflow:6 out of the last 6 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7f00641280d0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n</pre> <pre>1/1 [==============================] - 0s 55ms/step\n</pre> <p>Strange, we trained for longer but our model performed worse?</p> <p>As it turns out, our model might've trained too long and has thus resulted in worse results (we'll see ways to prevent training for too long later on).</p> In\u00a0[52]: Copied! <pre># Calculate model_3 metrics\nmae_3 = mae(y_test, y_preds_3.squeeze()).numpy()\nmse_3 = mse(y_test, y_preds_3.squeeze()).numpy()\nmae_3, mse_3\n</pre> # Calculate model_3 metrics mae_3 = mae(y_test, y_preds_3.squeeze()).numpy() mse_3 = mse(y_test, y_preds_3.squeeze()).numpy() mae_3, mse_3 Out[52]: <pre>(67.224594, 4601.822)</pre> In\u00a0[53]: Copied! <pre>model_results = [[\"model_1\", mae_1, mse_1],\n                 [\"model_2\", mae_2, mse_2],\n                 [\"model_3\", mae_3, mae_3]]\n</pre> model_results = [[\"model_1\", mae_1, mse_1],                  [\"model_2\", mae_2, mse_2],                  [\"model_3\", mae_3, mae_3]] In\u00a0[54]: Copied! <pre>import pandas as pd\nall_results = pd.DataFrame(model_results, columns=[\"model\", \"mae\", \"mse\"])\nall_results\n</pre> import pandas as pd all_results = pd.DataFrame(model_results, columns=[\"model\", \"mae\", \"mse\"]) all_results Out[54]: model mae mse 0 model_1 30.638134 949.130859 1 model_2 10.610324 120.355423 2 model_3 67.224594 67.224594 <p>From our experiments, it looks like <code>model_2</code> performed the best.</p> <p>And now, you might be thinking, \"wow, comparing models is tedious...\" and it definitely can be, we've only compared 3 models here.</p> <p>But this is part of what machine learning modelling is about, trying many different combinations of models and seeing which performs best.</p> <p>Each model you build is a small experiment.</p> <p>\ud83d\udd11 Note: One of your main goals should be to minimize the time between your experiments. The more experiments you do, the more things you'll figure out which don't work and in turn, get closer to figuring out what does work. Remember the machine learning practitioner's motto: \"experiment, experiment, experiment\".</p> <p>Another thing you'll also find is what you thought may work (such as training a model for longer) may not always work and the exact opposite is also often the case.</p> In\u00a0[55]: Copied! <pre># Save a model using the SavedModel format\nmodel_2.save('best_model_SavedModel_format')\n</pre> # Save a model using the SavedModel format model_2.save('best_model_SavedModel_format') <pre>WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n</pre> In\u00a0[56]: Copied! <pre># Check it out - outputs a protobuf binary file (.pb) as well as other files\n!ls best_model_SavedModel_format\n</pre> # Check it out - outputs a protobuf binary file (.pb) as well as other files !ls best_model_SavedModel_format <pre>assets\tfingerprint.pb\tkeras_metadata.pb  saved_model.pb  variables\n</pre> <p>Now let's save the model in the HDF5 format, we'll use the same method but with a different filename.</p> In\u00a0[57]: Copied! <pre># Save a model using the HDF5 format\nmodel_2.save(\"best_model_HDF5_format.h5\") # note the addition of '.h5' on the end\n</pre> # Save a model using the HDF5 format model_2.save(\"best_model_HDF5_format.h5\") # note the addition of '.h5' on the end In\u00a0[58]: Copied! <pre># Check it out\n!ls best_model_HDF5_format.h5\n</pre> # Check it out !ls best_model_HDF5_format.h5 <pre>best_model_HDF5_format.h5\n</pre> In\u00a0[59]: Copied! <pre># Load a model from the SavedModel format\nloaded_saved_model = tf.keras.models.load_model(\"best_model_SavedModel_format\")\nloaded_saved_model.summary()\n</pre> # Load a model from the SavedModel format loaded_saved_model = tf.keras.models.load_model(\"best_model_SavedModel_format\") loaded_saved_model.summary() <pre>Model: \"sequential_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_5 (Dense)             (None, 1)                 2         \n                                                                 \n dense_6 (Dense)             (None, 1)                 2         \n                                                                 \n=================================================================\nTotal params: 4\nTrainable params: 4\nNon-trainable params: 0\n_________________________________________________________________\n</pre> <p>Now let's test it out.</p> In\u00a0[60]: Copied! <pre># Compare model_2 with the SavedModel version (should return True)\nmodel_2_preds = model_2.predict(X_test)\nsaved_model_preds = loaded_saved_model.predict(X_test)\nmae(y_test, saved_model_preds.squeeze()).numpy() == mae(y_test, model_2_preds.squeeze()).numpy()\n</pre> # Compare model_2 with the SavedModel version (should return True) model_2_preds = model_2.predict(X_test) saved_model_preds = loaded_saved_model.predict(X_test) mae(y_test, saved_model_preds.squeeze()).numpy() == mae(y_test, model_2_preds.squeeze()).numpy() <pre>1/1 [==============================] - 0s 34ms/step\n1/1 [==============================] - 0s 56ms/step\n</pre> Out[60]: <pre>True</pre> <p>Loading in from the HDF5 is much the same.</p> In\u00a0[61]: Copied! <pre># Load a model from the HDF5 format\nloaded_h5_model = tf.keras.models.load_model(\"best_model_HDF5_format.h5\")\nloaded_h5_model.summary()\n</pre> # Load a model from the HDF5 format loaded_h5_model = tf.keras.models.load_model(\"best_model_HDF5_format.h5\") loaded_h5_model.summary() <pre>Model: \"sequential_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_5 (Dense)             (None, 1)                 2         \n                                                                 \n dense_6 (Dense)             (None, 1)                 2         \n                                                                 \n=================================================================\nTotal params: 4\nTrainable params: 4\nNon-trainable params: 0\n_________________________________________________________________\n</pre> In\u00a0[62]: Copied! <pre># Compare model_2 with the loaded HDF5 version (should return True)\nh5_model_preds = loaded_h5_model.predict(X_test)\nmae(y_test, h5_model_preds.squeeze()).numpy() == mae(y_test, model_2_preds.squeeze()).numpy()\n</pre> # Compare model_2 with the loaded HDF5 version (should return True) h5_model_preds = loaded_h5_model.predict(X_test) mae(y_test, h5_model_preds.squeeze()).numpy() == mae(y_test, model_2_preds.squeeze()).numpy() <pre>1/1 [==============================] - 0s 53ms/step\n</pre> Out[62]: <pre>True</pre> In\u00a0[63]: Copied! <pre># Download the model (or any file) from Google Colab\nfrom google.colab import files\nfiles.download(\"best_model_HDF5_format.h5\")\n</pre> # Download the model (or any file) from Google Colab from google.colab import files files.download(\"best_model_HDF5_format.h5\") In\u00a0[64]: Copied! <pre># Import required libraries\nimport tensorflow as tf\nimport pandas as pd\nimport matplotlib.pyplot as plt\n</pre> # Import required libraries import tensorflow as tf import pandas as pd import matplotlib.pyplot as plt In\u00a0[65]: Copied! <pre># Read in the insurance dataset\ninsurance = pd.read_csv(\"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\")\n</pre> # Read in the insurance dataset insurance = pd.read_csv(\"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\") In\u00a0[66]: Copied! <pre># Check out the insurance dataset\ninsurance.head()\n</pre> # Check out the insurance dataset insurance.head() Out[66]: age sex bmi children smoker region charges 0 19 female 27.900 0 yes southwest 16884.92400 1 18 male 33.770 1 no southeast 1725.55230 2 28 male 33.000 3 no southeast 4449.46200 3 33 male 22.705 0 no northwest 21984.47061 4 32 male 28.880 0 no northwest 3866.85520 <p>We're going to have to turn the non-numerical columns into numbers (because a neural network can't handle non-numerical inputs).</p> <p>To do so, we'll use the <code>get_dummies()</code> method in pandas.</p> <p>It converts categorical variables (like the <code>sex</code>, <code>smoker</code> and <code>region</code> columns) into numerical variables using one-hot encoding.</p> In\u00a0[67]: Copied! <pre># Turn all categories into numbers\ninsurance_one_hot = pd.get_dummies(insurance)\ninsurance_one_hot.head() # view the converted columns\n</pre> # Turn all categories into numbers insurance_one_hot = pd.get_dummies(insurance) insurance_one_hot.head() # view the converted columns Out[67]: age bmi children charges sex_female sex_male smoker_no smoker_yes region_northeast region_northwest region_southeast region_southwest 0 19 27.900 0 16884.92400 1 0 0 1 0 0 0 1 1 18 33.770 1 1725.55230 0 1 1 0 0 0 1 0 2 28 33.000 3 4449.46200 0 1 1 0 0 0 1 0 3 33 22.705 0 21984.47061 0 1 1 0 0 1 0 0 4 32 28.880 0 3866.85520 0 1 1 0 0 1 0 0 <p>Now we'll split data into features (<code>X</code>) and labels (<code>y</code>).</p> In\u00a0[68]: Copied! <pre># Create X &amp; y values\nX = insurance_one_hot.drop(\"charges\", axis=1)\ny = insurance_one_hot[\"charges\"]\n</pre> # Create X &amp; y values X = insurance_one_hot.drop(\"charges\", axis=1) y = insurance_one_hot[\"charges\"] In\u00a0[69]: Copied! <pre># View features\nX.head()\n</pre> # View features X.head() Out[69]: age bmi children sex_female sex_male smoker_no smoker_yes region_northeast region_northwest region_southeast region_southwest 0 19 27.900 0 1 0 0 1 0 0 0 1 1 18 33.770 1 0 1 1 0 0 0 1 0 2 28 33.000 3 0 1 1 0 0 0 1 0 3 33 22.705 0 0 1 1 0 0 1 0 0 4 32 28.880 0 0 1 1 0 0 1 0 0 <p>And create training and test sets. We could do this manually, but to make it easier, we'll leverage the already available <code>train_test_split</code> function available from Scikit-Learn.</p> In\u00a0[70]: Copied! <pre># Create training and test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    test_size=0.2, \n                                                    random_state=42) # set random state for reproducible splits\n</pre> # Create training and test sets from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X,                                                      y,                                                      test_size=0.2,                                                      random_state=42) # set random state for reproducible splits <p>Now we can build and fit a model (we'll make it the same as <code>model_2</code>).</p> In\u00a0[71]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create a new model (same as model_2)\ninsurance_model = tf.keras.Sequential([\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Dense(1)\n])\n\n# Compile the model\ninsurance_model.compile(loss=tf.keras.losses.mae,\n                        optimizer=tf.keras.optimizers.SGD(),\n                        metrics=['mae'])\n\n# Fit the model\ninsurance_model.fit(X_train, y_train, epochs=100)\n</pre> # Set random seed tf.random.set_seed(42)  # Create a new model (same as model_2) insurance_model = tf.keras.Sequential([   tf.keras.layers.Dense(1),   tf.keras.layers.Dense(1) ])  # Compile the model insurance_model.compile(loss=tf.keras.losses.mae,                         optimizer=tf.keras.optimizers.SGD(),                         metrics=['mae'])  # Fit the model insurance_model.fit(X_train, y_train, epochs=100) <pre>Epoch 1/100\n34/34 [==============================] - 1s 3ms/step - loss: 9369.4971 - mae: 9369.4971\nEpoch 2/100\n34/34 [==============================] - 0s 3ms/step - loss: 7851.0615 - mae: 7851.0615\nEpoch 3/100\n34/34 [==============================] - 0s 3ms/step - loss: 7567.6006 - mae: 7567.6006\nEpoch 4/100\n34/34 [==============================] - 0s 3ms/step - loss: 7540.6812 - mae: 7540.6812\nEpoch 5/100\n34/34 [==============================] - 0s 3ms/step - loss: 7695.3062 - mae: 7695.3062\nEpoch 6/100\n34/34 [==============================] - 0s 3ms/step - loss: 7608.9712 - mae: 7608.9712\nEpoch 7/100\n34/34 [==============================] - 0s 3ms/step - loss: 7517.2896 - mae: 7517.2896\nEpoch 8/100\n34/34 [==============================] - 0s 3ms/step - loss: 7788.9150 - mae: 7788.9150\nEpoch 9/100\n34/34 [==============================] - 0s 3ms/step - loss: 7584.7583 - mae: 7584.7583\nEpoch 10/100\n34/34 [==============================] - 0s 3ms/step - loss: 7721.3062 - mae: 7721.3062\nEpoch 11/100\n34/34 [==============================] - 0s 3ms/step - loss: 7769.6841 - mae: 7769.6841\nEpoch 12/100\n34/34 [==============================] - 0s 3ms/step - loss: 7637.6289 - mae: 7637.6289\nEpoch 13/100\n34/34 [==============================] - 0s 3ms/step - loss: 7783.9155 - mae: 7783.9155\nEpoch 14/100\n34/34 [==============================] - 0s 3ms/step - loss: 7717.6284 - mae: 7717.6284\nEpoch 15/100\n34/34 [==============================] - 0s 3ms/step - loss: 7542.4185 - mae: 7542.4185\nEpoch 16/100\n34/34 [==============================] - 0s 3ms/step - loss: 7749.9731 - mae: 7749.9731\nEpoch 17/100\n34/34 [==============================] - 0s 3ms/step - loss: 7598.9355 - mae: 7598.9355\nEpoch 18/100\n34/34 [==============================] - 0s 3ms/step - loss: 7808.7837 - mae: 7808.7837\nEpoch 19/100\n34/34 [==============================] - 0s 3ms/step - loss: 7794.0264 - mae: 7794.0264\nEpoch 20/100\n34/34 [==============================] - 0s 3ms/step - loss: 7882.9180 - mae: 7882.9180\nEpoch 21/100\n34/34 [==============================] - 0s 3ms/step - loss: 7522.1748 - mae: 7522.1748\nEpoch 22/100\n34/34 [==============================] - 0s 3ms/step - loss: 7838.5420 - mae: 7838.5420\nEpoch 23/100\n34/34 [==============================] - 0s 3ms/step - loss: 7580.5273 - mae: 7580.5273\nEpoch 24/100\n34/34 [==============================] - 0s 3ms/step - loss: 7542.0605 - mae: 7542.0605\nEpoch 25/100\n34/34 [==============================] - 0s 3ms/step - loss: 7628.8145 - mae: 7628.8145\nEpoch 26/100\n34/34 [==============================] - 0s 3ms/step - loss: 7606.2163 - mae: 7606.2163\nEpoch 27/100\n34/34 [==============================] - 0s 3ms/step - loss: 7559.1895 - mae: 7559.1895\nEpoch 28/100\n34/34 [==============================] - 0s 3ms/step - loss: 7375.6968 - mae: 7375.6968\nEpoch 29/100\n34/34 [==============================] - 0s 3ms/step - loss: 7684.6680 - mae: 7684.6680\nEpoch 30/100\n34/34 [==============================] - 0s 3ms/step - loss: 7640.9189 - mae: 7640.9189\nEpoch 31/100\n34/34 [==============================] - 0s 3ms/step - loss: 7714.8374 - mae: 7714.8374\nEpoch 32/100\n34/34 [==============================] - 0s 3ms/step - loss: 7506.1108 - mae: 7506.1108\nEpoch 33/100\n34/34 [==============================] - 0s 3ms/step - loss: 7406.3838 - mae: 7406.3838\nEpoch 34/100\n34/34 [==============================] - 0s 3ms/step - loss: 7449.5298 - mae: 7449.5298\nEpoch 35/100\n34/34 [==============================] - 0s 3ms/step - loss: 7560.9849 - mae: 7560.9849\nEpoch 36/100\n34/34 [==============================] - 0s 3ms/step - loss: 7600.5376 - mae: 7600.5376\nEpoch 37/100\n34/34 [==============================] - 0s 3ms/step - loss: 7553.1353 - mae: 7553.1353\nEpoch 38/100\n34/34 [==============================] - 0s 3ms/step - loss: 7440.0786 - mae: 7440.0786\nEpoch 39/100\n34/34 [==============================] - 0s 3ms/step - loss: 7537.5098 - mae: 7537.5098\nEpoch 40/100\n34/34 [==============================] - 0s 2ms/step - loss: 7317.4824 - mae: 7317.4824\nEpoch 41/100\n34/34 [==============================] - 0s 3ms/step - loss: 7758.7363 - mae: 7758.7363\nEpoch 42/100\n34/34 [==============================] - 0s 3ms/step - loss: 7428.0835 - mae: 7428.0835\nEpoch 43/100\n34/34 [==============================] - 0s 3ms/step - loss: 7663.6904 - mae: 7663.6904\nEpoch 44/100\n34/34 [==============================] - 0s 3ms/step - loss: 7451.4316 - mae: 7451.4316\nEpoch 45/100\n34/34 [==============================] - 0s 3ms/step - loss: 7423.6045 - mae: 7423.6045\nEpoch 46/100\n34/34 [==============================] - 0s 3ms/step - loss: 7639.4028 - mae: 7639.4028\nEpoch 47/100\n34/34 [==============================] - 0s 3ms/step - loss: 7432.8540 - mae: 7432.8540\nEpoch 48/100\n34/34 [==============================] - 0s 3ms/step - loss: 7417.3125 - mae: 7417.3125\nEpoch 49/100\n34/34 [==============================] - 0s 3ms/step - loss: 7534.0508 - mae: 7534.0508\nEpoch 50/100\n34/34 [==============================] - 0s 3ms/step - loss: 7494.0288 - mae: 7494.0288\nEpoch 51/100\n34/34 [==============================] - 0s 3ms/step - loss: 7388.7905 - mae: 7388.7905\nEpoch 52/100\n34/34 [==============================] - 0s 3ms/step - loss: 7539.9224 - mae: 7539.9224\nEpoch 53/100\n34/34 [==============================] - 0s 3ms/step - loss: 7613.3428 - mae: 7613.3428\nEpoch 54/100\n34/34 [==============================] - 0s 3ms/step - loss: 7376.6582 - mae: 7376.6582\nEpoch 55/100\n34/34 [==============================] - 0s 3ms/step - loss: 7305.9424 - mae: 7305.9424\nEpoch 56/100\n34/34 [==============================] - 0s 3ms/step - loss: 7302.3154 - mae: 7302.3154\nEpoch 57/100\n34/34 [==============================] - 0s 3ms/step - loss: 7345.5010 - mae: 7345.5010\nEpoch 58/100\n34/34 [==============================] - 0s 3ms/step - loss: 7603.5664 - mae: 7603.5664\nEpoch 59/100\n34/34 [==============================] - 0s 3ms/step - loss: 7538.8198 - mae: 7538.8198\nEpoch 60/100\n34/34 [==============================] - 0s 3ms/step - loss: 7510.0747 - mae: 7510.0747\nEpoch 61/100\n34/34 [==============================] - 0s 3ms/step - loss: 7485.2583 - mae: 7485.2583\nEpoch 62/100\n34/34 [==============================] - 0s 3ms/step - loss: 7396.9038 - mae: 7396.9038\nEpoch 63/100\n34/34 [==============================] - 0s 3ms/step - loss: 7275.6465 - mae: 7275.6465\nEpoch 64/100\n34/34 [==============================] - 0s 3ms/step - loss: 7422.1836 - mae: 7422.1836\nEpoch 65/100\n34/34 [==============================] - 0s 3ms/step - loss: 7321.6035 - mae: 7321.6035\nEpoch 66/100\n34/34 [==============================] - 0s 3ms/step - loss: 7287.3623 - mae: 7287.3623\nEpoch 67/100\n34/34 [==============================] - 0s 3ms/step - loss: 7259.7354 - mae: 7259.7354\nEpoch 68/100\n34/34 [==============================] - 0s 3ms/step - loss: 7494.6240 - mae: 7494.6240\nEpoch 69/100\n34/34 [==============================] - 0s 3ms/step - loss: 7641.2607 - mae: 7641.2607\nEpoch 70/100\n34/34 [==============================] - 0s 3ms/step - loss: 7518.6045 - mae: 7518.6045\nEpoch 71/100\n34/34 [==============================] - 0s 3ms/step - loss: 7506.5884 - mae: 7506.5884\nEpoch 72/100\n34/34 [==============================] - 0s 3ms/step - loss: 7324.0532 - mae: 7324.0532\nEpoch 73/100\n34/34 [==============================] - 0s 3ms/step - loss: 7277.3755 - mae: 7277.3755\nEpoch 74/100\n34/34 [==============================] - 0s 3ms/step - loss: 7425.9590 - mae: 7425.9590\nEpoch 75/100\n34/34 [==============================] - 0s 3ms/step - loss: 7308.2617 - mae: 7308.2617\nEpoch 76/100\n34/34 [==============================] - 0s 3ms/step - loss: 7206.4956 - mae: 7206.4956\nEpoch 77/100\n34/34 [==============================] - 0s 3ms/step - loss: 7439.0884 - mae: 7439.0884\nEpoch 78/100\n34/34 [==============================] - 0s 3ms/step - loss: 7026.8892 - mae: 7026.8892\nEpoch 79/100\n34/34 [==============================] - 0s 3ms/step - loss: 7536.8247 - mae: 7536.8247\nEpoch 80/100\n34/34 [==============================] - 0s 3ms/step - loss: 7216.5439 - mae: 7216.5439\nEpoch 81/100\n34/34 [==============================] - 0s 3ms/step - loss: 7262.0186 - mae: 7262.0186\nEpoch 82/100\n34/34 [==============================] - 0s 3ms/step - loss: 7100.8447 - mae: 7100.8447\nEpoch 83/100\n34/34 [==============================] - 0s 3ms/step - loss: 7499.2349 - mae: 7499.2349\nEpoch 84/100\n34/34 [==============================] - 0s 3ms/step - loss: 7417.9360 - mae: 7417.9360\nEpoch 85/100\n34/34 [==============================] - 0s 3ms/step - loss: 7291.6318 - mae: 7291.6318\nEpoch 86/100\n34/34 [==============================] - 0s 3ms/step - loss: 7259.4048 - mae: 7259.4048\nEpoch 87/100\n34/34 [==============================] - 0s 3ms/step - loss: 7278.6729 - mae: 7278.6729\nEpoch 88/100\n34/34 [==============================] - 0s 3ms/step - loss: 7334.4409 - mae: 7334.4409\nEpoch 89/100\n34/34 [==============================] - 0s 3ms/step - loss: 7472.9878 - mae: 7472.9878\nEpoch 90/100\n34/34 [==============================] - 0s 3ms/step - loss: 7145.0088 - mae: 7145.0088\nEpoch 91/100\n34/34 [==============================] - 0s 3ms/step - loss: 7247.7705 - mae: 7247.7705\nEpoch 92/100\n34/34 [==============================] - 0s 3ms/step - loss: 7322.7412 - mae: 7322.7412\nEpoch 93/100\n34/34 [==============================] - 0s 3ms/step - loss: 7422.8809 - mae: 7422.8809\nEpoch 94/100\n34/34 [==============================] - 0s 3ms/step - loss: 7359.2515 - mae: 7359.2515\nEpoch 95/100\n34/34 [==============================] - 0s 3ms/step - loss: 7475.3711 - mae: 7475.3711\nEpoch 96/100\n34/34 [==============================] - 0s 3ms/step - loss: 6932.3413 - mae: 6932.3413\nEpoch 97/100\n34/34 [==============================] - 0s 3ms/step - loss: 7036.0708 - mae: 7036.0708\nEpoch 98/100\n34/34 [==============================] - 0s 3ms/step - loss: 7209.4790 - mae: 7209.4790\nEpoch 99/100\n34/34 [==============================] - 0s 3ms/step - loss: 7393.4707 - mae: 7393.4707\nEpoch 100/100\n34/34 [==============================] - 0s 3ms/step - loss: 7257.4365 - mae: 7257.4365\n</pre> Out[71]: <pre>&lt;keras.callbacks.History at 0x7f002a284730&gt;</pre> In\u00a0[72]: Copied! <pre># Check the results of the insurance model\ninsurance_model.evaluate(X_test, y_test)\n</pre> # Check the results of the insurance model insurance_model.evaluate(X_test, y_test) <pre>9/9 [==============================] - 0s 2ms/step - loss: 6392.2939 - mae: 6392.2939\n</pre> Out[72]: <pre>[6392.2939453125, 6392.2939453125]</pre> <p>Our model didn't perform very well, let's try a bigger model.</p> <p>We'll try 3 things:</p> <ul> <li>Increasing the number of layers (2 -&gt; 3).</li> <li>Increasing the number of units in each layer (except for the output layer).</li> <li>Changing the optimizer (from SGD to Adam).</li> </ul> <p>Everything else will stay the same.</p> In\u00a0[73]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Add an extra layer and increase number of units\ninsurance_model_2 = tf.keras.Sequential([\n  tf.keras.layers.Dense(100), # 100 units\n  tf.keras.layers.Dense(10), # 10 units\n  tf.keras.layers.Dense(1) # 1 unit (important for output layer)\n])\n\n# Compile the model\ninsurance_model_2.compile(loss=tf.keras.losses.mae,\n                          optimizer=tf.keras.optimizers.Adam(), # Adam works but SGD doesn't \n                          metrics=['mae'])\n\n# Fit the model and save the history (we can plot this)\nhistory = insurance_model_2.fit(X_train, y_train, epochs=100, verbose=0)\n</pre> # Set random seed tf.random.set_seed(42)  # Add an extra layer and increase number of units insurance_model_2 = tf.keras.Sequential([   tf.keras.layers.Dense(100), # 100 units   tf.keras.layers.Dense(10), # 10 units   tf.keras.layers.Dense(1) # 1 unit (important for output layer) ])  # Compile the model insurance_model_2.compile(loss=tf.keras.losses.mae,                           optimizer=tf.keras.optimizers.Adam(), # Adam works but SGD doesn't                            metrics=['mae'])  # Fit the model and save the history (we can plot this) history = insurance_model_2.fit(X_train, y_train, epochs=100, verbose=0) In\u00a0[74]: Copied! <pre># Evaluate our larger model\ninsurance_model_2.evaluate(X_test, y_test)\n</pre> # Evaluate our larger model insurance_model_2.evaluate(X_test, y_test) <pre>9/9 [==============================] - 0s 2ms/step - loss: 4629.1626 - mae: 4629.1626\n</pre> Out[74]: <pre>[4629.16259765625, 4629.16259765625]</pre> <p>Much better! Using a larger model and the Adam optimizer results in almost half the error as the previous model.</p> <p>\ud83d\udd11 Note: For many problems, the Adam optimizer is a great starting choice. See Andrei Karpathy's \"Adam is safe\" point from A Recipe for Training Neural Networks for more.</p> <p>Let's check out the loss curves of our model, we should see a downward trend.</p> In\u00a0[75]: Copied! <pre># Plot history (also known as a loss curve)\npd.DataFrame(history.history).plot()\nplt.ylabel(\"loss\")\nplt.xlabel(\"epochs\");\n</pre> # Plot history (also known as a loss curve) pd.DataFrame(history.history).plot() plt.ylabel(\"loss\") plt.xlabel(\"epochs\"); <p>From this, it looks like our model's loss (and MAE) were both still decreasing (in our case, MAE and loss are the same, hence the lines in the plot overlap eachother).</p> <p>What this tells us is the loss might go down if we try training it for longer.</p> <p>\ud83e\udd14 Question: How long should you train for?</p> <p>It depends on what problem you're working on. Sometimes training won't take very long, other times it'll take longer than you expect. A common method is to set your model training for a very long time (e.g. 1000's of epochs) but set it up with an EarlyStopping callback so it stops automatically when it stops improving. We'll see this in another module.</p> <p>Let's train the same model as above for a little longer. We can do this but calling fit on it again.</p> In\u00a0[76]: Copied! <pre># Try training for a little longer (100 more epochs)\nhistory_2 = insurance_model_2.fit(X_train, y_train, epochs=100, verbose=0)\n</pre> # Try training for a little longer (100 more epochs) history_2 = insurance_model_2.fit(X_train, y_train, epochs=100, verbose=0) <p>How did the extra training go?</p> In\u00a0[77]: Copied! <pre># Evaluate the model trained for 200 total epochs\ninsurance_model_2_loss, insurance_model_2_mae = insurance_model_2.evaluate(X_test, y_test)\ninsurance_model_2_loss, insurance_model_2_mae\n</pre> # Evaluate the model trained for 200 total epochs insurance_model_2_loss, insurance_model_2_mae = insurance_model_2.evaluate(X_test, y_test) insurance_model_2_loss, insurance_model_2_mae <pre>9/9 [==============================] - 0s 3ms/step - loss: 3483.4031 - mae: 3483.4031\n</pre> Out[77]: <pre>(3483.403076171875, 3483.403076171875)</pre> <p>Boom! Training for an extra 100 epochs we see about a 10% decrease in error.</p> <p>How does the visual look?</p> In\u00a0[78]: Copied! <pre># Plot the model trained for 200 total epochs loss curves\npd.DataFrame(history_2.history).plot()\nplt.ylabel(\"loss\")\nplt.xlabel(\"epochs\"); # note: epochs will only show 100 since we overrid the history variable\n</pre> # Plot the model trained for 200 total epochs loss curves pd.DataFrame(history_2.history).plot() plt.ylabel(\"loss\") plt.xlabel(\"epochs\"); # note: epochs will only show 100 since we overrid the history variable In\u00a0[79]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\n# Read in the insurance dataset\ninsurance = pd.read_csv(\"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\")\n</pre> import pandas as pd import matplotlib.pyplot as plt import tensorflow as tf  # Read in the insurance dataset insurance = pd.read_csv(\"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\") In\u00a0[80]: Copied! <pre># Check out the data\ninsurance.head()\n</pre> # Check out the data insurance.head() Out[80]: age sex bmi children smoker region charges 0 19 female 27.900 0 yes southwest 16884.92400 1 18 male 33.770 1 no southeast 1725.55230 2 28 male 33.000 3 no southeast 4449.46200 3 33 male 22.705 0 no northwest 21984.47061 4 32 male 28.880 0 no northwest 3866.85520 <p>Now, just as before, we need to transform the non-numerical columns into numbers and this time we'll also be normalizing the numerical columns with different ranges (to make sure they're all between 0 and 1).</p> <p>To do this, we're going to use a few classes from Scikit-Learn:</p> <ul> <li><code>make_column_transformer</code> - build a multi-step data preprocessing function for the folllowing transformations:<ul> <li><code>MinMaxScaler</code> - make sure all numerical columns are normalized (between 0 and 1).</li> <li><code>OneHotEncoder</code> - one hot encode the non-numerical columns.</li> </ul> </li> </ul> <p>Let's see them in action.</p> In\u00a0[81]: Copied! <pre>from sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n\n# Create column transformer (this will help us normalize/preprocess our data)\nct = make_column_transformer(\n    (MinMaxScaler(), [\"age\", \"bmi\", \"children\"]), # get all values between 0 and 1\n    (OneHotEncoder(handle_unknown=\"ignore\"), [\"sex\", \"smoker\", \"region\"])\n)\n\n# Create X &amp; y\nX = insurance.drop(\"charges\", axis=1)\ny = insurance[\"charges\"]\n\n# Build our train and test sets (use random state to ensure same split as before)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Fit column transformer on the training data only (doing so on test data would result in data leakage)\nct.fit(X_train)\n\n# Transform training and test data with normalization (MinMaxScalar) and one hot encoding (OneHotEncoder)\nX_train_normal = ct.transform(X_train)\nX_test_normal = ct.transform(X_test)\n</pre> from sklearn.compose import make_column_transformer from sklearn.preprocessing import MinMaxScaler, OneHotEncoder  # Create column transformer (this will help us normalize/preprocess our data) ct = make_column_transformer(     (MinMaxScaler(), [\"age\", \"bmi\", \"children\"]), # get all values between 0 and 1     (OneHotEncoder(handle_unknown=\"ignore\"), [\"sex\", \"smoker\", \"region\"]) )  # Create X &amp; y X = insurance.drop(\"charges\", axis=1) y = insurance[\"charges\"]  # Build our train and test sets (use random state to ensure same split as before) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Fit column transformer on the training data only (doing so on test data would result in data leakage) ct.fit(X_train)  # Transform training and test data with normalization (MinMaxScalar) and one hot encoding (OneHotEncoder) X_train_normal = ct.transform(X_train) X_test_normal = ct.transform(X_test) <p>Now we've normalized it and one-hot encoding it, what does our data look like now?</p> In\u00a0[82]: Copied! <pre># Non-normalized and non-one-hot encoded data example\nX_train.loc[0]\n</pre> # Non-normalized and non-one-hot encoded data example X_train.loc[0] Out[82]: <pre>age                19\nsex            female\nbmi              27.9\nchildren            0\nsmoker            yes\nregion      southwest\nName: 0, dtype: object</pre> In\u00a0[83]: Copied! <pre># Normalized and one-hot encoded example\nX_train_normal[0]\n</pre> # Normalized and one-hot encoded example X_train_normal[0] Out[83]: <pre>array([0.60869565, 0.10734463, 0.4       , 1.        , 0.        ,\n       1.        , 0.        , 0.        , 1.        , 0.        ,\n       0.        ])</pre> <p>How about the shapes?</p> In\u00a0[84]: Copied! <pre># Notice the normalized/one-hot encoded shape is larger because of the extra columns\nX_train_normal.shape, X_train.shape\n</pre> # Notice the normalized/one-hot encoded shape is larger because of the extra columns X_train_normal.shape, X_train.shape Out[84]: <pre>((1070, 11), (1070, 6))</pre> <p>Our data is normalized and numerical, let's model it.</p> <p>We'll use the same model as <code>insurance_model_2</code>.</p> In\u00a0[85]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Build the model (3 layers, 100, 10, 1 units)\ninsurance_model_3 = tf.keras.Sequential([\n  tf.keras.layers.Dense(100),\n  tf.keras.layers.Dense(10),\n  tf.keras.layers.Dense(1)\n])\n\n# Compile the model\ninsurance_model_3.compile(loss=tf.keras.losses.mae,\n                          optimizer=tf.keras.optimizers.Adam(),\n                          metrics=['mae'])\n\n# Fit the model for 200 epochs (same as insurance_model_2)\ninsurance_model_3.fit(X_train_normal, y_train, epochs=200, verbose=0)\n</pre> # Set random seed tf.random.set_seed(42)  # Build the model (3 layers, 100, 10, 1 units) insurance_model_3 = tf.keras.Sequential([   tf.keras.layers.Dense(100),   tf.keras.layers.Dense(10),   tf.keras.layers.Dense(1) ])  # Compile the model insurance_model_3.compile(loss=tf.keras.losses.mae,                           optimizer=tf.keras.optimizers.Adam(),                           metrics=['mae'])  # Fit the model for 200 epochs (same as insurance_model_2) insurance_model_3.fit(X_train_normal, y_train, epochs=200, verbose=0)  Out[85]: <pre>&lt;keras.callbacks.History at 0x7f004cd64af0&gt;</pre> <p>Let's evaluate the model on normalized test set.</p> In\u00a0[86]: Copied! <pre># Evaulate 3rd model\ninsurance_model_3_loss, insurance_model_3_mae = insurance_model_3.evaluate(X_test_normal, y_test)\n</pre> # Evaulate 3rd model insurance_model_3_loss, insurance_model_3_mae = insurance_model_3.evaluate(X_test_normal, y_test) <pre>9/9 [==============================] - 0s 3ms/step - loss: 3171.2595 - mae: 3171.2595\n</pre> <p>And finally, let's compare the results from <code>insurance_model_2</code> (trained on non-normalized data) and <code>insurance_model_3</code> (trained on normalized data).</p> In\u00a0[87]: Copied! <pre># Compare modelling results from non-normalized data and normalized data\ninsurance_model_2_mae, insurance_model_3_mae\n</pre> # Compare modelling results from non-normalized data and normalized data insurance_model_2_mae, insurance_model_3_mae Out[87]: <pre>(3483.403076171875, 3171.259521484375)</pre> <p>From this we can see normalizing the data results in 10% less error using the same model than not normalizing the data.</p> <p>This is one of the main benefits of normalization: faster convergence time (a fancy way of saying, your model gets to better results faster).</p> <p><code>insurance_model_2</code> may have eventually achieved the same results as <code>insurance_model_3</code> if we left it training for longer.</p> <p>Also, the results may change if we were to alter the architectures of the models, e.g. more hidden units per layer or more layers.</p> <p>But since our main goal as neural network practitioners is to decrease the time between experiments, anything that helps us get better results sooner is a plus.</p>"},{"location":"01_neural_network_regression_in_tensorflow/#01-neural-network-regression-with-tensorflow","title":"01. Neural Network Regression with TensorFlow\u00b6","text":"<p>There are many definitions for a regression problem but in our case, we're going to simplify it to be: predicting a number.</p> <p>For example, you might want to:</p> <ul> <li>Predict the selling price of houses given information about them (such as number of rooms, size, number of bathrooms).</li> <li>Predict the coordinates of a bounding box of an item in an image.</li> <li>Predict the cost of medical insurance for an individual given their demographics (age, sex, gender, race).</li> </ul> <p>In this notebook, we're going to set the foundations for how you can take a sample of inputs (this is your data), build a neural network to discover patterns in those inputs and then make a prediction (in the form of a number) based on those inputs.</p>"},{"location":"01_neural_network_regression_in_tensorflow/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>Specifically, we're going to go through doing the following with TensorFlow:</p> <ul> <li>Architecture of a regression model</li> <li>Input shapes and output shapes<ul> <li><code>X</code>: features/data (inputs)</li> <li><code>y</code>: labels (outputs)</li> </ul> </li> <li>Creating custom data to view and fit</li> <li>Steps in modelling<ul> <li>Creating a model</li> <li>Compiling a model<ul> <li>Defining a loss function</li> <li>Setting up an optimizer</li> <li>Creating evaluation metrics</li> </ul> </li> <li>Fitting a model (getting it to find patterns in our data)</li> </ul> </li> <li>Evaluating a model<ul> <li>Visualizng the model (\"visualize, visualize, visualize\")</li> <li>Looking at training curves</li> <li>Compare predictions to ground truth (using our evaluation metrics)</li> </ul> </li> <li>Saving a model (so we can use it later)</li> <li>Loading a model</li> </ul> <p>Don't worry if none of these make sense now, we're going to go through each.</p>"},{"location":"01_neural_network_regression_in_tensorflow/#how-you-can-use-this-notebook","title":"How you can use this notebook\u00b6","text":"<p>You can read through the descriptions and the code (it should all run), but there's a better option.</p> <p>Write all of the code yourself.</p> <p>Yes. I'm serious. Create a new notebook, and rewrite each line by yourself. Investigate it, see if you can break it, why does it break?</p> <p>You don't have to write the text descriptions but writing the code yourself is a great way to get hands-on experience.</p> <p>Don't worry if you make mistakes, we all do. The way to get better and make less mistakes is to write more code.</p>"},{"location":"01_neural_network_regression_in_tensorflow/#typical-architecture-of-a-regresison-neural-network","title":"Typical architecture of a regresison neural network\u00b6","text":"<p>The word typical is on purpose.</p> <p>Why?</p> <p>Because there are many different ways (actually, there's almost an infinite number of ways) to write neural networks.</p> <p>But the following is a generic setup for ingesting a collection of numbers, finding patterns in them and then outputting some kind of target number.</p> <p>Yes, the previous sentence is vague but we'll see this in action shortly.</p> Hyperparameter Typical value Input layer shape Same shape as number of features (e.g. 3 for # bedrooms, # bathrooms, # car spaces in housing price prediction) Hidden layer(s) Problem specific, minimum = 1, maximum = unlimited Neurons per hidden layer Problem specific, generally 10 to 100 Output layer shape Same shape as desired prediction shape (e.g. 1 for house price) Hidden activation Usually ReLU (rectified linear unit) Output activation None, ReLU, logistic/tanh Loss function MSE (mean square error) or MAE (mean absolute error)/Huber (combination of MAE/MSE) if outliers Optimizer SGD (stochastic gradient descent), Adam <p>Table 1: Typical architecture of a regression network. Source: Adapted from page 293 of Hands-On Machine Learning with Scikit-Learn, Keras &amp; TensorFlow Book by Aur\u00e9lien G\u00e9ron</p> <p>Again, if you're new to neural networks and deep learning in general, much of the above table won't make sense. But don't worry, we'll be getting hands-on with all of it soon.</p> <p>\ud83d\udd11 Note: A hyperparameter in machine learning is something a data analyst or developer can set themselves, where as a parameter usually describes something a model learns on its own (a value not explicitly set by an analyst).</p> <p>Okay, enough talk, let's get started writing code.</p> <p>To use TensorFlow, we'll import it as the common alias <code>tf</code> (short for TensorFlow).</p>"},{"location":"01_neural_network_regression_in_tensorflow/#creating-data-to-view-and-fit","title":"Creating data to view and fit\u00b6","text":"<p>Since we're working on a regression problem (predicting a number) let's create some linear data (a straight line) to model.</p>"},{"location":"01_neural_network_regression_in_tensorflow/#regression-input-shapes-and-output-shapes","title":"Regression input shapes and output shapes\u00b6","text":"<p>One of the most important concepts when working with neural networks are the input and output shapes.</p> <p>The input shape is the shape of your data that goes into the model.</p> <p>The output shape is the shape of your data you want to come out of your model.</p> <p>These will differ depending on the problem you're working on.</p> <p>Neural networks accept numbers and output numbers. These numbers are typically represented as tensors (or arrays).</p> <p>Before, we created data using NumPy arrays, but we could do the same with tensors.</p>"},{"location":"01_neural_network_regression_in_tensorflow/#steps-in-modelling-with-tensorflow","title":"Steps in modelling with TensorFlow\u00b6","text":"<p>Now we know what data we have as well as the input and output shapes, let's see how we'd build a neural network to model it.</p> <p>In TensorFlow, there are typically 3 fundamental steps to creating and training a model.</p> <ol> <li>Creating a model - piece together the layers of a neural network yourself (using the Functional or Sequential API) or import a previously built model (known as transfer learning).</li> <li>Compiling a model - defining how a models performance should be measured (loss/metrics) as well as defining how it should improve (optimizer).</li> <li>Fitting a model - letting the model try to find patterns in the data (how does <code>X</code> get to <code>y</code>).</li> </ol> <p>Let's see these in action using the Keras Sequential API to build a model for our regression data. And then we'll step through each.</p> <p>Note: If you're using TensorFlow 2.7.0+, the <code>fit()</code> function no longer upscales input data to go from <code>(batch_size, )</code> to <code>(batch_size, 1)</code>. To fix this, you'll need to expand the dimension of input data using <code>tf.expand_dims(input_data, axis=-1)</code>.</p> <p>In our case, this means instead of using <code>model.fit(X, y, epochs=5)</code>, use <code>model.fit(tf.expand_dims(X, axis=-1), y, epochs=5)</code>.</p>"},{"location":"01_neural_network_regression_in_tensorflow/#improving-a-model","title":"Improving a model\u00b6","text":"<p>How do you think you'd improve upon our current model?</p> <p>If you guessed by tweaking some of the things we did above, you'd be correct.</p> <p>To improve our model, we alter almost every part of the 3 steps we went through before.</p> <ol> <li>Creating a model - here you might want to add more layers, increase the number of hidden units (also called neurons) within each layer, change the activation functions of each layer.</li> <li>Compiling a model - you might want to choose optimization function or perhaps change the learning rate of the optimization function.</li> <li>Fitting a model - perhaps you could fit a model for more epochs (leave it training for longer) or on more data (give the model more examples to learn from).</li> </ol> <p> There are many different ways to potentially improve a neural network. Some of the most common include: increasing the number of layers (making the network deeper), increasing the number of hidden units (making the network wider) and changing the learning rate. Because these values are all human-changeable, they're referred to as hyperparameters) and the practice of trying to find the best hyperparameters is referred to as hyperparameter tuning.</p> <p>Woah. We just introduced a bunch of possible steps. The important thing to remember is how you alter each of these will depend on the problem you're working on.</p> <p>And the good thing is, over the next few problems, we'll get hands-on with all of them.</p> <p>For now, let's keep it simple, all we'll do is train our model for longer (everything else will stay the same).</p>"},{"location":"01_neural_network_regression_in_tensorflow/#evaluating-a-model","title":"Evaluating a model\u00b6","text":"<p>A typical workflow you'll go through when building neural networks is:</p> <pre><code>Build a model -&gt; evaluate it -&gt; build (tweak) a model -&gt; evaulate it -&gt; build (tweak) a model -&gt; evaluate it...\n</code></pre> <p>The tweaking comes from maybe not building a model from scratch but adjusting an existing one.</p>"},{"location":"01_neural_network_regression_in_tensorflow/#visualize-visualize-visualize","title":"Visualize, visualize, visualize\u00b6","text":"<p>When it comes to evaluation, you'll want to remember the words: \"visualize, visualize, visualize.\"</p> <p>This is because you're probably better looking at something (doing) than you are thinking about something.</p> <p>It's a good idea to visualize:</p> <ul> <li>The data - what data are you working with? What does it look like?</li> <li>The model itself - what does the architecture look like? What are the different shapes?</li> <li>The training of a model - how does a model perform while it learns?</li> <li>The predictions of a model - how do the predictions of a model line up against the ground truth (the original labels)?</li> </ul> <p>Let's start by visualizing the model.</p> <p>But first, we'll create a little bit of a bigger dataset and a new model we can use (it'll be the same as before, but the more practice the better).</p>"},{"location":"01_neural_network_regression_in_tensorflow/#split-data-into-trainingtest-set","title":"Split data into training/test set\u00b6","text":"<p>One of the other most common and important steps in a machine learning project is creating a training and test set (and when required, a validation set).</p> <p>Each set serves a specific purpose:</p> <ul> <li>Training set - the model learns from this data, which is typically 70-80% of the total data available (like the course materials you study during the semester).</li> <li>Validation set - the model gets tuned on this data, which is typically 10-15% of the total data available (like the practice exam you take before the final exam).</li> <li>Test set - the model gets evaluated on this data to test what it has learned, it's typically 10-15% of the total data available (like the final exam you take at the end of the semester).</li> </ul> <p>For now, we'll just use a training and test set, this means we'll have a dataset for our model to learn on as well as be evaluated on.</p> <p>We can create them by splitting our <code>X</code> and <code>y</code> arrays.</p> <p>\ud83d\udd11 Note: When dealing with real-world data, this step is typically done right at the start of a project (the test set should always be kept separate from all other data). We want our model to learn on training data and then evaluate it on test data to get an indication of how well it generalizes to unseen examples.</p>"},{"location":"01_neural_network_regression_in_tensorflow/#visualizing-the-data","title":"Visualizing the data\u00b6","text":"<p>Now we've got our training and test data, it's a good idea to visualize it.</p> <p>Let's plot it with some nice colours to differentiate what's what.</p>"},{"location":"01_neural_network_regression_in_tensorflow/#visualizing-the-model","title":"Visualizing the model\u00b6","text":"<p>After you've built a model, you might want to take a look at it (especially if you haven't built many before).</p> <p>You can take a look at the layers and shapes of your model by calling <code>summary()</code> on it.</p> <p>\ud83d\udd11 Note: Visualizing a model is particularly helpful when you run into input and output shape mismatches.</p>"},{"location":"01_neural_network_regression_in_tensorflow/#visualizing-the-predictions","title":"Visualizing the predictions\u00b6","text":"<p>Now we've got a trained model, let's visualize some predictions.</p> <p>To visualize predictions, it's always a good idea to plot them against the ground truth labels.</p> <p>Often you'll see this in the form of <code>y_test</code> vs. <code>y_pred</code> (ground truth vs. predictions).</p> <p>First, we'll make some predictions on the test data (<code>X_test</code>), remember the model has never seen the test data.</p>"},{"location":"01_neural_network_regression_in_tensorflow/#evaluating-predictions","title":"Evaluating predictions\u00b6","text":"<p>Alongisde visualizations, evaulation metrics are your alternative best option for evaluating your model.</p> <p>Depending on the problem you're working on, different models have different evaluation metrics.</p> <p>Two of the main metrics used for regression problems are:</p> <ul> <li>Mean absolute error (MAE) - the mean difference between each of the predictions.</li> <li>Mean squared error (MSE) - the squared mean difference between of the predictions (use if larger errors are more detrimental than smaller errors).</li> </ul> <p>The lower each of these values, the better.</p> <p>You can also use <code>model.evaluate()</code> which will return the loss of the model as well as any metrics setup during the compile step.</p>"},{"location":"01_neural_network_regression_in_tensorflow/#running-experiments-to-improve-a-model","title":"Running experiments to improve a model\u00b6","text":"<p>After seeing the evaluation metrics and the predictions your model makes, it's likely you'll want to improve it.</p> <p>Again, there are many different ways you can do this, but 3 of the main ones are:</p> <ol> <li>Get more data - get more examples for your model to train on (more opportunities to learn patterns).</li> <li>Make your model larger (use a more complex model) - this might come in the form of more layers or more hidden units in each layer.</li> <li>Train for longer - give your model more of a chance to find the patterns in the data.</li> </ol> <p>Since we created our dataset, we could easily make more data but this isn't always the case when you're working with real-world datasets.</p> <p>So let's take a look at how we can improve our model using 2 and 3.</p> <p>To do so, we'll build 3 models and compare their results:</p> <ol> <li><code>model_1</code> - same as original model, 1 layer, trained for 100 epochs.</li> <li><code>model_2</code> - 2 layers, trained for 100 epochs.</li> <li><code>model_3</code> - 2 layers, trained for 500 epochs.</li> </ol> <p>Build <code>model_1</code></p>"},{"location":"01_neural_network_regression_in_tensorflow/#comparing-results","title":"Comparing results\u00b6","text":"<p>Now we've got results for 3 similar but slightly different results, let's compare them.</p>"},{"location":"01_neural_network_regression_in_tensorflow/#tracking-your-experiments","title":"Tracking your experiments\u00b6","text":"<p>One really good habit to get into is tracking your modelling experiments to see which perform better than others.</p> <p>We've done a simple version of this above (keeping the results in different variables).</p> <p>\ud83d\udcd6 Resource: But as you build more models, you'll want to look into using tools such as:</p> <ul> <li>TensorBoard - a component of the TensorFlow library to help track modelling experiments (we'll see this later).</li> <li>Weights &amp; Biases - a tool for tracking all kinds of machine learning experiments (the good news for Weights &amp; Biases is it plugs into TensorBoard).</li> </ul>"},{"location":"01_neural_network_regression_in_tensorflow/#saving-a-model","title":"Saving a model\u00b6","text":"<p>Once you've trained a model and found one which performs to your liking, you'll probably want to save it for use elsewhere (like a web application or mobile device).</p> <p>You can save a TensorFlow/Keras model using <code>model.save()</code>.</p> <p>There are two ways to save a model in TensorFlow:</p> <ol> <li>The SavedModel format (default).</li> <li>The HDF5 format.</li> </ol> <p>The main difference between the two is the SavedModel is automatically able to save custom objects (such as special layers) without additional modifications when loading the model back in.</p> <p>Which one should you use?</p> <p>It depends on your situation but the SavedModel format will suffice most of the time.</p> <p>Both methods use the same method call.</p>"},{"location":"01_neural_network_regression_in_tensorflow/#loading-a-model","title":"Loading a model\u00b6","text":"<p>We can load a saved model using the <code>load_model()</code> method.</p> <p>Loading a model for the different formats (SavedModel and HDF5) is the same (as long as the pathnames to the particular formats are correct).</p>"},{"location":"01_neural_network_regression_in_tensorflow/#downloading-a-model-from-google-colab","title":"Downloading a model (from Google Colab)\u00b6","text":"<p>Say you wanted to get your model from Google Colab to your local machine, you can do one of the following things:</p> <ul> <li>Right click on the file in the files pane and click 'download'.</li> <li>Use the code below.</li> </ul>"},{"location":"01_neural_network_regression_in_tensorflow/#a-larger-example","title":"A larger example\u00b6","text":"<p>Alright, we've seen the fundamentals of building neural network regression models in TensorFlow.</p> <p>Let's step it up a notch and build a model for a more feature rich dataset.</p> <p>More specifically we're going to try predict the cost of medical insurance for individuals based on a number of different parameters such as, <code>age</code>, <code>sex</code>, <code>bmi</code>, <code>children</code>, <code>smoking_status</code> and <code>residential_region</code>.</p> <p>To do, we'll leverage the pubically available Medical Cost dataset available from Kaggle and hosted on GitHub.</p> <p>\ud83d\udd11 Note: When learning machine learning paradigms, you'll often go through a series of foundational techniques and then practice them by working with open-source datasets and examples. Just as we're doing now, learn foundations, put them to work with different problems. Every time you work on something new, it's a good idea to search for something like \"problem X example with Python/TensorFlow\" where you substitute X for your problem.</p>"},{"location":"01_neural_network_regression_in_tensorflow/#preprocessing-data-normalization-and-standardization","title":"Preprocessing data (normalization and standardization)\u00b6","text":"<p>A common practice when working with neural networks is to make sure all of the data you pass to them is in the range 0 to 1.</p> <p>This practice is called normalization (scaling all values from their original range to, e.g. between 0 and 100,000 to be between 0 and 1).</p> <p>There is another process call standardization which converts all of your data to unit variance and 0 mean.</p> <p>These two practices are often part of a preprocessing pipeline (a series of functions to prepare your data for use with neural networks).</p> <p>Knowing this, some of the major steps you'll take to preprocess your data for a neural network include:</p> <ul> <li>Turning all of your data to numbers (a neural network can't handle strings).</li> <li>Making sure your data is in the right shape (verifying input and output shapes).</li> <li>Feature scaling:<ul> <li>Normalizing data (making sure all values are between 0 and 1). This is done by subtracting the minimum value then dividing by the maximum value minus the minimum. This is also referred to as min-max scaling.</li> <li>Standardization (making sure all values have a mean of 0 and a variance of 1). This is done by subtracting the mean value from the target feature and then dividing it by the standard deviation.</li> <li>Which one should you use?<ul> <li>With neural networks you'll tend to favour normalization as they tend to prefer values between 0 and 1 (you'll see this espcially with image processing), however, you'll often find a neural network can perform pretty well with minimal feature scaling.</li> </ul> </li> </ul> </li> </ul> <p>\ud83d\udcd6 Resource: For more on preprocessing data, I'd recommend reading the following resources:</p> <ul> <li>Scikit-Learn's documentation on preprocessing data.</li> <li>Scale, Standardize or Normalize with Scikit-Learn by Jeff Hale.</li> </ul> <p>We've already turned our data into numbers using <code>get_dummies()</code>, let's see how we'd normalize it as well.</p>"},{"location":"01_neural_network_regression_in_tensorflow/#exercises","title":"\ud83d\udee0 Exercises\u00b6","text":"<p>We've a covered a whole lot pretty quickly.</p> <p>So now it's time to have a play around with a few things and start to build up your intuition.</p> <p>I emphasise the words play around because that's very important. Try a few things out, run the code and see what happens.</p> <ol> <li>Create your own regression dataset (or make the one we created in \"Create data to view and fit\" bigger) and build fit a model to it.</li> <li>Try building a neural network with 4 Dense layers and fitting it to your own regression dataset, how does it perform?</li> <li>Try and improve the results we got on the insurance dataset, some things you might want to try include:</li> </ol> <ul> <li>Building a larger model (how does one with 4 dense layers go?).</li> <li>Increasing the number of units in each layer.</li> <li>Lookup the documentation of Adam and find out what the first parameter is, what happens if you increase it by 10x?</li> <li>What happens if you train for longer (say 300 epochs instead of 200)?</li> </ul> <ol> <li>Import the Boston pricing dataset from TensorFlow <code>tf.keras.datasets</code> and model it.</li> </ol>"},{"location":"01_neural_network_regression_in_tensorflow/#extra-curriculum","title":"\ud83d\udcd6 Extra curriculum\u00b6","text":"<p>If you're looking for extra materials relating to this notebook, I'd check out the following:</p> <ul> <li>MIT introduction deep learning lecture 1 - gives a great overview of what's happening behind all of the code we're running.</li> <li>Reading: 1-hour of Chapter 1 of Neural Networks and Deep Learning by Michael Nielson - a great in-depth and hands-on example of the intuition behind neural networks.</li> </ul> <p>To practice your regression modelling with TensorFlow, I'd also encourage you to look through Lion Bridge's collection of datasets or Kaggle's datasets, find a regression dataset which sparks your interest and try to model.</p>"},{"location":"02_neural_network_classification_in_tensorflow/","title":"02. Neural Network Classification with TensorFlow","text":"In\u00a0[1]: Copied! <pre>import tensorflow as tf\nprint(tf.__version__)\n\nimport datetime\nprint(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\")\n</pre> import tensorflow as tf print(tf.__version__)  import datetime print(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\") <pre>2.13.0\nNotebook last run (end-to-end): 2023-10-12 04:07:12.774646\n</pre> In\u00a0[2]: Copied! <pre>from sklearn.datasets import make_circles\n\n# Make 1000 examples\nn_samples = 1000\n\n# Create circles\nX, y = make_circles(n_samples,\n                    noise=0.03,\n                    random_state=42)\n</pre> from sklearn.datasets import make_circles  # Make 1000 examples n_samples = 1000  # Create circles X, y = make_circles(n_samples,                     noise=0.03,                     random_state=42) <p>Wonderful, now we've created some data, let's look at the features (<code>X</code>) and labels (<code>y</code>).</p> In\u00a0[3]: Copied! <pre># Check out the features\nX\n</pre> # Check out the features X Out[3]: <pre>array([[ 0.75424625,  0.23148074],\n       [-0.75615888,  0.15325888],\n       [-0.81539193,  0.17328203],\n       ...,\n       [-0.13690036, -0.81001183],\n       [ 0.67036156, -0.76750154],\n       [ 0.28105665,  0.96382443]])</pre> In\u00a0[4]: Copied! <pre># See the first 10 labels\ny[:10]\n</pre> # See the first 10 labels y[:10] Out[4]: <pre>array([1, 1, 1, 1, 0, 1, 1, 1, 1, 0])</pre> <p>Okay, we've seen some of our data and labels, how about we move towards visualizing?</p> <p>\ud83d\udd11 Note: One important step of starting any kind of machine learning project is to become one with the data. And one of the best ways to do this is to visualize the data you're working with as much as possible. The data explorer's motto is \"visualize, visualize, visualize\".</p> <p>We'll start with a DataFrame.</p> In\u00a0[5]: Copied! <pre># Make dataframe of features and labels\nimport pandas as pd\ncircles = pd.DataFrame({\"X0\":X[:, 0], \"X1\":X[:, 1], \"label\":y})\ncircles.head()\n</pre> # Make dataframe of features and labels import pandas as pd circles = pd.DataFrame({\"X0\":X[:, 0], \"X1\":X[:, 1], \"label\":y}) circles.head() Out[5]: X0 X1 label 0 0.754246 0.231481 1 1 -0.756159 0.153259 1 2 -0.815392 0.173282 1 3 -0.393731 0.692883 1 4 0.442208 -0.896723 0 <p>What kind of labels are we dealing with?</p> In\u00a0[6]: Copied! <pre># Check out the different labels\ncircles.label.value_counts()\n</pre> # Check out the different labels circles.label.value_counts() Out[6]: <pre>1    500\n0    500\nName: label, dtype: int64</pre> <p>Alright, looks like we're dealing with a binary classification problem. It's binary because there are only two labels (0 or 1).</p> <p>If there were more label options (e.g. 0, 1, 2, 3 or 4), it would be called multiclass classification.</p> <p>Let's take our visualization a step further and plot our data.</p> In\u00a0[7]: Copied! <pre># Visualize with a plot\nimport matplotlib.pyplot as plt\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu);\n</pre> # Visualize with a plot import matplotlib.pyplot as plt plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu); <p>Nice! From the plot, can you guess what kind of model we might want to build?</p> <p>How about we try and build one to classify blue or red dots? As in, a model which is able to distinguish blue from red dots.</p> <p>\ud83d\udee0 Practice: Before pushing forward, you might want to spend 10 minutes playing around with the TensorFlow Playground. Try adjusting the different hyperparameters you see and click play to see a neural network train. I think you'll find the data very similar to what we've just created.</p> In\u00a0[8]: Copied! <pre># Check the shapes of our features and labels\nX.shape, y.shape\n</pre> # Check the shapes of our features and labels X.shape, y.shape Out[8]: <pre>((1000, 2), (1000,))</pre> <p>Hmm, where do these numbers come from?</p> In\u00a0[9]: Copied! <pre># Check how many samples we have\nlen(X), len(y)\n</pre> # Check how many samples we have len(X), len(y) Out[9]: <pre>(1000, 1000)</pre> <p>So we've got as many <code>X</code> values as we do <code>y</code> values, that makes sense.</p> <p>Let's check out one example of each.</p> In\u00a0[10]: Copied! <pre># View the first example of features and labels\nX[0], y[0]\n</pre> # View the first example of features and labels X[0], y[0] Out[10]: <pre>(array([0.75424625, 0.23148074]), 1)</pre> <p>Alright, so we've got two <code>X</code> features which lead to one <code>y</code> value.</p> <p>This means our neural network input shape will has to accept a tensor with at least one dimension being two and output a tensor with at least one value.</p> <p>\ud83e\udd14 Note: <code>y</code> having a shape of (1000,) can seem confusing. However, this is because all <code>y</code> values are actually scalars (single values) and therefore don't have a dimension. For now, think of your output shape as being at least the same value as one example of <code>y</code> (in our case, the output from our neural network has to be at least one value).</p> In\u00a0[11]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# 1. Create the model using the Sequential API\nmodel_1 = tf.keras.Sequential([\n  tf.keras.layers.Dense(1)\n])\n\n# 2. Compile the model\nmodel_1.compile(loss=tf.keras.losses.BinaryCrossentropy(), # binary since we are working with 2 clases (0 &amp; 1)\n                optimizer=tf.keras.optimizers.SGD(),\n                metrics=['accuracy'])\n\n# 3. Fit the model\nmodel_1.fit(X, y, epochs=5)\n</pre> # Set random seed tf.random.set_seed(42)  # 1. Create the model using the Sequential API model_1 = tf.keras.Sequential([   tf.keras.layers.Dense(1) ])  # 2. Compile the model model_1.compile(loss=tf.keras.losses.BinaryCrossentropy(), # binary since we are working with 2 clases (0 &amp; 1)                 optimizer=tf.keras.optimizers.SGD(),                 metrics=['accuracy'])  # 3. Fit the model model_1.fit(X, y, epochs=5) <pre>Epoch 1/5\n32/32 [==============================] - 5s 3ms/step - loss: 5.9177 - accuracy: 0.4800\nEpoch 2/5\n32/32 [==============================] - 0s 3ms/step - loss: 5.1146 - accuracy: 0.4620\nEpoch 3/5\n32/32 [==============================] - 0s 3ms/step - loss: 4.6022 - accuracy: 0.4720\nEpoch 4/5\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 5/5\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\n</pre> Out[11]: <pre>&lt;keras.src.callbacks.History at 0x7a25080c58d0&gt;</pre> <p>Looking at the accuracy metric, our model performs poorly (50% accuracy on a binary classification problem is the equivalent of guessing), but what if we trained it for longer?</p> In\u00a0[12]: Copied! <pre># Train our model for longer (more chances to look at the data)\nmodel_1.fit(X, y, epochs=200, verbose=0) # set verbose=0 to remove training updates\nmodel_1.evaluate(X, y)\n</pre> # Train our model for longer (more chances to look at the data) model_1.fit(X, y, epochs=200, verbose=0) # set verbose=0 to remove training updates model_1.evaluate(X, y) <pre>32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000\n</pre> Out[12]: <pre>[7.712474346160889, 0.5]</pre> <p>Even after 200 passes of the data, it's still performing as if it's guessing.</p> <p>What if we added an extra layer and trained for a little longer?</p> In\u00a0[13]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# 1. Create the model (same as model_1 but with an extra layer)\nmodel_2 = tf.keras.Sequential([\n  tf.keras.layers.Dense(1), # add an extra layer\n  tf.keras.layers.Dense(1)\n])\n\n# 2. Compile the model\nmodel_2.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n                optimizer=tf.keras.optimizers.SGD(),\n                metrics=['accuracy'])\n\n# 3. Fit the model\nmodel_2.fit(X, y, epochs=100, verbose=0) # set verbose=0 to make the output print less\n</pre> # Set random seed tf.random.set_seed(42)  # 1. Create the model (same as model_1 but with an extra layer) model_2 = tf.keras.Sequential([   tf.keras.layers.Dense(1), # add an extra layer   tf.keras.layers.Dense(1) ])  # 2. Compile the model model_2.compile(loss=tf.keras.losses.BinaryCrossentropy(),                 optimizer=tf.keras.optimizers.SGD(),                 metrics=['accuracy'])  # 3. Fit the model model_2.fit(X, y, epochs=100, verbose=0) # set verbose=0 to make the output print less Out[13]: <pre>&lt;keras.src.callbacks.History at 0x7a24b4b323e0&gt;</pre> In\u00a0[14]: Copied! <pre># Evaluate the model\nmodel_2.evaluate(X, y)\n</pre> # Evaluate the model model_2.evaluate(X, y) <pre>32/32 [==============================] - 0s 2ms/step - loss: 0.6934 - accuracy: 0.5000\n</pre> Out[14]: <pre>[0.6933949589729309, 0.5]</pre> <p>Still not even as good as guessing (~50% accuracy)... hmm...?</p> <p>Let's remind ourselves of a couple more ways we can use to improve our models.</p> In\u00a0[15]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# 1. Create the model (this time 3 layers)\nmodel_3 = tf.keras.Sequential([\n  # Before TensorFlow 2.7.0\n  # tf.keras.layers.Dense(100), # add 100 dense neurons\n\n  # With TensorFlow 2.7.0\n  # tf.keras.layers.Dense(100, input_shape=(None, 1)), # add 100 dense neurons\n\n  ## After TensorFlow 2.8.0 ##\n  tf.keras.layers.Dense(100), # add 100 dense neurons\n  tf.keras.layers.Dense(10), # add another layer with 10 neurons\n  tf.keras.layers.Dense(1)\n])\n\n# 2. Compile the model\nmodel_3.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n                optimizer=tf.keras.optimizers.Adam(), # use Adam instead of SGD\n                metrics=['accuracy'])\n\n# 3. Fit the model\nmodel_3.fit(X, y, epochs=100, verbose=1) # fit for 100 passes of the data\n</pre> # Set random seed tf.random.set_seed(42)  # 1. Create the model (this time 3 layers) model_3 = tf.keras.Sequential([   # Before TensorFlow 2.7.0   # tf.keras.layers.Dense(100), # add 100 dense neurons    # With TensorFlow 2.7.0   # tf.keras.layers.Dense(100, input_shape=(None, 1)), # add 100 dense neurons    ## After TensorFlow 2.8.0 ##   tf.keras.layers.Dense(100), # add 100 dense neurons   tf.keras.layers.Dense(10), # add another layer with 10 neurons   tf.keras.layers.Dense(1) ])  # 2. Compile the model model_3.compile(loss=tf.keras.losses.BinaryCrossentropy(),                 optimizer=tf.keras.optimizers.Adam(), # use Adam instead of SGD                 metrics=['accuracy'])  # 3. Fit the model model_3.fit(X, y, epochs=100, verbose=1) # fit for 100 passes of the data <pre>Epoch 1/100\n32/32 [==============================] - 2s 3ms/step - loss: 3.5433 - accuracy: 0.4520\nEpoch 2/100\n32/32 [==============================] - 0s 3ms/step - loss: 1.0533 - accuracy: 0.4910\nEpoch 3/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7210 - accuracy: 0.5000\nEpoch 4/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6998 - accuracy: 0.5000\nEpoch 5/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6952 - accuracy: 0.4830\nEpoch 6/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6941 - accuracy: 0.4910\nEpoch 7/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6938 - accuracy: 0.4940\nEpoch 8/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6945 - accuracy: 0.4990\nEpoch 9/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6943 - accuracy: 0.4880\nEpoch 10/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6945 - accuracy: 0.4550\nEpoch 11/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6954 - accuracy: 0.4490\nEpoch 12/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6946 - accuracy: 0.4860\nEpoch 13/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6958 - accuracy: 0.4920\nEpoch 14/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6949 - accuracy: 0.5150\nEpoch 15/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6961 - accuracy: 0.4720\nEpoch 16/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6943 - accuracy: 0.4880\nEpoch 17/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6983 - accuracy: 0.4930\nEpoch 18/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6945 - accuracy: 0.4730\nEpoch 19/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6954 - accuracy: 0.5030\nEpoch 20/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6957 - accuracy: 0.4600\nEpoch 21/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6956 - accuracy: 0.4790\nEpoch 22/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6948 - accuracy: 0.4440\nEpoch 23/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6943 - accuracy: 0.4850\nEpoch 24/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6962 - accuracy: 0.4690\nEpoch 25/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6973 - accuracy: 0.5070\nEpoch 26/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6978 - accuracy: 0.4870\nEpoch 27/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6965 - accuracy: 0.5010\nEpoch 28/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6947 - accuracy: 0.4690\nEpoch 29/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6974 - accuracy: 0.4860\nEpoch 30/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6997 - accuracy: 0.4870\nEpoch 31/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6947 - accuracy: 0.5060\nEpoch 32/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6965 - accuracy: 0.4710\nEpoch 33/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6955 - accuracy: 0.4590\nEpoch 34/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6972 - accuracy: 0.4770\nEpoch 35/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6961 - accuracy: 0.5020\nEpoch 36/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6946 - accuracy: 0.4680\nEpoch 37/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6952 - accuracy: 0.4980\nEpoch 38/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6976 - accuracy: 0.4930\nEpoch 39/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6950 - accuracy: 0.4750\nEpoch 40/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6966 - accuracy: 0.4970\nEpoch 41/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6978 - accuracy: 0.4840\nEpoch 42/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6994 - accuracy: 0.4770\nEpoch 43/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6954 - accuracy: 0.5060\nEpoch 44/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6982 - accuracy: 0.4900\nEpoch 45/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6960 - accuracy: 0.5040\nEpoch 46/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6948 - accuracy: 0.4810\nEpoch 47/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6974 - accuracy: 0.5120\nEpoch 48/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6967 - accuracy: 0.4930\nEpoch 49/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6975 - accuracy: 0.4820\nEpoch 50/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6970 - accuracy: 0.4640\nEpoch 51/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6977 - accuracy: 0.4810\nEpoch 52/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6963 - accuracy: 0.5080\nEpoch 53/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6969 - accuracy: 0.5070\nEpoch 54/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6997 - accuracy: 0.5120\nEpoch 55/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6952 - accuracy: 0.5180\nEpoch 56/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6965 - accuracy: 0.4890\nEpoch 57/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6980 - accuracy: 0.4730\nEpoch 58/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6965 - accuracy: 0.5080\nEpoch 59/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7003 - accuracy: 0.4970\nEpoch 60/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7009 - accuracy: 0.4930\nEpoch 61/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6997 - accuracy: 0.4710\nEpoch 62/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6965 - accuracy: 0.4950\nEpoch 63/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6948 - accuracy: 0.4840\nEpoch 64/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6961 - accuracy: 0.4920\nEpoch 65/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6993 - accuracy: 0.4830\nEpoch 66/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6966 - accuracy: 0.4980\nEpoch 67/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6977 - accuracy: 0.4490\nEpoch 68/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6958 - accuracy: 0.5060\nEpoch 69/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6949 - accuracy: 0.5280\nEpoch 70/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6987 - accuracy: 0.4720\nEpoch 71/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6976 - accuracy: 0.4720\nEpoch 72/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6965 - accuracy: 0.5010\nEpoch 73/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6960 - accuracy: 0.4890\nEpoch 74/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6958 - accuracy: 0.5050\nEpoch 75/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6960 - accuracy: 0.5070\nEpoch 76/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6980 - accuracy: 0.4810\nEpoch 77/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6972 - accuracy: 0.4990\nEpoch 78/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6972 - accuracy: 0.4710\nEpoch 79/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7007 - accuracy: 0.5130\nEpoch 80/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6978 - accuracy: 0.5000\nEpoch 81/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6975 - accuracy: 0.5020\nEpoch 82/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6966 - accuracy: 0.4880\nEpoch 83/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7003 - accuracy: 0.4510\nEpoch 84/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6964 - accuracy: 0.5010\nEpoch 85/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6968 - accuracy: 0.4660\nEpoch 86/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7005 - accuracy: 0.4940\nEpoch 87/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6976 - accuracy: 0.4540\nEpoch 88/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6981 - accuracy: 0.4570\nEpoch 89/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6982 - accuracy: 0.4740\nEpoch 90/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6978 - accuracy: 0.4560\nEpoch 91/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6980 - accuracy: 0.4880\nEpoch 92/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6984 - accuracy: 0.4730\nEpoch 93/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6982 - accuracy: 0.4710\nEpoch 94/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7001 - accuracy: 0.4790\nEpoch 95/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6972 - accuracy: 0.4560\nEpoch 96/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6979 - accuracy: 0.4860\nEpoch 97/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6972 - accuracy: 0.4580\nEpoch 98/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6987 - accuracy: 0.4800\nEpoch 99/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6975 - accuracy: 0.5080\nEpoch 100/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6967 - accuracy: 0.4810\n</pre> Out[15]: <pre>&lt;keras.src.callbacks.History at 0x7a249bd86620&gt;</pre> <p>Still!</p> <p>We've pulled out a few tricks but our model isn't even doing better than guessing.</p> <p>Let's make some visualizations to see what's happening.</p> <p>\ud83d\udd11 Note: Whenever your model is performing strangely or there's something going on with your data you're not quite sure of, remember these three words: visualize, visualize, visualize. Inspect your data, inspect your model, inpsect your model's predictions.</p> <p>To visualize our model's predictions we're going to create a function <code>plot_decision_boundary()</code> which:</p> <ul> <li>Takes in a trained model, features (<code>X</code>) and labels (<code>y</code>).</li> <li>Creates a meshgrid of the different <code>X</code> values.</li> <li>Makes predictions across the meshgrid.</li> <li>Plots the predictions as well as a line between the different zones (where each unique class falls).</li> </ul> <p>If this sounds confusing, let's see it in code and then see the output.</p> <p>\ud83d\udd11 Note: If you're ever unsure of what a function does, try unraveling it and writing it line by line for yourself to see what it does. Break it into small parts and see what each part outputs.</p> In\u00a0[16]: Copied! <pre>import numpy as np\n\ndef plot_decision_boundary(model, X, y):\n  \"\"\"\n  Plots the decision boundary created by a model predicting on X.\n  This function has been adapted from two phenomenal resources:\n   1. CS231n - https://cs231n.github.io/neural-networks-case-study/\n   2. Made with ML basics - https://github.com/GokuMohandas/MadeWithML/blob/main/notebooks/08_Neural_Networks.ipynb\n  \"\"\"\n  # Define the axis boundaries of the plot and create a meshgrid\n  x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n  y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n  xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n                       np.linspace(y_min, y_max, 100))\n\n  # Create X values (we're going to predict on all of these)\n  x_in = np.c_[xx.ravel(), yy.ravel()] # stack 2D arrays together: https://numpy.org/devdocs/reference/generated/numpy.c_.html\n\n  # Make predictions using the trained model\n  y_pred = model.predict(x_in)\n\n  # Check for multi-class\n  if model.output_shape[-1] &gt; 1: # checks the final dimension of the model's output shape, if this is &gt; (greater than) 1, it's multi-class\n    print(\"doing multiclass classification...\")\n    # We have to reshape our predictions to get them ready for plotting\n    y_pred = np.argmax(y_pred, axis=1).reshape(xx.shape)\n  else:\n    print(\"doing binary classifcation...\")\n    y_pred = np.round(np.max(y_pred, axis=1)).reshape(xx.shape)\n\n  # Plot decision boundary\n  plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n  plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n  plt.xlim(xx.min(), xx.max())\n  plt.ylim(yy.min(), yy.max())\n</pre> import numpy as np  def plot_decision_boundary(model, X, y):   \"\"\"   Plots the decision boundary created by a model predicting on X.   This function has been adapted from two phenomenal resources:    1. CS231n - https://cs231n.github.io/neural-networks-case-study/    2. Made with ML basics - https://github.com/GokuMohandas/MadeWithML/blob/main/notebooks/08_Neural_Networks.ipynb   \"\"\"   # Define the axis boundaries of the plot and create a meshgrid   x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1   y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1   xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),                        np.linspace(y_min, y_max, 100))    # Create X values (we're going to predict on all of these)   x_in = np.c_[xx.ravel(), yy.ravel()] # stack 2D arrays together: https://numpy.org/devdocs/reference/generated/numpy.c_.html    # Make predictions using the trained model   y_pred = model.predict(x_in)    # Check for multi-class   if model.output_shape[-1] &gt; 1: # checks the final dimension of the model's output shape, if this is &gt; (greater than) 1, it's multi-class     print(\"doing multiclass classification...\")     # We have to reshape our predictions to get them ready for plotting     y_pred = np.argmax(y_pred, axis=1).reshape(xx.shape)   else:     print(\"doing binary classifcation...\")     y_pred = np.round(np.max(y_pred, axis=1)).reshape(xx.shape)    # Plot decision boundary   plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)   plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)   plt.xlim(xx.min(), xx.max())   plt.ylim(yy.min(), yy.max()) <p>Now we've got a function to plot our model's decision boundary (the cut off point its making between red and blue dots), let's try it out.</p> In\u00a0[17]: Copied! <pre># Check out the predictions our model is making\nplot_decision_boundary(model_3, X, y)\n</pre> # Check out the predictions our model is making plot_decision_boundary(model_3, X, y) <pre>313/313 [==============================] - 0s 1ms/step\ndoing binary classifcation...\n</pre> <p>Looks like our model is trying to draw a straight line through the data.</p> <p>What's wrong with doing this?</p> <p>The main issue is our data isn't separable by a straight line.</p> <p>In a regression problem, our model might work. In fact, let's try it.</p> In\u00a0[18]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create some regression data\nX_regression = np.arange(0, 1000, 5)\ny_regression = np.arange(100, 1100, 5)\n\n# Split it into training and test sets\nX_reg_train = X_regression[:150]\nX_reg_test = X_regression[150:]\ny_reg_train = y_regression[:150]\ny_reg_test = y_regression[150:]\n\n# Fit our model to the data\n# Note: Before TensorFlow 2.7.0, this line would work\n# model_3.fit(X_reg_train, y_reg_train, epochs=100)\n\n# After TensorFlow 2.7.0, see here for more: https://github.com/mrdbourke/tensorflow-deep-learning/discussions/278\nmodel_3.fit(tf.expand_dims(X_reg_train, axis=-1),\n            y_reg_train,\n            epochs=100)\n</pre> # Set random seed tf.random.set_seed(42)  # Create some regression data X_regression = np.arange(0, 1000, 5) y_regression = np.arange(100, 1100, 5)  # Split it into training and test sets X_reg_train = X_regression[:150] X_reg_test = X_regression[150:] y_reg_train = y_regression[:150] y_reg_test = y_regression[150:]  # Fit our model to the data # Note: Before TensorFlow 2.7.0, this line would work # model_3.fit(X_reg_train, y_reg_train, epochs=100)  # After TensorFlow 2.7.0, see here for more: https://github.com/mrdbourke/tensorflow-deep-learning/discussions/278 model_3.fit(tf.expand_dims(X_reg_train, axis=-1),             y_reg_train,             epochs=100) <pre>Epoch 1/100\n</pre> <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-18-2f683d96fa34&gt; in &lt;cell line: 19&gt;()\n     17 \n     18 # After TensorFlow 2.7.0, see here for more: https://github.com/mrdbourke/tensorflow-deep-learning/discussions/278\n---&gt; 19 model_3.fit(tf.expand_dims(X_reg_train, axis=-1),\n     20             y_reg_train,\n     21             epochs=100)\n\n/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py in error_handler(*args, **kwargs)\n     68             # To get the full stack trace, call:\n     69             # `tf.debugging.disable_traceback_filtering()`\n---&gt; 70             raise e.with_traceback(filtered_tb) from None\n     71         finally:\n     72             del filtered_tb\n\n/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py in tf__train_function(iterator)\n     13                 try:\n     14                     do_return = True\n---&gt; 15                     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\n     16                 except:\n     17                     do_return = False\n\nValueError: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1080, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 280, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_2' (type Sequential).\n    \n    Input 0 of layer \"dense_3\" is incompatible with the layer: expected axis -1 of input shape to have value 2, but received input with shape (None, 1)\n    \n    Call arguments received by layer 'sequential_2' (type Sequential):\n      \u2022 inputs=tf.Tensor(shape=(None, 1), dtype=int64)\n      \u2022 training=True\n      \u2022 mask=None\n</pre> In\u00a0[19]: Copied! <pre>model_3.summary()\n</pre> model_3.summary() <pre>Model: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_3 (Dense)             (None, 100)               300       \n                                                                 \n dense_4 (Dense)             (None, 10)                1010      \n                                                                 \n dense_5 (Dense)             (None, 1)                 11        \n                                                                 \n=================================================================\nTotal params: 1321 (5.16 KB)\nTrainable params: 1321 (5.16 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n</pre> <p>Oh wait... we compiled our model for a binary classification problem.</p> <p>No trouble, we can recreate it for a regression problem.</p> In\u00a0[20]: Copied! <pre># Setup random seed\ntf.random.set_seed(42)\n\n# Recreate the model\nmodel_3 = tf.keras.Sequential([\n  tf.keras.layers.Dense(100),\n  tf.keras.layers.Dense(10),\n  tf.keras.layers.Dense(1)\n])\n\n# Change the loss and metrics of our compiled model\nmodel_3.compile(loss=tf.keras.losses.mae, # change the loss function to be regression-specific\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=['mae']) # change the metric to be regression-specific\n\n# Fit the recompiled model\nmodel_3.fit(tf.expand_dims(X_reg_train, axis=-1),\n            y_reg_train,\n            epochs=100)\n</pre> # Setup random seed tf.random.set_seed(42)  # Recreate the model model_3 = tf.keras.Sequential([   tf.keras.layers.Dense(100),   tf.keras.layers.Dense(10),   tf.keras.layers.Dense(1) ])  # Change the loss and metrics of our compiled model model_3.compile(loss=tf.keras.losses.mae, # change the loss function to be regression-specific                 optimizer=tf.keras.optimizers.Adam(),                 metrics=['mae']) # change the metric to be regression-specific  # Fit the recompiled model model_3.fit(tf.expand_dims(X_reg_train, axis=-1),             y_reg_train,             epochs=100) <pre>Epoch 1/100\n5/5 [==============================] - 1s 5ms/step - loss: 447.6102 - mae: 447.6102\nEpoch 2/100\n5/5 [==============================] - 0s 4ms/step - loss: 313.3108 - mae: 313.3108\nEpoch 3/100\n5/5 [==============================] - 0s 4ms/step - loss: 182.3118 - mae: 182.3118\nEpoch 4/100\n5/5 [==============================] - 0s 4ms/step - loss: 58.6317 - mae: 58.6317\nEpoch 5/100\n5/5 [==============================] - 0s 4ms/step - loss: 83.0025 - mae: 83.0025\nEpoch 6/100\n5/5 [==============================] - 0s 4ms/step - loss: 86.4671 - mae: 86.4671\nEpoch 7/100\n5/5 [==============================] - 0s 3ms/step - loss: 49.6151 - mae: 49.6151\nEpoch 8/100\n5/5 [==============================] - 0s 4ms/step - loss: 57.7703 - mae: 57.7703\nEpoch 9/100\n5/5 [==============================] - 0s 3ms/step - loss: 50.1641 - mae: 50.1641\nEpoch 10/100\n5/5 [==============================] - 0s 4ms/step - loss: 47.7698 - mae: 47.7698\nEpoch 11/100\n5/5 [==============================] - 0s 4ms/step - loss: 48.6194 - mae: 48.6194\nEpoch 12/100\n5/5 [==============================] - 0s 4ms/step - loss: 43.2044 - mae: 43.2044\nEpoch 13/100\n5/5 [==============================] - 0s 4ms/step - loss: 42.6293 - mae: 42.6293\nEpoch 14/100\n5/5 [==============================] - 0s 4ms/step - loss: 42.4557 - mae: 42.4557\nEpoch 15/100\n5/5 [==============================] - 0s 4ms/step - loss: 41.9446 - mae: 41.9446\nEpoch 16/100\n5/5 [==============================] - 0s 4ms/step - loss: 41.7290 - mae: 41.7290\nEpoch 17/100\n5/5 [==============================] - 0s 4ms/step - loss: 41.5669 - mae: 41.5669\nEpoch 18/100\n5/5 [==============================] - 0s 4ms/step - loss: 41.2955 - mae: 41.2955\nEpoch 19/100\n5/5 [==============================] - 0s 4ms/step - loss: 41.1274 - mae: 41.1274\nEpoch 20/100\n5/5 [==============================] - 0s 4ms/step - loss: 41.1121 - mae: 41.1121\nEpoch 21/100\n5/5 [==============================] - 0s 4ms/step - loss: 41.1519 - mae: 41.1519\nEpoch 22/100\n5/5 [==============================] - 0s 4ms/step - loss: 41.0565 - mae: 41.0565\nEpoch 23/100\n5/5 [==============================] - 0s 4ms/step - loss: 41.1133 - mae: 41.1133\nEpoch 24/100\n5/5 [==============================] - 0s 60ms/step - loss: 41.0074 - mae: 41.0074\nEpoch 25/100\n5/5 [==============================] - 0s 4ms/step - loss: 40.9870 - mae: 40.9870\nEpoch 26/100\n5/5 [==============================] - 0s 4ms/step - loss: 41.0249 - mae: 41.0249\nEpoch 27/100\n5/5 [==============================] - 0s 3ms/step - loss: 40.8403 - mae: 40.8403\nEpoch 28/100\n5/5 [==============================] - 0s 4ms/step - loss: 40.9965 - mae: 40.9965\nEpoch 29/100\n5/5 [==============================] - 0s 4ms/step - loss: 41.0225 - mae: 41.0225\nEpoch 30/100\n5/5 [==============================] - 0s 4ms/step - loss: 40.8058 - mae: 40.8058\nEpoch 31/100\n5/5 [==============================] - 0s 4ms/step - loss: 41.3589 - mae: 41.3589\nEpoch 32/100\n5/5 [==============================] - 0s 4ms/step - loss: 41.0084 - mae: 41.0084\nEpoch 33/100\n5/5 [==============================] - 0s 4ms/step - loss: 41.0701 - mae: 41.0701\nEpoch 34/100\n5/5 [==============================] - 0s 4ms/step - loss: 41.2035 - mae: 41.2035\nEpoch 35/100\n5/5 [==============================] - 0s 3ms/step - loss: 40.5885 - mae: 40.5885\nEpoch 36/100\n5/5 [==============================] - 0s 4ms/step - loss: 41.0615 - mae: 41.0615\nEpoch 37/100\n5/5 [==============================] - 0s 4ms/step - loss: 40.6438 - mae: 40.6438\nEpoch 38/100\n5/5 [==============================] - 0s 4ms/step - loss: 40.3412 - mae: 40.3412\nEpoch 39/100\n5/5 [==============================] - 0s 4ms/step - loss: 40.6498 - mae: 40.6498\nEpoch 40/100\n5/5 [==============================] - 0s 4ms/step - loss: 40.4421 - mae: 40.4421\nEpoch 41/100\n5/5 [==============================] - 0s 4ms/step - loss: 40.3558 - mae: 40.3558\nEpoch 42/100\n5/5 [==============================] - 0s 4ms/step - loss: 40.3041 - mae: 40.3041\nEpoch 43/100\n5/5 [==============================] - 0s 4ms/step - loss: 40.5277 - mae: 40.5277\nEpoch 44/100\n5/5 [==============================] - 0s 4ms/step - loss: 40.1808 - mae: 40.1808\nEpoch 45/100\n5/5 [==============================] - 0s 4ms/step - loss: 40.6292 - mae: 40.6292\nEpoch 46/100\n5/5 [==============================] - 0s 3ms/step - loss: 40.4382 - mae: 40.4382\nEpoch 47/100\n5/5 [==============================] - 0s 4ms/step - loss: 40.1801 - mae: 40.1801\nEpoch 48/100\n5/5 [==============================] - 0s 4ms/step - loss: 40.2386 - mae: 40.2386\nEpoch 49/100\n5/5 [==============================] - 0s 3ms/step - loss: 40.7914 - mae: 40.7914\nEpoch 50/100\n5/5 [==============================] - 0s 3ms/step - loss: 40.1259 - mae: 40.1259\nEpoch 51/100\n5/5 [==============================] - 0s 4ms/step - loss: 40.4617 - mae: 40.4617\nEpoch 52/100\n5/5 [==============================] - 0s 4ms/step - loss: 40.8686 - mae: 40.8686\nEpoch 53/100\n5/5 [==============================] - 0s 4ms/step - loss: 41.0441 - mae: 41.0441\nEpoch 54/100\n5/5 [==============================] - 0s 4ms/step - loss: 41.1022 - mae: 41.1022\nEpoch 55/100\n5/5 [==============================] - 0s 4ms/step - loss: 42.1498 - mae: 42.1498\nEpoch 56/100\n5/5 [==============================] - 0s 4ms/step - loss: 42.3732 - mae: 42.3732\nEpoch 57/100\n5/5 [==============================] - 0s 4ms/step - loss: 40.9868 - mae: 40.9868\nEpoch 58/100\n5/5 [==============================] - 0s 4ms/step - loss: 40.4022 - mae: 40.4022\nEpoch 59/100\n5/5 [==============================] - 0s 4ms/step - loss: 41.1762 - mae: 41.1762\nEpoch 60/100\n5/5 [==============================] - 0s 4ms/step - loss: 40.0280 - mae: 40.0280\nEpoch 61/100\n5/5 [==============================] - 0s 4ms/step - loss: 39.4377 - mae: 39.4377\nEpoch 62/100\n5/5 [==============================] - 0s 4ms/step - loss: 40.2227 - mae: 40.2227\nEpoch 63/100\n5/5 [==============================] - 0s 4ms/step - loss: 39.7379 - mae: 39.7379\nEpoch 64/100\n5/5 [==============================] - 0s 4ms/step - loss: 39.4851 - mae: 39.4851\nEpoch 65/100\n5/5 [==============================] - 0s 4ms/step - loss: 39.8735 - mae: 39.8735\nEpoch 66/100\n5/5 [==============================] - 0s 4ms/step - loss: 39.5498 - mae: 39.5498\nEpoch 67/100\n5/5 [==============================] - 0s 3ms/step - loss: 39.5944 - mae: 39.5944\nEpoch 68/100\n5/5 [==============================] - 0s 3ms/step - loss: 39.5087 - mae: 39.5087\nEpoch 69/100\n5/5 [==============================] - 0s 3ms/step - loss: 39.3758 - mae: 39.3758\nEpoch 70/100\n5/5 [==============================] - 0s 4ms/step - loss: 39.8543 - mae: 39.8543\nEpoch 71/100\n5/5 [==============================] - 0s 4ms/step - loss: 41.2924 - mae: 41.2924\nEpoch 72/100\n5/5 [==============================] - 0s 4ms/step - loss: 39.0309 - mae: 39.0309\nEpoch 73/100\n5/5 [==============================] - 0s 3ms/step - loss: 39.7582 - mae: 39.7582\nEpoch 74/100\n5/5 [==============================] - 0s 4ms/step - loss: 39.1621 - mae: 39.1621\nEpoch 75/100\n5/5 [==============================] - 0s 4ms/step - loss: 39.9158 - mae: 39.9158\nEpoch 76/100\n5/5 [==============================] - 0s 4ms/step - loss: 40.2419 - mae: 40.2419\nEpoch 77/100\n5/5 [==============================] - 0s 4ms/step - loss: 38.9030 - mae: 38.9030\nEpoch 78/100\n5/5 [==============================] - 0s 3ms/step - loss: 39.5400 - mae: 39.5400\nEpoch 79/100\n5/5 [==============================] - 0s 3ms/step - loss: 39.2044 - mae: 39.2044\nEpoch 80/100\n5/5 [==============================] - 0s 3ms/step - loss: 38.7893 - mae: 38.7893\nEpoch 81/100\n5/5 [==============================] - 0s 3ms/step - loss: 38.8879 - mae: 38.8879\nEpoch 82/100\n5/5 [==============================] - 0s 4ms/step - loss: 38.9441 - mae: 38.9441\nEpoch 83/100\n5/5 [==============================] - 0s 4ms/step - loss: 38.6721 - mae: 38.6721\nEpoch 84/100\n5/5 [==============================] - 0s 4ms/step - loss: 38.7601 - mae: 38.7601\nEpoch 85/100\n5/5 [==============================] - 0s 4ms/step - loss: 39.0045 - mae: 39.0045\nEpoch 86/100\n5/5 [==============================] - 0s 4ms/step - loss: 38.9378 - mae: 38.9378\nEpoch 87/100\n5/5 [==============================] - 0s 3ms/step - loss: 38.3988 - mae: 38.3988\nEpoch 88/100\n5/5 [==============================] - 0s 3ms/step - loss: 38.5840 - mae: 38.5840\nEpoch 89/100\n5/5 [==============================] - 0s 3ms/step - loss: 38.4868 - mae: 38.4868\nEpoch 90/100\n5/5 [==============================] - 0s 4ms/step - loss: 38.3730 - mae: 38.3730\nEpoch 91/100\n5/5 [==============================] - 0s 3ms/step - loss: 38.2209 - mae: 38.2209\nEpoch 92/100\n5/5 [==============================] - 0s 4ms/step - loss: 38.3540 - mae: 38.3540\nEpoch 93/100\n5/5 [==============================] - 0s 4ms/step - loss: 38.6931 - mae: 38.6931\nEpoch 94/100\n5/5 [==============================] - 0s 4ms/step - loss: 37.9931 - mae: 37.9931\nEpoch 95/100\n5/5 [==============================] - 0s 4ms/step - loss: 38.0585 - mae: 38.0585\nEpoch 96/100\n5/5 [==============================] - 0s 4ms/step - loss: 38.4031 - mae: 38.4031\nEpoch 97/100\n5/5 [==============================] - 0s 4ms/step - loss: 38.0610 - mae: 38.0610\nEpoch 98/100\n5/5 [==============================] - 0s 4ms/step - loss: 38.3810 - mae: 38.3810\nEpoch 99/100\n5/5 [==============================] - 0s 4ms/step - loss: 38.4900 - mae: 38.4900\nEpoch 100/100\n5/5 [==============================] - 0s 4ms/step - loss: 37.9673 - mae: 37.9673\n</pre> Out[20]: <pre>&lt;keras.src.callbacks.History at 0x7a24c5b179a0&gt;</pre> <p>Okay, it seems like our model is learning something (the <code>mae</code> value trends down with each epoch), let's plot its predictions.</p> In\u00a0[21]: Copied! <pre># Make predictions with our trained model\ny_reg_preds = model_3.predict(X_reg_test)\n\n# Plot the model's predictions against our regression data\nplt.figure(figsize=(10, 7))\nplt.scatter(X_reg_train, y_reg_train, c='b', label='Training data')\nplt.scatter(X_reg_test, y_reg_test, c='g', label='Testing data')\nplt.scatter(X_reg_test, y_reg_preds.squeeze(), c='r', label='Predictions')\nplt.legend();\n</pre> # Make predictions with our trained model y_reg_preds = model_3.predict(X_reg_test)  # Plot the model's predictions against our regression data plt.figure(figsize=(10, 7)) plt.scatter(X_reg_train, y_reg_train, c='b', label='Training data') plt.scatter(X_reg_test, y_reg_test, c='g', label='Testing data') plt.scatter(X_reg_test, y_reg_preds.squeeze(), c='r', label='Predictions') plt.legend(); <pre>2/2 [==============================] - 0s 4ms/step\n</pre> <p>Okay, the predictions aren't perfect (if the predictions were perfect, the red would line up with the green), but they look better than complete guessing.</p> <p>So this means our model must be learning something...</p> <p>There must be something we're missing out on for our classification problem.</p> In\u00a0[22]: Copied! <pre># Set the random seed\ntf.random.set_seed(42)\n\n# Create the model\nmodel_4 = tf.keras.Sequential([\n  tf.keras.layers.Dense(1, activation=tf.keras.activations.linear), # 1 hidden layer with linear activation\n  tf.keras.layers.Dense(1) # output layer\n])\n\n# Compile the model\nmodel_4.compile(loss=tf.keras.losses.binary_crossentropy,\n                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), # note: \"lr\" used to be what was used, now \"learning_rate\" is favoured\n                metrics=[\"accuracy\"])\n\n# Fit the model\nhistory = model_4.fit(X, y, epochs=100)\n</pre> # Set the random seed tf.random.set_seed(42)  # Create the model model_4 = tf.keras.Sequential([   tf.keras.layers.Dense(1, activation=tf.keras.activations.linear), # 1 hidden layer with linear activation   tf.keras.layers.Dense(1) # output layer ])  # Compile the model model_4.compile(loss=tf.keras.losses.binary_crossentropy,                 optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), # note: \"lr\" used to be what was used, now \"learning_rate\" is favoured                 metrics=[\"accuracy\"])  # Fit the model history = model_4.fit(X, y, epochs=100) <pre>Epoch 1/100\n32/32 [==============================] - 1s 3ms/step - loss: 4.2584 - accuracy: 0.5000\nEpoch 2/100\n32/32 [==============================] - 0s 3ms/step - loss: 4.0332 - accuracy: 0.5000\nEpoch 3/100\n32/32 [==============================] - 0s 3ms/step - loss: 3.8635 - accuracy: 0.4970\nEpoch 4/100\n32/32 [==============================] - 0s 3ms/step - loss: 3.6277 - accuracy: 0.4670\nEpoch 5/100\n32/32 [==============================] - 0s 3ms/step - loss: 3.3772 - accuracy: 0.4490\nEpoch 6/100\n32/32 [==============================] - 0s 3ms/step - loss: 3.0635 - accuracy: 0.4460\nEpoch 7/100\n32/32 [==============================] - 0s 3ms/step - loss: 2.7472 - accuracy: 0.4450\nEpoch 8/100\n32/32 [==============================] - 0s 3ms/step - loss: 2.1925 - accuracy: 0.4470\nEpoch 9/100\n32/32 [==============================] - 0s 3ms/step - loss: 1.1191 - accuracy: 0.4790\nEpoch 10/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.9511 - accuracy: 0.4900\nEpoch 11/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.9207 - accuracy: 0.4880\nEpoch 12/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.8991 - accuracy: 0.4850\nEpoch 13/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.8814 - accuracy: 0.4770\nEpoch 14/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.8662 - accuracy: 0.4720\nEpoch 15/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.8530 - accuracy: 0.4620\nEpoch 16/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.8421 - accuracy: 0.4510\nEpoch 17/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.8320 - accuracy: 0.4470\nEpoch 18/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.8227 - accuracy: 0.4420\nEpoch 19/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.8146 - accuracy: 0.4330\nEpoch 20/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.8070 - accuracy: 0.4290\nEpoch 21/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.8001 - accuracy: 0.4210\nEpoch 22/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7937 - accuracy: 0.4140\nEpoch 23/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7877 - accuracy: 0.4090\nEpoch 24/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7823 - accuracy: 0.4090\nEpoch 25/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7771 - accuracy: 0.4100\nEpoch 26/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7722 - accuracy: 0.4140\nEpoch 27/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7677 - accuracy: 0.4220\nEpoch 28/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7633 - accuracy: 0.4320\nEpoch 29/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7594 - accuracy: 0.4350\nEpoch 30/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7557 - accuracy: 0.4460\nEpoch 31/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7522 - accuracy: 0.4480\nEpoch 32/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7489 - accuracy: 0.4470\nEpoch 33/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7460 - accuracy: 0.4540\nEpoch 34/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7432 - accuracy: 0.4520\nEpoch 35/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7406 - accuracy: 0.4590\nEpoch 36/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7381 - accuracy: 0.4620\nEpoch 37/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7357 - accuracy: 0.4640\nEpoch 38/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7335 - accuracy: 0.4640\nEpoch 39/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7314 - accuracy: 0.4660\nEpoch 40/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7295 - accuracy: 0.4670\nEpoch 41/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7276 - accuracy: 0.4720\nEpoch 42/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7259 - accuracy: 0.4740\nEpoch 43/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7241 - accuracy: 0.4780\nEpoch 44/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7227 - accuracy: 0.4770\nEpoch 45/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7212 - accuracy: 0.4770\nEpoch 46/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7198 - accuracy: 0.4760\nEpoch 47/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7186 - accuracy: 0.4760\nEpoch 48/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7173 - accuracy: 0.4780\nEpoch 49/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7161 - accuracy: 0.4810\nEpoch 50/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7150 - accuracy: 0.4780\nEpoch 51/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7139 - accuracy: 0.4800\nEpoch 52/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7129 - accuracy: 0.4820\nEpoch 53/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7119 - accuracy: 0.4850\nEpoch 54/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7110 - accuracy: 0.4850\nEpoch 55/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7101 - accuracy: 0.4880\nEpoch 56/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7092 - accuracy: 0.4870\nEpoch 57/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7085 - accuracy: 0.4860\nEpoch 58/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7078 - accuracy: 0.4910\nEpoch 59/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7072 - accuracy: 0.4870\nEpoch 60/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7064 - accuracy: 0.4900\nEpoch 61/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7058 - accuracy: 0.4890\nEpoch 62/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7053 - accuracy: 0.4900\nEpoch 63/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7046 - accuracy: 0.4900\nEpoch 64/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7041 - accuracy: 0.4900\nEpoch 65/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7036 - accuracy: 0.4910\nEpoch 66/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7031 - accuracy: 0.4870\nEpoch 67/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7026 - accuracy: 0.4880\nEpoch 68/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7022 - accuracy: 0.4880\nEpoch 69/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7017 - accuracy: 0.4860\nEpoch 70/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7013 - accuracy: 0.4860\nEpoch 71/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7009 - accuracy: 0.4870\nEpoch 72/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7005 - accuracy: 0.4900\nEpoch 73/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7002 - accuracy: 0.4890\nEpoch 74/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6999 - accuracy: 0.4890\nEpoch 75/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6996 - accuracy: 0.4900\nEpoch 76/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6993 - accuracy: 0.4900\nEpoch 77/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6990 - accuracy: 0.4890\nEpoch 78/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6987 - accuracy: 0.4890\nEpoch 79/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6984 - accuracy: 0.4890\nEpoch 80/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6981 - accuracy: 0.4870\nEpoch 81/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6978 - accuracy: 0.4850\nEpoch 82/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6976 - accuracy: 0.4850\nEpoch 83/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6974 - accuracy: 0.4860\nEpoch 84/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6971 - accuracy: 0.4870\nEpoch 85/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6969 - accuracy: 0.4880\nEpoch 86/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6968 - accuracy: 0.4880\nEpoch 87/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6965 - accuracy: 0.4900\nEpoch 88/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6964 - accuracy: 0.4860\nEpoch 89/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6963 - accuracy: 0.4840\nEpoch 90/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6962 - accuracy: 0.4900\nEpoch 91/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6959 - accuracy: 0.4890\nEpoch 92/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6958 - accuracy: 0.4880\nEpoch 93/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6957 - accuracy: 0.4910\nEpoch 94/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6956 - accuracy: 0.4860\nEpoch 95/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6954 - accuracy: 0.4880\nEpoch 96/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6953 - accuracy: 0.4850\nEpoch 97/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6952 - accuracy: 0.4880\nEpoch 98/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6951 - accuracy: 0.4880\nEpoch 99/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6950 - accuracy: 0.4850\nEpoch 100/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6948 - accuracy: 0.4900\n</pre> <p>Okay, our model performs a little worse than guessing.</p> <p>Let's remind ourselves what our data looks like.</p> In\u00a0[23]: Copied! <pre># Check out our data\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu);\n</pre> # Check out our data plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu); <p>And let's see how our model is making predictions on it.</p> In\u00a0[24]: Copied! <pre># Check the deicison boundary (blue is blue class, yellow is the crossover, red is red class)\nplot_decision_boundary(model_4, X, y)\n</pre> # Check the deicison boundary (blue is blue class, yellow is the crossover, red is red class) plot_decision_boundary(model_4, X, y) <pre>313/313 [==============================] - 0s 1ms/step\ndoing binary classifcation...\n</pre> <p>Well, it looks like we're getting a straight (linear) line prediction again.</p> <p>But our data is non-linear (not a straight line)...</p> <p>What we're going to have to do is add some non-linearity to our model.</p> <p>To do so, we'll use the <code>activation</code> parameter in on of our layers.</p> In\u00a0[25]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create a model with a non-linear activation\nmodel_5 = tf.keras.Sequential([\n  tf.keras.layers.Dense(1, activation=tf.keras.activations.relu), # can also do activation='relu'\n  tf.keras.layers.Dense(1) # output layer\n])\n\n# Compile the model\nmodel_5.compile(loss=tf.keras.losses.binary_crossentropy,\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=[\"accuracy\"])\n\n# Fit the model\nhistory = model_5.fit(X, y, epochs=100)\n</pre> # Set random seed tf.random.set_seed(42)  # Create a model with a non-linear activation model_5 = tf.keras.Sequential([   tf.keras.layers.Dense(1, activation=tf.keras.activations.relu), # can also do activation='relu'   tf.keras.layers.Dense(1) # output layer ])  # Compile the model model_5.compile(loss=tf.keras.losses.binary_crossentropy,               optimizer=tf.keras.optimizers.Adam(),               metrics=[\"accuracy\"])  # Fit the model history = model_5.fit(X, y, epochs=100) <pre>Epoch 1/100\n32/32 [==============================] - 1s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 2/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 3/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 4/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 5/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 6/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 7/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 8/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 9/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 10/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 11/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 12/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 13/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 14/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 15/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 16/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 17/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 18/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 19/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 20/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 21/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 22/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 23/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 24/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 25/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 26/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 27/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 28/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 29/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 30/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 31/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 32/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 33/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 34/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 35/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 36/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 37/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 38/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 39/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 40/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 41/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 42/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 43/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 44/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 45/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 46/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 47/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 48/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 49/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 50/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 51/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 52/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 53/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 54/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 55/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 56/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 57/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 58/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 59/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 60/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 61/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 62/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 63/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 64/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 65/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 66/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 67/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 68/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 69/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 70/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 71/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 72/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 73/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 74/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 75/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 76/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 77/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 78/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 79/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 80/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 81/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 82/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 83/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 84/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 85/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 86/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 87/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 88/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 89/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 90/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 91/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 92/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 93/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 94/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 95/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 96/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 97/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 98/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 99/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\nEpoch 100/100\n32/32 [==============================] - 0s 3ms/step - loss: 7.7125 - accuracy: 0.5000\n</pre> <p>Hmm... still not learning...</p> <p>What we if increased the number of neurons and layers?</p> <p>Say, 2 hidden layers, with ReLU, pronounced \"rel-u\", (short for rectified linear unit), activation on the first one, and 4 neurons each?</p> <p>To see this network in action, check out the TensorFlow Playground demo.</p> <p> The neural network we're going to recreate with TensorFlow code. See it live at TensorFlow Playground.</p> <p>Let's try.</p> <p>Note: in the course, Daniel used <code>lr</code> instead of <code>learning_rate</code>. But for the update, we had changed to <code>learning_rate</code> instead of <code>lr</code>.</p> In\u00a0[26]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create a model\nmodel_6 = tf.keras.Sequential([\n  tf.keras.layers.Dense(4, activation=tf.keras.activations.relu), # hidden layer 1, 4 neurons, ReLU activation\n  tf.keras.layers.Dense(4, activation=tf.keras.activations.relu), # hidden layer 2, 4 neurons, ReLU activation\n  tf.keras.layers.Dense(1) # ouput layer\n])\n\n# Compile the model\nmodel_6.compile(loss=tf.keras.losses.binary_crossentropy,\n                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), # Adam's default learning rate is 0.001\n                metrics=['accuracy'])\n\n# Fit the model\nhistory = model_6.fit(X, y, epochs=100)\n</pre> # Set random seed tf.random.set_seed(42)  # Create a model model_6 = tf.keras.Sequential([   tf.keras.layers.Dense(4, activation=tf.keras.activations.relu), # hidden layer 1, 4 neurons, ReLU activation   tf.keras.layers.Dense(4, activation=tf.keras.activations.relu), # hidden layer 2, 4 neurons, ReLU activation   tf.keras.layers.Dense(1) # ouput layer ])  # Compile the model model_6.compile(loss=tf.keras.losses.binary_crossentropy,                 optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), # Adam's default learning rate is 0.001                 metrics=['accuracy'])  # Fit the model history = model_6.fit(X, y, epochs=100) <pre>Epoch 1/100\n32/32 [==============================] - 2s 3ms/step - loss: 4.3069 - accuracy: 0.5000\nEpoch 2/100\n32/32 [==============================] - 0s 3ms/step - loss: 4.0916 - accuracy: 0.5000\nEpoch 3/100\n32/32 [==============================] - 0s 3ms/step - loss: 3.9820 - accuracy: 0.4520\nEpoch 4/100\n32/32 [==============================] - 0s 3ms/step - loss: 3.8302 - accuracy: 0.4150\nEpoch 5/100\n32/32 [==============================] - 0s 3ms/step - loss: 3.7048 - accuracy: 0.4500\nEpoch 6/100\n32/32 [==============================] - 0s 3ms/step - loss: 3.5944 - accuracy: 0.4650\nEpoch 7/100\n32/32 [==============================] - 0s 3ms/step - loss: 3.1921 - accuracy: 0.4650\nEpoch 8/100\n32/32 [==============================] - 0s 3ms/step - loss: 2.6196 - accuracy: 0.4680\nEpoch 9/100\n32/32 [==============================] - 0s 3ms/step - loss: 1.3332 - accuracy: 0.4740\nEpoch 10/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.9951 - accuracy: 0.4750\nEpoch 11/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.9489 - accuracy: 0.4750\nEpoch 12/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.9082 - accuracy: 0.4730\nEpoch 13/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.8345 - accuracy: 0.4720\nEpoch 14/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7435 - accuracy: 0.4690\nEpoch 15/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7288 - accuracy: 0.4550\nEpoch 16/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7245 - accuracy: 0.4620\nEpoch 17/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7213 - accuracy: 0.4630\nEpoch 18/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7186 - accuracy: 0.4670\nEpoch 19/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7166 - accuracy: 0.4600\nEpoch 20/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7143 - accuracy: 0.4560\nEpoch 21/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7127 - accuracy: 0.4600\nEpoch 22/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7107 - accuracy: 0.4580\nEpoch 23/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7090 - accuracy: 0.4590\nEpoch 24/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7075 - accuracy: 0.4640\nEpoch 25/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7061 - accuracy: 0.4690\nEpoch 26/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7046 - accuracy: 0.4640\nEpoch 27/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7033 - accuracy: 0.4720\nEpoch 28/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7012 - accuracy: 0.4650\nEpoch 29/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.7002 - accuracy: 0.4660\nEpoch 30/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6987 - accuracy: 0.4650\nEpoch 31/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6973 - accuracy: 0.4660\nEpoch 32/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6961 - accuracy: 0.4710\nEpoch 33/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6944 - accuracy: 0.4760\nEpoch 34/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6935 - accuracy: 0.4690\nEpoch 35/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6922 - accuracy: 0.4850\nEpoch 36/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6907 - accuracy: 0.4740\nEpoch 37/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6895 - accuracy: 0.4940\nEpoch 38/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6887 - accuracy: 0.4770\nEpoch 39/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6875 - accuracy: 0.4680\nEpoch 40/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.4800\nEpoch 41/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6862 - accuracy: 0.4850\nEpoch 42/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6847 - accuracy: 0.4640\nEpoch 43/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6837 - accuracy: 0.4730\nEpoch 44/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6841 - accuracy: 0.4590\nEpoch 45/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6820 - accuracy: 0.4520\nEpoch 46/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6812 - accuracy: 0.4500\nEpoch 47/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6813 - accuracy: 0.4730\nEpoch 48/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6798 - accuracy: 0.4570\nEpoch 49/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6791 - accuracy: 0.5190\nEpoch 50/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6784 - accuracy: 0.5360\nEpoch 51/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6779 - accuracy: 0.5270\nEpoch 52/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6773 - accuracy: 0.5030\nEpoch 53/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6766 - accuracy: 0.5350\nEpoch 54/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6757 - accuracy: 0.5400\nEpoch 55/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6756 - accuracy: 0.5410\nEpoch 56/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6749 - accuracy: 0.5430\nEpoch 57/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6744 - accuracy: 0.5420\nEpoch 58/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6741 - accuracy: 0.5430\nEpoch 59/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6737 - accuracy: 0.5420\nEpoch 60/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6729 - accuracy: 0.5400\nEpoch 61/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6727 - accuracy: 0.5410\nEpoch 62/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6722 - accuracy: 0.5420\nEpoch 63/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6715 - accuracy: 0.5390\nEpoch 64/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6712 - accuracy: 0.5410\nEpoch 65/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6709 - accuracy: 0.5400\nEpoch 66/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6704 - accuracy: 0.5390\nEpoch 67/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6701 - accuracy: 0.5400\nEpoch 68/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6698 - accuracy: 0.5400\nEpoch 69/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6691 - accuracy: 0.5420\nEpoch 70/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6691 - accuracy: 0.5420\nEpoch 71/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6684 - accuracy: 0.5410\nEpoch 72/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6682 - accuracy: 0.5440\nEpoch 73/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6680 - accuracy: 0.5480\nEpoch 74/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6673 - accuracy: 0.5400\nEpoch 75/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6675 - accuracy: 0.5500\nEpoch 76/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6671 - accuracy: 0.5420\nEpoch 77/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6666 - accuracy: 0.5530\nEpoch 78/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6661 - accuracy: 0.5440\nEpoch 79/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6657 - accuracy: 0.5490\nEpoch 80/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6648 - accuracy: 0.5570\nEpoch 81/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6647 - accuracy: 0.5580\nEpoch 82/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6643 - accuracy: 0.5510\nEpoch 83/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6637 - accuracy: 0.5460\nEpoch 84/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6628 - accuracy: 0.5590\nEpoch 85/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6625 - accuracy: 0.5540\nEpoch 86/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6624 - accuracy: 0.5600\nEpoch 87/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6615 - accuracy: 0.5560\nEpoch 88/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6606 - accuracy: 0.5590\nEpoch 89/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6605 - accuracy: 0.5650\nEpoch 90/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6604 - accuracy: 0.5480\nEpoch 91/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6588 - accuracy: 0.5580\nEpoch 92/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6587 - accuracy: 0.5580\nEpoch 93/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6586 - accuracy: 0.5490\nEpoch 94/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6574 - accuracy: 0.5610\nEpoch 95/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6567 - accuracy: 0.5560\nEpoch 96/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6559 - accuracy: 0.5560\nEpoch 97/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6551 - accuracy: 0.5590\nEpoch 98/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6535 - accuracy: 0.5580\nEpoch 99/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6538 - accuracy: 0.5760\nEpoch 100/100\n32/32 [==============================] - 0s 3ms/step - loss: 0.6523 - accuracy: 0.5640\n</pre> In\u00a0[27]: Copied! <pre># Evaluate the model\nmodel_6.evaluate(X, y)\n</pre> # Evaluate the model model_6.evaluate(X, y) <pre>32/32 [==============================] - 0s 2ms/step - loss: 0.6508 - accuracy: 0.5640\n</pre> Out[27]: <pre>[0.6507635116577148, 0.5640000104904175]</pre> <p>We're still hitting 50% accuracy, our model is still practically as good as guessing.</p> <p>How do the predictions look?</p> In\u00a0[28]: Copied! <pre># Check out the predictions using 2 hidden layers\nplot_decision_boundary(model_6, X, y)\n</pre> # Check out the predictions using 2 hidden layers plot_decision_boundary(model_6, X, y) <pre>313/313 [==============================] - 0s 1ms/step\ndoing binary classifcation...\n</pre> <p>What gives?</p> <p>It seems like our model is the same as the one in the TensorFlow Playground but model it's still drawing straight lines...</p> <p>Ideally, the yellow lines go on the inside of the red circle and the blue circle.</p> <p>Okay, okay, let's model this circle once and for all.</p> <p>One more model (I promise... actually, I'm going to have to break that promise... we'll be building plenty more models).</p> <p>This time we'll change the activation function on our output layer too. Remember the architecture of a classification model? For binary classification, the output layer activation is usually the Sigmoid activation function.</p> In\u00a0[29]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create a model\nmodel_7 = tf.keras.Sequential([\n  tf.keras.layers.Dense(4, activation=tf.keras.activations.relu), # hidden layer 1, ReLU activation\n  tf.keras.layers.Dense(4, activation=tf.keras.activations.relu), # hidden layer 2, ReLU activation\n  tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid) # ouput layer, sigmoid activation\n])\n\n# Compile the model\nmodel_7.compile(loss=tf.keras.losses.binary_crossentropy,\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=['accuracy'])\n\n# Fit the model\nhistory = model_7.fit(X, y, epochs=100, verbose=0)\n</pre> # Set random seed tf.random.set_seed(42)  # Create a model model_7 = tf.keras.Sequential([   tf.keras.layers.Dense(4, activation=tf.keras.activations.relu), # hidden layer 1, ReLU activation   tf.keras.layers.Dense(4, activation=tf.keras.activations.relu), # hidden layer 2, ReLU activation   tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid) # ouput layer, sigmoid activation ])  # Compile the model model_7.compile(loss=tf.keras.losses.binary_crossentropy,                 optimizer=tf.keras.optimizers.Adam(),                 metrics=['accuracy'])  # Fit the model history = model_7.fit(X, y, epochs=100, verbose=0) In\u00a0[30]: Copied! <pre># Evaluate our model\nmodel_7.evaluate(X, y)\n</pre> # Evaluate our model model_7.evaluate(X, y) <pre>32/32 [==============================] - 0s 2ms/step - loss: 0.2082 - accuracy: 0.9950\n</pre> Out[30]: <pre>[0.20816797018051147, 0.9950000047683716]</pre> <p>Woah! It looks like our model is getting some incredible results, let's check them out.</p> In\u00a0[31]: Copied! <pre># View the predictions of the model with relu and sigmoid activations\nplot_decision_boundary(model_7, X, y)\n</pre> # View the predictions of the model with relu and sigmoid activations plot_decision_boundary(model_7, X, y) <pre>313/313 [==============================] - 0s 1ms/step\ndoing binary classifcation...\n</pre> <p>Nice! It looks like our model is almost perfectly (apart from a few examples) separating the two circles.</p> <p>\ud83e\udd14 Question: What's wrong with the predictions we've made? Are we really evaluating our model correctly here? Hint: what data did the model learn on and what did we predict on?</p> <p>Before we answer that, it's important to recognize what we've just covered.</p> <p>\ud83d\udd11 Note: The combination of linear (straight lines) and non-linear (non-straight lines) functions is one of the key fundamentals of neural networks.</p> <p>Think of it like this:</p> <p>If I gave you an unlimited amount of straight lines and non-straight lines, what kind of patterns could you draw?</p> <p>That's essentially what neural networks do to find patterns in data.</p> <p>Now you might be thinking, \"but I haven't seen a linear function or a non-linear function before...\"</p> <p>Oh but you have.</p> <p>We've been using them the whole time.</p> <p>They're what power the layers in the models we just built.</p> <p>To get some intuition about the activation functions we've just used, let's create them and then try them on some toy data.</p> In\u00a0[32]: Copied! <pre># Create a toy tensor (similar to the data we pass into our model)\nA = tf.cast(tf.range(-10, 10), tf.float32)\nA\n</pre> # Create a toy tensor (similar to the data we pass into our model) A = tf.cast(tf.range(-10, 10), tf.float32) A Out[32]: <pre>&lt;tf.Tensor: shape=(20,), dtype=float32, numpy=\narray([-10.,  -9.,  -8.,  -7.,  -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,\n         1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],\n      dtype=float32)&gt;</pre> <p>How does this look?</p> In\u00a0[33]: Copied! <pre># Visualize our toy tensor\nplt.plot(A);\n</pre> # Visualize our toy tensor plt.plot(A); <p>A straight (linear) line!</p> <p>Nice, now let's recreate the sigmoid function and see what it does to our data. You can also find a pre-built sigmoid function at <code>tf.keras.activations.sigmoid</code>.</p> In\u00a0[34]: Copied! <pre># Sigmoid - https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid\ndef sigmoid(x):\n  return 1 / (1 + tf.exp(-x))\n\n# Use the sigmoid function on our tensor\nsigmoid(A)\n</pre> # Sigmoid - https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid def sigmoid(x):   return 1 / (1 + tf.exp(-x))  # Use the sigmoid function on our tensor sigmoid(A) Out[34]: <pre>&lt;tf.Tensor: shape=(20,), dtype=float32, numpy=\narray([4.5397868e-05, 1.2339458e-04, 3.3535014e-04, 9.1105117e-04,\n       2.4726230e-03, 6.6928510e-03, 1.7986210e-02, 4.7425874e-02,\n       1.1920292e-01, 2.6894143e-01, 5.0000000e-01, 7.3105854e-01,\n       8.8079703e-01, 9.5257413e-01, 9.8201376e-01, 9.9330717e-01,\n       9.9752742e-01, 9.9908900e-01, 9.9966466e-01, 9.9987662e-01],\n      dtype=float32)&gt;</pre> <p>And how does it look?</p> In\u00a0[35]: Copied! <pre># Plot sigmoid modified tensor\nplt.plot(sigmoid(A));\n</pre> # Plot sigmoid modified tensor plt.plot(sigmoid(A)); <p>A non-straight (non-linear) line!</p> <p>Okay, how about the ReLU function (ReLU turns all negatives to 0 and positive numbers stay the same)?</p> In\u00a0[36]: Copied! <pre># ReLU - https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu\ndef relu(x):\n  return tf.maximum(0, x)\n\n# Pass toy tensor through ReLU function\nrelu(A)\n</pre> # ReLU - https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu def relu(x):   return tf.maximum(0, x)  # Pass toy tensor through ReLU function relu(A) Out[36]: <pre>&lt;tf.Tensor: shape=(20,), dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 3., 4., 5., 6.,\n       7., 8., 9.], dtype=float32)&gt;</pre> <p>How does the ReLU-modified tensor look?</p> In\u00a0[37]: Copied! <pre># Plot ReLU-modified tensor\nplt.plot(relu(A));\n</pre> # Plot ReLU-modified tensor plt.plot(relu(A)); <p>Another non-straight line!</p> <p>Well, how about TensorFlow's linear activation function?</p> In\u00a0[38]: Copied! <pre># Linear - https://www.tensorflow.org/api_docs/python/tf/keras/activations/linear (returns input non-modified...)\ntf.keras.activations.linear(A)\n</pre> # Linear - https://www.tensorflow.org/api_docs/python/tf/keras/activations/linear (returns input non-modified...) tf.keras.activations.linear(A) Out[38]: <pre>&lt;tf.Tensor: shape=(20,), dtype=float32, numpy=\narray([-10.,  -9.,  -8.,  -7.,  -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,\n         1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],\n      dtype=float32)&gt;</pre> <p>Hmm, it looks like our inputs are unmodified...</p> In\u00a0[39]: Copied! <pre># Does the linear activation change anything?\nA == tf.keras.activations.linear(A)\n</pre> # Does the linear activation change anything? A == tf.keras.activations.linear(A) Out[39]: <pre>&lt;tf.Tensor: shape=(20,), dtype=bool, numpy=\narray([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n        True,  True])&gt;</pre> <p>Okay, so it makes sense now the model doesn't really learn anything when using only linear activation functions, because the linear activation function doesn't change our input data in anyway.</p> <p>Where as, with our non-linear functions, our data gets manipulated. A neural network uses these kind of transformations at a large scale to figure draw patterns between its inputs and outputs.</p> <p>Now rather than dive into the guts of neural networks, we're going to keep coding applying what we've learned to different problems but if you want a more in-depth look at what's going on behind the scenes, check out the Extra Curriculum section below.</p> <p>\ud83d\udcd6  Resource: For more on activation functions, check out the machine learning cheatsheet page on them.</p> In\u00a0[40]: Copied! <pre># How many examples are in the whole dataset?\nlen(X)\n</pre> # How many examples are in the whole dataset? len(X) Out[40]: <pre>1000</pre> In\u00a0[41]: Copied! <pre># Split data into train and test sets\nX_train, y_train = X[:800], y[:800] # 80% of the data for the training set\nX_test, y_test = X[800:], y[800:] # 20% of the data for the test set\n\n# Check the shapes of the data\nX_train.shape, X_test.shape # 800 examples in the training set, 200 examples in the test set\n</pre> # Split data into train and test sets X_train, y_train = X[:800], y[:800] # 80% of the data for the training set X_test, y_test = X[800:], y[800:] # 20% of the data for the test set  # Check the shapes of the data X_train.shape, X_test.shape # 800 examples in the training set, 200 examples in the test set Out[41]: <pre>((800, 2), (200, 2))</pre> <p>Great, now we've got training and test sets, let's model the training data and evaluate what our model has learned on the test set.</p> In\u00a0[42]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create the model (same as model_7)\nmodel_8 = tf.keras.Sequential([\n  tf.keras.layers.Dense(4, activation=\"relu\"), # hidden layer 1, using \"relu\" for activation (same as tf.keras.activations.relu)\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(1, activation=\"sigmoid\") # output layer, using 'sigmoid' for the output\n])\n\n# Compile the model\nmodel_8.compile(loss=tf.keras.losses.binary_crossentropy,\n                optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), # increase learning rate from 0.001 to 0.01 for faster learning\n                metrics=['accuracy'])\n\n# Fit the model\nhistory = model_8.fit(X_train, y_train, epochs=25)\n</pre> # Set random seed tf.random.set_seed(42)  # Create the model (same as model_7) model_8 = tf.keras.Sequential([   tf.keras.layers.Dense(4, activation=\"relu\"), # hidden layer 1, using \"relu\" for activation (same as tf.keras.activations.relu)   tf.keras.layers.Dense(4, activation=\"relu\"),   tf.keras.layers.Dense(1, activation=\"sigmoid\") # output layer, using 'sigmoid' for the output ])  # Compile the model model_8.compile(loss=tf.keras.losses.binary_crossentropy,                 optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), # increase learning rate from 0.001 to 0.01 for faster learning                 metrics=['accuracy'])  # Fit the model history = model_8.fit(X_train, y_train, epochs=25) <pre>Epoch 1/25\n25/25 [==============================] - 1s 3ms/step - loss: 0.6902 - accuracy: 0.5075\nEpoch 2/25\n25/25 [==============================] - 0s 3ms/step - loss: 0.6878 - accuracy: 0.5325\nEpoch 3/25\n25/25 [==============================] - 0s 3ms/step - loss: 0.6862 - accuracy: 0.5200\nEpoch 4/25\n25/25 [==============================] - 0s 3ms/step - loss: 0.6830 - accuracy: 0.5462\nEpoch 5/25\n25/25 [==============================] - 0s 3ms/step - loss: 0.6770 - accuracy: 0.5700\nEpoch 6/25\n25/25 [==============================] - 0s 3ms/step - loss: 0.6676 - accuracy: 0.5450\nEpoch 7/25\n25/25 [==============================] - 0s 3ms/step - loss: 0.6511 - accuracy: 0.6650\nEpoch 8/25\n25/25 [==============================] - 0s 3ms/step - loss: 0.6341 - accuracy: 0.7075\nEpoch 9/25\n25/25 [==============================] - 0s 3ms/step - loss: 0.6085 - accuracy: 0.7588\nEpoch 10/25\n25/25 [==============================] - 0s 3ms/step - loss: 0.5821 - accuracy: 0.7538\nEpoch 11/25\n25/25 [==============================] - 0s 3ms/step - loss: 0.5565 - accuracy: 0.7763\nEpoch 12/25\n25/25 [==============================] - 0s 3ms/step - loss: 0.5284 - accuracy: 0.7962\nEpoch 13/25\n25/25 [==============================] - 0s 3ms/step - loss: 0.5035 - accuracy: 0.8037\nEpoch 14/25\n25/25 [==============================] - 0s 3ms/step - loss: 0.4585 - accuracy: 0.8450\nEpoch 15/25\n25/25 [==============================] - 0s 3ms/step - loss: 0.4156 - accuracy: 0.8838\nEpoch 16/25\n25/25 [==============================] - 0s 3ms/step - loss: 0.3831 - accuracy: 0.8975\nEpoch 17/25\n25/25 [==============================] - 0s 3ms/step - loss: 0.3542 - accuracy: 0.9125\nEpoch 18/25\n25/25 [==============================] - 0s 3ms/step - loss: 0.3288 - accuracy: 0.9312\nEpoch 19/25\n25/25 [==============================] - 0s 3ms/step - loss: 0.2976 - accuracy: 0.9588\nEpoch 20/25\n25/25 [==============================] - 0s 3ms/step - loss: 0.2850 - accuracy: 0.9450\nEpoch 21/25\n25/25 [==============================] - 0s 3ms/step - loss: 0.2571 - accuracy: 0.9563\nEpoch 22/25\n25/25 [==============================] - 0s 3ms/step - loss: 0.2353 - accuracy: 0.9613\nEpoch 23/25\n25/25 [==============================] - 0s 3ms/step - loss: 0.2209 - accuracy: 0.9725\nEpoch 24/25\n25/25 [==============================] - 0s 3ms/step - loss: 0.2108 - accuracy: 0.9588\nEpoch 25/25\n25/25 [==============================] - 0s 3ms/step - loss: 0.2010 - accuracy: 0.9700\n</pre> In\u00a0[43]: Copied! <pre># Evaluate our model on the test set\nloss, accuracy = model_8.evaluate(X_test, y_test)\nprint(f\"Model loss on the test set: {loss}\")\nprint(f\"Model accuracy on the test set: {100*accuracy:.2f}%\")\n</pre> # Evaluate our model on the test set loss, accuracy = model_8.evaluate(X_test, y_test) print(f\"Model loss on the test set: {loss}\") print(f\"Model accuracy on the test set: {100*accuracy:.2f}%\") <pre>7/7 [==============================] - 0s 3ms/step - loss: 0.2052 - accuracy: 0.9700\nModel loss on the test set: 0.20516908168792725\nModel accuracy on the test set: 97.00%\n</pre> <p>100% accuracy? Nice!</p> <p>Now, when we started to create <code>model_8</code> we said it was going to be the same as <code>model_7</code> but you might've found that to be a little lie.</p> <p>That's because we changed a few things:</p> <ul> <li>The <code>activation</code> parameter - We used strings (<code>\"relu\"</code> &amp; <code>\"sigmoid\"</code>) instead of using library paths (<code>tf.keras.activations.relu</code>), in TensorFlow, they both offer the same functionality.</li> <li>The <code>learning_rate</code> (also <code>lr</code>) parameter - We increased the learning rate parameter in the Adam optimizer to <code>0.01</code> instead of <code>0.001</code> (an increase of 10x).<ul> <li>You can think of the learning rate as how quickly a model learns. The higher the learning rate, the faster the model's capacity to learn, however, there's such a thing as a too high learning rate, where a model tries to learn too fast and doesn't learn anything. We'll see a trick to find the ideal learning rate soon.</li> </ul> </li> <li>The number of epochs - We lowered the number of epochs (using the <code>epochs</code> parameter) from 100 to 25 but our model still got an incredible result on both the training and test sets.<ul> <li>One of the reasons our model performed well in even less epochs (remember a single epoch is the model trying to learn patterns in the data by looking at it once, so 25 epochs means the model gets 25 chances) than before is because we increased the learning rate.</li> </ul> </li> </ul> <p>We know our model is performing well based on the evaluation metrics but let's see how it performs visually.</p> In\u00a0[44]: Copied! <pre># Plot the decision boundaries for the training and test sets\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_8, X=X_train, y=y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_8, X=X_test, y=y_test)\nplt.show()\n</pre> # Plot the decision boundaries for the training and test sets plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.title(\"Train\") plot_decision_boundary(model_8, X=X_train, y=y_train) plt.subplot(1, 2, 2) plt.title(\"Test\") plot_decision_boundary(model_8, X=X_test, y=y_test) plt.show() <pre>313/313 [==============================] - 0s 1ms/step\ndoing binary classifcation...\n313/313 [==============================] - 0s 1ms/step\ndoing binary classifcation...\n</pre> <p>Check that out! How cool. With a few tweaks, our model is now predicting the blue and red circles almost perfectly.</p> In\u00a0[45]: Copied! <pre># You can access the information in the history variable using the .history attribute\npd.DataFrame(history.history)\n</pre> # You can access the information in the history variable using the .history attribute pd.DataFrame(history.history) Out[45]: loss accuracy 0 0.690183 0.50750 1 0.687798 0.53250 2 0.686171 0.52000 3 0.683011 0.54625 4 0.677036 0.57000 5 0.667617 0.54500 6 0.651129 0.66500 7 0.634132 0.70750 8 0.608484 0.75875 9 0.582073 0.75375 10 0.556544 0.77625 11 0.528435 0.79625 12 0.503492 0.80375 13 0.458543 0.84500 14 0.415571 0.88375 15 0.383102 0.89750 16 0.354211 0.91250 17 0.328809 0.93125 18 0.297566 0.95875 19 0.285039 0.94500 20 0.257121 0.95625 21 0.235265 0.96125 22 0.220926 0.97250 23 0.210754 0.95875 24 0.201047 0.97000 <p>Inspecting the outputs, we can see the loss values going down and the accuracy going up.</p> <p>How's it look (visualize, visualize, visualize)?</p> In\u00a0[46]: Copied! <pre># Plot the loss curves\npd.DataFrame(history.history).plot()\nplt.title(\"Model_8 training curves\")\n</pre> # Plot the loss curves pd.DataFrame(history.history).plot() plt.title(\"Model_8 training curves\") Out[46]: <pre>Text(0.5, 1.0, 'Model_8 training curves')</pre> <p>Beautiful. This is the ideal plot we'd be looking for when dealing with a classification problem, loss going down, accuracy going up.</p> <p>\ud83d\udd11 Note: For many problems, the loss function going down means the model is improving (the predictions it's making are getting closer to the ground truth labels).</p> In\u00a0[47]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create a model (same as model_8)\nmodel_9 = tf.keras.Sequential([\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(1, activation=\"sigmoid\")\n])\n\n# Compile the model\nmodel_9.compile(loss=\"binary_crossentropy\", # we can use strings here too\n              optimizer=\"Adam\", # same as tf.keras.optimizers.Adam() with default settings\n              metrics=[\"accuracy\"])\n\n# Create a learning rate scheduler callback\nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch/20)) # traverse a set of learning rate values starting from 1e-4, increasing by 10**(epoch/20) every epoch\n\n# Fit the model (passing the lr_scheduler callback)\nhistory = model_9.fit(X_train,\n                      y_train,\n                      epochs=100,\n                      callbacks=[lr_scheduler])\n</pre> # Set random seed tf.random.set_seed(42)  # Create a model (same as model_8) model_9 = tf.keras.Sequential([   tf.keras.layers.Dense(4, activation=\"relu\"),   tf.keras.layers.Dense(4, activation=\"relu\"),   tf.keras.layers.Dense(1, activation=\"sigmoid\") ])  # Compile the model model_9.compile(loss=\"binary_crossentropy\", # we can use strings here too               optimizer=\"Adam\", # same as tf.keras.optimizers.Adam() with default settings               metrics=[\"accuracy\"])  # Create a learning rate scheduler callback lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch/20)) # traverse a set of learning rate values starting from 1e-4, increasing by 10**(epoch/20) every epoch  # Fit the model (passing the lr_scheduler callback) history = model_9.fit(X_train,                       y_train,                       epochs=100,                       callbacks=[lr_scheduler]) <pre>Epoch 1/100\n25/25 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5088 - lr: 1.0000e-04\nEpoch 2/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6918 - accuracy: 0.5038 - lr: 1.1220e-04\nEpoch 3/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6917 - accuracy: 0.5038 - lr: 1.2589e-04\nEpoch 4/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6917 - accuracy: 0.5025 - lr: 1.4125e-04\nEpoch 5/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6917 - accuracy: 0.5063 - lr: 1.5849e-04\nEpoch 6/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6916 - accuracy: 0.5050 - lr: 1.7783e-04\nEpoch 7/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6915 - accuracy: 0.5088 - lr: 1.9953e-04\nEpoch 8/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6915 - accuracy: 0.5075 - lr: 2.2387e-04\nEpoch 9/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6914 - accuracy: 0.5088 - lr: 2.5119e-04\nEpoch 10/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6913 - accuracy: 0.5088 - lr: 2.8184e-04\nEpoch 11/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6912 - accuracy: 0.5075 - lr: 3.1623e-04\nEpoch 12/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6912 - accuracy: 0.5100 - lr: 3.5481e-04\nEpoch 13/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6910 - accuracy: 0.5113 - lr: 3.9811e-04\nEpoch 14/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6909 - accuracy: 0.5138 - lr: 4.4668e-04\nEpoch 15/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6907 - accuracy: 0.5150 - lr: 5.0119e-04\nEpoch 16/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6905 - accuracy: 0.5188 - lr: 5.6234e-04\nEpoch 17/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6903 - accuracy: 0.5225 - lr: 6.3096e-04\nEpoch 18/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6901 - accuracy: 0.5238 - lr: 7.0795e-04\nEpoch 19/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6899 - accuracy: 0.5250 - lr: 7.9433e-04\nEpoch 20/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6895 - accuracy: 0.5288 - lr: 8.9125e-04\nEpoch 21/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6891 - accuracy: 0.5300 - lr: 0.0010\nEpoch 22/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6885 - accuracy: 0.5375 - lr: 0.0011\nEpoch 23/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6880 - accuracy: 0.5350 - lr: 0.0013\nEpoch 24/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6868 - accuracy: 0.5412 - lr: 0.0014\nEpoch 25/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6856 - accuracy: 0.5350 - lr: 0.0016\nEpoch 26/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6844 - accuracy: 0.5387 - lr: 0.0018\nEpoch 27/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6830 - accuracy: 0.5425 - lr: 0.0020\nEpoch 28/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6811 - accuracy: 0.5437 - lr: 0.0022\nEpoch 29/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6794 - accuracy: 0.5475 - lr: 0.0025\nEpoch 30/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6773 - accuracy: 0.5587 - lr: 0.0028\nEpoch 31/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6743 - accuracy: 0.5600 - lr: 0.0032\nEpoch 32/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6712 - accuracy: 0.5788 - lr: 0.0035\nEpoch 33/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6668 - accuracy: 0.5938 - lr: 0.0040\nEpoch 34/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6611 - accuracy: 0.6300 - lr: 0.0045\nEpoch 35/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6541 - accuracy: 0.6513 - lr: 0.0050\nEpoch 36/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6456 - accuracy: 0.6400 - lr: 0.0056\nEpoch 37/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6316 - accuracy: 0.6862 - lr: 0.0063\nEpoch 38/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6141 - accuracy: 0.6850 - lr: 0.0071\nEpoch 39/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.5986 - accuracy: 0.7287 - lr: 0.0079\nEpoch 40/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.5662 - accuracy: 0.7588 - lr: 0.0089\nEpoch 41/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.5288 - accuracy: 0.7663 - lr: 0.0100\nEpoch 42/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.5150 - accuracy: 0.7588 - lr: 0.0112\nEpoch 43/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.5029 - accuracy: 0.7837 - lr: 0.0126\nEpoch 44/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.4472 - accuracy: 0.8213 - lr: 0.0141\nEpoch 45/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.3744 - accuracy: 0.8662 - lr: 0.0158\nEpoch 46/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.2755 - accuracy: 0.9312 - lr: 0.0178\nEpoch 47/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.1886 - accuracy: 0.9600 - lr: 0.0200\nEpoch 48/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.1305 - accuracy: 0.9812 - lr: 0.0224\nEpoch 49/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.1161 - accuracy: 0.9762 - lr: 0.0251\nEpoch 50/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.0870 - accuracy: 0.9900 - lr: 0.0282\nEpoch 51/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.1532 - accuracy: 0.9325 - lr: 0.0316\nEpoch 52/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.0809 - accuracy: 0.9775 - lr: 0.0355\nEpoch 53/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.1048 - accuracy: 0.9600 - lr: 0.0398\nEpoch 54/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.0923 - accuracy: 0.9600 - lr: 0.0447\nEpoch 55/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.1136 - accuracy: 0.9538 - lr: 0.0501\nEpoch 56/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.2233 - accuracy: 0.9200 - lr: 0.0562\nEpoch 57/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.1119 - accuracy: 0.9563 - lr: 0.0631\nEpoch 58/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.2204 - accuracy: 0.9125 - lr: 0.0708\nEpoch 59/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.0781 - accuracy: 0.9725 - lr: 0.0794\nEpoch 60/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9787 - lr: 0.0891\nEpoch 61/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.0916 - accuracy: 0.9688 - lr: 0.1000\nEpoch 62/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.4185 - accuracy: 0.8838 - lr: 0.1122\nEpoch 63/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.1851 - accuracy: 0.9350 - lr: 0.1259\nEpoch 64/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.0893 - accuracy: 0.9688 - lr: 0.1413\nEpoch 65/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.0730 - accuracy: 0.9775 - lr: 0.1585\nEpoch 66/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.3190 - accuracy: 0.9050 - lr: 0.1778\nEpoch 67/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6285 - accuracy: 0.6950 - lr: 0.1995\nEpoch 68/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.4084 - accuracy: 0.7812 - lr: 0.2239\nEpoch 69/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.1803 - accuracy: 0.9350 - lr: 0.2512\nEpoch 70/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.1364 - accuracy: 0.9500 - lr: 0.2818\nEpoch 71/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.3171 - accuracy: 0.9013 - lr: 0.3162\nEpoch 72/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.2477 - accuracy: 0.9150 - lr: 0.3548\nEpoch 73/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.1354 - accuracy: 0.9525 - lr: 0.3981\nEpoch 74/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.3741 - accuracy: 0.8712 - lr: 0.4467\nEpoch 75/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.3701 - accuracy: 0.8550 - lr: 0.5012\nEpoch 76/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.2392 - accuracy: 0.9125 - lr: 0.5623\nEpoch 77/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.1684 - accuracy: 0.9312 - lr: 0.6310\nEpoch 78/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.1817 - accuracy: 0.9312 - lr: 0.7079\nEpoch 79/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.4512 - accuracy: 0.8250 - lr: 0.7943\nEpoch 80/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6677 - accuracy: 0.5938 - lr: 0.8913\nEpoch 81/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6764 - accuracy: 0.5400 - lr: 1.0000\nEpoch 82/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.6678 - accuracy: 0.5775 - lr: 1.1220\nEpoch 83/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.7416 - accuracy: 0.4900 - lr: 1.2589\nEpoch 84/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.7138 - accuracy: 0.5063 - lr: 1.4125\nEpoch 85/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.7237 - accuracy: 0.5038 - lr: 1.5849\nEpoch 86/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.7751 - accuracy: 0.5063 - lr: 1.7783\nEpoch 87/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.7665 - accuracy: 0.5063 - lr: 1.9953\nEpoch 88/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.7566 - accuracy: 0.5163 - lr: 2.2387\nEpoch 89/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.7552 - accuracy: 0.4938 - lr: 2.5119\nEpoch 90/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.7816 - accuracy: 0.5238 - lr: 2.8184\nEpoch 91/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.8113 - accuracy: 0.5213 - lr: 3.1623\nEpoch 92/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.7351 - accuracy: 0.4888 - lr: 3.5481\nEpoch 93/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.7429 - accuracy: 0.5063 - lr: 3.9811\nEpoch 94/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.7601 - accuracy: 0.5063 - lr: 4.4668\nEpoch 95/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.8247 - accuracy: 0.4863 - lr: 5.0119\nEpoch 96/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.7877 - accuracy: 0.4737 - lr: 5.6234\nEpoch 97/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.8081 - accuracy: 0.5013 - lr: 6.3096\nEpoch 98/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.9653 - accuracy: 0.4963 - lr: 7.0795\nEpoch 99/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.9762 - accuracy: 0.4913 - lr: 7.9433\nEpoch 100/100\n25/25 [==============================] - 0s 3ms/step - loss: 0.8582 - accuracy: 0.4613 - lr: 8.9125\n</pre> <p>Now our model has finished training, let's have a look at the training history.</p> In\u00a0[48]: Copied! <pre># Checkout the history\npd.DataFrame(history.history).plot(figsize=(10,7), xlabel=\"epochs\");\n</pre> # Checkout the history pd.DataFrame(history.history).plot(figsize=(10,7), xlabel=\"epochs\"); <p>As you you see the learning rate exponentially increases as the number of epochs increases.</p> <p>And you can see the model's accuracy goes up (and loss goes down) at a specific point when the learning rate slowly increases.</p> <p>To figure out where this infliction point is, we can plot the loss versus the log-scale learning rate.</p> In\u00a0[49]: Copied! <pre># Plot the learning rate versus the loss\nlrs = 1e-4 * (10 ** (np.arange(100)/20))\nplt.figure(figsize=(10, 7))\nplt.semilogx(lrs, history.history[\"loss\"]) # we want the x-axis (learning rate) to be log scale\nplt.xlabel(\"Learning Rate\")\nplt.ylabel(\"Loss\")\nplt.title(\"Learning rate vs. loss\");\n</pre> # Plot the learning rate versus the loss lrs = 1e-4 * (10 ** (np.arange(100)/20)) plt.figure(figsize=(10, 7)) plt.semilogx(lrs, history.history[\"loss\"]) # we want the x-axis (learning rate) to be log scale plt.xlabel(\"Learning Rate\") plt.ylabel(\"Loss\") plt.title(\"Learning rate vs. loss\"); <p>To figure out the ideal value of the learning rate (at least the ideal value to begin training our model), the rule of thumb is to take the learning rate value where the loss is still decreasing but not quite flattened out (usually about 10x smaller than the bottom of the curve).</p> <p>In this case, our ideal learning rate ends up between <code>0.01</code> ($10^{-2}$) and <code>0.02</code>.</p> <p></p> <p>The ideal learning rate at the start of model training is somewhere just before the loss curve bottoms out (a value where the loss is still decreasing).</p> In\u00a0[50]: Copied! <pre># Example of other typical learning rate values\n10**0, 10**-1, 10**-2, 10**-3, 1e-4\n</pre> # Example of other typical learning rate values 10**0, 10**-1, 10**-2, 10**-3, 1e-4 Out[50]: <pre>(1, 0.1, 0.01, 0.001, 0.0001)</pre> <p>Now we've estimated the ideal learning rate (we'll use <code>0.02</code>) for our model, let's refit it.</p> In\u00a0[51]: Copied! <pre># Set the random seed\ntf.random.set_seed(42)\n\n# Create the model\nmodel_10 = tf.keras.Sequential([\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(1, activation=\"sigmoid\")\n])\n\n# Compile the model with the ideal learning rate\nmodel_10.compile(loss=\"binary_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(learning_rate=0.02), # to adjust the learning rate, you need to use tf.keras.optimizers.Adam (not \"adam\")\n                metrics=[\"accuracy\"])\n\n# Fit the model for 20 epochs (5 less than before)\nhistory = model_10.fit(X_train, y_train, epochs=20)\n</pre> # Set the random seed tf.random.set_seed(42)  # Create the model model_10 = tf.keras.Sequential([   tf.keras.layers.Dense(4, activation=\"relu\"),   tf.keras.layers.Dense(4, activation=\"relu\"),   tf.keras.layers.Dense(1, activation=\"sigmoid\") ])  # Compile the model with the ideal learning rate model_10.compile(loss=\"binary_crossentropy\",                 optimizer=tf.keras.optimizers.Adam(learning_rate=0.02), # to adjust the learning rate, you need to use tf.keras.optimizers.Adam (not \"adam\")                 metrics=[\"accuracy\"])  # Fit the model for 20 epochs (5 less than before) history = model_10.fit(X_train, y_train, epochs=20) <pre>Epoch 1/20\n25/25 [==============================] - 2s 3ms/step - loss: 0.6844 - accuracy: 0.5650\nEpoch 2/20\n25/25 [==============================] - 0s 3ms/step - loss: 0.6692 - accuracy: 0.6612\nEpoch 3/20\n25/25 [==============================] - 0s 3ms/step - loss: 0.6408 - accuracy: 0.7250\nEpoch 4/20\n25/25 [==============================] - 0s 3ms/step - loss: 0.5839 - accuracy: 0.7812\nEpoch 5/20\n25/25 [==============================] - 0s 3ms/step - loss: 0.5135 - accuracy: 0.8250\nEpoch 6/20\n25/25 [==============================] - 0s 3ms/step - loss: 0.4106 - accuracy: 0.9187\nEpoch 7/20\n25/25 [==============================] - 0s 3ms/step - loss: 0.3194 - accuracy: 0.9513\nEpoch 8/20\n25/25 [==============================] - 0s 3ms/step - loss: 0.2379 - accuracy: 0.9762\nEpoch 9/20\n25/25 [==============================] - 0s 3ms/step - loss: 0.1835 - accuracy: 0.9850\nEpoch 10/20\n25/25 [==============================] - 0s 3ms/step - loss: 0.1439 - accuracy: 0.9925\nEpoch 11/20\n25/25 [==============================] - 0s 3ms/step - loss: 0.1122 - accuracy: 0.9950\nEpoch 12/20\n25/25 [==============================] - 0s 3ms/step - loss: 0.0928 - accuracy: 0.9937\nEpoch 13/20\n25/25 [==============================] - 0s 3ms/step - loss: 0.0849 - accuracy: 0.9937\nEpoch 14/20\n25/25 [==============================] - 0s 3ms/step - loss: 0.0818 - accuracy: 0.9875\nEpoch 15/20\n25/25 [==============================] - 0s 3ms/step - loss: 0.0714 - accuracy: 0.9925\nEpoch 16/20\n25/25 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.9950\nEpoch 17/20\n25/25 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9912\nEpoch 18/20\n25/25 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9975\nEpoch 19/20\n25/25 [==============================] - 0s 3ms/step - loss: 0.0588 - accuracy: 0.9875\nEpoch 20/20\n25/25 [==============================] - 0s 3ms/step - loss: 0.0470 - accuracy: 0.9887\n</pre> <p>Nice! With a little higher learning rate (<code>0.02</code> instead of <code>0.01</code>) we reach a higher accuracy than <code>model_8</code> in less epochs (<code>20</code> instead of <code>25</code>).</p> <p>\ud83d\udee0 Practice: Now you've seen an example of what can happen when you change the learning rate, try changing the learning rate value in the TensorFlow Playground and see what happens. What happens if you increase it? What happens if you decrease it?</p> In\u00a0[52]: Copied! <pre># Evaluate model on the test dataset\nmodel_10.evaluate(X_test, y_test)\n</pre> # Evaluate model on the test dataset model_10.evaluate(X_test, y_test) <pre>7/7 [==============================] - 0s 3ms/step - loss: 0.0425 - accuracy: 1.0000\n</pre> Out[52]: <pre>[0.042508091777563095, 1.0]</pre> <p>Let's see how the predictions look.</p> In\u00a0[53]: Copied! <pre># Plot the decision boundaries for the training and test sets\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_10, X=X_train, y=y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_10, X=X_test, y=y_test)\nplt.show()\n</pre> # Plot the decision boundaries for the training and test sets plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.title(\"Train\") plot_decision_boundary(model_10, X=X_train, y=y_train) plt.subplot(1, 2, 2) plt.title(\"Test\") plot_decision_boundary(model_10, X=X_test, y=y_test) plt.show() <pre>313/313 [==============================] - 0s 1ms/step\ndoing binary classifcation...\n313/313 [==============================] - 0s 1ms/step\ndoing binary classifcation...\n</pre> <p>And as we can see, almost perfect again.</p> <p>These are the kind of experiments you'll be running often when building your own models.</p> <p>Start with default settings and see how they perform on your data.</p> <p>And if they don't perform as well as you'd like, improve them.</p> <p>Let's look at a few more ways to evaluate our classification models.</p> In\u00a0[54]: Copied! <pre># Check the accuracy of our model\nloss, accuracy = model_10.evaluate(X_test, y_test)\nprint(f\"Model loss on test set: {loss}\")\nprint(f\"Model accuracy on test set: {(accuracy*100):.2f}%\")\n</pre> # Check the accuracy of our model loss, accuracy = model_10.evaluate(X_test, y_test) print(f\"Model loss on test set: {loss}\") print(f\"Model accuracy on test set: {(accuracy*100):.2f}%\") <pre>7/7 [==============================] - 0s 3ms/step - loss: 0.0425 - accuracy: 1.0000\nModel loss on test set: 0.042508091777563095\nModel accuracy on test set: 100.00%\n</pre> <p>How about a confusion matrix?</p> <p> Anatomy of a confusion matrix (what we're going to be creating). Correct predictions appear down the diagonal (from top left to bottom right).</p> <p>We can make a confusion matrix using Scikit-Learn's <code>confusion_matrix</code> method.</p> In\u00a0[58]: Copied! <pre># Create a confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\n# Make predictions\ny_preds = model_10.predict(X_test)\n\n# Create confusion matrix\nconfusion_matrix(y_test, y_preds)\n</pre> # Create a confusion matrix from sklearn.metrics import confusion_matrix  # Make predictions y_preds = model_10.predict(X_test)  # Create confusion matrix confusion_matrix(y_test, y_preds) <pre>7/7 [==============================] - 0s 2ms/step\n</pre> <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-58-f9843efd97f5&gt; in &lt;cell line: 8&gt;()\n      6 \n      7 # Create confusion matrix\n----&gt; 8 confusion_matrix(y_test, y_preds)\n\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py in confusion_matrix(y_true, y_pred, labels, sample_weight, normalize)\n    315     (0, 2, 1, 1)\n    316     \"\"\"\n--&gt; 317     y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n    318     if y_type not in (\"binary\", \"multiclass\"):\n    319         raise ValueError(\"%s is not supported\" % y_type)\n\n/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py in _check_targets(y_true, y_pred)\n     93 \n     94     if len(y_type) &gt; 1:\n---&gt; 95         raise ValueError(\n     96             \"Classification metrics can't handle a mix of {0} and {1} targets\".format(\n     97                 type_true, type_pred\n\nValueError: Classification metrics can't handle a mix of binary and continuous targets</pre> <p>Ahh, it seems our predictions aren't in the format they need to be.</p> <p>Let's check them out.</p> In\u00a0[59]: Copied! <pre># View the first 10 predictions\ny_preds[:10]\n</pre> # View the first 10 predictions y_preds[:10] Out[59]: <pre>array([[0.9740965 ],\n       [0.9740965 ],\n       [0.9740965 ],\n       [0.9740965 ],\n       [0.47072026],\n       [0.00771922],\n       [0.9740965 ],\n       [0.00127994],\n       [0.9740965 ],\n       [0.00113649]], dtype=float32)</pre> <p>What about our test labels?</p> In\u00a0[60]: Copied! <pre># View the first 10 test labels\ny_test[:10]\n</pre> # View the first 10 test labels y_test[:10] Out[60]: <pre>array([1, 1, 1, 1, 0, 0, 1, 0, 1, 0])</pre> <p>It looks like we need to get our predictions into the binary format (0 or 1).</p> <p>But you might be wondering, what format are they currently in?</p> <p>In their current format (<code>9.8526537e-01</code>), they're in a form called prediction probabilities.</p> <p>You'll see this often with the outputs of neural networks. Often they won't be exact values but more a probability of how likely they are to be one value or another.</p> <p>So one of the steps you'll often see after making predicitons with a neural network is converting the prediction probabilities into labels.</p> <p>In our case, since our ground truth labels (<code>y_test</code>) are binary (0 or 1), we can convert the prediction probabilities using to their binary form using <code>tf.round()</code>.</p> In\u00a0[61]: Copied! <pre># Convert prediction probabilities to binary format and view the first 10\ntf.round(y_preds)[:10]\n</pre> # Convert prediction probabilities to binary format and view the first 10 tf.round(y_preds)[:10] Out[61]: <pre>&lt;tf.Tensor: shape=(10, 1), dtype=float32, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       [1.],\n       [0.],\n       [0.],\n       [1.],\n       [0.],\n       [1.],\n       [0.]], dtype=float32)&gt;</pre> <p>Wonderful! Now we can use the <code>confusion_matrix</code> function.</p> In\u00a0[62]: Copied! <pre># Create a confusion matrix\nconfusion_matrix(y_test, tf.round(y_preds))\n</pre> # Create a confusion matrix confusion_matrix(y_test, tf.round(y_preds)) Out[62]: <pre>array([[101,   0],\n       [  0,  99]])</pre> <p>Alright, we can see the highest numbers are down the diagonal (from top left to bottom right) so this a good sign, but the rest of the matrix doesn't really tell us much.</p> <p>How about we make a function to make our confusion matrix a little more visual?</p> In\u00a0[63]: Copied! <pre># Note: The following confusion matrix code is a remix of Scikit-Learn's\n# plot_confusion_matrix function - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html\n# and Made with ML's introductory notebook - https://github.com/GokuMohandas/MadeWithML/blob/main/notebooks/08_Neural_Networks.ipynb\nimport itertools\n\nfigsize = (10, 10)\n\n# Create the confusion matrix\ncm = confusion_matrix(y_test, tf.round(y_preds))\ncm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it\nn_classes = cm.shape[0]\n\n# Let's prettify it\nfig, ax = plt.subplots(figsize=figsize)\n# Create a matrix plot\ncax = ax.matshow(cm, cmap=plt.cm.Blues) # https://matplotlib.org/3.2.0/api/_as_gen/matplotlib.axes.Axes.matshow.html\nfig.colorbar(cax)\n\n# Create classes\nclasses = False\n\nif classes:\n  labels = classes\nelse:\n  labels = np.arange(cm.shape[0])\n\n# Label the axes\nax.set(title=\"Confusion Matrix\",\n       xlabel=\"Predicted label\",\n       ylabel=\"True label\",\n       xticks=np.arange(n_classes),\n       yticks=np.arange(n_classes),\n       xticklabels=labels,\n       yticklabels=labels)\n\n# Set x-axis labels to bottom\nax.xaxis.set_label_position(\"bottom\")\nax.xaxis.tick_bottom()\n\n# Adjust label size\nax.xaxis.label.set_size(20)\nax.yaxis.label.set_size(20)\nax.title.set_size(20)\n\n# Set threshold for different colors\nthreshold = (cm.max() + cm.min()) / 2.\n\n# Plot the text on each cell\nfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n  plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",\n           horizontalalignment=\"center\",\n           color=\"white\" if cm[i, j] &gt; threshold else \"black\",\n           size=15)\n</pre> # Note: The following confusion matrix code is a remix of Scikit-Learn's # plot_confusion_matrix function - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html # and Made with ML's introductory notebook - https://github.com/GokuMohandas/MadeWithML/blob/main/notebooks/08_Neural_Networks.ipynb import itertools  figsize = (10, 10)  # Create the confusion matrix cm = confusion_matrix(y_test, tf.round(y_preds)) cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it n_classes = cm.shape[0]  # Let's prettify it fig, ax = plt.subplots(figsize=figsize) # Create a matrix plot cax = ax.matshow(cm, cmap=plt.cm.Blues) # https://matplotlib.org/3.2.0/api/_as_gen/matplotlib.axes.Axes.matshow.html fig.colorbar(cax)  # Create classes classes = False  if classes:   labels = classes else:   labels = np.arange(cm.shape[0])  # Label the axes ax.set(title=\"Confusion Matrix\",        xlabel=\"Predicted label\",        ylabel=\"True label\",        xticks=np.arange(n_classes),        yticks=np.arange(n_classes),        xticklabels=labels,        yticklabels=labels)  # Set x-axis labels to bottom ax.xaxis.set_label_position(\"bottom\") ax.xaxis.tick_bottom()  # Adjust label size ax.xaxis.label.set_size(20) ax.yaxis.label.set_size(20) ax.title.set_size(20)  # Set threshold for different colors threshold = (cm.max() + cm.min()) / 2.  # Plot the text on each cell for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):   plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",            horizontalalignment=\"center\",            color=\"white\" if cm[i, j] &gt; threshold else \"black\",            size=15) <p>That looks much better. It seems our model has made almost perfect predictions on the test set except for two false positives (top right corner).</p> In\u00a0[64]: Copied! <pre># What does itertools.product do? Combines two things into each combination\nimport itertools\nfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n  print(i, j)\n</pre> # What does itertools.product do? Combines two things into each combination import itertools for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):   print(i, j) <pre>0 0\n0 1\n1 0\n1 1\n</pre> In\u00a0[65]: Copied! <pre>import tensorflow as tf\nfrom tensorflow.keras.datasets import fashion_mnist\n\n# The data has already been sorted into training and test sets for us\n(train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()\n</pre> import tensorflow as tf from tensorflow.keras.datasets import fashion_mnist  # The data has already been sorted into training and test sets for us (train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data() <pre>Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n29515/29515 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n26421880/26421880 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n5148/5148 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n4422102/4422102 [==============================] - 0s 0us/step\n</pre> <p>Now let's check out an example.</p> In\u00a0[66]: Copied! <pre># Show the first training example\nprint(f\"Training sample:\\n{train_data[0]}\\n\")\nprint(f\"Training label: {train_labels[0]}\")\n</pre> # Show the first training example print(f\"Training sample:\\n{train_data[0]}\\n\") print(f\"Training label: {train_labels[0]}\") <pre>Training sample:\n[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0\n    0   1   4   0   0   0   0   1   1   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62\n   54   0   0   0   1   3   4   0   0   3]\n [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134\n  144 123  23   0   0   0   0  12  10   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178\n  107 156 161 109  64  23  77 130  72  15]\n [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216\n  216 163 127 121 122 146 141  88 172  66]\n [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229\n  223 223 215 213 164 127 123 196 229   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228\n  235 227 224 222 224 221 223 245 173   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198\n  180 212 210 211 213 223 220 243 202   0]\n [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192\n  169 227 208 218 224 212 226 197 209  52]\n [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203\n  198 221 215 213 222 220 245 119 167  56]\n [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240\n  232 213 218 223 234 217 217 209  92   0]\n [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219\n  222 221 216 223 229 215 218 255  77   0]\n [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208\n  211 218 224 223 219 215 224 244 159   0]\n [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230\n  224 234 176 188 250 248 233 238 215   0]\n [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223\n  255 255 221 234 221 211 220 232 246   0]\n [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221\n  188 154 191 210 204 209 222 228 225   0]\n [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117\n  168 219 221 215 217 223 223 224 229  29]\n [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245\n  239 223 218 212 209 222 220 221 230  67]\n [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216\n  199 206 186 181 177 172 181 205 206 115]\n [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191\n  195 191 198 192 176 156 167 177 210  92]\n [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209\n  210 210 211 188 188 194 192 216 170   0]\n [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179\n  182 182 181 176 166 168  99  58   0   0]\n [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]]\n\nTraining label: 9\n</pre> <p>Woah, we get a large list of numbers, followed (the data) by a single number (the class label).</p> <p>What about the shapes?</p> In\u00a0[67]: Copied! <pre># Check the shape of our data\ntrain_data.shape, train_labels.shape, test_data.shape, test_labels.shape\n</pre> # Check the shape of our data train_data.shape, train_labels.shape, test_data.shape, test_labels.shape Out[67]: <pre>((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))</pre> In\u00a0[68]: Copied! <pre># Check shape of a single example\ntrain_data[0].shape, train_labels[0].shape\n</pre> # Check shape of a single example train_data[0].shape, train_labels[0].shape Out[68]: <pre>((28, 28), ())</pre> <p>Okay, 60,000 training examples each with shape (28, 28) and a label each as well as 10,000 test examples of shape (28, 28).</p> <p>But these are just numbers, let's visualize.</p> In\u00a0[69]: Copied! <pre># Plot a single example\nimport matplotlib.pyplot as plt\nplt.imshow(train_data[7]);\n</pre> # Plot a single example import matplotlib.pyplot as plt plt.imshow(train_data[7]); <p>Hmm, but what about its label?</p> In\u00a0[70]: Copied! <pre># Check our samples label\ntrain_labels[7]\n</pre> # Check our samples label train_labels[7] Out[70]: <pre>2</pre> <p>It looks like our labels are in numerical form. And while this is fine for a neural network, you might want to have them in human readable form.</p> <p>Let's create a small list of the class names (we can find them on the dataset's GitHub page).</p> <p>\ud83d\udd11 Note: Whilst this dataset has been prepared for us and ready to go, it's important to remember many datasets won't be ready to go like this one. Often you'll have to do a few preprocessing steps to have it ready to use with a neural network (we'll see more of this when we work with our own data later).</p> In\u00a0[71]: Copied! <pre>class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\n# How many classes are there (this'll be our output shape)?\nlen(class_names)\n</pre> class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',                'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']  # How many classes are there (this'll be our output shape)? len(class_names) Out[71]: <pre>10</pre> <p>Now we have these, let's plot another example.</p> <p>\ud83e\udd14 Question: Pay particular attention to what the data we're working with looks like. Is it only straight lines? Or does it have non-straight lines as well? Do you think if we wanted to find patterns in the photos of clothes (which are actually collections of pixels), will our model need non-linearities (non-straight lines) or not?</p> In\u00a0[72]: Copied! <pre># Plot an example image and its label\nplt.imshow(train_data[17], cmap=plt.cm.binary) # change the colours to black &amp; white\nplt.title(class_names[train_labels[17]]);\n</pre> # Plot an example image and its label plt.imshow(train_data[17], cmap=plt.cm.binary) # change the colours to black &amp; white plt.title(class_names[train_labels[17]]); In\u00a0[74]: Copied! <pre># Plot multiple random images of fashion MNIST\nimport random\nplt.figure(figsize=(7, 7))\nfor i in range(4):\n  ax = plt.subplot(2, 2, i + 1)\n  rand_index = random.choice(range(len(train_data)))\n  plt.imshow(train_data[rand_index], cmap=plt.cm.binary)\n  plt.title(class_names[train_labels[rand_index]])\n  plt.axis(False)\n</pre> # Plot multiple random images of fashion MNIST import random plt.figure(figsize=(7, 7)) for i in range(4):   ax = plt.subplot(2, 2, i + 1)   rand_index = random.choice(range(len(train_data)))   plt.imshow(train_data[rand_index], cmap=plt.cm.binary)   plt.title(class_names[train_labels[rand_index]])   plt.axis(False) <p>Alright, let's build a model to figure out the relationship between the pixel values and their labels.</p> <p>Since this is a multiclass classification problem, we'll need to make a few changes to our architecture (inline with Table 1 above):</p> <ul> <li>The input shape will have to deal with 28x28 tensors (the height and width of our images).<ul> <li>We're actually going to squash the input into a tensor (vector) of shape <code>(784)</code>.</li> </ul> </li> <li>The output shape will have to be 10 because we need our model to predict for 10 different classes.<ul> <li>We'll also change the <code>activation</code> parameter of our output layer to be <code>\"softmax\"</code> instead of <code>'sigmoid'</code>. As we'll see the <code>\"softmax\"</code> activation function outputs a series of values between 0 &amp; 1 (the same shape as output shape, which together add up to ~1. The index with the highest value is predicted by the model to be the most likely class.</li> </ul> </li> <li>We'll need to change our loss function from a binary loss function to a multiclass loss function.<ul> <li>More specifically, since our labels are in integer form, we'll use <code>tf.keras.losses.SparseCategoricalCrossentropy()</code>, if our labels were one-hot encoded (e.g. they looked something like <code>[0, 0, 1, 0, 0...]</code>), we'd use <code>tf.keras.losses.CategoricalCrossentropy()</code>.</li> </ul> </li> <li>We'll also use the <code>validation_data</code> parameter when calling the <code>fit()</code> function. This will give us an idea of how the model performs on the test set during training.</li> </ul> <p>You ready? Let's go.</p> In\u00a0[75]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create the model\nmodel_11 = tf.keras.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)), # input layer (we had to reshape 28x28 to 784, the Flatten layer does this for us)\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(10, activation=\"softmax\") # output shape is 10, activation is softmax\n])\n\n# Compile the model\nmodel_11.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), # different loss function for multiclass classifcation\n                 optimizer=tf.keras.optimizers.Adam(),\n                 metrics=[\"accuracy\"])\n\n# Fit the model\nnon_norm_history = model_11.fit(train_data,\n                                train_labels,\n                                epochs=10,\n                                validation_data=(test_data, test_labels)) # see how the model performs on the test set during training\n</pre> # Set random seed tf.random.set_seed(42)  # Create the model model_11 = tf.keras.Sequential([   tf.keras.layers.Flatten(input_shape=(28, 28)), # input layer (we had to reshape 28x28 to 784, the Flatten layer does this for us)   tf.keras.layers.Dense(4, activation=\"relu\"),   tf.keras.layers.Dense(4, activation=\"relu\"),   tf.keras.layers.Dense(10, activation=\"softmax\") # output shape is 10, activation is softmax ])  # Compile the model model_11.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), # different loss function for multiclass classifcation                  optimizer=tf.keras.optimizers.Adam(),                  metrics=[\"accuracy\"])  # Fit the model non_norm_history = model_11.fit(train_data,                                 train_labels,                                 epochs=10,                                 validation_data=(test_data, test_labels)) # see how the model performs on the test set during training <pre>Epoch 1/10\n1875/1875 [==============================] - 8s 3ms/step - loss: 2.1829 - accuracy: 0.1931 - val_loss: 2.1994 - val_accuracy: 0.2037\nEpoch 2/10\n1875/1875 [==============================] - 6s 3ms/step - loss: 1.8995 - accuracy: 0.2442 - val_loss: 1.8438 - val_accuracy: 0.2579\nEpoch 3/10\n1875/1875 [==============================] - 6s 3ms/step - loss: 1.7651 - accuracy: 0.2615 - val_loss: 1.7387 - val_accuracy: 0.2779\nEpoch 4/10\n1875/1875 [==============================] - 6s 3ms/step - loss: 1.6032 - accuracy: 0.2936 - val_loss: 1.5459 - val_accuracy: 0.3104\nEpoch 5/10\n1875/1875 [==============================] - 6s 3ms/step - loss: 1.5402 - accuracy: 0.3038 - val_loss: 1.5040 - val_accuracy: 0.3089\nEpoch 6/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 1.4902 - accuracy: 0.3207 - val_loss: 1.4725 - val_accuracy: 0.3131\nEpoch 7/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 1.4667 - accuracy: 0.3323 - val_loss: 1.4497 - val_accuracy: 0.3634\nEpoch 8/10\n1875/1875 [==============================] - 6s 3ms/step - loss: 1.4550 - accuracy: 0.3469 - val_loss: 1.4640 - val_accuracy: 0.3559\nEpoch 9/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 1.4280 - accuracy: 0.3549 - val_loss: 1.4428 - val_accuracy: 0.3422\nEpoch 10/10\n1875/1875 [==============================] - 6s 3ms/step - loss: 1.4292 - accuracy: 0.3542 - val_loss: 1.5142 - val_accuracy: 0.3405\n</pre> In\u00a0[76]: Copied! <pre># Check the shapes of our model\n# Note: the \"None\" in (None, 784) is for batch_size, we'll cover this in a later module\nmodel_11.summary()\n</pre> # Check the shapes of our model # Note: the \"None\" in (None, 784) is for batch_size, we'll cover this in a later module model_11.summary() <pre>Model: \"sequential_11\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n flatten (Flatten)           (None, 784)               0         \n                                                                 \n dense_28 (Dense)            (None, 4)                 3140      \n                                                                 \n dense_29 (Dense)            (None, 4)                 20        \n                                                                 \n dense_30 (Dense)            (None, 10)                50        \n                                                                 \n=================================================================\nTotal params: 3210 (12.54 KB)\nTrainable params: 3210 (12.54 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n</pre> <p>Alright, our model gets to about ~35% accuracy after 10 epochs using a similar style model to what we used on our binary classification problem.</p> <p>Which is better than guessing (guessing with 10 classes would result in about 10% accuracy) but we can do better.</p> <p>Do you remember when we talked about neural networks preferring numbers between 0 and 1? (if not, treat this as a reminder)</p> <p>Well, right now, the data we have isn't between 0 and 1, in other words, it's not normalized (hence why we used the <code>non_norm_history</code> variable when calling <code>fit()</code>). It's pixel values are between 0 and 255.</p> <p>Let's see.</p> In\u00a0[77]: Copied! <pre># Check the min and max values of the training data\ntrain_data.min(), train_data.max()\n</pre> # Check the min and max values of the training data train_data.min(), train_data.max() Out[77]: <pre>(0, 255)</pre> <p>We can get these values between 0 and 1 by dividing the entire array by the maximum: <code>255.0</code> (dividing by a float also converts to a float).</p> <p>Doing so will result in all of our data being between 0 and 1 (known as scaling or normalization).</p> In\u00a0[78]: Copied! <pre># Divide train and test images by the maximum value (normalize it)\ntrain_data = train_data / 255.0\ntest_data = test_data / 255.0\n\n# Check the min and max values of the training data\ntrain_data.min(), train_data.max()\n</pre> # Divide train and test images by the maximum value (normalize it) train_data = train_data / 255.0 test_data = test_data / 255.0  # Check the min and max values of the training data train_data.min(), train_data.max() Out[78]: <pre>(0.0, 1.0)</pre> <p>Beautiful! Now our data is between 0 and 1. Let's see what happens when we model it.</p> <p>We'll use the same model as before (<code>model_11</code>) except this time the data will be normalized.</p> In\u00a0[79]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create the model\nmodel_12 = tf.keras.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)), # input layer (we had to reshape 28x28 to 784)\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(10, activation=\"softmax\") # output shape is 10, activation is softmax\n])\n\n# Compile the model\nmodel_12.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n                 optimizer=tf.keras.optimizers.Adam(),\n                 metrics=[\"accuracy\"])\n\n# Fit the model (to the normalized data)\nnorm_history = model_12.fit(train_data,\n                            train_labels,\n                            epochs=10,\n                            validation_data=(test_data, test_labels))\n</pre> # Set random seed tf.random.set_seed(42)  # Create the model model_12 = tf.keras.Sequential([   tf.keras.layers.Flatten(input_shape=(28, 28)), # input layer (we had to reshape 28x28 to 784)   tf.keras.layers.Dense(4, activation=\"relu\"),   tf.keras.layers.Dense(4, activation=\"relu\"),   tf.keras.layers.Dense(10, activation=\"softmax\") # output shape is 10, activation is softmax ])  # Compile the model model_12.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),                  optimizer=tf.keras.optimizers.Adam(),                  metrics=[\"accuracy\"])  # Fit the model (to the normalized data) norm_history = model_12.fit(train_data,                             train_labels,                             epochs=10,                             validation_data=(test_data, test_labels)) <pre>Epoch 1/10\n1875/1875 [==============================] - 7s 3ms/step - loss: 1.2368 - accuracy: 0.5151 - val_loss: 0.9158 - val_accuracy: 0.6197\nEpoch 2/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.8054 - accuracy: 0.6939 - val_loss: 0.7300 - val_accuracy: 0.7337\nEpoch 3/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.6805 - accuracy: 0.7524 - val_loss: 0.6827 - val_accuracy: 0.7570\nEpoch 4/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.6427 - accuracy: 0.7661 - val_loss: 0.6599 - val_accuracy: 0.7663\nEpoch 5/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.6258 - accuracy: 0.7735 - val_loss: 0.6568 - val_accuracy: 0.7681\nEpoch 6/10\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.6138 - accuracy: 0.7784 - val_loss: 0.6378 - val_accuracy: 0.7772\nEpoch 7/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.6056 - accuracy: 0.7817 - val_loss: 0.6611 - val_accuracy: 0.7562\nEpoch 8/10\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.5993 - accuracy: 0.7842 - val_loss: 0.6351 - val_accuracy: 0.7798\nEpoch 9/10\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.5905 - accuracy: 0.7881 - val_loss: 0.6232 - val_accuracy: 0.7782\nEpoch 10/10\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.5867 - accuracy: 0.7911 - val_loss: 0.6203 - val_accuracy: 0.7818\n</pre> <p>Woah, we used the exact same model as before but we with normalized data we're now seeing a much higher accuracy value!</p> <p>Let's plot each model's history (their loss curves).</p> In\u00a0[80]: Copied! <pre>import pandas as pd\n# Plot non-normalized data loss curves\npd.DataFrame(non_norm_history.history).plot(title=\"Non-normalized Data\")\n# Plot normalized data loss curves\npd.DataFrame(norm_history.history).plot(title=\"Normalized data\");\n</pre> import pandas as pd # Plot non-normalized data loss curves pd.DataFrame(non_norm_history.history).plot(title=\"Non-normalized Data\") # Plot normalized data loss curves pd.DataFrame(norm_history.history).plot(title=\"Normalized data\"); <p>Wow. From these two plots, we can see how much quicker our model with the normalized data (<code>model_12</code>) improved than the model with the non-normalized data (<code>model_11</code>).</p> <p>\ud83d\udd11 Note: The same model with even slightly different data can produce dramatically different results. So when you're comparing models, it's important to make sure you're comparing them on the same criteria (e.g. same architecture but different data or same data but different architecture).</p> <p>How about we find the ideal learning rate and see what happens?</p> <p>We'll use the same architecture we've been using.</p> In\u00a0[81]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create the model\nmodel_13 = tf.keras.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)), # input layer (we had to reshape 28x28 to 784)\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(10, activation=\"softmax\") # output shape is 10, activation is softmax\n])\n\n# Compile the model\nmodel_13.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n                 optimizer=tf.keras.optimizers.Adam(),\n                 metrics=[\"accuracy\"])\n\n# Create the learning rate callback\nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10**(epoch/20))\n\n# Fit the model\nfind_lr_history = model_13.fit(train_data,\n                               train_labels,\n                               epochs=40, # model already doing pretty good with current LR, probably don't need 100 epochs\n                               validation_data=(test_data, test_labels),\n                               callbacks=[lr_scheduler])\n</pre> # Set random seed tf.random.set_seed(42)  # Create the model model_13 = tf.keras.Sequential([   tf.keras.layers.Flatten(input_shape=(28, 28)), # input layer (we had to reshape 28x28 to 784)   tf.keras.layers.Dense(4, activation=\"relu\"),   tf.keras.layers.Dense(4, activation=\"relu\"),   tf.keras.layers.Dense(10, activation=\"softmax\") # output shape is 10, activation is softmax ])  # Compile the model model_13.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),                  optimizer=tf.keras.optimizers.Adam(),                  metrics=[\"accuracy\"])  # Create the learning rate callback lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10**(epoch/20))  # Fit the model find_lr_history = model_13.fit(train_data,                                train_labels,                                epochs=40, # model already doing pretty good with current LR, probably don't need 100 epochs                                validation_data=(test_data, test_labels),                                callbacks=[lr_scheduler]) <pre>Epoch 1/40\n1875/1875 [==============================] - 7s 3ms/step - loss: 1.3489 - accuracy: 0.5091 - val_loss: 1.0140 - val_accuracy: 0.6485 - lr: 0.0010\nEpoch 2/40\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.8974 - accuracy: 0.6739 - val_loss: 0.8554 - val_accuracy: 0.6812 - lr: 0.0011\nEpoch 3/40\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.7930 - accuracy: 0.7102 - val_loss: 0.7868 - val_accuracy: 0.6940 - lr: 0.0013\nEpoch 4/40\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.7509 - accuracy: 0.7236 - val_loss: 0.7557 - val_accuracy: 0.7129 - lr: 0.0014\nEpoch 5/40\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.7246 - accuracy: 0.7306 - val_loss: 0.7407 - val_accuracy: 0.7340 - lr: 0.0016\nEpoch 6/40\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.7055 - accuracy: 0.7378 - val_loss: 0.7294 - val_accuracy: 0.7424 - lr: 0.0018\nEpoch 7/40\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.6916 - accuracy: 0.7442 - val_loss: 0.7072 - val_accuracy: 0.7379 - lr: 0.0020\nEpoch 8/40\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.6764 - accuracy: 0.7509 - val_loss: 0.7037 - val_accuracy: 0.7459 - lr: 0.0022\nEpoch 9/40\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.6649 - accuracy: 0.7562 - val_loss: 0.6898 - val_accuracy: 0.7578 - lr: 0.0025\nEpoch 10/40\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.6596 - accuracy: 0.7585 - val_loss: 0.7213 - val_accuracy: 0.7497 - lr: 0.0028\nEpoch 11/40\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.6526 - accuracy: 0.7628 - val_loss: 0.7046 - val_accuracy: 0.7515 - lr: 0.0032\nEpoch 12/40\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.6484 - accuracy: 0.7644 - val_loss: 0.6996 - val_accuracy: 0.7602 - lr: 0.0035\nEpoch 13/40\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.6383 - accuracy: 0.7674 - val_loss: 0.6949 - val_accuracy: 0.7657 - lr: 0.0040\nEpoch 14/40\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.6361 - accuracy: 0.7695 - val_loss: 0.6696 - val_accuracy: 0.7616 - lr: 0.0045\nEpoch 15/40\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.6343 - accuracy: 0.7717 - val_loss: 0.6948 - val_accuracy: 0.7563 - lr: 0.0050\nEpoch 16/40\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.6297 - accuracy: 0.7751 - val_loss: 0.6591 - val_accuracy: 0.7772 - lr: 0.0056\nEpoch 17/40\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.6207 - accuracy: 0.7827 - val_loss: 0.6500 - val_accuracy: 0.7791 - lr: 0.0063\nEpoch 18/40\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.6184 - accuracy: 0.7862 - val_loss: 0.6448 - val_accuracy: 0.7823 - lr: 0.0071\nEpoch 19/40\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.6161 - accuracy: 0.7865 - val_loss: 0.6276 - val_accuracy: 0.7931 - lr: 0.0079\nEpoch 20/40\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.6152 - accuracy: 0.7868 - val_loss: 0.6336 - val_accuracy: 0.7889 - lr: 0.0089\nEpoch 21/40\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.6146 - accuracy: 0.7876 - val_loss: 0.6349 - val_accuracy: 0.7811 - lr: 0.0100\nEpoch 22/40\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.6176 - accuracy: 0.7863 - val_loss: 0.6349 - val_accuracy: 0.7908 - lr: 0.0112\nEpoch 23/40\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.6217 - accuracy: 0.7857 - val_loss: 0.7139 - val_accuracy: 0.7555 - lr: 0.0126\nEpoch 24/40\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.6278 - accuracy: 0.7825 - val_loss: 0.7166 - val_accuracy: 0.7612 - lr: 0.0141\nEpoch 25/40\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.6356 - accuracy: 0.7805 - val_loss: 0.7001 - val_accuracy: 0.7508 - lr: 0.0158\nEpoch 26/40\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.6377 - accuracy: 0.7787 - val_loss: 0.7146 - val_accuracy: 0.7597 - lr: 0.0178\nEpoch 27/40\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.6791 - accuracy: 0.7617 - val_loss: 0.6618 - val_accuracy: 0.7748 - lr: 0.0200\nEpoch 28/40\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.6538 - accuracy: 0.7726 - val_loss: 0.6899 - val_accuracy: 0.7610 - lr: 0.0224\nEpoch 29/40\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.6791 - accuracy: 0.7612 - val_loss: 0.6711 - val_accuracy: 0.7719 - lr: 0.0251\nEpoch 30/40\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.6992 - accuracy: 0.7521 - val_loss: 0.7585 - val_accuracy: 0.7172 - lr: 0.0282\nEpoch 31/40\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.7305 - accuracy: 0.7406 - val_loss: 0.7314 - val_accuracy: 0.7392 - lr: 0.0316\nEpoch 32/40\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.7689 - accuracy: 0.7179 - val_loss: 0.8037 - val_accuracy: 0.6799 - lr: 0.0355\nEpoch 33/40\n1875/1875 [==============================] - 5s 3ms/step - loss: 1.0027 - accuracy: 0.6074 - val_loss: 1.0677 - val_accuracy: 0.5509 - lr: 0.0398\nEpoch 34/40\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.9718 - accuracy: 0.6132 - val_loss: 1.0379 - val_accuracy: 0.5977 - lr: 0.0447\nEpoch 35/40\n1875/1875 [==============================] - 5s 3ms/step - loss: 1.0091 - accuracy: 0.6153 - val_loss: 0.9773 - val_accuracy: 0.6447 - lr: 0.0501\nEpoch 36/40\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.9577 - accuracy: 0.6454 - val_loss: 0.8809 - val_accuracy: 0.6806 - lr: 0.0562\nEpoch 37/40\n1875/1875 [==============================] - 5s 3ms/step - loss: 1.0135 - accuracy: 0.6252 - val_loss: 1.0258 - val_accuracy: 0.6005 - lr: 0.0631\nEpoch 38/40\n1875/1875 [==============================] - 6s 3ms/step - loss: 1.0135 - accuracy: 0.6163 - val_loss: 0.9384 - val_accuracy: 0.6446 - lr: 0.0708\nEpoch 39/40\n1875/1875 [==============================] - 5s 3ms/step - loss: 1.1192 - accuracy: 0.5747 - val_loss: 1.0548 - val_accuracy: 0.5785 - lr: 0.0794\nEpoch 40/40\n1875/1875 [==============================] - 6s 3ms/step - loss: 1.2500 - accuracy: 0.5285 - val_loss: 1.5707 - val_accuracy: 0.4179 - lr: 0.0891\n</pre> In\u00a0[82]: Copied! <pre># Plot the learning rate decay curve\nimport numpy as np\nimport matplotlib.pyplot as plt\nlrs = 1e-3 * (10**(np.arange(40)/20))\nplt.semilogx(lrs, find_lr_history.history[\"loss\"]) # want the x-axis to be log-scale\nplt.xlabel(\"Learning rate\")\nplt.ylabel(\"Loss\")\nplt.title(\"Finding the ideal learning rate\");\n</pre> # Plot the learning rate decay curve import numpy as np import matplotlib.pyplot as plt lrs = 1e-3 * (10**(np.arange(40)/20)) plt.semilogx(lrs, find_lr_history.history[\"loss\"]) # want the x-axis to be log-scale plt.xlabel(\"Learning rate\") plt.ylabel(\"Loss\") plt.title(\"Finding the ideal learning rate\"); <p>In this case, it looks like somewhere close to the default learning rate of the Adam optimizer (<code>0.001</code>) is the ideal learning rate.</p> <p>Let's refit a model using the ideal learning rate.</p> In\u00a0[83]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create the model\nmodel_14 = tf.keras.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)), # input layer (we had to reshape 28x28 to 784)\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(10, activation=\"softmax\") # output shape is 10, activation is softmax\n])\n\n# Compile the model\nmodel_14.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n                 optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), # ideal learning rate (same as default)\n                 metrics=[\"accuracy\"])\n\n# Fit the model\nhistory = model_14.fit(train_data,\n                       train_labels,\n                       epochs=20,\n                       validation_data=(test_data, test_labels))\n</pre> # Set random seed tf.random.set_seed(42)  # Create the model model_14 = tf.keras.Sequential([   tf.keras.layers.Flatten(input_shape=(28, 28)), # input layer (we had to reshape 28x28 to 784)   tf.keras.layers.Dense(4, activation=\"relu\"),   tf.keras.layers.Dense(4, activation=\"relu\"),   tf.keras.layers.Dense(10, activation=\"softmax\") # output shape is 10, activation is softmax ])  # Compile the model model_14.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), # ideal learning rate (same as default)                  metrics=[\"accuracy\"])  # Fit the model history = model_14.fit(train_data,                        train_labels,                        epochs=20,                        validation_data=(test_data, test_labels)) <pre>Epoch 1/20\n1875/1875 [==============================] - 7s 3ms/step - loss: 1.1588 - accuracy: 0.6050 - val_loss: 0.7818 - val_accuracy: 0.7258\nEpoch 2/20\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.7097 - accuracy: 0.7484 - val_loss: 0.7062 - val_accuracy: 0.7526\nEpoch 3/20\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.6528 - accuracy: 0.7655 - val_loss: 0.6678 - val_accuracy: 0.7645\nEpoch 4/20\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.6249 - accuracy: 0.7756 - val_loss: 0.6516 - val_accuracy: 0.7684\nEpoch 5/20\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.6082 - accuracy: 0.7798 - val_loss: 0.6405 - val_accuracy: 0.7733\nEpoch 6/20\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.5946 - accuracy: 0.7838 - val_loss: 0.6344 - val_accuracy: 0.7743\nEpoch 7/20\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.5855 - accuracy: 0.7868 - val_loss: 0.6231 - val_accuracy: 0.7768\nEpoch 8/20\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.5773 - accuracy: 0.7880 - val_loss: 0.6256 - val_accuracy: 0.7737\nEpoch 9/20\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.5716 - accuracy: 0.7909 - val_loss: 0.6134 - val_accuracy: 0.7812\nEpoch 10/20\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.5653 - accuracy: 0.7930 - val_loss: 0.6030 - val_accuracy: 0.7843\nEpoch 11/20\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.5604 - accuracy: 0.7944 - val_loss: 0.5985 - val_accuracy: 0.7849\nEpoch 12/20\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.5544 - accuracy: 0.7972 - val_loss: 0.5951 - val_accuracy: 0.7868\nEpoch 13/20\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.5513 - accuracy: 0.7966 - val_loss: 0.5940 - val_accuracy: 0.7893\nEpoch 14/20\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.5462 - accuracy: 0.7994 - val_loss: 0.5953 - val_accuracy: 0.7864\nEpoch 15/20\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.5420 - accuracy: 0.8008 - val_loss: 0.5860 - val_accuracy: 0.7911\nEpoch 16/20\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.5388 - accuracy: 0.8016 - val_loss: 0.5901 - val_accuracy: 0.7927\nEpoch 17/20\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.5359 - accuracy: 0.8025 - val_loss: 0.5889 - val_accuracy: 0.7898\nEpoch 18/20\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.5341 - accuracy: 0.8028 - val_loss: 0.5746 - val_accuracy: 0.7959\nEpoch 19/20\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.5305 - accuracy: 0.8036 - val_loss: 0.5861 - val_accuracy: 0.7897\nEpoch 20/20\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.5271 - accuracy: 0.8071 - val_loss: 0.5726 - val_accuracy: 0.7970\n</pre> <p>Now we've got a model trained with a close-to-ideal learning rate and performing pretty well, we've got a couple of options.</p> <p>We could:</p> <ul> <li>Evaluate its performance using other classification metrics (such as a confusion matrix or classification report).</li> <li>Assess some of its predictions (through visualizations).</li> <li>Improve its accuracy (by training it for longer or changing the architecture).</li> <li>Save and export it for use in an application.</li> </ul> <p>Let's go through the first two options.</p> <p>First we'll create a classification matrix to visualize its predictions across the different classes.</p> In\u00a0[84]: Copied! <pre># Note: The following confusion matrix code is a remix of Scikit-Learn's\n# plot_confusion_matrix function - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html\n# and Made with ML's introductory notebook - https://github.com/GokuMohandas/MadeWithML/blob/main/notebooks/08_Neural_Networks.ipynb\nimport itertools\nfrom sklearn.metrics import confusion_matrix\n\n# Our function needs a different name to sklearn's plot_confusion_matrix\ndef make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15):\n  \"\"\"Makes a labelled confusion matrix comparing predictions and ground truth labels.\n\n  If classes is passed, confusion matrix will be labelled, if not, integer class values\n  will be used.\n\n  Args:\n    y_true: Array of truth labels (must be same shape as y_pred).\n    y_pred: Array of predicted labels (must be same shape as y_true).\n    classes: Array of class labels (e.g. string form). If `None`, integer labels are used.\n    figsize: Size of output figure (default=(10, 10)).\n    text_size: Size of output figure text (default=15).\n\n  Returns:\n    A labelled confusion matrix plot comparing y_true and y_pred.\n\n  Example usage:\n    make_confusion_matrix(y_true=test_labels, # ground truth test labels\n                          y_pred=y_preds, # predicted labels\n                          classes=class_names, # array of class label names\n                          figsize=(15, 15),\n                          text_size=10)\n  \"\"\"\n  # Create the confustion matrix\n  cm = confusion_matrix(y_true, y_pred)\n  cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it\n  n_classes = cm.shape[0] # find the number of classes we're dealing with\n\n  # Plot the figure and make it pretty\n  fig, ax = plt.subplots(figsize=figsize)\n  cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better\n  fig.colorbar(cax)\n\n  # Are there a list of classes?\n  if classes:\n    labels = classes\n  else:\n    labels = np.arange(cm.shape[0])\n\n  # Label the axes\n  ax.set(title=\"Confusion Matrix\",\n         xlabel=\"Predicted label\",\n         ylabel=\"True label\",\n         xticks=np.arange(n_classes), # create enough axis slots for each class\n         yticks=np.arange(n_classes),\n         xticklabels=labels, # axes will labeled with class names (if they exist) or ints\n         yticklabels=labels)\n\n  # Make x-axis labels appear on bottom\n  ax.xaxis.set_label_position(\"bottom\")\n  ax.xaxis.tick_bottom()\n\n  # Set the threshold for different colors\n  threshold = (cm.max() + cm.min()) / 2.\n\n  # Plot the text on each cell\n  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n    plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",\n             horizontalalignment=\"center\",\n             color=\"white\" if cm[i, j] &gt; threshold else \"black\",\n             size=text_size)\n</pre> # Note: The following confusion matrix code is a remix of Scikit-Learn's # plot_confusion_matrix function - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html # and Made with ML's introductory notebook - https://github.com/GokuMohandas/MadeWithML/blob/main/notebooks/08_Neural_Networks.ipynb import itertools from sklearn.metrics import confusion_matrix  # Our function needs a different name to sklearn's plot_confusion_matrix def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15):   \"\"\"Makes a labelled confusion matrix comparing predictions and ground truth labels.    If classes is passed, confusion matrix will be labelled, if not, integer class values   will be used.    Args:     y_true: Array of truth labels (must be same shape as y_pred).     y_pred: Array of predicted labels (must be same shape as y_true).     classes: Array of class labels (e.g. string form). If `None`, integer labels are used.     figsize: Size of output figure (default=(10, 10)).     text_size: Size of output figure text (default=15).    Returns:     A labelled confusion matrix plot comparing y_true and y_pred.    Example usage:     make_confusion_matrix(y_true=test_labels, # ground truth test labels                           y_pred=y_preds, # predicted labels                           classes=class_names, # array of class label names                           figsize=(15, 15),                           text_size=10)   \"\"\"   # Create the confustion matrix   cm = confusion_matrix(y_true, y_pred)   cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it   n_classes = cm.shape[0] # find the number of classes we're dealing with    # Plot the figure and make it pretty   fig, ax = plt.subplots(figsize=figsize)   cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better   fig.colorbar(cax)    # Are there a list of classes?   if classes:     labels = classes   else:     labels = np.arange(cm.shape[0])    # Label the axes   ax.set(title=\"Confusion Matrix\",          xlabel=\"Predicted label\",          ylabel=\"True label\",          xticks=np.arange(n_classes), # create enough axis slots for each class          yticks=np.arange(n_classes),          xticklabels=labels, # axes will labeled with class names (if they exist) or ints          yticklabels=labels)    # Make x-axis labels appear on bottom   ax.xaxis.set_label_position(\"bottom\")   ax.xaxis.tick_bottom()    # Set the threshold for different colors   threshold = (cm.max() + cm.min()) / 2.    # Plot the text on each cell   for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):     plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",              horizontalalignment=\"center\",              color=\"white\" if cm[i, j] &gt; threshold else \"black\",              size=text_size) <p>Since a confusion matrix compares the truth labels (<code>test_labels</code>) to the predicted labels, we have to make some predictions with our model.</p> In\u00a0[85]: Copied! <pre># Make predictions with the most recent model\ny_probs = model_14.predict(test_data) # \"probs\" is short for probabilities\n\n# View the first 5 predictions\ny_probs[:5]\n</pre> # Make predictions with the most recent model y_probs = model_14.predict(test_data) # \"probs\" is short for probabilities  # View the first 5 predictions y_probs[:5] <pre>313/313 [==============================] - 1s 1ms/step\n</pre> Out[85]: <pre>array([[7.1295396e-08, 0.0000000e+00, 3.3289107e-12, 5.1906879e-20,\n        2.2590930e-15, 2.9898265e-01, 1.3648585e-07, 7.8765094e-02,\n        8.4200781e-03, 6.1383194e-01],\n       [2.3802532e-02, 4.3833437e-03, 7.4165797e-01, 9.3923630e-03,\n        4.8104558e-02, 1.6303338e-03, 1.6198176e-01, 2.8633869e-07,\n        9.0436097e-03, 3.2003202e-06],\n       [1.6573335e-06, 9.9551845e-01, 9.1274740e-07, 4.4626528e-03,\n        1.4068096e-05, 4.6464342e-16, 2.2328247e-06, 6.8920781e-22,\n        2.8975861e-10, 3.0581186e-15],\n       [2.8563186e-08, 9.9863142e-01, 4.3472301e-09, 1.3678216e-03,\n        6.5454623e-07, 6.7064722e-21, 3.1206884e-08, 3.7914244e-28,\n        6.2471518e-13, 5.6674063e-19],\n       [2.8196907e-01, 5.4220832e-06, 4.3520968e-02, 3.0695686e-02,\n        2.1239575e-02, 2.4910512e-05, 5.9459400e-01, 1.5045735e-09,\n        2.7928762e-02, 2.1589773e-05]], dtype=float32)</pre> <p>Our model outputs a list of prediction probabilities, meaning, it outputs a number for how likely it thinks a particular class is to be the label.</p> <p>The higher the number in the prediction probabilities list, the more likely the model believes that is the right class.</p> <p>To find the highest value we can use the <code>argmax()</code> method.</p> In\u00a0[86]: Copied! <pre># See the predicted class number and label for the first example\ny_probs[0].argmax(), class_names[y_probs[0].argmax()]\n</pre> # See the predicted class number and label for the first example y_probs[0].argmax(), class_names[y_probs[0].argmax()] Out[86]: <pre>(9, 'Ankle boot')</pre> <p>Now let's do the same for all of the predictions.</p> In\u00a0[87]: Copied! <pre># Convert all of the predictions from probabilities to labels\ny_preds = y_probs.argmax(axis=1)\n\n# View the first 10 prediction labels\ny_preds[:10]\n</pre> # Convert all of the predictions from probabilities to labels y_preds = y_probs.argmax(axis=1)  # View the first 10 prediction labels y_preds[:10] Out[87]: <pre>array([9, 2, 1, 1, 6, 1, 4, 4, 5, 7])</pre> <p>Wonderful, now we've got our model's predictions in label form, let's create a confusion matrix to view them against the truth labels.</p> In\u00a0[88]: Copied! <pre># Check out the non-prettified confusion matrix\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_true=test_labels,\n                 y_pred=y_preds)\n</pre> # Check out the non-prettified confusion matrix from sklearn.metrics import confusion_matrix confusion_matrix(y_true=test_labels,                  y_pred=y_preds) Out[88]: <pre>array([[759,   0,  30,  71,   6,   1, 124,   0,   8,   1],\n       [  0, 939,  11,  40,   5,   0,   5,   0,   0,   0],\n       [ 21,   2, 730,   9, 176,   0,  59,   0,   3,   0],\n       [ 38,  11,  17, 839,  41,   0,  50,   0,   4,   0],\n       [  0,   0, 125,  28, 805,   0,  38,   0,   4,   0],\n       [  1,   0,   0,   0,   0, 901,   1,  55,  11,  31],\n       [177,   2, 185,  42, 321,   0, 264,   0,   9,   0],\n       [  0,   0,   0,   0,   0,  50,   0, 917,   0,  33],\n       [  5,   0,   8,   3,  16,  17,  49,   4, 897,   1],\n       [  0,   0,   0,   0,   0,  21,   0,  49,  11, 919]])</pre> <p>That confusion matrix is hard to comprehend, let's make it prettier using the function we created before.</p> In\u00a0[89]: Copied! <pre># Make a prettier confusion matrix\nmake_confusion_matrix(y_true=test_labels,\n                      y_pred=y_preds,\n                      classes=class_names,\n                      figsize=(15, 15),\n                      text_size=10)\n</pre> # Make a prettier confusion matrix make_confusion_matrix(y_true=test_labels,                       y_pred=y_preds,                       classes=class_names,                       figsize=(15, 15),                       text_size=10) <p>That looks much better! (one of my favourites sights in the world is a confusion matrix with dark squares down the diagonal)</p> <p>Except the results aren't as good as they could be...</p> <p>It looks like our model is getting confused between the <code>Shirt</code> and <code>T-shirt/top</code> classes (e.g. predicting <code>Shirt</code> when it's actually a <code>T-shirt/top</code>).</p> <p>\ud83e\udd14 Question: Does it make sense that our model is getting confused between the <code>Shirt</code> and <code>T-shirt/top</code> classes? Why do you think this might be? What's one way you could investigate?</p> <p>We've seen how our models predictions line up to the truth labels using a confusion matrix, but how about we visualize some?</p> <p>Let's create a function to plot a random image along with its prediction.</p> <p>\ud83d\udd11 Note: Often when working with images and other forms of visual data, it's a good idea to visualize as much as possible to develop a further understanding of the data and the outputs of your model.</p> In\u00a0[90]: Copied! <pre>import random\n\n# Create a function for plotting a random image along with its prediction\ndef plot_random_image(model, images, true_labels, classes):\n  \"\"\"Picks a random image, plots it and labels it with a predicted and truth label.\n\n  Args:\n    model: a trained model (trained on data similar to what's in images).\n    images: a set of random images (in tensor form).\n    true_labels: array of ground truth labels for images.\n    classes: array of class names for images.\n\n  Returns:\n    A plot of a random image from `images` with a predicted class label from `model`\n    as well as the truth class label from `true_labels`.\n  \"\"\"\n  # Setup random integer\n  i = random.randint(0, len(images))\n\n  # Create predictions and targets\n  target_image = images[i]\n  pred_probs = model.predict(target_image.reshape(1, 28, 28)) # have to reshape to get into right size for model\n  pred_label = classes[pred_probs.argmax()]\n  true_label = classes[true_labels[i]]\n\n  # Plot the target image\n  plt.imshow(target_image, cmap=plt.cm.binary)\n\n  # Change the color of the titles depending on if the prediction is right or wrong\n  if pred_label == true_label:\n    color = \"green\"\n  else:\n    color = \"red\"\n\n  # Add xlabel information (prediction/true label)\n  plt.xlabel(\"Pred: {} {:2.0f}% (True: {})\".format(pred_label,\n                                                   100*tf.reduce_max(pred_probs),\n                                                   true_label),\n             color=color) # set the color to green or red\n</pre> import random  # Create a function for plotting a random image along with its prediction def plot_random_image(model, images, true_labels, classes):   \"\"\"Picks a random image, plots it and labels it with a predicted and truth label.    Args:     model: a trained model (trained on data similar to what's in images).     images: a set of random images (in tensor form).     true_labels: array of ground truth labels for images.     classes: array of class names for images.    Returns:     A plot of a random image from `images` with a predicted class label from `model`     as well as the truth class label from `true_labels`.   \"\"\"   # Setup random integer   i = random.randint(0, len(images))    # Create predictions and targets   target_image = images[i]   pred_probs = model.predict(target_image.reshape(1, 28, 28)) # have to reshape to get into right size for model   pred_label = classes[pred_probs.argmax()]   true_label = classes[true_labels[i]]    # Plot the target image   plt.imshow(target_image, cmap=plt.cm.binary)    # Change the color of the titles depending on if the prediction is right or wrong   if pred_label == true_label:     color = \"green\"   else:     color = \"red\"    # Add xlabel information (prediction/true label)   plt.xlabel(\"Pred: {} {:2.0f}% (True: {})\".format(pred_label,                                                    100*tf.reduce_max(pred_probs),                                                    true_label),              color=color) # set the color to green or red In\u00a0[91]: Copied! <pre># Check out a random image as well as its prediction\nplot_random_image(model=model_14,\n                  images=test_data,\n                  true_labels=test_labels,\n                  classes=class_names)\n</pre> # Check out a random image as well as its prediction plot_random_image(model=model_14,                   images=test_data,                   true_labels=test_labels,                   classes=class_names) <pre>1/1 [==============================] - 0s 22ms/step\n</pre> <p>After running the cell above a few times you'll start to get a visual understanding of the relationship between the model's predictions and the true labels.</p> <p>Did you figure out which predictions the model gets confused on?</p> <p>It seems to mix up classes which are similar, for example, <code>Sneaker</code> with <code>Ankle boot</code>.</p> <p>Looking at the images, you can see how this might be the case.</p> <p>The overall shape of a <code>Sneaker</code> and an <code>Ankle Boot</code> are similar.</p> <p>The overall shape might be one of the patterns the model has learned and so therefore when two images have a similar shape, their predictions get mixed up.</p> In\u00a0[92]: Copied! <pre># Find the layers of our most recent model\nmodel_14.layers\n</pre> # Find the layers of our most recent model model_14.layers Out[92]: <pre>[&lt;keras.src.layers.reshaping.flatten.Flatten at 0x7a2407e038b0&gt;,\n &lt;keras.src.layers.core.dense.Dense at 0x7a2407e02260&gt;,\n &lt;keras.src.layers.core.dense.Dense at 0x7a2407e02f50&gt;,\n &lt;keras.src.layers.core.dense.Dense at 0x7a2407e03070&gt;]</pre> <p>We can access a target layer using indexing.</p> In\u00a0[93]: Copied! <pre># Extract a particular layer\nmodel_14.layers[1]\n</pre> # Extract a particular layer model_14.layers[1] Out[93]: <pre>&lt;keras.src.layers.core.dense.Dense at 0x7a2407e02260&gt;</pre> <p>And we can find the patterns learned by a particular layer using the <code>get_weights()</code> method.</p> <p>The <code>get_weights()</code> method returns the weights (also known as a weights matrix) and biases (also known as a bias vector) of a particular layer.</p> In\u00a0[94]: Copied! <pre># Get the patterns of a layer in our network\nweights, biases = model_14.layers[1].get_weights()\n\n# Shape = 1 weight matrix the size of our input data (28x28) per neuron (4)\nweights, weights.shape\n</pre> # Get the patterns of a layer in our network weights, biases = model_14.layers[1].get_weights()  # Shape = 1 weight matrix the size of our input data (28x28) per neuron (4) weights, weights.shape Out[94]: <pre>(array([[ 0.41736275,  0.16016883,  0.32807097,  0.51932573],\n        [ 0.38906115, -0.11863507, -0.98573697,  0.45491368],\n        [ 0.18559982, -1.1637362 , -0.4268363 , -0.01036255],\n        ...,\n        [-0.11902154, -0.19244409, -0.49631563, -0.086859  ],\n        [-0.18131663,  0.0480358 ,  0.13416374, -0.32421213],\n        [ 0.37639815, -0.5836524 ,  0.3026454 ,  0.4631417 ]],\n       dtype=float32),\n (784, 4))</pre> <p>The weights matrix is the same shape as the input data, which in our case is 784 (28x28 pixels). And there's a copy of the weights matrix for each neuron the in the selected layer (our selected layer has 4 neurons).</p> <p>Each value in the weights matrix corresponds to how a particular value in the input data influences the network's decisions.</p> <p>These values start out as random numbers (they're set by the <code>kernel_initializer</code> parameter when creating a layer, the default is <code>\"glorot_uniform\"</code>) and are then updated to better representative values of the data (non-random) by the neural network during training.</p> <p> Example workflow of how a supervised neural network starts with random weights and updates them to better represent the data by looking at examples of ideal outputs.</p> <p>Now let's check out the bias vector.</p> In\u00a0[95]: Copied! <pre># Shape = 1 bias per neuron (we use 4 neurons in the first layer)\nbiases, biases.shape\n</pre> # Shape = 1 bias per neuron (we use 4 neurons in the first layer) biases, biases.shape Out[95]: <pre>(array([1.2710664 , 1.8170385 , 0.24927875, 1.4434327 ], dtype=float32), (4,))</pre> <p>Every neuron has a bias vector. Each of these is paired with a weight matrix.</p> <p>The bias values get initialized as zeroes by default (using the <code>bias_initializer</code> parameter).</p> <p>The bias vector dictates how much the patterns within the corresponding weights matrix should influence the next layer.</p> In\u00a0[96]: Copied! <pre># Can now calculate the number of paramters in our model\nmodel_14.summary()\n</pre> # Can now calculate the number of paramters in our model model_14.summary() <pre>Model: \"sequential_14\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n flatten_3 (Flatten)         (None, 784)               0         \n                                                                 \n dense_37 (Dense)            (None, 4)                 3140      \n                                                                 \n dense_38 (Dense)            (None, 4)                 20        \n                                                                 \n dense_39 (Dense)            (None, 10)                50        \n                                                                 \n=================================================================\nTotal params: 3210 (12.54 KB)\nTrainable params: 3210 (12.54 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n</pre> <p>Now we've built a few deep learning models, it's a good time to point out the whole concept of inputs and outputs not only relates to a model as a whole but to every layer within a model.</p> <p>You might've already guessed this, but starting from the input layer, each subsequent layer's input is the output of the previous layer.</p> <p>We can see this clearly using the utility <code>plot_model()</code>.</p> In\u00a0[97]: Copied! <pre>from tensorflow.keras.utils import plot_model\n\n# See the inputs and outputs of each layer\nplot_model(model_14, show_shapes=True)\n</pre> from tensorflow.keras.utils import plot_model  # See the inputs and outputs of each layer plot_model(model_14, show_shapes=True) Out[97]:"},{"location":"02_neural_network_classification_in_tensorflow/#02-neural-network-classification-with-tensorflow","title":"02. Neural Network Classification with TensorFlow\u00b6","text":"<p>Okay, we've seen how to deal with a regression problem in TensorFlow, let's look at how we can approach a classification problem.</p> <p>A classification problem involves predicting whether something is one thing or another.</p> <p>For example, you might want to:</p> <ul> <li>Predict whether or not someone has heart disease based on their health parameters. This is called binary classification since there are only two options.</li> <li>Decide whether a photo of is of food, a person or a dog. This is called multi-class classification since there are more than two options.</li> <li>Predict what categories should be assigned to a Wikipedia article. This is called multi-label classification since a single article could have more than one category assigned.</li> </ul> <p>In this notebook, we're going to work through a number of different classification problems with TensorFlow. In other words, taking a set of inputs and predicting what class those set of inputs belong to.</p>"},{"location":"02_neural_network_classification_in_tensorflow/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>Specifically, we're going to go through doing the following with TensorFlow:</p> <ul> <li>Architecture of a classification model</li> <li>Input shapes and output shapes<ul> <li><code>X</code>: features/data (inputs)</li> <li><code>y</code>: labels (outputs)<ul> <li>\"What class do the inputs belong to?\"</li> </ul> </li> </ul> </li> <li>Creating custom data to view and fit</li> <li>Steps in modelling for binary and mutliclass classification<ul> <li>Creating a model</li> <li>Compiling a model<ul> <li>Defining a loss function</li> <li>Setting up an optimizer<ul> <li>Finding the best learning rate</li> </ul> </li> <li>Creating evaluation metrics</li> </ul> </li> <li>Fitting a model (getting it to find patterns in our data)</li> <li>Improving a model</li> </ul> </li> <li>The power of non-linearity</li> <li>Evaluating classification models<ul> <li>Visualizng the model (\"visualize, visualize, visualize\")</li> <li>Looking at training curves</li> <li>Compare predictions to ground truth (using our evaluation metrics)</li> </ul> </li> </ul>"},{"location":"02_neural_network_classification_in_tensorflow/#how-you-can-use-this-notebook","title":"How you can use this notebook\u00b6","text":"<p>You can read through the descriptions and the code (it should all run, except for the cells which error on purpose), but there's a better option.</p> <p>Write all of the code yourself.</p> <p>Yes. I'm serious. Create a new notebook, and rewrite each line by yourself. Investigate it, see if you can break it, why does it break?</p> <p>You don't have to write the text descriptions but writing the code yourself is a great way to get hands-on experience.</p> <p>Don't worry if you make mistakes, we all do. The way to get better and make less mistakes is to write more code.</p>"},{"location":"02_neural_network_classification_in_tensorflow/#typical-architecture-of-a-classification-neural-network","title":"Typical architecture of a classification neural network\u00b6","text":"<p>The word typical is on purpose.</p> <p>Because the architecture of a classification neural network can widely vary depending on the problem you're working on.</p> <p>However, there are some fundamentals all deep neural networks contain:</p> <ul> <li>An input layer.</li> <li>Some hidden layers.</li> <li>An output layer.</li> </ul> <p>Much of the rest is up to the data analyst creating the model.</p> <p>The following are some standard values you'll often use in your classification neural networks.</p> Hyperparameter Binary Classification Multiclass classification Input layer shape Same as number of features (e.g. 5 for age, sex, height, weight, smoking status in heart disease prediction) Same as binary classification Hidden layer(s) Problem specific, minimum = 1, maximum = unlimited Same as binary classification Neurons per hidden layer Problem specific, generally 10 to 100 Same as binary classification Output layer shape 1 (one class or the other) 1 per class (e.g. 3 for food, person or dog photo) Hidden activation Usually ReLU (rectified linear unit) Same as binary classification Output activation Sigmoid Softmax Loss function Cross entropy (<code>tf.keras.losses.BinaryCrossentropy</code> in TensorFlow) Cross entropy (<code>tf.keras.losses.CategoricalCrossentropy</code> in TensorFlow) Optimizer SGD (stochastic gradient descent), Adam Same as binary classification <p>Table 1: Typical architecture of a classification network. Source: Adapted from page 295 of Hands-On Machine Learning with Scikit-Learn, Keras &amp; TensorFlow Book by Aur\u00e9lien G\u00e9ron</p> <p>Don't worry if not much of the above makes sense right now, we'll get plenty of experience as we go through this notebook.</p> <p>Let's start by importing TensorFlow as the common alias <code>tf</code>. For this notebook, make sure you're using version 2.x+.</p>"},{"location":"02_neural_network_classification_in_tensorflow/#creating-data-to-view-and-fit","title":"Creating data to view and fit\u00b6","text":"<p>We could start by importing a classification dataset but let's practice making some of our own classification data.</p> <p>\ud83d\udd11 Note: It's a common practice to get you and model you build working on a toy (or simple) dataset before moving to your actual problem. Treat it as a rehersal experiment before the actual experiment(s).</p> <p>Since classification is predicting whether something is one thing or another, let's make some data to reflect that.</p> <p>To do so, we'll use Scikit-Learn's <code>make_circles()</code> function.</p>"},{"location":"02_neural_network_classification_in_tensorflow/#input-and-output-shapes","title":"Input and output shapes\u00b6","text":"<p>One of the most common issues you'll run into when building neural networks is shape mismatches.</p> <p>More specifically, the shape of the input data and the shape of the output data.</p> <p>In our case, we want to input <code>X</code> and get our model to predict <code>y</code>.</p> <p>So let's check out the shapes of <code>X</code> and <code>y</code>.</p>"},{"location":"02_neural_network_classification_in_tensorflow/#steps-in-modelling","title":"Steps in modelling\u00b6","text":"<p>Now we know what data we have as well as the input and output shapes, let's see how we'd build a neural network to model it.</p> <p>In TensorFlow, there are typically 3 fundamental steps to creating and training a model.</p> <ol> <li>Creating a model - piece together the layers of a neural network yourself (using the functional or sequential API) or import a previously built model (known as transfer learning).</li> <li>Compiling a model - defining how a model's performance should be measured (loss/metrics) as well as defining how it should improve (optimizer).</li> <li>Fitting a model - letting the model try to find patterns in the data (how does <code>X</code> get to <code>y</code>).</li> </ol> <p>Let's see these in action using the Sequential API to build a model for our regression data. And then we'll step through each.</p>"},{"location":"02_neural_network_classification_in_tensorflow/#improving-a-model","title":"Improving a model\u00b6","text":"<p>To improve our model, we can alter almost every part of the 3 steps we went through before.</p> <ol> <li>Creating a model - here you might want to add more layers, increase the number of hidden units (also called neurons) within each layer, change the activation functions of each layer.</li> <li>Compiling a model - you might want to choose a different optimization function (such as the Adam optimizer, which is usually pretty good for many problems) or perhaps change the learning rate of the optimization function.</li> <li>Fitting a model - perhaps you could fit a model for more epochs (leave it training for longer).</li> </ol> <p> There are many different ways to potentially improve a neural network. Some of the most common include: increasing the number of layers (making the network deeper), increasing the number of hidden units (making the network wider) and changing the learning rate. Because these values are all human-changeable, they're referred to as hyperparameters) and the practice of trying to find the best hyperparameters is referred to as hyperparameter tuning.</p> <p>How about we try adding more neurons, an extra layer and our friend the Adam optimizer?</p> <p>Surely doing this will result in predictions better than guessing...</p> <p>Note: The following message (below this one) can be ignored if you're running TensorFlow 2.8.0+, the error seems to have been fixed.</p> <p>Note: If you're using TensorFlow 2.7.0+ (but not 2.8.0+) the original code from the following cells may have caused some errors. They've since been updated to fix those errors. You can see explanations on what happened at the following resources:</p> <ul> <li>Example Colab Notebook</li> <li>TensorFlow for Deep Learning GitHub Discussion on TensorFlow 2.7.0 breaking changes</li> </ul>"},{"location":"02_neural_network_classification_in_tensorflow/#the-missing-piece-non-linearity","title":"The missing piece: Non-linearity\u00b6","text":"<p>Okay, so we saw our neural network can model straight lines (with ability a little bit better than guessing).</p> <p>What about non-straight (non-linear) lines?</p> <p>If we're going to model our classification data (the red and blue circles), we're going to need some non-linear lines.</p> <p>\ud83d\udd28 Practice: Before we get to the next steps, I'd encourage you to play around with the TensorFlow Playground (check out what the data has in common with our own classification data) for 10-minutes. In particular the tab which says \"activation\". Once you're done, come back.</p> <p>Did you try out the activation options? If so, what did you find?</p> <p>If you didn't, don't worry, let's see it in code.</p> <p>We're going to replicate the neural network you can see at this link: TensorFlow Playground.</p> <p> The neural network we're going to recreate with TensorFlow code. See it live at TensorFlow Playground.</p> <p>The main change we'll add to models we've built before is the use of the <code>activation</code> keyword.</p>"},{"location":"02_neural_network_classification_in_tensorflow/#evaluating-and-improving-our-classification-model","title":"Evaluating and improving our classification model\u00b6","text":"<p>If you answered the question above, you might've picked up what we've been doing wrong.</p> <p>We've been evaluating our model on the same data it was trained on.</p> <p>A better approach would be to split our data into training, validation (optional) and test sets.</p> <p>Once we've done that, we'll train our model on the training set (let it find patterns in the data) and then see how well it learned the patterns by using it to predict values on the test set.</p> <p>Let's do it.</p>"},{"location":"02_neural_network_classification_in_tensorflow/#plot-the-loss-curves","title":"Plot the loss curves\u00b6","text":"<p>Looking at the plots above, we can see the outputs of our model are very good.</p> <p>But how did our model go whilst it was learning?</p> <p>As in, how did the performance change everytime the model had a chance to look at the data (once every epoch)?</p> <p>To figure this out, we can check the loss curves (also referred to as the learning curves).</p> <p>You might've seen we've been using the variable <code>history</code> when calling the <code>fit()</code> function on a model (<code>fit()</code> returns a <code>History</code> object).</p> <p>This is where we'll get the information for how our model is performing as it learns.</p> <p>Let's see how we might use it.</p>"},{"location":"02_neural_network_classification_in_tensorflow/#finding-the-best-learning-rate","title":"Finding the best learning rate\u00b6","text":"<p>Aside from the architecture itself (the layers, number of neurons, activations, etc), the most important hyperparameter you can tune for your neural network models is the learning rate.</p> <p>In <code>model_8</code> you saw we lowered the Adam optimizer's learning rate from the default of <code>0.001</code> (default) to <code>0.01</code>.</p> <p>And you might be wondering why we did this.</p> <p>Put it this way, it was a lucky guess.</p> <p>I just decided to try a lower learning rate and see how the model went.</p> <p>Now you might be thinking, \"Seriously? You can do that?\"</p> <p>And the answer is yes. You can change any of the hyperparamaters of your neural networks.</p> <p>With practice, you'll start to see what kind of hyperparameters work and what don't.</p> <p>That's an important thing to understand about machine learning and deep learning in general. It's very experimental. You build a model and evaluate it, build a model and evaluate it.</p> <p>That being said, I want to introduce you a trick which will help you find the optimal learning rate (at least to begin training with) for your models going forward.</p> <p>To do so, we're going to use the following:</p> <ul> <li>A learning rate callback.<ul> <li>You can think of a callback as an extra piece of functionality you can add to your model while its training.</li> </ul> </li> <li>Another model (we could use the same ones as above, we we're practicing building models here).</li> <li>A modified loss curves plot.</li> </ul> <p>We'll go through each with code, then explain what's going on.</p> <p>\ud83d\udd11 Note: The default hyperparameters of many neural network building blocks in TensorFlow are setup in a way which usually work right out of the box (e.g. the Adam optimizer's default settings can usually get good results on many datasets). So it's a good idea to try the defaults first, then adjust as needed.</p>"},{"location":"02_neural_network_classification_in_tensorflow/#more-classification-evaluation-methods","title":"More classification evaluation methods\u00b6","text":"<p>Alongside the visualizations we've been making, there are a number of different evaluation metrics we can use to evaluate our classification models.</p> Metric name/Evaluation method Defintion Code Accuracy Out of 100 predictions, how many does your model get correct? E.g. 95% accuracy means it gets 95/100 predictions correct. <code>sklearn.metrics.accuracy_score()</code> or <code>tf.keras.metrics.Accuracy()</code> Precision Proportion of true positives over total number of samples. Higher precision leads to less false positives (model predicts 1 when it should've been 0). <code>sklearn.metrics.precision_score()</code> or <code>tf.keras.metrics.Precision()</code> Recall Proportion of true positives over total number of true positives and false negatives (model predicts 0 when it should've been 1). Higher recall leads to less false negatives. <code>sklearn.metrics.recall_score()</code> or <code>tf.keras.metrics.Recall()</code> F1-score Combines precision and recall into one metric. 1 is best, 0 is worst. <code>sklearn.metrics.f1_score()</code> Confusion matrix Compares the predicted values with the true values in a tabular way, if 100% correct, all values in the matrix will be top left to bottom right (diagnol line). Custom function or <code>sklearn.metrics.plot_confusion_matrix()</code> Classification report Collection of some of the main classification metrics such as precision, recall and f1-score. <code>sklearn.metrics.classification_report()</code> <p>\ud83d\udd11 Note: Every classification problem will require different kinds of evaluation methods. But you should be familiar with at least the ones above.</p> <p>Let's start with accuracy.</p> <p>Because we passed <code>[\"accuracy\"]</code> to the <code>metrics</code> parameter when we compiled our model, calling <code>evaluate()</code> on it will return the loss as well as accuracy.</p>"},{"location":"02_neural_network_classification_in_tensorflow/#working-with-a-larger-example-multiclass-classification","title":"Working with a larger example (multiclass classification)\u00b6","text":"<p>We've seen a binary classification example (predicting if a data point is part of a red circle or blue circle) but what if you had multiple different classes of things?</p> <p>For example, say you were a fashion company and you wanted to build a neural network to predict whether a piece of clothing was a shoe, a shirt or a jacket (3 different options).</p> <p>When you have more than two classes as an option, this is known as multiclass classification.</p> <p>The good news is, the things we've learned so far (with a few tweaks) can be applied to multiclass classification problems as well.</p> <p>Let's see it in action.</p> <p>To start, we'll need some data. The good thing for us is TensorFlow has a multiclass classication dataset known as Fashion MNIST built-in. Meaning we can get started straight away.</p> <p>We can import it using the <code>tf.keras.datasets</code> module.</p> <p>\ud83d\udcd6 Resource: The following multiclass classification problem has been adapted from the TensorFlow classification guide. A good exercise would be to once you've gone through the following example, replicate the TensorFlow guide.</p>"},{"location":"02_neural_network_classification_in_tensorflow/#what-patterns-is-our-model-learning","title":"What patterns is our model learning?\u00b6","text":"<p>We've been talking a lot about how a neural network finds patterns in numbers, but what exactly do these patterns look like?</p> <p>Let's crack open one of our models and find out.</p> <p>First, we'll get a list of layers in our most recent model (<code>model_14</code>) using the <code>layers</code> attribute.</p>"},{"location":"02_neural_network_classification_in_tensorflow/#how-a-model-learns-in-brief","title":"How a model learns (in brief)\u00b6","text":"<p>Alright, we've trained a bunch of models, but we've never really discussed what's going on under the hood. So how exactly does a model learn?</p> <p>A model learns by updating and improving its weight matrices and biases values every epoch (in our case, when we call the <code>fit()</code> fucntion).</p> <p>It does so by comparing the patterns its learned between the data and labels to the actual labels.</p> <p>If the current patterns (weight matrices and bias values) don't result in a desirable decrease in the loss function (higher loss means worse predictions), the optimizer tries to steer the model to update its patterns in the right way (using the real labels as a reference).</p> <p>This process of using the real labels as a reference to improve the model's predictions is called backpropagation.</p> <p>In other words, data and labels pass through a model (forward pass) and it attempts to learn the relationship between the data and labels.</p> <p>And if this learned relationship isn't close to the actual relationship or it could be improved, the model does so by going back through itself (backward pass) and tweaking its weights matrices and bias values to better represent the data.</p> <p>If all of this sounds confusing (and it's fine if it does, the above is a very succinct description), check out the resources in the extra-curriculum section for more.</p>"},{"location":"02_neural_network_classification_in_tensorflow/#exercises","title":"Exercises \ud83d\udee0\u00b6","text":"<ol> <li>Play with neural networks in the TensorFlow Playground for 10-minutes. Especially try different values of the learning, what happens when you decrease it? What happens when you increase it?</li> <li>Replicate the model pictured in the TensorFlow Playground diagram below using TensorFlow code. Compile it using the Adam optimizer, binary crossentropy loss and accuracy metric. Once it's compiled check a summary of the model.  Try this network out for yourself on the TensorFlow Playground website. Hint: there are 5 hidden layers but the output layer isn't pictured, you'll have to decide what the output layer should be based on the input data.</li> <li>Create a classification dataset using Scikit-Learn's <code>make_moons()</code> function, visualize it and then build a model to fit it at over 85% accuracy.</li> <li>Create a function (or write code) to visualize multiple image predictions for the fashion MNIST at the same time. Plot at least three different images and their prediciton labels at the same time. Hint: see the classifcation tutorial in the TensorFlow documentation for ideas.</li> <li>Recreate TensorFlow's softmax activation function in your own code. Make sure it can accept a tensor and return that tensor after having the softmax function applied to it.</li> <li>Train a model to get 88%+ accuracy on the fashion MNIST test set. Plot a confusion matrix to see the results after.</li> <li>Make a function to show an image of a certain class of the fashion MNIST dataset and make a prediction on it. For example, plot 3 images of the <code>T-shirt</code> class with their predictions.</li> </ol>"},{"location":"02_neural_network_classification_in_tensorflow/#extra-curriculum","title":"Extra curriculum \ud83d\udcd6\u00b6","text":"<ul> <li>Watch 3Blue1Brown's neural networks video 2: Gradient descent, how neural networks learn. After you're done, write 100 words about what you've learned.<ul> <li>If you haven't already, watch video 1: But what is a Neural Network?. Note the activation function they talk about at the end.</li> </ul> </li> <li>Watch MIT's introduction to deep learning lecture 1 (if you haven't already) to get an idea of the concepts behind using linear and non-linear functions.</li> <li>Spend 1-hour reading Michael Nielsen's Neural Networks and Deep Learning book.</li> <li>Read the ML-Glossary documentation on activation functions. Which one is your favourite?<ul> <li>After you've read the ML-Glossary, see which activation functions are available in TensorFlow by searching \"tensorflow activation functions\".</li> </ul> </li> </ul>"},{"location":"03_convolutional_neural_networks_in_tensorflow/","title":"03. Convolutional Neural Networks and Computer Vision with TensorFlow","text":"In\u00a0[1]: Copied! <pre>import datetime\nprint(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\")\n</pre> import datetime print(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\") <pre>Notebook last run (end-to-end): 2023-05-11 03:50:16.074113\n</pre> In\u00a0[2]: Copied! <pre>import zipfile\n\n# Download zip file of pizza_steak images\n!wget https://storage.googleapis.com/ztm_tf_course/food_vision/pizza_steak.zip \n\n# Unzip the downloaded file\nzip_ref = zipfile.ZipFile(\"pizza_steak.zip\", \"r\")\nzip_ref.extractall()\nzip_ref.close()\n</pre> import zipfile  # Download zip file of pizza_steak images !wget https://storage.googleapis.com/ztm_tf_course/food_vision/pizza_steak.zip   # Unzip the downloaded file zip_ref = zipfile.ZipFile(\"pizza_steak.zip\", \"r\") zip_ref.extractall() zip_ref.close() <pre>--2023-05-11 03:50:16--  https://storage.googleapis.com/ztm_tf_course/food_vision/pizza_steak.zip\nResolving storage.googleapis.com (storage.googleapis.com)... 74.125.128.128, 74.125.143.128, 173.194.69.128, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|74.125.128.128|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 109540975 (104M) [application/zip]\nSaving to: \u2018pizza_steak.zip\u2019\n\npizza_steak.zip     100%[===================&gt;] 104.47M  29.4MB/s    in 3.6s    \n\n2023-05-11 03:50:20 (29.4 MB/s) - \u2018pizza_steak.zip\u2019 saved [109540975/109540975]\n\n</pre> <p>\ud83d\udd11 Note: If you're using Google Colab and your runtime disconnects, you may have to redownload the files. You can do this by rerunning the cell above.</p> In\u00a0[3]: Copied! <pre>!ls pizza_steak\n</pre> !ls pizza_steak <pre>test  train\n</pre> <p>We can see we've got a <code>train</code> and <code>test</code> folder.</p> <p>Let's see what's inside one of them.</p> In\u00a0[4]: Copied! <pre>!ls pizza_steak/train/\n</pre> !ls pizza_steak/train/ <pre>pizza  steak\n</pre> <p>And how about insde the <code>steak</code> directory?</p> In\u00a0[5]: Copied! <pre>!ls pizza_steak/train/steak/\n</pre> !ls pizza_steak/train/steak/ <pre>1000205.jpg  1647351.jpg  2238681.jpg  2824680.jpg  3375959.jpg  417368.jpg\n100135.jpg   1650002.jpg  2238802.jpg  2825100.jpg  3381560.jpg  4176.jpg\n101312.jpg   165639.jpg   2254705.jpg  2826987.jpg  3382936.jpg  42125.jpg\n1021458.jpg  1658186.jpg  225990.jpg   2832499.jpg  3386119.jpg  421476.jpg\n1032846.jpg  1658443.jpg  2260231.jpg  2832960.jpg  3388717.jpg  421561.jpg\n10380.jpg    165964.jpg   2268692.jpg  285045.jpg   3389138.jpg  438871.jpg\n1049459.jpg  167069.jpg   2271133.jpg  285147.jpg   3393547.jpg  43924.jpg\n1053665.jpg  1675632.jpg  227576.jpg   2855315.jpg  3393688.jpg  440188.jpg\n1068516.jpg  1678108.jpg  2283057.jpg  2856066.jpg  3396589.jpg  442757.jpg\n1068975.jpg  168006.jpg   2286639.jpg  2859933.jpg  339891.jpg\t 443210.jpg\n1081258.jpg  1682496.jpg  2287136.jpg  286219.jpg   3417789.jpg  444064.jpg\n1090122.jpg  1684438.jpg  2291292.jpg  2862562.jpg  3425047.jpg  444709.jpg\n1093966.jpg  168775.jpg   229323.jpg   2865730.jpg  3434983.jpg  447557.jpg\n1098844.jpg  1697339.jpg  2300534.jpg  2878151.jpg  3435358.jpg  461187.jpg\n1100074.jpg  1710569.jpg  2300845.jpg  2880035.jpg  3438319.jpg  461689.jpg\n1105280.jpg  1714605.jpg  231296.jpg   2881783.jpg  3444407.jpg  465494.jpg\n1117936.jpg  1724387.jpg  2315295.jpg  2884233.jpg  345734.jpg\t 468384.jpg\n1126126.jpg  1724717.jpg  2323132.jpg  2890573.jpg  3460673.jpg  477486.jpg\n114601.jpg   172936.jpg   2324994.jpg  2893832.jpg  3465327.jpg  482022.jpg\n1147047.jpg  1736543.jpg  2327701.jpg  2893892.jpg  3466159.jpg  482465.jpg\n1147883.jpg  1736968.jpg  2331076.jpg  2907177.jpg  3469024.jpg  483788.jpg\n1155665.jpg  1746626.jpg  233964.jpg   290850.jpg   3470083.jpg  493029.jpg\n1163977.jpg  1752330.jpg  2344227.jpg  2909031.jpg  3476564.jpg  503589.jpg\n1190233.jpg  1761285.jpg  234626.jpg   2910418.jpg  3478318.jpg  510757.jpg\n1208405.jpg  176508.jpg   234704.jpg   2912290.jpg  3488748.jpg  513129.jpg\n1209120.jpg  1772039.jpg  2357281.jpg  2916448.jpg  3492328.jpg  513842.jpg\n1212161.jpg  1777107.jpg  2361812.jpg  2916967.jpg  3518960.jpg  523535.jpg\n1213988.jpg  1787505.jpg  2365287.jpg  2927833.jpg  3522209.jpg  525041.jpg\n1219039.jpg  179293.jpg   2374582.jpg  2928643.jpg  3524429.jpg  534560.jpg\n1225762.jpg  1816235.jpg  239025.jpg   2929179.jpg  3528458.jpg  534633.jpg\n1230968.jpg  1822407.jpg  2390628.jpg  2936477.jpg  3531805.jpg  536535.jpg\n1236155.jpg  1823263.jpg  2392910.jpg  2938012.jpg  3536023.jpg  541410.jpg\n1241193.jpg  1826066.jpg  2394465.jpg  2938151.jpg  3538682.jpg  543691.jpg\n1248337.jpg  1828502.jpg  2395127.jpg  2939678.jpg  3540750.jpg  560503.jpg\n1257104.jpg  1828969.jpg  2396291.jpg  2940544.jpg  354329.jpg\t 561972.jpg\n126345.jpg   1829045.jpg  2400975.jpg  2940621.jpg  3547166.jpg  56240.jpg\n1264050.jpg  1829088.jpg  2403776.jpg  2949079.jpg  3553911.jpg  56409.jpg\n1264154.jpg  1836332.jpg  2403907.jpg  295491.jpg   3556871.jpg  564530.jpg\n1264858.jpg  1839025.jpg  240435.jpg   296268.jpg   355715.jpg\t 568972.jpg\n127029.jpg   1839481.jpg  2404695.jpg  2964732.jpg  356234.jpg\t 576725.jpg\n1289900.jpg  183995.jpg   2404884.jpg  2965021.jpg  3571963.jpg  588739.jpg\n1290362.jpg  184110.jpg   2407770.jpg  2966859.jpg  3576078.jpg  590142.jpg\n1295457.jpg  184226.jpg   2412263.jpg  2977966.jpg  3577618.jpg  60633.jpg\n1312841.jpg  1846706.jpg  2425062.jpg  2979061.jpg  3577732.jpg  60655.jpg\n1313316.jpg  1849364.jpg  2425389.jpg  2983260.jpg  3578934.jpg  606820.jpg\n1324791.jpg  1849463.jpg  2435316.jpg  2984311.jpg  358042.jpg\t 612551.jpg\n1327567.jpg  1849542.jpg  2437268.jpg  2988960.jpg  358045.jpg\t 614975.jpg\n1327667.jpg  1853564.jpg  2437843.jpg  2989882.jpg  3591821.jpg  616809.jpg\n1333055.jpg  1869467.jpg  2440131.jpg  2995169.jpg  359330.jpg\t 628628.jpg\n1334054.jpg  1870942.jpg  2443168.jpg  2996324.jpg  3601483.jpg  632427.jpg\n1335556.jpg  187303.jpg   2446660.jpg  3000131.jpg  3606642.jpg  636594.jpg\n1337814.jpg  187521.jpg   2455944.jpg  3002350.jpg  3609394.jpg  637374.jpg\n1340977.jpg  1888450.jpg  2458401.jpg  3007772.jpg  361067.jpg\t 640539.jpg\n1343209.jpg  1889336.jpg  2487306.jpg  3008192.jpg  3613455.jpg  644777.jpg\n134369.jpg   1907039.jpg  248841.jpg   3009617.jpg  3621464.jpg  644867.jpg\n1344105.jpg  1925230.jpg  2489716.jpg  3011642.jpg  3621562.jpg  658189.jpg\n134598.jpg   1927984.jpg  2490489.jpg  3020591.jpg  3621565.jpg  660900.jpg\n1346387.jpg  1930577.jpg  2495884.jpg  3030578.jpg  3623556.jpg  663014.jpg\n1348047.jpg  1937872.jpg  2495903.jpg  3047807.jpg  3640915.jpg  664545.jpg\n1351372.jpg  1941807.jpg  2499364.jpg  3059843.jpg  3643951.jpg  667075.jpg\n1362989.jpg  1942333.jpg  2500292.jpg  3074367.jpg  3653129.jpg  669180.jpg\n1367035.jpg  1945132.jpg  2509017.jpg  3082120.jpg  3656752.jpg  669960.jpg\n1371177.jpg  1961025.jpg  250978.jpg   3094354.jpg  3663518.jpg  6709.jpg\n1375640.jpg  1966300.jpg  2514432.jpg  3095301.jpg  3663800.jpg  674001.jpg\n1382427.jpg  1966967.jpg  2526838.jpg  3099645.jpg  3664376.jpg  676189.jpg\n1392718.jpg  1969596.jpg  252858.jpg   3100476.jpg  3670607.jpg  681609.jpg\n1395906.jpg  1971757.jpg  2532239.jpg  3110387.jpg  3671021.jpg  6926.jpg\n1400760.jpg  1976160.jpg  2534567.jpg  3113772.jpg  3671877.jpg  703556.jpg\n1403005.jpg  1984271.jpg  2535431.jpg  3116018.jpg  368073.jpg\t 703909.jpg\n1404770.jpg  1987213.jpg  2535456.jpg  3128952.jpg  368162.jpg\t 704316.jpg\n140832.jpg   1987639.jpg  2538000.jpg  3130412.jpg  368170.jpg\t 714298.jpg\n141056.jpg   1995118.jpg  2543081.jpg  3136.jpg     3693649.jpg  720060.jpg\n141135.jpg   1995252.jpg  2544643.jpg  313851.jpg   3700079.jpg  726083.jpg\n1413972.jpg  199754.jpg   2547797.jpg  3140083.jpg  3704103.jpg  728020.jpg\n1421393.jpg  2002400.jpg  2548974.jpg  3140147.jpg  3707493.jpg  732986.jpg\n1428947.jpg  2011264.jpg  2549316.jpg  3142045.jpg  3716881.jpg  734445.jpg\n1433912.jpg  2012996.jpg  2561199.jpg  3142618.jpg  3724677.jpg  735441.jpg\n143490.jpg   2013535.jpg  2563233.jpg  3142674.jpg  3727036.jpg  740090.jpg\n1445352.jpg  2017387.jpg  256592.jpg   3143192.jpg  3727491.jpg  745189.jpg\n1446401.jpg  2018173.jpg  2568848.jpg  314359.jpg   3736065.jpg  752203.jpg\n1453991.jpg  2020613.jpg  2573392.jpg  3157832.jpg  37384.jpg\t 75537.jpg\n1456841.jpg  2032669.jpg  2592401.jpg  3159818.jpg  3743286.jpg  756655.jpg\n146833.jpg   203450.jpg   2599817.jpg  3162376.jpg  3745515.jpg  762210.jpg\n1476404.jpg  2034628.jpg  2603058.jpg  3168620.jpg  3750472.jpg  763690.jpg\n1485083.jpg  2036920.jpg  2606444.jpg  3171085.jpg  3752362.jpg  767442.jpg\n1487113.jpg  2038418.jpg  2614189.jpg  317206.jpg   3766099.jpg  786409.jpg\n148916.jpg   2042975.jpg  2614649.jpg  3173444.jpg  3770370.jpg  80215.jpg\n149087.jpg   2045647.jpg  2615718.jpg  3180182.jpg  377190.jpg\t 802348.jpg\n1493169.jpg  2050584.jpg  2619625.jpg  31881.jpg    3777020.jpg  804684.jpg\n149682.jpg   2052542.jpg  2622140.jpg  3191589.jpg  3777482.jpg  812163.jpg\n1508094.jpg  2056627.jpg  262321.jpg   3204977.jpg  3781152.jpg  813486.jpg\n1512226.jpg  2062248.jpg  2625330.jpg  320658.jpg   3787809.jpg  819027.jpg\n1512347.jpg  2081995.jpg  2628106.jpg  3209173.jpg  3788729.jpg  822550.jpg\n1524526.jpg  2087958.jpg  2629750.jpg  3223400.jpg  3790962.jpg  823766.jpg\n1530833.jpg  2088030.jpg  2643906.jpg  3223601.jpg  3792514.jpg  827764.jpg\n1539499.jpg  2088195.jpg  2644457.jpg  3241894.jpg  379737.jpg\t 830007.jpg\n1541672.jpg  2090493.jpg  2648423.jpg  3245533.jpg  3807440.jpg  838344.jpg\n1548239.jpg  2090504.jpg  2651300.jpg  3245622.jpg  381162.jpg\t 853327.jpg\n1550997.jpg  2125877.jpg  2653594.jpg  3247009.jpg  3812039.jpg  854150.jpg\n1552530.jpg  2129685.jpg  2661577.jpg  3253588.jpg  3829392.jpg  864997.jpg\n15580.jpg    2133717.jpg  2668916.jpg  3260624.jpg  3830872.jpg  885571.jpg\n1559052.jpg  2136662.jpg  268444.jpg   326587.jpg   38442.jpg\t 907107.jpg\n1563266.jpg  213765.jpg   2691461.jpg  32693.jpg    3855584.jpg  908261.jpg\n1567554.jpg  2138335.jpg  2706403.jpg  3271253.jpg  3857508.jpg  910672.jpg\n1575322.jpg  2140776.jpg  270687.jpg   3274423.jpg  386335.jpg\t 911803.jpg\n1588879.jpg  214320.jpg   2707522.jpg  3280453.jpg  3867460.jpg  91432.jpg\n1594719.jpg  2146963.jpg  2711806.jpg  3298495.jpg  3868959.jpg  914570.jpg\n1595869.jpg  215222.jpg   2716993.jpg  330182.jpg   3869679.jpg  922752.jpg\n1598345.jpg  2154126.jpg  2724554.jpg  3306627.jpg  388776.jpg\t 923772.jpg\n1598885.jpg  2154779.jpg  2738227.jpg  3315727.jpg  3890465.jpg  926414.jpg\n1600179.jpg  2159975.jpg  2748917.jpg  331860.jpg   3894222.jpg  931356.jpg\n1600794.jpg  2163079.jpg  2760475.jpg  332232.jpg   3895825.jpg  937133.jpg\n160552.jpg   217250.jpg   2761427.jpg  3322909.jpg  389739.jpg\t 945791.jpg\n1606596.jpg  2172600.jpg  2765887.jpg  332557.jpg   3916407.jpg  947877.jpg\n1615395.jpg  2173084.jpg  2768451.jpg  3326734.jpg  393349.jpg\t 952407.jpg\n1618011.jpg  217996.jpg   2771149.jpg  3330642.jpg  393494.jpg\t 952437.jpg\n1619357.jpg  2193684.jpg  2779040.jpg  3333128.jpg  398288.jpg\t 955466.jpg\n1621763.jpg  220341.jpg   2788312.jpg  3333735.jpg  40094.jpg\t 9555.jpg\n1623325.jpg  22080.jpg\t  2788759.jpg  3334973.jpg  401094.jpg\t 961341.jpg\n1624450.jpg  2216146.jpg  2796102.jpg  3335013.jpg  401144.jpg\t 97656.jpg\n1624747.jpg  2222018.jpg  280284.jpg   3335267.jpg  401651.jpg\t 979110.jpg\n1628861.jpg  2223787.jpg  2807888.jpg  3346787.jpg  405173.jpg\t 980247.jpg\n1632774.jpg  2230959.jpg  2815172.jpg  3364420.jpg  405794.jpg\t 982988.jpg\n1636831.jpg  2232310.jpg  2818805.jpg  336637.jpg   40762.jpg\t 987732.jpg\n1645470.jpg  2233395.jpg  2823872.jpg  3372616.jpg  413325.jpg\t 996684.jpg\n</pre> <p>Woah, a whole bunch of images. But how many?</p> <p>\ud83d\udee0 Practice: Try listing the same information for the <code>pizza</code> directory in the <code>test</code> folder.</p> In\u00a0[6]: Copied! <pre>import os\n\n# Walk through pizza_steak directory and list number of files\nfor dirpath, dirnames, filenames in os.walk(\"pizza_steak\"):\n  print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n</pre> import os  # Walk through pizza_steak directory and list number of files for dirpath, dirnames, filenames in os.walk(\"pizza_steak\"):   print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\") <pre>There are 2 directories and 0 images in 'pizza_steak'.\nThere are 2 directories and 0 images in 'pizza_steak/test'.\nThere are 0 directories and 250 images in 'pizza_steak/test/steak'.\nThere are 0 directories and 250 images in 'pizza_steak/test/pizza'.\nThere are 2 directories and 0 images in 'pizza_steak/train'.\nThere are 0 directories and 750 images in 'pizza_steak/train/steak'.\nThere are 0 directories and 750 images in 'pizza_steak/train/pizza'.\n</pre> In\u00a0[7]: Copied! <pre># Another way to find out how many images are in a file\nnum_steak_images_train = len(os.listdir(\"pizza_steak/train/steak\"))\n\nnum_steak_images_train\n</pre> # Another way to find out how many images are in a file num_steak_images_train = len(os.listdir(\"pizza_steak/train/steak\"))  num_steak_images_train Out[7]: <pre>750</pre> In\u00a0[8]: Copied! <pre># Get the class names (programmatically, this is much more helpful with a longer list of classes)\nimport pathlib\nimport numpy as np\ndata_dir = pathlib.Path(\"pizza_steak/train/\") # turn our training path into a Python path\nclass_names = np.array(sorted([item.name for item in data_dir.glob('*')])) # created a list of class_names from the subdirectories\nprint(class_names)\n</pre> # Get the class names (programmatically, this is much more helpful with a longer list of classes) import pathlib import numpy as np data_dir = pathlib.Path(\"pizza_steak/train/\") # turn our training path into a Python path class_names = np.array(sorted([item.name for item in data_dir.glob('*')])) # created a list of class_names from the subdirectories print(class_names) <pre>['pizza' 'steak']\n</pre> <p>Okay, so we've got a collection of 750 training images and 250 testing images of pizza and steak.</p> <p>Let's look at some.</p> <p>\ud83e\udd14 Note: Whenever you're working with data, it's always good to visualize it as much as possible. Treat your first couple of steps of a project as becoming one with the data. Visualize, visualize, visualize.</p> In\u00a0[9]: Copied! <pre># View an image\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport random\n\ndef view_random_image(target_dir, target_class):\n  # Setup target directory (we'll view images from here)\n  target_folder = target_dir+target_class\n\n  # Get a random image path\n  random_image = random.sample(os.listdir(target_folder), 1)\n\n  # Read in the image and plot it using matplotlib\n  img = mpimg.imread(target_folder + \"/\" + random_image[0])\n  plt.imshow(img)\n  plt.title(target_class)\n  plt.axis(\"off\");\n\n  print(f\"Image shape: {img.shape}\") # show the shape of the image\n\n  return img\n</pre> # View an image import matplotlib.pyplot as plt import matplotlib.image as mpimg import random  def view_random_image(target_dir, target_class):   # Setup target directory (we'll view images from here)   target_folder = target_dir+target_class    # Get a random image path   random_image = random.sample(os.listdir(target_folder), 1)    # Read in the image and plot it using matplotlib   img = mpimg.imread(target_folder + \"/\" + random_image[0])   plt.imshow(img)   plt.title(target_class)   plt.axis(\"off\");    print(f\"Image shape: {img.shape}\") # show the shape of the image    return img In\u00a0[10]: Copied! <pre># View a random image from the training dataset\nimg = view_random_image(target_dir=\"pizza_steak/train/\",\n                        target_class=\"steak\")\n</pre> # View a random image from the training dataset img = view_random_image(target_dir=\"pizza_steak/train/\",                         target_class=\"steak\") <pre>Image shape: (512, 512, 3)\n</pre> <p>After going through a dozen or so images from the different classes, you can start to get an idea of what we're working with.</p> <p>The entire Food101 dataset comprises of similar images from 101 different classes.</p> <p>You might've noticed we've been printing the image shape alongside the plotted image.</p> <p>This is because the way our computer sees the image is in the form of a big array (tensor).</p> In\u00a0[11]: Copied! <pre># View the img (actually just a big array/tensor)\nimg\n</pre> # View the img (actually just a big array/tensor) img Out[11]: <pre>array([[[ 80,  32,  18],\n        [ 77,  29,  15],\n        [ 75,  27,  13],\n        ...,\n        [189, 163, 130],\n        [196, 170, 137],\n        [192, 166, 133]],\n\n       [[ 79,  31,  17],\n        [ 76,  28,  14],\n        [ 74,  26,  12],\n        ...,\n        [155, 129,  96],\n        [177, 151, 118],\n        [192, 166, 133]],\n\n       [[ 78,  30,  16],\n        [ 76,  28,  14],\n        [ 74,  26,  12],\n        ...,\n        [135, 109,  76],\n        [149, 123,  90],\n        [170, 144, 111]],\n\n       ...,\n\n       [[ 52,  18,  17],\n        [ 63,  29,  28],\n        [ 53,  19,  17],\n        ...,\n        [253, 237, 203],\n        [253, 237, 203],\n        [253, 237, 203]],\n\n       [[ 53,  22,  20],\n        [ 63,  32,  30],\n        [ 56,  22,  21],\n        ...,\n        [253, 237, 203],\n        [253, 237, 203],\n        [252, 236, 202]],\n\n       [[ 41,  10,   8],\n        [ 62,  31,  29],\n        [ 53,  19,  18],\n        ...,\n        [253, 237, 203],\n        [252, 236, 202],\n        [252, 236, 202]]], dtype=uint8)</pre> In\u00a0[12]: Copied! <pre># View the image shape\nimg.shape # returns (width, height, colour channels)\n</pre> # View the image shape img.shape # returns (width, height, colour channels) Out[12]: <pre>(512, 512, 3)</pre> <p>Looking at the image shape more closely, you'll see it's in the form <code>(Width, Height, Colour Channels)</code>.</p> <p>In our case, the width and height vary but because we're dealing with colour images, the colour channels value is always 3. This is for different values of red, green and blue (RGB) pixels.</p> <p>You'll notice all of the values in the <code>img</code> array are between 0 and 255. This is because that's the possible range for red, green and blue values.</p> <p>For example, a pixel with a value <code>red=0, green=0, blue=255</code> will look very blue.</p> <p>So when we build a model to differentiate between our images of <code>pizza</code> and <code>steak</code>, it will be finding patterns in these different pixel values which determine what each class looks like.</p> <p>\ud83d\udd11 Note: As we've discussed before, many machine learning models, including neural networks prefer the values they work with to be between 0 and 1. Knowing this, one of the most common preprocessing steps for working with images is to scale (also referred to as normalize) their pixel values by dividing the image arrays by 255.</p> In\u00a0[13]: Copied! <pre># Get all the pixel values between 0 &amp; 1\nimg/255.\n</pre> # Get all the pixel values between 0 &amp; 1 img/255.  Out[13]: <pre>array([[[0.31372549, 0.1254902 , 0.07058824],\n        [0.30196078, 0.11372549, 0.05882353],\n        [0.29411765, 0.10588235, 0.05098039],\n        ...,\n        [0.74117647, 0.63921569, 0.50980392],\n        [0.76862745, 0.66666667, 0.5372549 ],\n        [0.75294118, 0.65098039, 0.52156863]],\n\n       [[0.30980392, 0.12156863, 0.06666667],\n        [0.29803922, 0.10980392, 0.05490196],\n        [0.29019608, 0.10196078, 0.04705882],\n        ...,\n        [0.60784314, 0.50588235, 0.37647059],\n        [0.69411765, 0.59215686, 0.4627451 ],\n        [0.75294118, 0.65098039, 0.52156863]],\n\n       [[0.30588235, 0.11764706, 0.0627451 ],\n        [0.29803922, 0.10980392, 0.05490196],\n        [0.29019608, 0.10196078, 0.04705882],\n        ...,\n        [0.52941176, 0.42745098, 0.29803922],\n        [0.58431373, 0.48235294, 0.35294118],\n        [0.66666667, 0.56470588, 0.43529412]],\n\n       ...,\n\n       [[0.20392157, 0.07058824, 0.06666667],\n        [0.24705882, 0.11372549, 0.10980392],\n        [0.20784314, 0.0745098 , 0.06666667],\n        ...,\n        [0.99215686, 0.92941176, 0.79607843],\n        [0.99215686, 0.92941176, 0.79607843],\n        [0.99215686, 0.92941176, 0.79607843]],\n\n       [[0.20784314, 0.08627451, 0.07843137],\n        [0.24705882, 0.1254902 , 0.11764706],\n        [0.21960784, 0.08627451, 0.08235294],\n        ...,\n        [0.99215686, 0.92941176, 0.79607843],\n        [0.99215686, 0.92941176, 0.79607843],\n        [0.98823529, 0.9254902 , 0.79215686]],\n\n       [[0.16078431, 0.03921569, 0.03137255],\n        [0.24313725, 0.12156863, 0.11372549],\n        [0.20784314, 0.0745098 , 0.07058824],\n        ...,\n        [0.99215686, 0.92941176, 0.79607843],\n        [0.98823529, 0.9254902 , 0.79215686],\n        [0.98823529, 0.9254902 , 0.79215686]]])</pre> <p>Components of a convolutional neural network:</p> Hyperparameter/Layer type What does it do? Typical values Input image(s) Target images you'd like to discover patterns in Whatever you can take a photo (or video) of Input layer Takes in target images and preprocesses them for further layers <code>input_shape = [batch_size, image_height, image_width, color_channels]</code> Convolution layer Extracts/learns the most important features from target images Multiple, can create with <code>tf.keras.layers.ConvXD</code> (X can be multiple values) Hidden activation Adds non-linearity to learned features (non-straight lines) Usually ReLU (<code>tf.keras.activations.relu</code>) Pooling layer Reduces the dimensionality of learned image features Average (<code>tf.keras.layers.AvgPool2D</code>) or Max (<code>tf.keras.layers.MaxPool2D</code>) Fully connected layer Further refines learned features from convolution layers <code>tf.keras.layers.Dense</code> Output layer Takes learned features and outputs them in shape of target labels <code>output_shape = [number_of_classes]</code> (e.g. 3 for pizza, steak or sushi) Output activation Adds non-linearities to output layer <code>tf.keras.activations.sigmoid</code> (binary classification) or <code>tf.keras.activations.softmax</code> <p>How they stack together:</p> <p> A simple example of how you might stack together the above layers into a convolutional neural network. Note the convolutional and pooling layers can often be arranged and rearranged into many different formations.</p> In\u00a0[14]: Copied! <pre>import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Set the seed\ntf.random.set_seed(42)\n\n# Preprocess data (get all of the pixel values between 1 and 0, also called scaling/normalization)\ntrain_datagen = ImageDataGenerator(rescale=1./255)\nvalid_datagen = ImageDataGenerator(rescale=1./255)\n\n# Setup the train and test directories\ntrain_dir = \"pizza_steak/train/\"\ntest_dir = \"pizza_steak/test/\"\n\n# Import data from directories and turn it into batches\ntrain_data = train_datagen.flow_from_directory(train_dir,\n                                               batch_size=32, # number of images to process at a time \n                                               target_size=(224, 224), # convert all images to be 224 x 224\n                                               class_mode=\"binary\", # type of problem we're working on\n                                               seed=42)\n\nvalid_data = valid_datagen.flow_from_directory(test_dir,\n                                               batch_size=32,\n                                               target_size=(224, 224),\n                                               class_mode=\"binary\",\n                                               seed=42)\n\n# Create a CNN model (same as Tiny VGG - https://poloclub.github.io/cnn-explainer/)\nmodel_1 = tf.keras.models.Sequential([\n  tf.keras.layers.Conv2D(filters=10, \n                         kernel_size=3, # can also be (3, 3)\n                         activation=\"relu\", \n                         input_shape=(224, 224, 3)), # first layer specifies input shape (height, width, colour channels)\n  tf.keras.layers.Conv2D(10, 3, activation=\"relu\"),\n  tf.keras.layers.MaxPool2D(pool_size=2, # pool_size can also be (2, 2)\n                            padding=\"valid\"), # padding can also be 'same'\n  tf.keras.layers.Conv2D(10, 3, activation=\"relu\"),\n  tf.keras.layers.Conv2D(10, 3, activation=\"relu\"), # activation='relu' == tf.keras.layers.Activations(tf.nn.relu)\n  tf.keras.layers.MaxPool2D(2),\n  tf.keras.layers.Flatten(),\n  tf.keras.layers.Dense(1, activation=\"sigmoid\") # binary activation output\n])\n\n# Compile the model\nmodel_1.compile(loss=\"binary_crossentropy\",\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=[\"accuracy\"])\n\n# Fit the model\nhistory_1 = model_1.fit(train_data,\n                        epochs=5,\n                        steps_per_epoch=len(train_data),\n                        validation_data=valid_data,\n                        validation_steps=len(valid_data))\n</pre> import tensorflow as tf from tensorflow.keras.preprocessing.image import ImageDataGenerator  # Set the seed tf.random.set_seed(42)  # Preprocess data (get all of the pixel values between 1 and 0, also called scaling/normalization) train_datagen = ImageDataGenerator(rescale=1./255) valid_datagen = ImageDataGenerator(rescale=1./255)  # Setup the train and test directories train_dir = \"pizza_steak/train/\" test_dir = \"pizza_steak/test/\"  # Import data from directories and turn it into batches train_data = train_datagen.flow_from_directory(train_dir,                                                batch_size=32, # number of images to process at a time                                                 target_size=(224, 224), # convert all images to be 224 x 224                                                class_mode=\"binary\", # type of problem we're working on                                                seed=42)  valid_data = valid_datagen.flow_from_directory(test_dir,                                                batch_size=32,                                                target_size=(224, 224),                                                class_mode=\"binary\",                                                seed=42)  # Create a CNN model (same as Tiny VGG - https://poloclub.github.io/cnn-explainer/) model_1 = tf.keras.models.Sequential([   tf.keras.layers.Conv2D(filters=10,                           kernel_size=3, # can also be (3, 3)                          activation=\"relu\",                           input_shape=(224, 224, 3)), # first layer specifies input shape (height, width, colour channels)   tf.keras.layers.Conv2D(10, 3, activation=\"relu\"),   tf.keras.layers.MaxPool2D(pool_size=2, # pool_size can also be (2, 2)                             padding=\"valid\"), # padding can also be 'same'   tf.keras.layers.Conv2D(10, 3, activation=\"relu\"),   tf.keras.layers.Conv2D(10, 3, activation=\"relu\"), # activation='relu' == tf.keras.layers.Activations(tf.nn.relu)   tf.keras.layers.MaxPool2D(2),   tf.keras.layers.Flatten(),   tf.keras.layers.Dense(1, activation=\"sigmoid\") # binary activation output ])  # Compile the model model_1.compile(loss=\"binary_crossentropy\",               optimizer=tf.keras.optimizers.Adam(),               metrics=[\"accuracy\"])  # Fit the model history_1 = model_1.fit(train_data,                         epochs=5,                         steps_per_epoch=len(train_data),                         validation_data=valid_data,                         validation_steps=len(valid_data)) <pre>Found 1500 images belonging to 2 classes.\nFound 500 images belonging to 2 classes.\nEpoch 1/5\n47/47 [==============================] - 23s 209ms/step - loss: 0.5981 - accuracy: 0.6773 - val_loss: 0.4158 - val_accuracy: 0.8060\nEpoch 2/5\n47/47 [==============================] - 9s 194ms/step - loss: 0.4584 - accuracy: 0.7967 - val_loss: 0.3675 - val_accuracy: 0.8500\nEpoch 3/5\n47/47 [==============================] - 9s 192ms/step - loss: 0.4656 - accuracy: 0.7893 - val_loss: 0.4424 - val_accuracy: 0.7940\nEpoch 4/5\n47/47 [==============================] - 9s 192ms/step - loss: 0.4098 - accuracy: 0.8380 - val_loss: 0.3485 - val_accuracy: 0.8780\nEpoch 5/5\n47/47 [==============================] - 9s 191ms/step - loss: 0.3708 - accuracy: 0.8453 - val_loss: 0.3162 - val_accuracy: 0.8820\n</pre> <p>\ud83e\udd14 Note: If the cell above takes more than ~12 seconds per epoch to run, you might not be using a GPU accelerator. If you're using a Colab notebook, you can access a GPU accelerator by going to Runtime -&gt; Change Runtime Type -&gt; Hardware Accelerator and select \"GPU\". After doing so, you might have to rerun all of the above cells as changing the runtime type causes Colab to have to reset.</p> <p>Nice! After 5 epochs, our model beat the baseline score of 50.76% accuracy (our model got ~85% accuaracy on the training set and ~85% accuracy on the test set).</p> <p>However, our model only went through a binary classificaiton problem rather than all of the 101 classes in the Food101 dataset, so we can't directly compare these metrics. That being said, the results so far show that our model is learning something.</p> <p>\ud83d\udee0 Practice: Step through each of the main blocks of code in the cell above, what do you think each is doing? It's okay if you're not sure, we'll go through this soon. In the meantime, spend 10-minutes playing around the incredible CNN explainer website. What do you notice about the layer names at the top of the webpage?</p> <p>Since we've already fit a model, let's check out its architecture.</p> In\u00a0[15]: Copied! <pre># Check out the layers in our model\nmodel_1.summary()\n</pre> # Check out the layers in our model model_1.summary() <pre>Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 222, 222, 10)      280       \n                                                                 \n conv2d_1 (Conv2D)           (None, 220, 220, 10)      910       \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 110, 110, 10)     0         \n )                                                               \n                                                                 \n conv2d_2 (Conv2D)           (None, 108, 108, 10)      910       \n                                                                 \n conv2d_3 (Conv2D)           (None, 106, 106, 10)      910       \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 53, 53, 10)       0         \n 2D)                                                             \n                                                                 \n flatten (Flatten)           (None, 28090)             0         \n                                                                 \n dense (Dense)               (None, 1)                 28091     \n                                                                 \n=================================================================\nTotal params: 31,101\nTrainable params: 31,101\nNon-trainable params: 0\n_________________________________________________________________\n</pre> <p>What do you notice about the names of <code>model_1</code>'s layers and the layer names at the top of the CNN explainer website?</p> <p>I'll let you in on a little secret: we've replicated the exact architecture they use for their model demo.</p> <p>Look at you go! You're already starting to replicate models you find in the wild.</p> <p>Now there are a few new things here we haven't discussed, namely:</p> <ul> <li>The <code>ImageDataGenerator</code> class and the <code>rescale</code> parameter</li> <li>The <code>flow_from_directory()</code> method<ul> <li>The <code>batch_size</code> parameter</li> <li>The <code>target_size</code> parameter</li> </ul> </li> <li><code>Conv2D</code> layers (and the parameters which come with them)</li> <li><code>MaxPool2D</code> layers (and their parameters).</li> <li>The <code>steps_per_epoch</code> and <code>validation_steps</code> parameters in the <code>fit()</code> function</li> </ul> <p>Before we dive into each of these, let's see what happens if we try to fit a model we've worked with previously to our data.</p> In\u00a0[16]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create a model to replicate the TensorFlow Playground model\nmodel_2 = tf.keras.Sequential([\n  tf.keras.layers.Flatten(input_shape=(224, 224, 3)), # dense layers expect a 1-dimensional vector as input\n  tf.keras.layers.Dense(4, activation='relu'),\n  tf.keras.layers.Dense(4, activation='relu'),\n  tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel_2.compile(loss='binary_crossentropy',\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=[\"accuracy\"])\n\n# Fit the model\nhistory_2 = model_2.fit(train_data, # use same training data created above\n                        epochs=5,\n                        steps_per_epoch=len(train_data),\n                        validation_data=valid_data, # use same validation data created above\n                        validation_steps=len(valid_data))\n</pre> # Set random seed tf.random.set_seed(42)  # Create a model to replicate the TensorFlow Playground model model_2 = tf.keras.Sequential([   tf.keras.layers.Flatten(input_shape=(224, 224, 3)), # dense layers expect a 1-dimensional vector as input   tf.keras.layers.Dense(4, activation='relu'),   tf.keras.layers.Dense(4, activation='relu'),   tf.keras.layers.Dense(1, activation='sigmoid') ])  # Compile the model model_2.compile(loss='binary_crossentropy',               optimizer=tf.keras.optimizers.Adam(),               metrics=[\"accuracy\"])  # Fit the model history_2 = model_2.fit(train_data, # use same training data created above                         epochs=5,                         steps_per_epoch=len(train_data),                         validation_data=valid_data, # use same validation data created above                         validation_steps=len(valid_data)) <pre>Epoch 1/5\n47/47 [==============================] - 11s 197ms/step - loss: 1.7892 - accuracy: 0.4920 - val_loss: 0.6932 - val_accuracy: 0.5000\nEpoch 2/5\n47/47 [==============================] - 9s 191ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\nEpoch 3/5\n47/47 [==============================] - 9s 189ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\nEpoch 4/5\n47/47 [==============================] - 9s 199ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\nEpoch 5/5\n47/47 [==============================] - 9s 196ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000\n</pre> <p>Hmmm... our model ran but it doesn't seem like it learned anything. It only reaches 50% accuracy on the training and test sets which in a binary classification problem is as good as guessing.</p> <p>Let's see the architecture.</p> In\u00a0[17]: Copied! <pre># Check out our second model's architecture\nmodel_2.summary()\n</pre> # Check out our second model's architecture model_2.summary() <pre>Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n flatten_1 (Flatten)         (None, 150528)            0         \n                                                                 \n dense_1 (Dense)             (None, 4)                 602116    \n                                                                 \n dense_2 (Dense)             (None, 4)                 20        \n                                                                 \n dense_3 (Dense)             (None, 1)                 5         \n                                                                 \n=================================================================\nTotal params: 602,141\nTrainable params: 602,141\nNon-trainable params: 0\n_________________________________________________________________\n</pre> <p>Wow. One of the most noticeable things here is the much larger number of parameters in <code>model_2</code> versus <code>model_1</code>.</p> <p><code>model_2</code> has 602,141 trainable parameters where as <code>model_1</code> has only 31,101. And despite this difference, <code>model_1</code> still far and large out performs <code>model_2</code>.</p> <p>\ud83d\udd11 Note: You can think of trainable parameters as patterns a model can learn from data. Intuitiely, you might think more is better. And in some cases it is. But in this case, the difference here is in the two different styles of model we're using. Where a series of dense layers have a number of different learnable parameters connected to each other and hence a higher number of possible learnable patterns, a convolutional neural network seeks to sort out and learn the most important patterns in an image. So even though there are less learnable parameters in our convolutional neural network, these are often more helpful in decphering between different features in an image.</p> <p>Since our previous model didn't work, do you have any ideas of how we might make it work?</p> <p>How about we increase the number of layers?</p> <p>And maybe even increase the number of neurons in each layer?</p> <p>More specifically, we'll increase the number of neurons (also called hidden units) in each dense layer from 4 to 100 and add an extra layer.</p> <p>\ud83d\udd11 Note: Adding extra layers or increasing the number of neurons in each layer is often referred to as increasing the complexity of your model.</p> In\u00a0[18]: Copied! <pre># Set random seed\ntf.random.set_seed(42)\n\n# Create a model similar to model_1 but add an extra layer and increase the number of hidden units in each layer\nmodel_3 = tf.keras.Sequential([\n  tf.keras.layers.Flatten(input_shape=(224, 224, 3)), # dense layers expect a 1-dimensional vector as input\n  tf.keras.layers.Dense(100, activation='relu'), # increase number of neurons from 4 to 100 (for each layer)\n  tf.keras.layers.Dense(100, activation='relu'),\n  tf.keras.layers.Dense(100, activation='relu'), # add an extra layer\n  tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel_3.compile(loss='binary_crossentropy',\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=[\"accuracy\"])\n\n# Fit the model\nhistory_3 = model_3.fit(train_data,\n                        epochs=5,\n                        steps_per_epoch=len(train_data),\n                        validation_data=valid_data,\n                        validation_steps=len(valid_data))\n</pre> # Set random seed tf.random.set_seed(42)  # Create a model similar to model_1 but add an extra layer and increase the number of hidden units in each layer model_3 = tf.keras.Sequential([   tf.keras.layers.Flatten(input_shape=(224, 224, 3)), # dense layers expect a 1-dimensional vector as input   tf.keras.layers.Dense(100, activation='relu'), # increase number of neurons from 4 to 100 (for each layer)   tf.keras.layers.Dense(100, activation='relu'),   tf.keras.layers.Dense(100, activation='relu'), # add an extra layer   tf.keras.layers.Dense(1, activation='sigmoid') ])  # Compile the model model_3.compile(loss='binary_crossentropy',               optimizer=tf.keras.optimizers.Adam(),               metrics=[\"accuracy\"])  # Fit the model history_3 = model_3.fit(train_data,                         epochs=5,                         steps_per_epoch=len(train_data),                         validation_data=valid_data,                         validation_steps=len(valid_data)) <pre>Epoch 1/5\n47/47 [==============================] - 11s 201ms/step - loss: 2.8247 - accuracy: 0.6413 - val_loss: 0.6214 - val_accuracy: 0.7720\nEpoch 2/5\n47/47 [==============================] - 9s 195ms/step - loss: 0.6881 - accuracy: 0.7300 - val_loss: 0.5762 - val_accuracy: 0.7140\nEpoch 3/5\n47/47 [==============================] - 9s 196ms/step - loss: 1.2195 - accuracy: 0.6860 - val_loss: 0.6234 - val_accuracy: 0.7320\nEpoch 4/5\n47/47 [==============================] - 9s 194ms/step - loss: 0.5172 - accuracy: 0.7893 - val_loss: 0.5061 - val_accuracy: 0.7600\nEpoch 5/5\n47/47 [==============================] - 9s 192ms/step - loss: 0.5041 - accuracy: 0.7833 - val_loss: 0.4380 - val_accuracy: 0.8000\n</pre> <p>Woah! Looks like our model is learning again. It got ~70% accuracy on the training set and ~70% accuracy on the validation set.</p> <p>How does the architecute look?</p> In\u00a0[19]: Copied! <pre># Check out model_3 architecture\nmodel_3.summary()\n</pre> # Check out model_3 architecture model_3.summary() <pre>Model: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n flatten_2 (Flatten)         (None, 150528)            0         \n                                                                 \n dense_4 (Dense)             (None, 100)               15052900  \n                                                                 \n dense_5 (Dense)             (None, 100)               10100     \n                                                                 \n dense_6 (Dense)             (None, 100)               10100     \n                                                                 \n dense_7 (Dense)             (None, 1)                 101       \n                                                                 \n=================================================================\nTotal params: 15,073,201\nTrainable params: 15,073,201\nNon-trainable params: 0\n_________________________________________________________________\n</pre> <p>My gosh, the number of trainable parameters has increased even more than <code>model_2</code>. And even with close to 500x (~15,000,000 vs. ~31,000) more trainable parameters, <code>model_3</code> still doesn't out perform <code>model_1</code>.</p> <p>This goes to show the power of convolutional neural networks and their ability to learn patterns despite using less parameters.</p> In\u00a0[20]: Copied! <pre># import zipfile\n\n# # Download zip file of pizza_steak images\n# !wget https://storage.googleapis.com/ztm_tf_course/food_vision/pizza_steak.zip\n\n# # Unzip the downloaded file\n# zip_ref = zipfile.ZipFile(\"pizza_steak.zip\", \"r\")\n# zip_ref.extractall()\n# zip_ref.close()\n</pre> # import zipfile  # # Download zip file of pizza_steak images # !wget https://storage.googleapis.com/ztm_tf_course/food_vision/pizza_steak.zip  # # Unzip the downloaded file # zip_ref = zipfile.ZipFile(\"pizza_steak.zip\", \"r\") # zip_ref.extractall() # zip_ref.close() In\u00a0[21]: Copied! <pre># Visualize data (requires function 'view_random_image' above)\nplt.figure()\nplt.subplot(1, 2, 1)\nsteak_img = view_random_image(\"pizza_steak/train/\", \"steak\")\nplt.subplot(1, 2, 2)\npizza_img = view_random_image(\"pizza_steak/train/\", \"pizza\")\n</pre> # Visualize data (requires function 'view_random_image' above) plt.figure() plt.subplot(1, 2, 1) steak_img = view_random_image(\"pizza_steak/train/\", \"steak\") plt.subplot(1, 2, 2) pizza_img = view_random_image(\"pizza_steak/train/\", \"pizza\") <pre>Image shape: (512, 512, 3)\nImage shape: (512, 512, 3)\n</pre> In\u00a0[22]: Copied! <pre># Define training and test directory paths\ntrain_dir = \"pizza_steak/train/\"\ntest_dir = \"pizza_steak/test/\"\n</pre> # Define training and test directory paths train_dir = \"pizza_steak/train/\" test_dir = \"pizza_steak/test/\" <p>Our next step is to turn our data into batches.</p> <p>A batch is a small subset of the dataset a model looks at during training. For example, rather than looking at 10,000 images at one time and trying to figure out the patterns, a model might only look at 32 images at a time.</p> <p>It does this for a couple of reasons:</p> <ul> <li>10,000 images (or more) might not fit into the memory of your processor (GPU).</li> <li>Trying to learn the patterns in 10,000 images in one hit could result in the model not being able to learn very well.</li> </ul> <p>Why 32?</p> <p>A batch size of 32 is good for your health.</p> <p>No seriously, there are many different batch sizes you could use but 32 has proven to be very effective in many different use cases and is often the default for many data preprocessing functions.</p> <p>To turn our data into batches, we'll first create an instance of <code>ImageDataGenerator</code> for each of our datasets.</p> In\u00a0[23]: Copied! <pre># Create train and test data generators and rescale the data \nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ntrain_datagen = ImageDataGenerator(rescale=1/255.)\ntest_datagen = ImageDataGenerator(rescale=1/255.)\n</pre> # Create train and test data generators and rescale the data  from tensorflow.keras.preprocessing.image import ImageDataGenerator train_datagen = ImageDataGenerator(rescale=1/255.) test_datagen = ImageDataGenerator(rescale=1/255.) <p>The <code>ImageDataGenerator</code> class helps us prepare our images into batches as well as perform transformations on them as they get loaded into the model.</p> <p>You might've noticed the <code>rescale</code> parameter. This is one example of the transformations we're doing.</p> <p>Remember from before how we imported an image and it's pixel values were between 0 and 255?</p> <p>The <code>rescale</code> parameter, along with <code>1/255.</code> is like saying \"divide all of the pixel values by 255\". This results in all of the image being imported and their pixel values being normalized (converted to be between 0 and 1).</p> <p>\ud83d\udd11 Note: For more transformation options such as data augmentation (we'll see this later), refer to the <code>ImageDataGenerator</code> documentation.</p> <p>Now we've got a couple of ImageDataGenerator instances, we can load our images from their respective directories using the <code>flow_from_directory</code> method.</p> In\u00a0[24]: Copied! <pre># Turn it into batches\ntrain_data = train_datagen.flow_from_directory(directory=train_dir,\n                                               target_size=(224, 224),\n                                               class_mode='binary',\n                                               batch_size=32)\n\ntest_data = test_datagen.flow_from_directory(directory=test_dir,\n                                             target_size=(224, 224),\n                                             class_mode='binary',\n                                             batch_size=32)\n</pre> # Turn it into batches train_data = train_datagen.flow_from_directory(directory=train_dir,                                                target_size=(224, 224),                                                class_mode='binary',                                                batch_size=32)  test_data = test_datagen.flow_from_directory(directory=test_dir,                                              target_size=(224, 224),                                              class_mode='binary',                                              batch_size=32) <pre>Found 1500 images belonging to 2 classes.\nFound 500 images belonging to 2 classes.\n</pre> <p>Wonderful! Looks like our training dataset has 1500 images belonging to 2 classes (pizza and steak) and our test dataset has 500 images also belonging to 2 classes.</p> <p>Some things to here:</p> <ul> <li>Due to how our directories are structured, the classes get inferred by the subdirectory names in <code>train_dir</code> and <code>test_dir</code>.</li> <li>The <code>target_size</code> parameter defines the input size of our images in <code>(height, width)</code> format.</li> <li>The <code>class_mode</code> value of <code>'binary'</code> defines our classification problem type. If we had more than two classes, we would use <code>'categorical'</code>.</li> <li>The <code>batch_size</code> defines how many images will be in each batch, we've used 32 which is the same as the default.</li> </ul> <p>We can take a look at our batched images and labels by inspecting the <code>train_data</code> object.</p> In\u00a0[25]: Copied! <pre># Get a sample of the training data batch \nimages, labels = train_data.next() # get the 'next' batch of images/labels\nlen(images), len(labels)\n</pre> # Get a sample of the training data batch  images, labels = train_data.next() # get the 'next' batch of images/labels len(images), len(labels) Out[25]: <pre>(32, 32)</pre> <p>Wonderful, it seems our images and labels are in batches of 32.</p> <p>Let's see what the images look like.</p> In\u00a0[26]: Copied! <pre># Get the first two images\nimages[:2], images[0].shape\n</pre> # Get the first two images images[:2], images[0].shape Out[26]: <pre>(array([[[[0.47058827, 0.40784317, 0.34509805],\n          [0.4784314 , 0.427451  , 0.3647059 ],\n          [0.48627454, 0.43529415, 0.37254903],\n          ...,\n          [0.8313726 , 0.70980394, 0.48627454],\n          [0.8431373 , 0.73333335, 0.5372549 ],\n          [0.87843144, 0.7725491 , 0.5882353 ]],\n \n         [[0.50980395, 0.427451  , 0.36078432],\n          [0.5058824 , 0.42352945, 0.35686275],\n          [0.5137255 , 0.4431373 , 0.3647059 ],\n          ...,\n          [0.82745105, 0.7058824 , 0.48235297],\n          [0.82745105, 0.70980394, 0.5058824 ],\n          [0.8431373 , 0.73333335, 0.5372549 ]],\n \n         [[0.5254902 , 0.427451  , 0.34901962],\n          [0.5372549 , 0.43921572, 0.36078432],\n          [0.5372549 , 0.45098042, 0.36078432],\n          ...,\n          [0.82745105, 0.7019608 , 0.4784314 ],\n          [0.82745105, 0.7058824 , 0.49411768],\n          [0.8352942 , 0.7176471 , 0.5137255 ]],\n \n         ...,\n \n         [[0.77647066, 0.5647059 , 0.2901961 ],\n          [0.7803922 , 0.53333336, 0.22352943],\n          [0.79215693, 0.5176471 , 0.18039216],\n          ...,\n          [0.30588236, 0.2784314 , 0.24705884],\n          [0.24705884, 0.23137257, 0.19607845],\n          [0.2784314 , 0.27450982, 0.25490198]],\n \n         [[0.7843138 , 0.57254905, 0.29803923],\n          [0.79215693, 0.54509807, 0.24313727],\n          [0.8000001 , 0.5254902 , 0.18823531],\n          ...,\n          [0.2627451 , 0.23529413, 0.20392159],\n          [0.24313727, 0.227451  , 0.19215688],\n          [0.26666668, 0.2627451 , 0.24313727]],\n \n         [[0.7960785 , 0.59607846, 0.3372549 ],\n          [0.7960785 , 0.5647059 , 0.26666668],\n          [0.81568635, 0.54901963, 0.22352943],\n          ...,\n          [0.23529413, 0.19607845, 0.16078432],\n          [0.3019608 , 0.26666668, 0.24705884],\n          [0.26666668, 0.2509804 , 0.24705884]]],\n \n \n        [[[0.38823533, 0.4666667 , 0.36078432],\n          [0.3921569 , 0.46274513, 0.36078432],\n          [0.38431376, 0.454902  , 0.36078432],\n          ...,\n          [0.5294118 , 0.627451  , 0.54509807],\n          [0.5294118 , 0.627451  , 0.54509807],\n          [0.5411765 , 0.6392157 , 0.5568628 ]],\n \n         [[0.38431376, 0.454902  , 0.3529412 ],\n          [0.3921569 , 0.46274513, 0.36078432],\n          [0.39607847, 0.4666667 , 0.37254903],\n          ...,\n          [0.54509807, 0.6431373 , 0.5686275 ],\n          [0.5529412 , 0.6509804 , 0.5764706 ],\n          [0.5647059 , 0.6627451 , 0.5882353 ]],\n \n         [[0.3921569 , 0.46274513, 0.36078432],\n          [0.38431376, 0.454902  , 0.3529412 ],\n          [0.4039216 , 0.47450984, 0.3803922 ],\n          ...,\n          [0.5764706 , 0.67058825, 0.6156863 ],\n          [0.5647059 , 0.6666667 , 0.6156863 ],\n          [0.5647059 , 0.6666667 , 0.6156863 ]],\n \n         ...,\n \n         [[0.47058827, 0.5647059 , 0.4784314 ],\n          [0.4784314 , 0.5764706 , 0.4901961 ],\n          [0.48235297, 0.5803922 , 0.49803925],\n          ...,\n          [0.39607847, 0.42352945, 0.3019608 ],\n          [0.37647063, 0.40000004, 0.2901961 ],\n          [0.3803922 , 0.4039216 , 0.3019608 ]],\n \n         [[0.45098042, 0.5529412 , 0.454902  ],\n          [0.46274513, 0.5647059 , 0.4666667 ],\n          [0.47058827, 0.57254905, 0.47450984],\n          ...,\n          [0.40784317, 0.43529415, 0.3137255 ],\n          [0.39607847, 0.41960788, 0.31764707],\n          [0.38823533, 0.40784317, 0.31764707]],\n \n         [[0.47450984, 0.5764706 , 0.47058827],\n          [0.47058827, 0.57254905, 0.4666667 ],\n          [0.46274513, 0.5647059 , 0.4666667 ],\n          ...,\n          [0.4039216 , 0.427451  , 0.31764707],\n          [0.3921569 , 0.4156863 , 0.3137255 ],\n          [0.4039216 , 0.42352945, 0.3372549 ]]]], dtype=float32),\n (224, 224, 3))</pre> <p>Due to our <code>rescale</code> parameter, the images are now in <code>(224, 224, 3)</code> shape tensors with values between 0 and 1.</p> <p>How about the labels?</p> In\u00a0[27]: Copied! <pre># View the first batch of labels\nlabels\n</pre> # View the first batch of labels labels Out[27]: <pre>array([1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1.,\n       1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n      dtype=float32)</pre> <p>Due to the <code>class_mode</code> parameter being <code>'binary'</code> our labels are either <code>0</code> (pizza) or <code>1</code> (steak).</p> <p>Now that our data is ready, our model is going to try and figure out the patterns between the image tensors and the labels.</p> In\u00a0[28]: Copied! <pre># Make the creating of our model a little easier\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Activation\nfrom tensorflow.keras import Sequential\n</pre> # Make the creating of our model a little easier from tensorflow.keras.optimizers import Adam from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Activation from tensorflow.keras import Sequential In\u00a0[29]: Copied! <pre># Create the model (this can be our baseline, a 3 layer Convolutional Neural Network)\nmodel_4 = Sequential([\n  Conv2D(filters=10, \n         kernel_size=3, \n         strides=1,\n         padding='valid',\n         activation='relu', \n         input_shape=(224, 224, 3)), # input layer (specify input shape)\n  Conv2D(10, 3, activation='relu'),\n  Conv2D(10, 3, activation='relu'),\n  Flatten(),\n  Dense(1, activation='sigmoid') # output layer (specify output shape)\n])\n</pre> # Create the model (this can be our baseline, a 3 layer Convolutional Neural Network) model_4 = Sequential([   Conv2D(filters=10,           kernel_size=3,           strides=1,          padding='valid',          activation='relu',           input_shape=(224, 224, 3)), # input layer (specify input shape)   Conv2D(10, 3, activation='relu'),   Conv2D(10, 3, activation='relu'),   Flatten(),   Dense(1, activation='sigmoid') # output layer (specify output shape) ]) <p>Great! We've got a simple convolutional neural network architecture ready to go.</p> <p>And it follows the typical CNN structure of:</p> <pre><code>Input -&gt; Conv + ReLU layers (non-linearities) -&gt; Pooling layer -&gt; Fully connected (dense layer) as Output\n</code></pre> <p>Let's discuss some of the components of the <code>Conv2D</code> layer:</p> <ul> <li>The \"<code>2D</code>\" means our inputs are two dimensional (height and width), even though they have 3 colour channels, the convolutions are run on each channel invididually.</li> <li><code>filters</code> - these are the number of \"feature extractors\" that will be moving over our images.</li> <li><code>kernel_size</code> - the size of our filters, for example, a <code>kernel_size</code> of <code>(3, 3)</code> (or just 3) will mean each filter will have the size 3x3, meaning it will look at a space of 3x3 pixels each time. The smaller the kernel, the more fine-grained features it will extract.</li> <li><code>stride</code> - the number of pixels a <code>filter</code> will move across as it covers the image. A <code>stride</code> of 1 means the filter moves across each pixel 1 by 1. A <code>stride</code> of 2 means it moves 2 pixels at a time.</li> <li><code>padding</code> - this can be either <code>'same'</code> or <code>'valid'</code>, <code>'same'</code> adds zeros the to outside of the image so the resulting output of the convolutional layer is the same as the input, where as <code>'valid'</code> (default) cuts off excess pixels where the <code>filter</code> doesn't fit (e.g. 224 pixels wide divided by a kernel size of 3 (224/3 = 74.6) means a single pixel will get cut off the end.</li> </ul> <p>What's a \"feature\"?</p> <p>A feature can be considered any significant part of an image. For example, in our case, a feature might be the circular shape of pizza. Or the rough edges on the outside of a steak.</p> <p>It's important to note that these features are not defined by us, instead, the model learns them as it applies different filters across the image.</p> <p>\ud83d\udcd6 Resources: For a great demonstration of these in action, be sure to spend some time going through the following:</p> <ul> <li>CNN Explainer Webpage - a great visual overview of many of the concepts we're replicating here with code.</li> <li>A guide to convolutional arithmetic for deep learning - a phenomenal introduction to the math going on behind the scenes of a convolutional neural network.</li> <li>For a great explanation of padding, see this Stack Overflow answer.</li> </ul> <p>Now our model is ready, let's compile it.</p> In\u00a0[30]: Copied! <pre># Compile the model\nmodel_4.compile(loss='binary_crossentropy',\n                optimizer=Adam(),\n                metrics=['accuracy'])\n</pre> # Compile the model model_4.compile(loss='binary_crossentropy',                 optimizer=Adam(),                 metrics=['accuracy']) <p>Since we're working on a binary classification problem (pizza vs. steak), the <code>loss</code> function we're using is <code>'binary_crossentropy'</code>, if it was mult-iclass, we might use something like <code>'categorical_crossentropy'</code>.</p> <p>Adam with all the default settings is our optimizer and our evaluation metric is accuracy.</p> In\u00a0[31]: Copied! <pre># Check lengths of training and test data generators\nlen(train_data), len(test_data)\n</pre> # Check lengths of training and test data generators len(train_data), len(test_data) Out[31]: <pre>(47, 16)</pre> In\u00a0[32]: Copied! <pre># Fit the model\nhistory_4 = model_4.fit(train_data,\n                        epochs=5,\n                        steps_per_epoch=len(train_data),\n                        validation_data=test_data,\n                        validation_steps=len(test_data))\n</pre> # Fit the model history_4 = model_4.fit(train_data,                         epochs=5,                         steps_per_epoch=len(train_data),                         validation_data=test_data,                         validation_steps=len(test_data)) <pre>Epoch 1/5\n47/47 [==============================] - 11s 198ms/step - loss: 0.6262 - accuracy: 0.6733 - val_loss: 0.4172 - val_accuracy: 0.8140\nEpoch 2/5\n47/47 [==============================] - 9s 192ms/step - loss: 0.4260 - accuracy: 0.8253 - val_loss: 0.3766 - val_accuracy: 0.8420\nEpoch 3/5\n47/47 [==============================] - 9s 191ms/step - loss: 0.2717 - accuracy: 0.9100 - val_loss: 0.3451 - val_accuracy: 0.8580\nEpoch 4/5\n47/47 [==============================] - 9s 191ms/step - loss: 0.1141 - accuracy: 0.9720 - val_loss: 0.3705 - val_accuracy: 0.8440\nEpoch 5/5\n47/47 [==============================] - 9s 191ms/step - loss: 0.0478 - accuracy: 0.9900 - val_loss: 0.5217 - val_accuracy: 0.8220\n</pre> <p>Oh yeah! Looks like our model is learning something.</p> <p>Let's check out its training curves.</p> In\u00a0[33]: Copied! <pre># Plot the training curves\nimport pandas as pd\npd.DataFrame(history_4.history).plot(figsize=(10, 7));\n</pre> # Plot the training curves import pandas as pd pd.DataFrame(history_4.history).plot(figsize=(10, 7)); <p>Hmm, judging by our loss curves, it looks like our model is overfitting the training dataset.</p> <p>\ud83d\udd11 Note: When a model's validation loss starts to increase, it's likely that it's overfitting the training dataset. This means, it's learning the patterns in the training dataset too well and thus its ability to generalize to unseen data will be diminished.</p> <p>To further inspect our model's training performance, let's separate the accuracy and loss curves.</p> In\u00a0[34]: Copied! <pre># Plot the validation and training data separately\ndef plot_loss_curves(history):\n  \"\"\"\n  Returns separate loss curves for training and validation metrics.\n  \"\"\" \n  loss = history.history['loss']\n  val_loss = history.history['val_loss']\n\n  accuracy = history.history['accuracy']\n  val_accuracy = history.history['val_accuracy']\n\n  epochs = range(len(history.history['loss']))\n\n  # Plot loss\n  plt.plot(epochs, loss, label='training_loss')\n  plt.plot(epochs, val_loss, label='val_loss')\n  plt.title('Loss')\n  plt.xlabel('Epochs')\n  plt.legend()\n\n  # Plot accuracy\n  plt.figure()\n  plt.plot(epochs, accuracy, label='training_accuracy')\n  plt.plot(epochs, val_accuracy, label='val_accuracy')\n  plt.title('Accuracy')\n  plt.xlabel('Epochs')\n  plt.legend();\n</pre> # Plot the validation and training data separately def plot_loss_curves(history):   \"\"\"   Returns separate loss curves for training and validation metrics.   \"\"\"    loss = history.history['loss']   val_loss = history.history['val_loss']    accuracy = history.history['accuracy']   val_accuracy = history.history['val_accuracy']    epochs = range(len(history.history['loss']))    # Plot loss   plt.plot(epochs, loss, label='training_loss')   plt.plot(epochs, val_loss, label='val_loss')   plt.title('Loss')   plt.xlabel('Epochs')   plt.legend()    # Plot accuracy   plt.figure()   plt.plot(epochs, accuracy, label='training_accuracy')   plt.plot(epochs, val_accuracy, label='val_accuracy')   plt.title('Accuracy')   plt.xlabel('Epochs')   plt.legend(); In\u00a0[35]: Copied! <pre># Check out the loss curves of model_4\nplot_loss_curves(history_4)\n</pre> # Check out the loss curves of model_4 plot_loss_curves(history_4) <p>The ideal position for these two curves is to follow each other. If anything, the validation curve should be slightly under the training curve. If there's a large gap between the training curve and validation curve, it means your model is probably overfitting.</p> In\u00a0[36]: Copied! <pre># Check out our model's architecture\nmodel_4.summary()\n</pre> # Check out our model's architecture model_4.summary() <pre>Model: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_4 (Conv2D)           (None, 222, 222, 10)      280       \n                                                                 \n conv2d_5 (Conv2D)           (None, 220, 220, 10)      910       \n                                                                 \n conv2d_6 (Conv2D)           (None, 218, 218, 10)      910       \n                                                                 \n flatten_3 (Flatten)         (None, 475240)            0         \n                                                                 \n dense_8 (Dense)             (None, 1)                 475241    \n                                                                 \n=================================================================\nTotal params: 477,341\nTrainable params: 477,341\nNon-trainable params: 0\n_________________________________________________________________\n</pre> In\u00a0[37]: Copied! <pre># Create the model (this can be our baseline, a 3 layer Convolutional Neural Network)\nmodel_5 = Sequential([\n  Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)),\n  MaxPool2D(pool_size=2), # reduce number of features by half\n  Conv2D(10, 3, activation='relu'),\n  MaxPool2D(),\n  Conv2D(10, 3, activation='relu'),\n  MaxPool2D(),\n  Flatten(),\n  Dense(1, activation='sigmoid')\n])\n</pre> # Create the model (this can be our baseline, a 3 layer Convolutional Neural Network) model_5 = Sequential([   Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)),   MaxPool2D(pool_size=2), # reduce number of features by half   Conv2D(10, 3, activation='relu'),   MaxPool2D(),   Conv2D(10, 3, activation='relu'),   MaxPool2D(),   Flatten(),   Dense(1, activation='sigmoid') ]) <p>Woah, we've got another layer type we haven't seen before.</p> <p>If convolutional layers learn the features of an image you can think of a Max Pooling layer as figuring out the most important of those features. We'll see this an example of this in a moment.</p> In\u00a0[38]: Copied! <pre># Compile model (same as model_4)\nmodel_5.compile(loss='binary_crossentropy',\n                optimizer=Adam(),\n                metrics=['accuracy'])\n</pre> # Compile model (same as model_4) model_5.compile(loss='binary_crossentropy',                 optimizer=Adam(),                 metrics=['accuracy']) In\u00a0[39]: Copied! <pre># Fit the model\nhistory_5 = model_5.fit(train_data,\n                        epochs=5,\n                        steps_per_epoch=len(train_data),\n                        validation_data=test_data,\n                        validation_steps=len(test_data))\n</pre> # Fit the model history_5 = model_5.fit(train_data,                         epochs=5,                         steps_per_epoch=len(train_data),                         validation_data=test_data,                         validation_steps=len(test_data)) <pre>Epoch 1/5\n47/47 [==============================] - 11s 196ms/step - loss: 0.6482 - accuracy: 0.6240 - val_loss: 0.5451 - val_accuracy: 0.7100\nEpoch 2/5\n47/47 [==============================] - 9s 190ms/step - loss: 0.4537 - accuracy: 0.8013 - val_loss: 0.3536 - val_accuracy: 0.8380\nEpoch 3/5\n47/47 [==============================] - 9s 192ms/step - loss: 0.4161 - accuracy: 0.8233 - val_loss: 0.3578 - val_accuracy: 0.8460\nEpoch 4/5\n47/47 [==============================] - 9s 192ms/step - loss: 0.3903 - accuracy: 0.8353 - val_loss: 0.3509 - val_accuracy: 0.8480\nEpoch 5/5\n47/47 [==============================] - 9s 192ms/step - loss: 0.3567 - accuracy: 0.8513 - val_loss: 0.3200 - val_accuracy: 0.8700\n</pre> <p>Okay, it looks like our model with max pooling (<code>model_5</code>) is performing worse on the training set but better on the validation set.</p> <p>Before we checkout its training curves, let's check out its architecture.</p> In\u00a0[40]: Copied! <pre># Check out the model architecture\nmodel_5.summary()\n</pre> # Check out the model architecture model_5.summary() <pre>Model: \"sequential_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_7 (Conv2D)           (None, 222, 222, 10)      280       \n                                                                 \n max_pooling2d_2 (MaxPooling  (None, 111, 111, 10)     0         \n 2D)                                                             \n                                                                 \n conv2d_8 (Conv2D)           (None, 109, 109, 10)      910       \n                                                                 \n max_pooling2d_3 (MaxPooling  (None, 54, 54, 10)       0         \n 2D)                                                             \n                                                                 \n conv2d_9 (Conv2D)           (None, 52, 52, 10)        910       \n                                                                 \n max_pooling2d_4 (MaxPooling  (None, 26, 26, 10)       0         \n 2D)                                                             \n                                                                 \n flatten_4 (Flatten)         (None, 6760)              0         \n                                                                 \n dense_9 (Dense)             (None, 1)                 6761      \n                                                                 \n=================================================================\nTotal params: 8,861\nTrainable params: 8,861\nNon-trainable params: 0\n_________________________________________________________________\n</pre> <p>Do you notice what's going on here with the output shape in each <code>MaxPooling2D</code> layer?</p> <p>It gets halved each time. This is effectively the <code>MaxPooling2D</code> layer taking the outputs of each Conv2D layer and saying \"I only want the most important features, get rid of the rest\".</p> <p>The bigger the <code>pool_size</code> parameter, the more the max pooling layer will squeeze the features out of the image. However, too big and the model might not be able to learn anything.</p> <p>The results of this pooling are seen in a major reduction of total trainable parameters (8,861 in <code>model_5</code> and 477,431 in <code>model_4</code>).</p> <p>Time to check out the loss curves.</p> In\u00a0[41]: Copied! <pre># Plot loss curves of model_5 results\nplot_loss_curves(history_5)\n</pre> # Plot loss curves of model_5 results plot_loss_curves(history_5) <p>Nice! We can see the training curves get a lot closer to eachother. However, our the validation loss looks to start increasing towards the end and in turn potentially leading to overfitting.</p> <p>Time to dig into our bag of tricks and try another method of overfitting prevention, data augmentation.</p> <p>First, we'll see how it's done with code then we'll discuss what it's doing.</p> <p>To implement data augmentation, we'll have to reinstantiate our <code>ImageDataGenerator</code> instances.</p> In\u00a0[42]: Copied! <pre># Create ImageDataGenerator training instance with data augmentation\ntrain_datagen_augmented = ImageDataGenerator(rescale=1/255.,\n                                             rotation_range=20, # rotate the image slightly between 0 and 20 degrees (note: this is an int not a float)\n                                             shear_range=0.2, # shear the image\n                                             zoom_range=0.2, # zoom into the image\n                                             width_shift_range=0.2, # shift the image width ways\n                                             height_shift_range=0.2, # shift the image height ways\n                                             horizontal_flip=True) # flip the image on the horizontal axis\n\n# Create ImageDataGenerator training instance without data augmentation\ntrain_datagen = ImageDataGenerator(rescale=1/255.) \n\n# Create ImageDataGenerator test instance without data augmentation\ntest_datagen = ImageDataGenerator(rescale=1/255.)\n</pre> # Create ImageDataGenerator training instance with data augmentation train_datagen_augmented = ImageDataGenerator(rescale=1/255.,                                              rotation_range=20, # rotate the image slightly between 0 and 20 degrees (note: this is an int not a float)                                              shear_range=0.2, # shear the image                                              zoom_range=0.2, # zoom into the image                                              width_shift_range=0.2, # shift the image width ways                                              height_shift_range=0.2, # shift the image height ways                                              horizontal_flip=True) # flip the image on the horizontal axis  # Create ImageDataGenerator training instance without data augmentation train_datagen = ImageDataGenerator(rescale=1/255.)   # Create ImageDataGenerator test instance without data augmentation test_datagen = ImageDataGenerator(rescale=1/255.) <p>\ud83e\udd14 Question: What's data augmentation?</p> <p>Data augmentation is the process of altering our training data, leading to it having more diversity and in turn allowing our models to learn more generalizable patterns. Altering might mean adjusting the rotation of an image, flipping it, cropping it or something similar.</p> <p>Doing this simulates the kind of data a model might be used on in the real world.</p> <p>If we're building a pizza vs. steak application, not all of the images our users take might be in similar setups to our training data. Using data augmentation gives us another way to prevent overfitting and in turn make our model more generalizable.</p> <p>\ud83d\udd11 Note: Data augmentation is usally only performed on the training data. Using the <code>ImageDataGenerator</code> built-in data augmentation parameters our images are left as they are in the directories but are randomly manipulated when loaded into the model.</p> In\u00a0[43]: Copied! <pre># Import data and augment it from training directory\nprint(\"Augmented training images:\")\ntrain_data_augmented = train_datagen_augmented.flow_from_directory(train_dir,\n                                                                   target_size=(224, 224),\n                                                                   batch_size=32,\n                                                                   class_mode='binary',\n                                                                   shuffle=False) # Don't shuffle for demonstration purposes, usually a good thing to shuffle\n\n# Create non-augmented data batches\nprint(\"Non-augmented training images:\")\ntrain_data = train_datagen.flow_from_directory(train_dir,\n                                               target_size=(224, 224),\n                                               batch_size=32,\n                                               class_mode='binary',\n                                               shuffle=False) # Don't shuffle for demonstration purposes\n\nprint(\"Unchanged test images:\")\ntest_data = test_datagen.flow_from_directory(test_dir,\n                                             target_size=(224, 224),\n                                             batch_size=32,\n                                             class_mode='binary')\n</pre> # Import data and augment it from training directory print(\"Augmented training images:\") train_data_augmented = train_datagen_augmented.flow_from_directory(train_dir,                                                                    target_size=(224, 224),                                                                    batch_size=32,                                                                    class_mode='binary',                                                                    shuffle=False) # Don't shuffle for demonstration purposes, usually a good thing to shuffle  # Create non-augmented data batches print(\"Non-augmented training images:\") train_data = train_datagen.flow_from_directory(train_dir,                                                target_size=(224, 224),                                                batch_size=32,                                                class_mode='binary',                                                shuffle=False) # Don't shuffle for demonstration purposes  print(\"Unchanged test images:\") test_data = test_datagen.flow_from_directory(test_dir,                                              target_size=(224, 224),                                              batch_size=32,                                              class_mode='binary') <pre>Augmented training images:\nFound 1500 images belonging to 2 classes.\nNon-augmented training images:\nFound 1500 images belonging to 2 classes.\nUnchanged test images:\nFound 500 images belonging to 2 classes.\n</pre> <p>Better than talk about data augmentation, how about we see it?</p> <p>(remember our motto? visualize, visualize, visualize...)</p> In\u00a0[44]: Copied! <pre># Get data batch samples\nimages, labels = train_data.next()\naugmented_images, augmented_labels = train_data_augmented.next() # Note: labels aren't augmented, they stay the same\n</pre> # Get data batch samples images, labels = train_data.next() augmented_images, augmented_labels = train_data_augmented.next() # Note: labels aren't augmented, they stay the same In\u00a0[45]: Copied! <pre># Show original image and augmented image\nrandom_number = random.randint(0, 31) # we're making batches of size 32, so we'll get a random instance\nplt.imshow(images[random_number])\nplt.title(f\"Original image\")\nplt.axis(False)\nplt.figure()\nplt.imshow(augmented_images[random_number])\nplt.title(f\"Augmented image\")\nplt.axis(False);\n</pre> # Show original image and augmented image random_number = random.randint(0, 31) # we're making batches of size 32, so we'll get a random instance plt.imshow(images[random_number]) plt.title(f\"Original image\") plt.axis(False) plt.figure() plt.imshow(augmented_images[random_number]) plt.title(f\"Augmented image\") plt.axis(False); <p>After going through a sample of original and augmented images, you can start to see some of the example transformations on the training images.</p> <p>Notice how some of the augmented images look like slightly warped versions of the original image. This means our model will be forced to try and learn patterns in less-than-perfect images, which is often the case when using real-world images.</p> <p>\ud83e\udd14 Question: Should I use data augmentation? And how much should I augment?</p> <p>Data augmentation is a way to try and prevent a model overfitting. If your model is overfiting (e.g. the validation loss keeps increasing), you may want to try using data augmentation.</p> <p>As for how much to data augment, there's no set practice for this. Best to check out the options in the <code>ImageDataGenerator</code> class and think about how a model in your use case might benefit from some data augmentation.</p> <p>Now we've got augmented data, let's try and refit a model on it and see how it affects training.</p> <p>We'll use the same model as <code>model_5</code>.</p> In\u00a0[46]: Copied! <pre># Create the model (same as model_5)\nmodel_6 = Sequential([\n  Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)),\n  MaxPool2D(pool_size=2), # reduce number of features by half\n  Conv2D(10, 3, activation='relu'),\n  MaxPool2D(),\n  Conv2D(10, 3, activation='relu'),\n  MaxPool2D(),\n  Flatten(),\n  Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel_6.compile(loss='binary_crossentropy',\n                optimizer=Adam(),\n                metrics=['accuracy'])\n\n# Fit the model\nhistory_6 = model_6.fit(train_data_augmented, # changed to augmented training data\n                        epochs=5,\n                        steps_per_epoch=len(train_data_augmented),\n                        validation_data=test_data,\n                        validation_steps=len(test_data))\n</pre> # Create the model (same as model_5) model_6 = Sequential([   Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)),   MaxPool2D(pool_size=2), # reduce number of features by half   Conv2D(10, 3, activation='relu'),   MaxPool2D(),   Conv2D(10, 3, activation='relu'),   MaxPool2D(),   Flatten(),   Dense(1, activation='sigmoid') ])  # Compile the model model_6.compile(loss='binary_crossentropy',                 optimizer=Adam(),                 metrics=['accuracy'])  # Fit the model history_6 = model_6.fit(train_data_augmented, # changed to augmented training data                         epochs=5,                         steps_per_epoch=len(train_data_augmented),                         validation_data=test_data,                         validation_steps=len(test_data)) <pre>Epoch 1/5\n47/47 [==============================] - 24s 472ms/step - loss: 0.8005 - accuracy: 0.4447 - val_loss: 0.7007 - val_accuracy: 0.5000\nEpoch 2/5\n47/47 [==============================] - 22s 475ms/step - loss: 0.6981 - accuracy: 0.4267 - val_loss: 0.6918 - val_accuracy: 0.5140\nEpoch 3/5\n47/47 [==============================] - 22s 474ms/step - loss: 0.6938 - accuracy: 0.5033 - val_loss: 0.6896 - val_accuracy: 0.5100\nEpoch 4/5\n47/47 [==============================] - 22s 477ms/step - loss: 0.6886 - accuracy: 0.5120 - val_loss: 0.6732 - val_accuracy: 0.5240\nEpoch 5/5\n47/47 [==============================] - 22s 477ms/step - loss: 0.7195 - accuracy: 0.5187 - val_loss: 0.6886 - val_accuracy: 0.5360\n</pre> <p>\ud83e\udd14 Question: Why didn't our model get very good results on the training set to begin with?</p> <p>It's because when we created <code>train_data_augmented</code> we turned off data shuffling using <code>shuffle=False</code> which means our model only sees a batch of a single kind of images at a time.</p> <p>For example, the pizza class gets loaded in first because it's the first class. Thus it's performance is measured on only a single class rather than both classes. The validation data performance improves steadily because it contains shuffled data.</p> <p>Since we only set <code>shuffle=False</code> for demonstration purposes (so we could plot the same augmented and non-augmented image), we can fix this by setting <code>shuffle=True</code> on future data generators.</p> <p>You may have also noticed each epoch taking longer when training with augmented data compared to when training with non-augmented data (~25s per epoch vs. ~10s per epoch).</p> <p>This is because the <code>ImageDataGenerator</code> instance augments the data as it's loaded into the model. The benefit of this is that it leaves the original images unchanged. The downside is that it takes longer to load them in.</p> <p>\ud83d\udd11 Note: One possible method to speed up dataset manipulation would be to look into TensorFlow's parrallel reads and buffered prefecting options.</p> In\u00a0[47]: Copied! <pre># Check model's performance history training on augmented data\nplot_loss_curves(history_6)\n</pre> # Check model's performance history training on augmented data plot_loss_curves(history_6) <p>It seems our validation loss curve is heading in the right direction but it's a bit jumpy (the most ideal loss curve isn't too spiky but a smooth descent, however, a perfectly smooth loss curve is the equivalent of a fairytale).</p> <p>Let's see what happens when we shuffle the augmented training data.</p> In\u00a0[48]: Copied! <pre># Import data and augment it from directories\ntrain_data_augmented_shuffled = train_datagen_augmented.flow_from_directory(train_dir,\n                                                                            target_size=(224, 224),\n                                                                            batch_size=32,\n                                                                            class_mode='binary',\n                                                                            shuffle=True) # Shuffle data (default)\n</pre> # Import data and augment it from directories train_data_augmented_shuffled = train_datagen_augmented.flow_from_directory(train_dir,                                                                             target_size=(224, 224),                                                                             batch_size=32,                                                                             class_mode='binary',                                                                             shuffle=True) # Shuffle data (default) <pre>Found 1500 images belonging to 2 classes.\n</pre> In\u00a0[49]: Copied! <pre># Create the model (same as model_5 and model_6)\nmodel_7 = Sequential([\n  Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)),\n  MaxPool2D(),\n  Conv2D(10, 3, activation='relu'),\n  MaxPool2D(),\n  Conv2D(10, 3, activation='relu'),\n  MaxPool2D(),\n  Flatten(),\n  Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel_7.compile(loss='binary_crossentropy',\n                optimizer=Adam(),\n                metrics=['accuracy'])\n\n# Fit the model\nhistory_7 = model_7.fit(train_data_augmented_shuffled, # now the augmented data is shuffled\n                        epochs=5,\n                        steps_per_epoch=len(train_data_augmented_shuffled),\n                        validation_data=test_data,\n                        validation_steps=len(test_data))\n</pre> # Create the model (same as model_5 and model_6) model_7 = Sequential([   Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)),   MaxPool2D(),   Conv2D(10, 3, activation='relu'),   MaxPool2D(),   Conv2D(10, 3, activation='relu'),   MaxPool2D(),   Flatten(),   Dense(1, activation='sigmoid') ])  # Compile the model model_7.compile(loss='binary_crossentropy',                 optimizer=Adam(),                 metrics=['accuracy'])  # Fit the model history_7 = model_7.fit(train_data_augmented_shuffled, # now the augmented data is shuffled                         epochs=5,                         steps_per_epoch=len(train_data_augmented_shuffled),                         validation_data=test_data,                         validation_steps=len(test_data)) <pre>Epoch 1/5\n47/47 [==============================] - 24s 474ms/step - loss: 0.6685 - accuracy: 0.5613 - val_loss: 0.5886 - val_accuracy: 0.6960\nEpoch 2/5\n47/47 [==============================] - 22s 478ms/step - loss: 0.5598 - accuracy: 0.7073 - val_loss: 0.4351 - val_accuracy: 0.7880\nEpoch 3/5\n47/47 [==============================] - 22s 477ms/step - loss: 0.4849 - accuracy: 0.7647 - val_loss: 0.4040 - val_accuracy: 0.8280\nEpoch 4/5\n47/47 [==============================] - 23s 487ms/step - loss: 0.4862 - accuracy: 0.7733 - val_loss: 0.4282 - val_accuracy: 0.8100\nEpoch 5/5\n47/47 [==============================] - 23s 486ms/step - loss: 0.4627 - accuracy: 0.7873 - val_loss: 0.3589 - val_accuracy: 0.8540\n</pre> In\u00a0[50]: Copied! <pre># Check model's performance history training on augmented data\nplot_loss_curves(history_7)\n</pre> # Check model's performance history training on augmented data plot_loss_curves(history_7) <p>Notice with <code>model_7</code> how the performance on the training dataset improves almost immediately compared to <code>model_6</code>. This is because we shuffled the training data as we passed it to the model using the parameter <code>shuffle=True</code> in the <code>flow_from_directory</code> method.</p> <p>This means the model was able to see examples of both pizza and steak images in each batch and in turn be evaluated on what it learned from both images rather than just one kind.</p> <p>Also, our loss curves look a little bit smoother with shuffled data (comparing <code>history_6</code> to <code>history_7</code>).</p> In\u00a0[51]: Copied! <pre># Create a CNN model (same as Tiny VGG but for binary classification - https://poloclub.github.io/cnn-explainer/ )\nmodel_8 = Sequential([\n  Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)), # same input shape as our images\n  Conv2D(10, 3, activation='relu'),\n  MaxPool2D(),\n  Conv2D(10, 3, activation='relu'),\n  Conv2D(10, 3, activation='relu'),\n  MaxPool2D(),\n  Flatten(),\n  Dense(1, activation='sigmoid')\n])\n\n# Compile the model\nmodel_8.compile(loss=\"binary_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])\n\n# Fit the model\nhistory_8 = model_8.fit(train_data_augmented_shuffled,\n                        epochs=5,\n                        steps_per_epoch=len(train_data_augmented_shuffled),\n                        validation_data=test_data,\n                        validation_steps=len(test_data))\n</pre> # Create a CNN model (same as Tiny VGG but for binary classification - https://poloclub.github.io/cnn-explainer/ ) model_8 = Sequential([   Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)), # same input shape as our images   Conv2D(10, 3, activation='relu'),   MaxPool2D(),   Conv2D(10, 3, activation='relu'),   Conv2D(10, 3, activation='relu'),   MaxPool2D(),   Flatten(),   Dense(1, activation='sigmoid') ])  # Compile the model model_8.compile(loss=\"binary_crossentropy\",                 optimizer=tf.keras.optimizers.Adam(),                 metrics=[\"accuracy\"])  # Fit the model history_8 = model_8.fit(train_data_augmented_shuffled,                         epochs=5,                         steps_per_epoch=len(train_data_augmented_shuffled),                         validation_data=test_data,                         validation_steps=len(test_data)) <pre>Epoch 1/5\n47/47 [==============================] - 25s 492ms/step - loss: 0.6345 - accuracy: 0.6267 - val_loss: 0.4843 - val_accuracy: 0.7500\nEpoch 2/5\n47/47 [==============================] - 23s 495ms/step - loss: 0.5293 - accuracy: 0.7327 - val_loss: 0.3829 - val_accuracy: 0.8400\nEpoch 3/5\n47/47 [==============================] - 23s 490ms/step - loss: 0.5296 - accuracy: 0.7360 - val_loss: 0.5177 - val_accuracy: 0.7160\nEpoch 4/5\n47/47 [==============================] - 22s 475ms/step - loss: 0.4971 - accuracy: 0.7560 - val_loss: 0.3477 - val_accuracy: 0.8600\nEpoch 5/5\n47/47 [==============================] - 22s 476ms/step - loss: 0.4670 - accuracy: 0.7927 - val_loss: 0.3511 - val_accuracy: 0.8540\n</pre> <p>\ud83d\udd11 Note: You might've noticed we used some slightly different code to build <code>model_8</code> as compared to <code>model_1</code>. This is because of the imports we did before, such as <code>from tensorflow.keras.layers import Conv2D</code> reduce the amount of code we had to write. Although the code is different, the architectures are the same.</p> In\u00a0[52]: Copied! <pre># Check model_1 architecture (same as model_8)\nmodel_1.summary()\n</pre> # Check model_1 architecture (same as model_8) model_1.summary() <pre>Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 222, 222, 10)      280       \n                                                                 \n conv2d_1 (Conv2D)           (None, 220, 220, 10)      910       \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 110, 110, 10)     0         \n )                                                               \n                                                                 \n conv2d_2 (Conv2D)           (None, 108, 108, 10)      910       \n                                                                 \n conv2d_3 (Conv2D)           (None, 106, 106, 10)      910       \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 53, 53, 10)       0         \n 2D)                                                             \n                                                                 \n flatten (Flatten)           (None, 28090)             0         \n                                                                 \n dense (Dense)               (None, 1)                 28091     \n                                                                 \n=================================================================\nTotal params: 31,101\nTrainable params: 31,101\nNon-trainable params: 0\n_________________________________________________________________\n</pre> In\u00a0[53]: Copied! <pre># Check model_8 architecture (same as model_1)\nmodel_8.summary()\n</pre> # Check model_8 architecture (same as model_1) model_8.summary() <pre>Model: \"sequential_7\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_16 (Conv2D)          (None, 222, 222, 10)      280       \n                                                                 \n conv2d_17 (Conv2D)          (None, 220, 220, 10)      910       \n                                                                 \n max_pooling2d_11 (MaxPoolin  (None, 110, 110, 10)     0         \n g2D)                                                            \n                                                                 \n conv2d_18 (Conv2D)          (None, 108, 108, 10)      910       \n                                                                 \n conv2d_19 (Conv2D)          (None, 106, 106, 10)      910       \n                                                                 \n max_pooling2d_12 (MaxPoolin  (None, 53, 53, 10)       0         \n g2D)                                                            \n                                                                 \n flatten_7 (Flatten)         (None, 28090)             0         \n                                                                 \n dense_12 (Dense)            (None, 1)                 28091     \n                                                                 \n=================================================================\nTotal params: 31,101\nTrainable params: 31,101\nNon-trainable params: 0\n_________________________________________________________________\n</pre> <p>Now let's check out our TinyVGG model's performance.</p> In\u00a0[54]: Copied! <pre># Check out the TinyVGG model performance\nplot_loss_curves(history_8)\n</pre> # Check out the TinyVGG model performance plot_loss_curves(history_8) In\u00a0[55]: Copied! <pre># How does this training curve look compared to the one above?\nplot_loss_curves(history_1)\n</pre> # How does this training curve look compared to the one above? plot_loss_curves(history_1) <p>Hmm, our training curves are looking good, but our model's performance on the training and test sets didn't improve much compared to the previous model.</p> <p>Taking another loook at the training curves, it looks like our model's performance might improve if we trained it a little longer (more epochs).</p> <p>Perhaps that's something you like to try?</p> In\u00a0[56]: Copied! <pre># Classes we're working with\nprint(class_names)\n</pre> # Classes we're working with print(class_names) <pre>['pizza' 'steak']\n</pre> <p>The first test image we're going to use is a delicious steak I cooked the other day.</p> In\u00a0[57]: Copied! <pre># View our example image\n!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-steak.jpeg \nsteak = mpimg.imread(\"03-steak.jpeg\")\nplt.imshow(steak)\nplt.axis(False);\n</pre> # View our example image !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-steak.jpeg  steak = mpimg.imread(\"03-steak.jpeg\") plt.imshow(steak) plt.axis(False); <pre>--2023-05-11 04:00:27--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-steak.jpeg\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1978213 (1.9M) [image/jpeg]\nSaving to: \u201803-steak.jpeg\u2019\n\n03-steak.jpeg       100%[===================&gt;]   1.89M  --.-KB/s    in 0.02s   \n\n2023-05-11 04:00:27 (90.1 MB/s) - \u201803-steak.jpeg\u2019 saved [1978213/1978213]\n\n</pre> In\u00a0[58]: Copied! <pre># Check the shape of our image\nsteak.shape\n</pre> # Check the shape of our image steak.shape Out[58]: <pre>(4032, 3024, 3)</pre> <p>Since our model takes in images of shapes <code>(224, 224, 3)</code>, we've got to reshape our custom image to use it with our model.</p> <p>To do so, we can import and decode our image using <code>tf.io.read_file</code> (for readining files) and <code>tf.image</code> (for resizing our image and turning it into a tensor).</p> <p>\ud83d\udd11 Note: For your model to make predictions on unseen data, for example, your own custom images, the custom image has to be in the same shape as your model has been trained on. In more general terms, to make predictions on custom data it has to be in the same form that your model has been trained on.</p> In\u00a0[59]: Copied! <pre># Create a function to import an image and resize it to be able to be used with our model\ndef load_and_prep_image(filename, img_shape=224):\n  \"\"\"\n  Reads an image from filename, turns it into a tensor\n  and reshapes it to (img_shape, img_shape, colour_channel).\n  \"\"\"\n  # Read in target file (an image)\n  img = tf.io.read_file(filename)\n\n  # Decode the read file into a tensor &amp; ensure 3 colour channels \n  # (our model is trained on images with 3 colour channels and sometimes images have 4 colour channels)\n  img = tf.image.decode_image(img, channels=3)\n\n  # Resize the image (to the same size our model was trained on)\n  img = tf.image.resize(img, size = [img_shape, img_shape])\n\n  # Rescale the image (get all values between 0 and 1)\n  img = img/255.\n  return img\n</pre> # Create a function to import an image and resize it to be able to be used with our model def load_and_prep_image(filename, img_shape=224):   \"\"\"   Reads an image from filename, turns it into a tensor   and reshapes it to (img_shape, img_shape, colour_channel).   \"\"\"   # Read in target file (an image)   img = tf.io.read_file(filename)    # Decode the read file into a tensor &amp; ensure 3 colour channels    # (our model is trained on images with 3 colour channels and sometimes images have 4 colour channels)   img = tf.image.decode_image(img, channels=3)    # Resize the image (to the same size our model was trained on)   img = tf.image.resize(img, size = [img_shape, img_shape])    # Rescale the image (get all values between 0 and 1)   img = img/255.   return img <p>Now we've got a function to load our custom image, let's load it in.</p> In\u00a0[60]: Copied! <pre># Load in and preprocess our custom image\nsteak = load_and_prep_image(\"03-steak.jpeg\")\nsteak\n</pre> # Load in and preprocess our custom image steak = load_and_prep_image(\"03-steak.jpeg\") steak Out[60]: <pre>&lt;tf.Tensor: shape=(224, 224, 3), dtype=float32, numpy=\narray([[[0.6377451 , 0.6220588 , 0.57892156],\n        [0.6504902 , 0.63186276, 0.5897059 ],\n        [0.63186276, 0.60833335, 0.5612745 ],\n        ...,\n        [0.52156866, 0.05098039, 0.09019608],\n        [0.49509802, 0.04215686, 0.07058824],\n        [0.52843136, 0.07745098, 0.10490196]],\n\n       [[0.6617647 , 0.6460784 , 0.6107843 ],\n        [0.6387255 , 0.6230392 , 0.57598037],\n        [0.65588236, 0.63235295, 0.5852941 ],\n        ...,\n        [0.5352941 , 0.06862745, 0.09215686],\n        [0.529902  , 0.05931373, 0.09460784],\n        [0.5142157 , 0.05539216, 0.08676471]],\n\n       [[0.6519608 , 0.6362745 , 0.5892157 ],\n        [0.6392157 , 0.6137255 , 0.56764704],\n        [0.65637255, 0.6269608 , 0.5828431 ],\n        ...,\n        [0.53137255, 0.06470589, 0.08039216],\n        [0.527451  , 0.06862745, 0.1       ],\n        [0.52254903, 0.05196078, 0.0872549 ]],\n\n       ...,\n\n       [[0.49313724, 0.42745098, 0.31029412],\n        [0.05441177, 0.01911765, 0.        ],\n        [0.2127451 , 0.16176471, 0.09509804],\n        ...,\n        [0.6132353 , 0.59362745, 0.57009804],\n        [0.65294117, 0.6333333 , 0.6098039 ],\n        [0.64166665, 0.62990195, 0.59460783]],\n\n       [[0.65392154, 0.5715686 , 0.45      ],\n        [0.6367647 , 0.54656863, 0.425     ],\n        [0.04656863, 0.01372549, 0.        ],\n        ...,\n        [0.6372549 , 0.61764705, 0.59411764],\n        [0.63529414, 0.6215686 , 0.5892157 ],\n        [0.6401961 , 0.62058824, 0.59705883]],\n\n       [[0.1       , 0.05539216, 0.        ],\n        [0.48333332, 0.40882352, 0.29117647],\n        [0.65      , 0.5686275 , 0.44019607],\n        ...,\n        [0.6308824 , 0.6161765 , 0.5808824 ],\n        [0.6519608 , 0.63186276, 0.5901961 ],\n        [0.6338235 , 0.6259804 , 0.57892156]]], dtype=float32)&gt;</pre> <p>Wonderful, our image is in tensor format, time to try it with our model!</p> In\u00a0[61]: Copied! <pre># Make a prediction on our custom image (spoiler: this won't work)\nmodel_8.predict(steak)\n</pre> # Make a prediction on our custom image (spoiler: this won't work) model_8.predict(steak) <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-61-fd7eef5274d1&gt; in &lt;cell line: 2&gt;()\n      1 # Make a prediction on our custom image (spoiler: this won't work)\n----&gt; 2 model_8.predict(steak)\n\n/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)\n     68             # To get the full stack trace, call:\n     69             # `tf.debugging.disable_traceback_filtering()`\n---&gt; 70             raise e.with_traceback(filtered_tb) from None\n     71         finally:\n     72             del filtered_tb\n\n/usr/local/lib/python3.10/dist-packages/keras/engine/training.py in tf__predict_function(iterator)\n     13                 try:\n     14                     do_return = True\n---&gt; 15                     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\n     16                 except:\n     17                     do_return = False\n\nValueError: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_7\" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(32, 224, 3)\n</pre> <p>There's one more problem...</p> <p>Although our image is in the same shape as the images our model has been trained on, we're still missing a dimension.</p> <p>Remember how our model was trained in batches?</p> <p>Well, the batch size becomes the first dimension.</p> <p>So in reality, our model was trained on data in the shape of <code>(batch_size, 224, 224, 3)</code>.</p> <p>We can fix this by adding an extra to our custom image tensor using <code>tf.expand_dims</code>.</p> In\u00a0[62]: Copied! <pre># Add an extra axis\nprint(f\"Shape before new dimension: {steak.shape}\")\nsteak = tf.expand_dims(steak, axis=0) # add an extra dimension at axis 0\n#steak = steak[tf.newaxis, ...] # alternative to the above, '...' is short for 'every other dimension'\nprint(f\"Shape after new dimension: {steak.shape}\")\nsteak\n</pre> # Add an extra axis print(f\"Shape before new dimension: {steak.shape}\") steak = tf.expand_dims(steak, axis=0) # add an extra dimension at axis 0 #steak = steak[tf.newaxis, ...] # alternative to the above, '...' is short for 'every other dimension' print(f\"Shape after new dimension: {steak.shape}\") steak <pre>Shape before new dimension: (224, 224, 3)\nShape after new dimension: (1, 224, 224, 3)\n</pre> Out[62]: <pre>&lt;tf.Tensor: shape=(1, 224, 224, 3), dtype=float32, numpy=\narray([[[[0.6377451 , 0.6220588 , 0.57892156],\n         [0.6504902 , 0.63186276, 0.5897059 ],\n         [0.63186276, 0.60833335, 0.5612745 ],\n         ...,\n         [0.52156866, 0.05098039, 0.09019608],\n         [0.49509802, 0.04215686, 0.07058824],\n         [0.52843136, 0.07745098, 0.10490196]],\n\n        [[0.6617647 , 0.6460784 , 0.6107843 ],\n         [0.6387255 , 0.6230392 , 0.57598037],\n         [0.65588236, 0.63235295, 0.5852941 ],\n         ...,\n         [0.5352941 , 0.06862745, 0.09215686],\n         [0.529902  , 0.05931373, 0.09460784],\n         [0.5142157 , 0.05539216, 0.08676471]],\n\n        [[0.6519608 , 0.6362745 , 0.5892157 ],\n         [0.6392157 , 0.6137255 , 0.56764704],\n         [0.65637255, 0.6269608 , 0.5828431 ],\n         ...,\n         [0.53137255, 0.06470589, 0.08039216],\n         [0.527451  , 0.06862745, 0.1       ],\n         [0.52254903, 0.05196078, 0.0872549 ]],\n\n        ...,\n\n        [[0.49313724, 0.42745098, 0.31029412],\n         [0.05441177, 0.01911765, 0.        ],\n         [0.2127451 , 0.16176471, 0.09509804],\n         ...,\n         [0.6132353 , 0.59362745, 0.57009804],\n         [0.65294117, 0.6333333 , 0.6098039 ],\n         [0.64166665, 0.62990195, 0.59460783]],\n\n        [[0.65392154, 0.5715686 , 0.45      ],\n         [0.6367647 , 0.54656863, 0.425     ],\n         [0.04656863, 0.01372549, 0.        ],\n         ...,\n         [0.6372549 , 0.61764705, 0.59411764],\n         [0.63529414, 0.6215686 , 0.5892157 ],\n         [0.6401961 , 0.62058824, 0.59705883]],\n\n        [[0.1       , 0.05539216, 0.        ],\n         [0.48333332, 0.40882352, 0.29117647],\n         [0.65      , 0.5686275 , 0.44019607],\n         ...,\n         [0.6308824 , 0.6161765 , 0.5808824 ],\n         [0.6519608 , 0.63186276, 0.5901961 ],\n         [0.6338235 , 0.6259804 , 0.57892156]]]], dtype=float32)&gt;</pre> <p>Our custom image has a batch size of 1! Let's make a prediction on it.</p> In\u00a0[63]: Copied! <pre># Make a prediction on custom image tensor\npred = model_8.predict(steak)\npred\n</pre> # Make a prediction on custom image tensor pred = model_8.predict(steak) pred <pre>1/1 [==============================] - 0s 401ms/step\n</pre> Out[63]: <pre>array([[0.7315909]], dtype=float32)</pre> <p>Ahh, the predictions come out in prediction probability form. In other words, this means how likely the image is to be one class or another.</p> <p>Since we're working with a binary classification problem, if the prediction probability is over 0.5, according to the model, the prediction is most likely to be the postive class (class 1).</p> <p>And if the prediction probability is under 0.5, according to the model, the predicted class is most likely to be the negative class (class 0).</p> <p>\ud83d\udd11 Note: The 0.5 cutoff can be adjusted to your liking. For example, you could set the limit to be 0.8 and over for the positive class and 0.2 for the negative class. However, doing this will almost always change your model's performance metrics so be sure to make sure they change in the right direction.</p> <p>But saying positive and negative class doesn't make much sense when we're working with pizza \ud83c\udf55 and steak \ud83e\udd69...</p> <p>So let's write a little function to convert predictions into their class names and then plot the target image.</p> In\u00a0[64]: Copied! <pre># Remind ourselves of our class names\nclass_names\n</pre> # Remind ourselves of our class names class_names Out[64]: <pre>array(['pizza', 'steak'], dtype='&lt;U5')</pre> In\u00a0[65]: Copied! <pre># We can index the predicted class by rounding the prediction probability\npred_class = class_names[int(tf.round(pred)[0][0])]\npred_class\n</pre> # We can index the predicted class by rounding the prediction probability pred_class = class_names[int(tf.round(pred)[0][0])] pred_class Out[65]: <pre>'steak'</pre> In\u00a0[66]: Copied! <pre>def pred_and_plot(model, filename, class_names):\n  \"\"\"\n  Imports an image located at filename, makes a prediction on it with\n  a trained model and plots the image with the predicted class as the title.\n  \"\"\"\n  # Import the target image and preprocess it\n  img = load_and_prep_image(filename)\n\n  # Make a prediction\n  pred = model.predict(tf.expand_dims(img, axis=0))\n\n  # Get the predicted class\n  pred_class = class_names[int(tf.round(pred)[0][0])]\n\n  # Plot the image and predicted class\n  plt.imshow(img)\n  plt.title(f\"Prediction: {pred_class}\")\n  plt.axis(False);\n</pre> def pred_and_plot(model, filename, class_names):   \"\"\"   Imports an image located at filename, makes a prediction on it with   a trained model and plots the image with the predicted class as the title.   \"\"\"   # Import the target image and preprocess it   img = load_and_prep_image(filename)    # Make a prediction   pred = model.predict(tf.expand_dims(img, axis=0))    # Get the predicted class   pred_class = class_names[int(tf.round(pred)[0][0])]    # Plot the image and predicted class   plt.imshow(img)   plt.title(f\"Prediction: {pred_class}\")   plt.axis(False); In\u00a0[67]: Copied! <pre># Test our model on a custom image\npred_and_plot(model_8, \"03-steak.jpeg\", class_names)\n</pre> # Test our model on a custom image pred_and_plot(model_8, \"03-steak.jpeg\", class_names) <pre>1/1 [==============================] - 0s 23ms/step\n</pre> <p>Nice! Our model got the prediction right.</p> <p>The only downside of working with food is this is making me hungry.</p> <p>Let's try one more image.</p> In\u00a0[68]: Copied! <pre># Download another test image and make a prediction on it\n!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-pizza-dad.jpeg \npred_and_plot(model_8, \"03-pizza-dad.jpeg\", class_names)\n</pre> # Download another test image and make a prediction on it !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-pizza-dad.jpeg  pred_and_plot(model_8, \"03-pizza-dad.jpeg\", class_names) <pre>--2023-05-11 04:00:57--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-pizza-dad.jpeg\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2874848 (2.7M) [image/jpeg]\nSaving to: \u201803-pizza-dad.jpeg\u2019\n\n03-pizza-dad.jpeg   100%[===================&gt;]   2.74M  --.-KB/s    in 0.02s   \n\n2023-05-11 04:00:57 (114 MB/s) - \u201803-pizza-dad.jpeg\u2019 saved [2874848/2874848]\n\n1/1 [==============================] - 0s 22ms/step\n</pre> <p>Two thumbs up! Woohoo!</p> In\u00a0[69]: Copied! <pre>import zipfile\n\n# Download zip file of 10_food_classes images\n# See how this data was created - https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/image_data_modification.ipynb\n!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip \n\n# Unzip the downloaded file\nzip_ref = zipfile.ZipFile(\"10_food_classes_all_data.zip\", \"r\")\nzip_ref.extractall()\nzip_ref.close()\n</pre> import zipfile  # Download zip file of 10_food_classes images # See how this data was created - https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/image_data_modification.ipynb !wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip   # Unzip the downloaded file zip_ref = zipfile.ZipFile(\"10_food_classes_all_data.zip\", \"r\") zip_ref.extractall() zip_ref.close() <pre>--2023-05-11 04:00:59--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip\nResolving storage.googleapis.com (storage.googleapis.com)... 108.177.127.128, 172.217.218.128, 142.250.153.128, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|108.177.127.128|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 519183241 (495M) [application/zip]\nSaving to: \u201810_food_classes_all_data.zip\u2019\n\n10_food_classes_all 100%[===================&gt;] 495.13M  35.8MB/s    in 15s     \n\n2023-05-11 04:01:15 (32.1 MB/s) - \u201810_food_classes_all_data.zip\u2019 saved [519183241/519183241]\n\n</pre> <p>Now let's check out all of the different directories and sub-directories in the <code>10_food_classes</code> file.</p> In\u00a0[70]: Copied! <pre>import os\n\n# Walk through 10_food_classes directory and list number of files\nfor dirpath, dirnames, filenames in os.walk(\"10_food_classes_all_data\"):\n  print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n</pre> import os  # Walk through 10_food_classes directory and list number of files for dirpath, dirnames, filenames in os.walk(\"10_food_classes_all_data\"):   print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\") <pre>There are 2 directories and 0 images in '10_food_classes_all_data'.\nThere are 10 directories and 0 images in '10_food_classes_all_data/test'.\nThere are 0 directories and 250 images in '10_food_classes_all_data/test/chicken_wings'.\nThere are 0 directories and 250 images in '10_food_classes_all_data/test/fried_rice'.\nThere are 0 directories and 250 images in '10_food_classes_all_data/test/ice_cream'.\nThere are 0 directories and 250 images in '10_food_classes_all_data/test/grilled_salmon'.\nThere are 0 directories and 250 images in '10_food_classes_all_data/test/chicken_curry'.\nThere are 0 directories and 250 images in '10_food_classes_all_data/test/hamburger'.\nThere are 0 directories and 250 images in '10_food_classes_all_data/test/steak'.\nThere are 0 directories and 250 images in '10_food_classes_all_data/test/pizza'.\nThere are 0 directories and 250 images in '10_food_classes_all_data/test/sushi'.\nThere are 0 directories and 250 images in '10_food_classes_all_data/test/ramen'.\nThere are 10 directories and 0 images in '10_food_classes_all_data/train'.\nThere are 0 directories and 750 images in '10_food_classes_all_data/train/chicken_wings'.\nThere are 0 directories and 750 images in '10_food_classes_all_data/train/fried_rice'.\nThere are 0 directories and 750 images in '10_food_classes_all_data/train/ice_cream'.\nThere are 0 directories and 750 images in '10_food_classes_all_data/train/grilled_salmon'.\nThere are 0 directories and 750 images in '10_food_classes_all_data/train/chicken_curry'.\nThere are 0 directories and 750 images in '10_food_classes_all_data/train/hamburger'.\nThere are 0 directories and 750 images in '10_food_classes_all_data/train/steak'.\nThere are 0 directories and 750 images in '10_food_classes_all_data/train/pizza'.\nThere are 0 directories and 750 images in '10_food_classes_all_data/train/sushi'.\nThere are 0 directories and 750 images in '10_food_classes_all_data/train/ramen'.\n</pre> <p>Looking good!</p> <p>We'll now setup the training and test directory paths.</p> In\u00a0[71]: Copied! <pre>train_dir = \"10_food_classes_all_data/train/\"\ntest_dir = \"10_food_classes_all_data/test/\"\n</pre> train_dir = \"10_food_classes_all_data/train/\" test_dir = \"10_food_classes_all_data/test/\" <p>And get the class names from the subdirectories.</p> In\u00a0[72]: Copied! <pre># Get the class names for our multi-class dataset\nimport pathlib\nimport numpy as np\ndata_dir = pathlib.Path(train_dir)\nclass_names = np.array(sorted([item.name for item in data_dir.glob('*')]))\nprint(class_names)\n</pre> # Get the class names for our multi-class dataset import pathlib import numpy as np data_dir = pathlib.Path(train_dir) class_names = np.array(sorted([item.name for item in data_dir.glob('*')])) print(class_names) <pre>['chicken_curry' 'chicken_wings' 'fried_rice' 'grilled_salmon' 'hamburger'\n 'ice_cream' 'pizza' 'ramen' 'steak' 'sushi']\n</pre> <p>How about we visualize an image from the training set?</p> In\u00a0[73]: Copied! <pre># View a random image from the training dataset\nimport random\nimg = view_random_image(target_dir=train_dir,\n                        target_class=random.choice(class_names)) # get a random class name\n</pre> # View a random image from the training dataset import random img = view_random_image(target_dir=train_dir,                         target_class=random.choice(class_names)) # get a random class name <pre>Image shape: (512, 384, 3)\n</pre> In\u00a0[74]: Copied! <pre>from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Rescale the data and create data generator instances\ntrain_datagen = ImageDataGenerator(rescale=1/255.)\ntest_datagen = ImageDataGenerator(rescale=1/255.)\n\n# Load data in from directories and turn it into batches\ntrain_data = train_datagen.flow_from_directory(train_dir,\n                                               target_size=(224, 224),\n                                               batch_size=32,\n                                               class_mode='categorical') # changed to categorical\n\ntest_data = train_datagen.flow_from_directory(test_dir,\n                                              target_size=(224, 224),\n                                              batch_size=32,\n                                              class_mode='categorical')\n</pre> from tensorflow.keras.preprocessing.image import ImageDataGenerator  # Rescale the data and create data generator instances train_datagen = ImageDataGenerator(rescale=1/255.) test_datagen = ImageDataGenerator(rescale=1/255.)  # Load data in from directories and turn it into batches train_data = train_datagen.flow_from_directory(train_dir,                                                target_size=(224, 224),                                                batch_size=32,                                                class_mode='categorical') # changed to categorical  test_data = train_datagen.flow_from_directory(test_dir,                                               target_size=(224, 224),                                               batch_size=32,                                               class_mode='categorical') <pre>Found 7500 images belonging to 10 classes.\nFound 2500 images belonging to 10 classes.\n</pre> <p>As with binary classifcation, we've creator image generators. The main change this time is that we've changed the <code>class_mode</code> parameter to <code>'categorical'</code> because we're dealing with 10 classes of food images.</p> <p>Everything else like rescaling the images, creating the batch size and target image size stay the same.</p> <p>\ud83e\udd14 Question: Why is the image size 224x224? This could actually be any size we wanted, however, 224x224 is a very common size for preprocessing images to. Depending on your problem you might want to use larger or smaller images.</p> In\u00a0[75]: Copied! <pre>import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n\n# Create our model (a clone of model_8, except to be multi-class)\nmodel_9 = Sequential([\n  Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)),\n  Conv2D(10, 3, activation='relu'),\n  MaxPool2D(),\n  Conv2D(10, 3, activation='relu'),\n  Conv2D(10, 3, activation='relu'),\n  MaxPool2D(),\n  Flatten(),\n  Dense(10, activation='softmax') # changed to have 10 neurons (same as number of classes) and 'softmax' activation\n])\n\n# Compile the model\nmodel_9.compile(loss=\"categorical_crossentropy\", # changed to categorical_crossentropy\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])\n</pre> import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense  # Create our model (a clone of model_8, except to be multi-class) model_9 = Sequential([   Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)),   Conv2D(10, 3, activation='relu'),   MaxPool2D(),   Conv2D(10, 3, activation='relu'),   Conv2D(10, 3, activation='relu'),   MaxPool2D(),   Flatten(),   Dense(10, activation='softmax') # changed to have 10 neurons (same as number of classes) and 'softmax' activation ])  # Compile the model model_9.compile(loss=\"categorical_crossentropy\", # changed to categorical_crossentropy                 optimizer=tf.keras.optimizers.Adam(),                 metrics=[\"accuracy\"]) In\u00a0[76]: Copied! <pre># Fit the model\nhistory_9 = model_9.fit(train_data, # now 10 different classes \n                        epochs=5,\n                        steps_per_epoch=len(train_data),\n                        validation_data=test_data,\n                        validation_steps=len(test_data))\n</pre> # Fit the model history_9 = model_9.fit(train_data, # now 10 different classes                          epochs=5,                         steps_per_epoch=len(train_data),                         validation_data=test_data,                         validation_steps=len(test_data)) <pre>Epoch 1/5\n235/235 [==============================] - 48s 194ms/step - loss: 2.1520 - accuracy: 0.2133 - val_loss: 1.9674 - val_accuracy: 0.2964\nEpoch 2/5\n235/235 [==============================] - 45s 193ms/step - loss: 1.8770 - accuracy: 0.3489 - val_loss: 1.8792 - val_accuracy: 0.3424\nEpoch 3/5\n235/235 [==============================] - 45s 190ms/step - loss: 1.5239 - accuracy: 0.4860 - val_loss: 1.9865 - val_accuracy: 0.3184\nEpoch 4/5\n235/235 [==============================] - 45s 193ms/step - loss: 0.9507 - accuracy: 0.6897 - val_loss: 2.4049 - val_accuracy: 0.2824\nEpoch 5/5\n235/235 [==============================] - 45s 190ms/step - loss: 0.3835 - accuracy: 0.8772 - val_loss: 3.6373 - val_accuracy: 0.2760\n</pre> <p>Why do you think each epoch takes longer than when working with only two classes of images?</p> <p>It's because we're now dealing with more images than we were before. We've got 10 classes with 750 training images and 250 validation images each totalling 10,000 images. Where as when we had two classes, we had 1500 training images and 500 validation images, totalling 2000.</p> <p>The intuitive reasoning here is the more data you have, the longer a model will take to find patterns.</p> In\u00a0[77]: Copied! <pre># Evaluate on the test data\nmodel_9.evaluate(test_data)\n</pre> # Evaluate on the test data model_9.evaluate(test_data) <pre>79/79 [==============================] - 11s 139ms/step - loss: 3.6373 - accuracy: 0.2760\n</pre> Out[77]: <pre>[3.6373109817504883, 0.2759999930858612]</pre> In\u00a0[78]: Copied! <pre># Check out the model's loss curves on the 10 classes of data (note: this function comes from above in the notebook)\nplot_loss_curves(history_9)\n</pre> # Check out the model's loss curves on the 10 classes of data (note: this function comes from above in the notebook) plot_loss_curves(history_9) <p>Woah, that's quite the gap between the training and validation loss curves.</p> <p>What does this tell us?</p> <p>It seems our model is overfitting the training set quite badly. In other words, it's getting great results on the training data but fails to generalize well to unseen data and performs poorly on the test data.</p> In\u00a0[79]: Copied! <pre># Try a simplified model (removed two layers)\nmodel_10 = Sequential([\n  Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)),\n  MaxPool2D(),\n  Conv2D(10, 3, activation='relu'),\n  MaxPool2D(),\n  Flatten(),\n  Dense(10, activation='softmax')\n])\n\nmodel_10.compile(loss='categorical_crossentropy',\n                 optimizer=tf.keras.optimizers.Adam(),\n                 metrics=['accuracy'])\n\nhistory_10 = model_10.fit(train_data,\n                          epochs=5,\n                          steps_per_epoch=len(train_data),\n                          validation_data=test_data,\n                          validation_steps=len(test_data))\n</pre> # Try a simplified model (removed two layers) model_10 = Sequential([   Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)),   MaxPool2D(),   Conv2D(10, 3, activation='relu'),   MaxPool2D(),   Flatten(),   Dense(10, activation='softmax') ])  model_10.compile(loss='categorical_crossentropy',                  optimizer=tf.keras.optimizers.Adam(),                  metrics=['accuracy'])  history_10 = model_10.fit(train_data,                           epochs=5,                           steps_per_epoch=len(train_data),                           validation_data=test_data,                           validation_steps=len(test_data)) <pre>Epoch 1/5\n235/235 [==============================] - 46s 193ms/step - loss: 2.2804 - accuracy: 0.2397 - val_loss: 1.9445 - val_accuracy: 0.3024\nEpoch 2/5\n235/235 [==============================] - 45s 192ms/step - loss: 1.8253 - accuracy: 0.3785 - val_loss: 1.9529 - val_accuracy: 0.3172\nEpoch 3/5\n235/235 [==============================] - 44s 186ms/step - loss: 1.5193 - accuracy: 0.4979 - val_loss: 2.0705 - val_accuracy: 0.3160\nEpoch 4/5\n235/235 [==============================] - 44s 187ms/step - loss: 1.1143 - accuracy: 0.6408 - val_loss: 2.2653 - val_accuracy: 0.3064\nEpoch 5/5\n235/235 [==============================] - 44s 188ms/step - loss: 0.7340 - accuracy: 0.7787 - val_loss: 2.5814 - val_accuracy: 0.2944\n</pre> In\u00a0[80]: Copied! <pre># Check out the loss curves of model_10\nplot_loss_curves(history_10)\n</pre> # Check out the loss curves of model_10 plot_loss_curves(history_10) <p>Hmm... even with a simplifed model, it looks like our model is still dramatically overfitting the training data.</p> <p>What else could we try?</p> <p>How about data augmentation?</p> <p>Data augmentation makes it harder for the model to learn on the training data and in turn, hopefully making the patterns it learns more generalizable to unseen data.</p> <p>To create augmented data, we'll recreate a new <code>ImageDataGenerator</code> instance, this time adding some parameters such as <code>rotation_range</code> and <code>horizontal_flip</code> to manipulate our images.</p> In\u00a0[81]: Copied! <pre># Create augmented data generator instance\ntrain_datagen_augmented = ImageDataGenerator(rescale=1/255.,\n                                             rotation_range=20, # note: this is an int not a float\n                                             width_shift_range=0.2,\n                                             height_shift_range=0.2,\n                                             zoom_range=0.2,\n                                             horizontal_flip=True)\n\ntrain_data_augmented = train_datagen_augmented.flow_from_directory(train_dir,\n                                                                  target_size=(224, 224),\n                                                                  batch_size=32,\n                                                                  class_mode='categorical')\n</pre> # Create augmented data generator instance train_datagen_augmented = ImageDataGenerator(rescale=1/255.,                                              rotation_range=20, # note: this is an int not a float                                              width_shift_range=0.2,                                              height_shift_range=0.2,                                              zoom_range=0.2,                                              horizontal_flip=True)  train_data_augmented = train_datagen_augmented.flow_from_directory(train_dir,                                                                   target_size=(224, 224),                                                                   batch_size=32,                                                                   class_mode='categorical') <pre>Found 7500 images belonging to 10 classes.\n</pre> <p>Now we've got augmented data, let's see how it works with the same model as before (<code>model_10</code>).</p> <p>Rather than rewrite the model from scratch, we can clone it using a handy function in TensorFlow called <code>clone_model</code> which can take an existing model and rebuild it in the same format.</p> <p>The cloned version will not include any of the weights (patterns) the original model has learned. So when we train it, it'll be like training a model from scratch.</p> <p>\ud83d\udd11 Note: One of the key practices in deep learning and machine learning in general is to be a serial experimenter. That's what we're doing here. Trying something, seeing if it works, then trying something else. A good experiment setup also keeps track of the things you change, for example, that's why we're using the same model as before but with different data. The model stays the same but the data changes, this will let us know if augmented training data has any influence over performance.</p> In\u00a0[82]: Copied! <pre># Clone the model (use the same architecture)\nmodel_11 = tf.keras.models.clone_model(model_10)\n\n# Compile the cloned model (same setup as used for model_10)\nmodel_11.compile(loss=\"categorical_crossentropy\",\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=[\"accuracy\"])\n\n# Fit the model\nhistory_11 = model_11.fit(train_data_augmented, # use augmented data\n                          epochs=5,\n                          steps_per_epoch=len(train_data_augmented),\n                          validation_data=test_data,\n                          validation_steps=len(test_data))\n</pre> # Clone the model (use the same architecture) model_11 = tf.keras.models.clone_model(model_10)  # Compile the cloned model (same setup as used for model_10) model_11.compile(loss=\"categorical_crossentropy\",               optimizer=tf.keras.optimizers.Adam(),               metrics=[\"accuracy\"])  # Fit the model history_11 = model_11.fit(train_data_augmented, # use augmented data                           epochs=5,                           steps_per_epoch=len(train_data_augmented),                           validation_data=test_data,                           validation_steps=len(test_data)) <pre>Epoch 1/5\n235/235 [==============================] - 112s 473ms/step - loss: 2.1903 - accuracy: 0.1997 - val_loss: 2.0500 - val_accuracy: 0.2668\nEpoch 2/5\n235/235 [==============================] - 114s 483ms/step - loss: 2.0740 - accuracy: 0.2592 - val_loss: 1.9318 - val_accuracy: 0.3236\nEpoch 3/5\n235/235 [==============================] - 114s 485ms/step - loss: 2.0149 - accuracy: 0.3044 - val_loss: 1.8862 - val_accuracy: 0.3348\nEpoch 4/5\n235/235 [==============================] - 114s 487ms/step - loss: 1.9739 - accuracy: 0.3172 - val_loss: 1.8206 - val_accuracy: 0.3868\nEpoch 5/5\n235/235 [==============================] - 115s 489ms/step - loss: 1.9415 - accuracy: 0.3292 - val_loss: 1.8619 - val_accuracy: 0.3696\n</pre> <p>You can see it each epoch takes longer than the previous model. This is because our data is being augmented on the fly on the CPU as it gets loaded onto the GPU, in turn, increasing the amount of time between each epoch.</p> <p>Note: One way to improve this time taken is to use augmentation layers directly as part of the model. For example, with <code>tf.keras.layers.RandomFlip</code>. You can also speed up data loading with the newer <code>tf.keras.utils.image_dataset_from_directory</code> image loading API (we cover this later in the course).</p> <p>How do our model's training curves look?</p> In\u00a0[83]: Copied! <pre># Check out our model's performance with augmented data\nplot_loss_curves(history_11)\n</pre> # Check out our model's performance with augmented data plot_loss_curves(history_11) <p>Woah! That's looking much better, the loss curves are much closer to eachother. Although our model didn't perform as well on the augmented training set, it performed much better on the validation dataset.</p> <p>It even looks like if we kept it training for longer (more epochs) the evaluation metrics might continue to improve.</p> In\u00a0[84]: Copied! <pre># What classes has our model been trained on?\nclass_names\n</pre> # What classes has our model been trained on? class_names Out[84]: <pre>array(['chicken_curry', 'chicken_wings', 'fried_rice', 'grilled_salmon',\n       'hamburger', 'ice_cream', 'pizza', 'ramen', 'steak', 'sushi'],\n      dtype='&lt;U14')</pre> <p>Beautiful, now let's get some of our custom images.</p> <p>If you're using Google Colab, you could also upload some of your own images via the files tab.</p> In\u00a0[85]: Copied! <pre># -q is for \"quiet\"\n!wget -q https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-pizza-dad.jpeg\n!wget -q https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-steak.jpeg\n!wget -q https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-hamburger.jpeg\n!wget -q https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-sushi.jpeg\n</pre> # -q is for \"quiet\" !wget -q https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-pizza-dad.jpeg !wget -q https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-steak.jpeg !wget -q https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-hamburger.jpeg !wget -q https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-sushi.jpeg <p>Okay, we've got some custom images to try, let's use the <code>pred_and_plot</code> function to make a prediction with <code>model_11</code> on one of the images and plot it.</p> In\u00a0[86]: Copied! <pre># Make a prediction using model_11\npred_and_plot(model=model_11, \n              filename=\"03-steak.jpeg\", \n              class_names=class_names)\n</pre> # Make a prediction using model_11 pred_and_plot(model=model_11,                filename=\"03-steak.jpeg\",                class_names=class_names) <pre>1/1 [==============================] - 0s 81ms/step\n</pre> <p>Hmm... it looks like our model got the prediction wrong, how about we try another?</p> In\u00a0[87]: Copied! <pre>pred_and_plot(model_11, \"03-sushi.jpeg\", class_names)\n</pre> pred_and_plot(model_11, \"03-sushi.jpeg\", class_names) <pre>1/1 [==============================] - 0s 23ms/step\n</pre> <p>And again, it's predicting <code>chicken_curry</code> for some reason.</p> <p>How about one more?</p> In\u00a0[88]: Copied! <pre>pred_and_plot(model_11, \"03-pizza-dad.jpeg\", class_names)\n</pre> pred_and_plot(model_11, \"03-pizza-dad.jpeg\", class_names) <pre>1/1 [==============================] - 0s 22ms/step\n</pre> <p><code>chicken_curry</code> again? There must be something wrong...</p> <p>I think it might have to do with our <code>pred_and_plot</code> function.</p> <p>Let's makes a prediction without using the function and see where it might be going wrong.</p> In\u00a0[89]: Copied! <pre># Load in and preprocess our custom image\nimg = load_and_prep_image(\"03-steak.jpeg\")\n\n# Make a prediction\npred = model_11.predict(tf.expand_dims(img, axis=0))\n\n# Match the prediction class to the highest prediction probability\npred_class = class_names[pred.argmax()]\nplt.imshow(img)\nplt.title(pred_class)\nplt.axis(False);\n</pre> # Load in and preprocess our custom image img = load_and_prep_image(\"03-steak.jpeg\")  # Make a prediction pred = model_11.predict(tf.expand_dims(img, axis=0))  # Match the prediction class to the highest prediction probability pred_class = class_names[pred.argmax()] plt.imshow(img) plt.title(pred_class) plt.axis(False); <pre>1/1 [==============================] - 0s 22ms/step\n</pre> <p>Much better! There must be something up with our <code>pred_and_plot</code> function.</p> <p>And I think I know what it is.</p> <p>The <code>pred_and_plot</code> function was designed to be used with binary classification models where as our current model is a multi-class classification model.</p> <p>The main difference lies in the output of the <code>predict</code> function.</p> In\u00a0[90]: Copied! <pre># Check the output of the predict function\npred = model_11.predict(tf.expand_dims(img, axis=0))\npred\n</pre> # Check the output of the predict function pred = model_11.predict(tf.expand_dims(img, axis=0)) pred <pre>1/1 [==============================] - 0s 22ms/step\n</pre> Out[90]: <pre>array([[0.00413127, 0.00698275, 0.00470221, 0.14550138, 0.0163848 ,\n        0.00341261, 0.00258247, 0.2162087 , 0.5952408 , 0.00485306]],\n      dtype=float32)</pre> <p>Since our model has a <code>'softmax'</code> activation function and 10 output neurons, it outputs a prediction probability for each of the classes in our model.</p> <p>The class with the highest probability is what the model believes the image contains.</p> <p>We can find the maximum value index using <code>argmax</code> and then use that to index our <code>class_names</code> list to output the predicted class.</p> In\u00a0[91]: Copied! <pre># Find the predicted class name\nclass_names[pred.argmax()]\n</pre> # Find the predicted class name class_names[pred.argmax()] Out[91]: <pre>'steak'</pre> <p>Knowing this, we can readjust our <code>pred_and_plot</code> function to work with multiple classes as well as binary classes.</p> In\u00a0[92]: Copied! <pre># Adjust function to work with multi-class\ndef pred_and_plot(model, filename, class_names):\n  \"\"\"\n  Imports an image located at filename, makes a prediction on it with\n  a trained model and plots the image with the predicted class as the title.\n  \"\"\"\n  # Import the target image and preprocess it\n  img = load_and_prep_image(filename)\n\n  # Make a prediction\n  pred = model.predict(tf.expand_dims(img, axis=0))\n\n  # Get the predicted class\n  if len(pred[0]) &gt; 1: # check for multi-class\n    pred_class = class_names[pred.argmax()] # if more than one output, take the max\n  else:\n    pred_class = class_names[int(tf.round(pred)[0][0])] # if only one output, round\n\n  # Plot the image and predicted class\n  plt.imshow(img)\n  plt.title(f\"Prediction: {pred_class}\")\n  plt.axis(False);\n</pre> # Adjust function to work with multi-class def pred_and_plot(model, filename, class_names):   \"\"\"   Imports an image located at filename, makes a prediction on it with   a trained model and plots the image with the predicted class as the title.   \"\"\"   # Import the target image and preprocess it   img = load_and_prep_image(filename)    # Make a prediction   pred = model.predict(tf.expand_dims(img, axis=0))    # Get the predicted class   if len(pred[0]) &gt; 1: # check for multi-class     pred_class = class_names[pred.argmax()] # if more than one output, take the max   else:     pred_class = class_names[int(tf.round(pred)[0][0])] # if only one output, round    # Plot the image and predicted class   plt.imshow(img)   plt.title(f\"Prediction: {pred_class}\")   plt.axis(False); <p>Let's try it out. If we've done it right, using different images should lead to different outputs (rather than <code>chicken_curry</code> every time).</p> In\u00a0[93]: Copied! <pre>pred_and_plot(model_11, \"03-steak.jpeg\", class_names)\n</pre> pred_and_plot(model_11, \"03-steak.jpeg\", class_names) <pre>1/1 [==============================] - 0s 23ms/step\n</pre> In\u00a0[94]: Copied! <pre>pred_and_plot(model_11, \"03-sushi.jpeg\", class_names)\n</pre> pred_and_plot(model_11, \"03-sushi.jpeg\", class_names) <pre>1/1 [==============================] - 0s 22ms/step\n</pre> In\u00a0[95]: Copied! <pre>pred_and_plot(model_11, \"03-pizza-dad.jpeg\", class_names)\n</pre> pred_and_plot(model_11, \"03-pizza-dad.jpeg\", class_names) <pre>1/1 [==============================] - 0s 22ms/step\n</pre> In\u00a0[96]: Copied! <pre>pred_and_plot(model_11, \"03-hamburger.jpeg\", class_names)\n</pre> pred_and_plot(model_11, \"03-hamburger.jpeg\", class_names) <pre>1/1 [==============================] - 0s 22ms/step\n</pre> <p>Our model's predictions aren't very good, this is because it's only performing at ~35% accuracy on the test dataset.</p> <p>We'll see how to improve this later on (spolier: transfer learning is going to step things up a notch).</p> In\u00a0[97]: Copied! <pre># Save a model\nmodel_11.save(\"saved_trained_model\")\n</pre> # Save a model model_11.save(\"saved_trained_model\") <pre>WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n</pre> In\u00a0[98]: Copied! <pre># Load in a model and evaluate it\nloaded_model_11 = tf.keras.models.load_model(\"saved_trained_model\")\nloaded_model_11.evaluate(test_data)\n</pre> # Load in a model and evaluate it loaded_model_11 = tf.keras.models.load_model(\"saved_trained_model\") loaded_model_11.evaluate(test_data) <pre>79/79 [==============================] - 11s 141ms/step - loss: 1.8619 - accuracy: 0.3696\n</pre> Out[98]: <pre>[1.8618727922439575, 0.36959999799728394]</pre> In\u00a0[99]: Copied! <pre># Compare our unsaved model's results (same as above)\nmodel_11.evaluate(test_data)\n</pre> # Compare our unsaved model's results (same as above) model_11.evaluate(test_data) <pre>79/79 [==============================] - 11s 140ms/step - loss: 1.8619 - accuracy: 0.3696\n</pre> Out[99]: <pre>[1.8618724346160889, 0.36959999799728394]</pre>"},{"location":"03_convolutional_neural_networks_in_tensorflow/#03-convolutional-neural-networks-and-computer-vision-with-tensorflow","title":"03. Convolutional Neural Networks and Computer Vision with TensorFlow\u00b6","text":"<p>So far we've covered the basics of TensorFlow and built a handful of models to work across different problems.</p> <p>Now we're going to get specific and see how a special kind of neural network, convolutional neural networks (CNNs) can be used for computer vision (detecting patterns in visual data).</p> <p>\ud83d\udd11 Note: In deep learning, many different kinds of model architectures can be used for different problems. For example, you could use a convolutional neural network for making predictions on image data and/or text data. However, in practice some architectures typically work better than others.</p> <p>For example, you might want to:</p> <ul> <li>Classify whether a picture of food contains pizza \ud83c\udf55 or steak \ud83e\udd69 (we're going to do this)</li> <li>Detect whether or not an object appears in an image (e.g. did a specific car pass through a security camera?)</li> </ul> <p>In this notebook, we're going to follow the TensorFlow modelling workflow we've been following so far whilst learning about how to build and use CNNs.</p>"},{"location":"03_convolutional_neural_networks_in_tensorflow/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>Specifically, we're going to go through the follow with TensorFlow:</p> <ul> <li>Getting a dataset to work with</li> <li>Architecture of a convolutional neural network</li> <li>A quick end-to-end example (what we're working towards)</li> <li>Steps in modelling for binary image classification with CNNs<ul> <li>Becoming one with the data</li> <li>Preparing data for modelling</li> <li>Creating a CNN model (starting with a baseline)</li> <li>Fitting a model (getting it to find patterns in our data)</li> <li>Evaluating a model</li> <li>Improving a model</li> <li>Making a prediction with a trained model</li> </ul> </li> <li>Steps in modelling for multi-class image classification with CNNs</li> <li>Same as above (but this time with a different dataset)</li> </ul>"},{"location":"03_convolutional_neural_networks_in_tensorflow/#how-you-can-use-this-notebook","title":"How you can use this notebook\u00b6","text":"<p>You can read through the descriptions and the code (it should all run, except for the cells which error on purpose), but there's a better option.</p> <p>Write all of the code yourself.</p> <p>Yes. I'm serious. Create a new notebook, and rewrite each line by yourself. Investigate it, see if you can break it, why does it break?</p> <p>You don't have to write the text descriptions but writing the code yourself is a great way to get hands-on experience.</p> <p>Don't worry if you make mistakes, we all do. The way to get better and make less mistakes is to write more code.</p>"},{"location":"03_convolutional_neural_networks_in_tensorflow/#get-the-data","title":"Get the data\u00b6","text":"<p>Because convolutional neural networks work so well with images, to learn more about them, we're going to start with a dataset of images.</p> <p>The images we're going to work with are from the Food-101 dataset, a collection of 101 different categories of 101,000 (1000 images per category) real-world images of food dishes.</p> <p>To begin, we're only going to use two of the categories, pizza \ud83c\udf55 and steak \ud83e\udd69 and build a binary classifier.</p> <p>\ud83d\udd11 Note: To prepare the data we're using, preprocessing steps such as, moving the images into different subset folders, have been done. To see these preprocessing steps check out the preprocessing notebook.</p> <p>We'll download the <code>pizza_steak</code> subset .zip file and unzip it.</p>"},{"location":"03_convolutional_neural_networks_in_tensorflow/#inspect-the-data-become-one-with-it","title":"Inspect the data (become one with it)\u00b6","text":"<p>A very crucial step at the beginning of any machine learning project is becoming one with the data. This usually means plenty of visualizing and folder scanning to understand the data you're working with.</p> <p>Wtih this being said, let's inspect the data we just downloaded.</p> <p>The file structure has been formatted to be in a typical format you might use for working with images.</p> <p>More specifically:</p> <ul> <li>A <code>train</code> directory which contains all of the images in the training dataset with subdirectories each named after a certain class containing images of that class.</li> <li>A <code>test</code> directory with the same structure as the <code>train</code> directory.</li> </ul> <pre><code>Example of file structure\n\npizza_steak &lt;- top level folder\n\u2514\u2500\u2500\u2500train &lt;- training images\n\u2502   \u2514\u2500\u2500\u2500pizza\n\u2502   \u2502   \u2502   1008104.jpg\n\u2502   \u2502   \u2502   1638227.jpg\n\u2502   \u2502   \u2502   ...      \n\u2502   \u2514\u2500\u2500\u2500steak\n\u2502       \u2502   1000205.jpg\n\u2502       \u2502   1647351.jpg\n\u2502       \u2502   ...\n\u2502   \n\u2514\u2500\u2500\u2500test &lt;- testing images\n\u2502   \u2514\u2500\u2500\u2500pizza\n\u2502   \u2502   \u2502   1001116.jpg\n\u2502   \u2502   \u2502   1507019.jpg\n\u2502   \u2502   \u2502   ...      \n\u2502   \u2514\u2500\u2500\u2500steak\n\u2502       \u2502   100274.jpg\n\u2502       \u2502   1653815.jpg\n\u2502       \u2502   ...    \n</code></pre> <p>Let's inspect each of the directories we've downloaded.</p> <p>To so do, we can use the command <code>ls</code> which stands for list.</p>"},{"location":"03_convolutional_neural_networks_in_tensorflow/#a-typical-architecture-of-a-convolutional-neural-network","title":"A (typical) architecture of a convolutional neural network\u00b6","text":"<p>Convolutional neural networks are no different to other kinds of deep learning neural networks in the fact they can be created in many different ways. What you see below are some components you'd expect to find in a traditional CNN.</p>"},{"location":"03_convolutional_neural_networks_in_tensorflow/#an-end-to-end-example","title":"An end-to-end example\u00b6","text":"<p>We've checked out our data and found there's 750 training images, as well as 250 test images per class and they're all of various different shapes.</p> <p>It's time to jump straight in the deep end.</p> <p>Reading the original dataset authors paper, we see they used a Random Forest machine learning model and averaged 50.76% accuracy at predicting what different foods different images had in them.</p> <p>From now on, that 50.76% will be our baseline.</p> <p>\ud83d\udd11 Note: A baseline is a score or evaluation metric you want to try and beat. Usually you'll start with a simple model, create a baseline and try to beat it by increasing the complexity of the model. A really fun way to learn machine learning is to find some kind of modelling paper with a published result and try to beat it.</p> <p>The code in the following cell replicates and end-to-end way to model our <code>pizza_steak</code> dataset with a convolutional neural network (CNN) using the components listed above.</p> <p>There will be a bunch of things you might not recognize but step through the code yourself and see if you can figure out what it's doing.</p> <p>We'll go through each of the steps later on in the notebook.</p> <p>For reference, the model we're using replicates TinyVGG, the computer vision architecture which fuels the CNN explainer webpage.</p> <p>\ud83d\udcd6 Resource: The architecture we're using below is a scaled-down version of VGG-16, a convolutional neural network which came 2nd in the 2014 ImageNet classification competition.</p>"},{"location":"03_convolutional_neural_networks_in_tensorflow/#using-the-same-model-as-before","title":"Using the same model as before\u00b6","text":"<p>To examplify how neural networks can be adapted to many different problems, let's see how a binary classification model we've previously built might work with our data.</p> <p>\ud83d\udd11 Note: If you haven't gone through the previous classification notebook, no troubles, we'll be bringing in the a simple 4 layer architecture used to separate dots replicated from the TensorFlow Playground environment.</p> <p>We can use all of the same parameters in our previous model except for changing two things:</p> <ul> <li>The data - we're now working with images instead of dots.</li> <li>The input shape - we have to tell our neural network the shape of the images we're working with.<ul> <li>A common practice is to reshape images all to one size. In our case, we'll resize the images to <code>(224, 224, 3)</code>, meaning a height and width of 224 pixels and a depth of 3 for the red, green, blue colour channels.</li> </ul> </li> </ul>"},{"location":"03_convolutional_neural_networks_in_tensorflow/#binary-classification-lets-break-it-down","title":"Binary classification: Let's break it down\u00b6","text":"<p>We just went through a whirlwind of steps:</p> <ol> <li>Become one with the data (visualize, visualize, visualize...)</li> <li>Preprocess the data (prepare it for a model)</li> <li>Create a model (start with a baseline)</li> <li>Fit the model</li> <li>Evaluate the model</li> <li>Adjust different parameters and improve model (try to beat your baseline)</li> <li>Repeat until satisfied</li> </ol> <p>Let's step through each.</p>"},{"location":"03_convolutional_neural_networks_in_tensorflow/#1-import-and-become-one-with-the-data","title":"1. Import and become one with the data\u00b6","text":"<p>Whatever kind of data you're dealing with, it's a good idea to visualize at least 10-100 samples to start to building your own mental model of the data.</p> <p>In our case, we might notice that the steak images tend to have darker colours where as pizza images tend to have a distinct circular shape in the middle. These might be patterns that our neural network picks up on.</p> <p>You an also notice if some of your data is messed up (for example, has the wrong label) and start to consider ways you might go about fixing it.</p> <p>\ud83d\udcd6 Resource: To see how this data was processed into the file format we're using, see the preprocessing notebook.</p> <p>If the visualization cell below doesn't work, make sure you've got the data by uncommenting the cell below.</p>"},{"location":"03_convolutional_neural_networks_in_tensorflow/#2-preprocess-the-data-prepare-it-for-a-model","title":"2. Preprocess the data (prepare it for a model)\u00b6","text":"<p>One of the most important steps for a machine learning project is creating a training and test set.</p> <p>In our case, our data is already split into training and test sets. Another option here might be to create a validation set as well, but we'll leave that for now.</p> <p>For an image classification project, it's standard to have your data seperated into <code>train</code> and <code>test</code> directories with subfolders in each for each class.</p> <p>To start we define the training and test directory paths.</p>"},{"location":"03_convolutional_neural_networks_in_tensorflow/#3-create-a-model-start-with-a-baseline","title":"3. Create a model (start with a baseline)\u00b6","text":"<p>You might be wondering what your default model architecture should be.</p> <p>And the truth is, there's many possible answers to this question.</p> <p>A simple heuristic for computer vision models is to use the model architecture which is performing best on ImageNet (a large collection of diverse images to benchmark different computer vision models).</p> <p>However, to begin with, it's good to build a smaller model to acquire a baseline result which you try to improve upon.</p> <p>\ud83d\udd11 Note: In deep learning a smaller model often refers to a model with less layers than the state of the art (SOTA). For example, a smaller model might have 3-4 layers where as a state of the art model, such as, ResNet50 might have 50+ layers.</p> <p>In our case, let's take a smaller version of the model that can be found on the CNN explainer website (<code>model_1</code> from above) and build a 3 layer convolutional neural network.</p>"},{"location":"03_convolutional_neural_networks_in_tensorflow/#4-fit-a-model","title":"4. Fit a model\u00b6","text":"<p>Our model is compiled, time to fit it.</p> <p>You'll notice two new parameters here:</p> <ul> <li><code>steps_per_epoch</code> - this is the number of batches a model will go through per epoch, in our case, we want our model to go through all batches so it's equal to the length of <code>train_data</code> (1500 images in batches of 32 = 1500/32 = ~47 steps)</li> <li><code>validation_steps</code> - same as above, except for the <code>validation_data</code> parameter (500 test images in batches of 32 = 500/32 = ~16 steps)</li> </ul>"},{"location":"03_convolutional_neural_networks_in_tensorflow/#5-evaluate-the-model","title":"5. Evaluate the model\u00b6","text":""},{"location":"03_convolutional_neural_networks_in_tensorflow/#6-adjust-the-model-parameters","title":"6. Adjust the model parameters\u00b6","text":"<p>Fitting a machine learning model  comes in 3 steps: 0. Create a basline.</p> <ol> <li>Beat the baseline by overfitting a larger model.</li> <li>Reduce overfitting.</li> </ol> <p>So far we've gone through steps 0 and 1.</p> <p>And there are even a few more things we could try to further overfit our model:</p> <ul> <li>Increase the number of convolutional layers.</li> <li>Increase the number of convolutional filters.</li> <li>Add another dense layer to the output of our flattened layer.</li> </ul> <p>But what we'll do instead is focus on getting our model's training curves to better align with eachother, in other words, we'll take on step 2.</p> <p>Why is reducing overfitting important?</p> <p>When a model performs too well on training data and poorly on unseen data, it's not much use to us if we wanted to use it in the real world.</p> <p>Say we were building a pizza vs. steak food classifier app, and our model performs very well on our training data but when users tried it out, they didn't get very good results on their own food images, is that a good experience?</p> <p>Not really...</p> <p>So for the next few models we build, we're going to adjust a number of parameters and inspect the training curves along the way.</p> <p>Namely, we'll build 2 more models:</p> <ul> <li>A ConvNet with max pooling</li> <li>A ConvNet with max pooling and data augmentation</li> </ul> <p>For the first model, we'll follow the modified basic CNN structure:</p> <pre><code>Input -&gt; Conv layers + ReLU layers (non-linearities) + Max Pooling layers -&gt; Fully connected (dense layer) as Output\n</code></pre> <p>Let's built it. It'll have the same structure as <code>model_4</code> but with a <code>MaxPool2D()</code> layer after each convolutional layer.</p>"},{"location":"03_convolutional_neural_networks_in_tensorflow/#7-repeat-until-satisified","title":"7. Repeat until satisified\u00b6","text":"<p>We've trained a few model's on our dataset already and so far they're performing pretty good.</p> <p>Since we've already beaten our baseline, there are a few things we could try to continue to improve our model:</p> <ul> <li>Increase the number of model layers (e.g. add more convolutional layers).</li> <li>Increase the number of filters in each convolutional layer (e.g. from 10 to 32, 64, or 128, these numbers aren't set in stone either, they are usually found through trial and error).</li> <li>Train for longer (more epochs).</li> <li>Finding an ideal learning rate.</li> <li>Get more data (give the model more opportunities to learn).</li> <li>Use transfer learning to leverage what another image model has learned and adjust it for our own use case.</li> </ul> <p>Adjusting each of these settings (except for the last two) during model development is usually referred to as hyperparameter tuning.</p> <p>You can think of hyperparameter tuning as simialr to adjusting the settings on your oven to cook your favourite dish. Although your oven does most of the cooking for you, you can help it by tweaking the dials.</p> <p>Let's go back to right where we started and try our original model (<code>model_1</code> or the TinyVGG architecture from CNN explainer).</p>"},{"location":"03_convolutional_neural_networks_in_tensorflow/#making-a-prediction-with-our-trained-model","title":"Making a prediction with our trained model\u00b6","text":"<p>What good is a trained model if you can't make predictions with it?</p> <p>To really test it out, we'll upload a couple of our own images and see how the model goes.</p> <p>First, let's remind ourselves of the classnames and view the image we're going to test on.</p>"},{"location":"03_convolutional_neural_networks_in_tensorflow/#multi-class-classification","title":"Multi-class Classification\u00b6","text":"<p>We've referenced the TinyVGG architecture from the CNN Explainer website multiple times through this notebook, however, the CNN Explainer website works with 10 different image classes, where as our current model only works with two classes (pizza and steak).</p> <p>\ud83d\udee0 Practice: Before scrolling down, how do you think we might change our model to work with 10 classes of the same kind of images? Assume the data is in the same style as our two class problem.</p> <p>Remember the steps we took before to build our pizza \ud83c\udf55 vs. steak \ud83e\udd69 classifier?</p> <p>How about we go through those steps again, except this time, we'll work with 10 different types of food.</p> <ol> <li>Become one with the data (visualize, visualize, visualize...)</li> <li>Preprocess the data (prepare it for a model)</li> <li>Create a model (start with a baseline)</li> <li>Fit the model</li> <li>Evaluate the model</li> <li>Adjust different parameters and improve model (try to beat your baseline)</li> <li>Repeat until satisfied</li> </ol> <p> The workflow we're about to go through is a slightly modified version of the above image. As you keep going through deep learning problems, you'll find the workflow above is more of an outline than a step-by-step guide.</p>"},{"location":"03_convolutional_neural_networks_in_tensorflow/#1-import-and-become-one-with-the-data","title":"1. Import and become one with the data\u00b6","text":"<p>Again, we've got a subset of the Food101 dataset. In addition to the pizza and steak images, we've pulled out another eight classes.</p>"},{"location":"03_convolutional_neural_networks_in_tensorflow/#2-preprocess-the-data-prepare-it-for-a-model","title":"2. Preprocess the data (prepare it for a model)\u00b6","text":"<p>After going through a handful of images (it's good to visualize at least 10-100 different examples), it looks like our data directories are setup correctly.</p> <p>Time to preprocess the data.</p>"},{"location":"03_convolutional_neural_networks_in_tensorflow/#3-create-a-model-start-with-a-baseline","title":"3. Create a model (start with a baseline)\u00b6","text":"<p>We can use the same model (TinyVGG) we used for the binary classification problem for our multi-class classification problem with a couple of small tweaks.</p> <p>Namely:</p> <ul> <li>Changing the output layer to use have 10 ouput neurons (the same number as the number of classes we have).</li> <li>Changing the output layer to use <code>'softmax'</code> activation instead of <code>'sigmoid'</code> activation.</li> <li>Changing the loss function to be <code>'categorical_crossentropy'</code> instead of <code>'binary_crossentropy'</code>.</li> </ul>"},{"location":"03_convolutional_neural_networks_in_tensorflow/#4-fit-a-model","title":"4. Fit a model\u00b6","text":"<p>Now we've got a model suited for working with multiple classes, let's fit it to our data.</p>"},{"location":"03_convolutional_neural_networks_in_tensorflow/#5-evaluate-the-model","title":"5. Evaluate the model\u00b6","text":"<p>Woohoo! We've just trained a model on 10 different classes of food images, let's see how it went.</p>"},{"location":"03_convolutional_neural_networks_in_tensorflow/#6-adjust-the-model-parameters","title":"6. Adjust the model parameters\u00b6","text":"<p>Due to its performance on the training data, it's clear our model is learning something. However, performing well on the training data is like going well in the classroom but failing to use your skills in real life.</p> <p>Ideally, we'd like our model to perform as well on the test data as it does on the training data.</p> <p>So our next steps will be to try and prevent our model overfitting. A couple of ways to prevent overfitting include:</p> <ul> <li>Get more data - Having more data gives the model more opportunities to learn patterns, patterns which may be more generalizable to new examples.</li> <li>Simplify model - If the current model is already overfitting the training data, it may be too complicated of a model. This means it's learning the patterns of the data too well and isn't able to generalize well to unseen data. One way to simplify a model is to reduce the number of layers it uses or to reduce the number of hidden units in each layer.</li> <li>Use data augmentation - Data augmentation manipulates the training data in a way so that's harder for the model to learn as it artificially adds more variety to the data. If a model is able to learn patterns in augmented data, the model may be able to generalize better to unseen data.</li> <li>Use transfer learning - Transfer learning involves leverages the patterns (also called pretrained weights) one model has learned to use as the foundation for your own task. In our case, we could use one computer vision model pretrained on a large variety of images and then tweak it slightly to be more specialized for food images.</li> </ul> <p>\ud83d\udd11 Note: Preventing overfitting is also referred to as regularization.</p> <p>If you've already got an existing dataset, you're probably most likely to try one or a combination of the last three above options first.</p> <p>Since collecting more data would involve us manually taking more images of food, let's try the ones we can do from right within the notebook.</p> <p>How about we simplify our model first?</p> <p>To do so, we'll remove two of the convolutional layers, taking the total number of convolutional layers from four to two.</p>"},{"location":"03_convolutional_neural_networks_in_tensorflow/#7-repeat-until-satisfied","title":"7. Repeat until satisfied\u00b6","text":"<p>We could keep going here. Restructuring our model's architecture, adding more layers, trying it out, adjusting the learning rate, trying it out, trying different methods of data augmentation, training for longer. But as you could image, this could take a fairly long time.</p> <p>Good thing there's still one trick we haven't tried yet and that's transfer learning.</p> <p>However, we'll save that for the next notebook where you'll see how rather than design our own models from scratch we leverage the patterns another model has learned for our own task.</p> <p>In the meantime, let's make a prediction with our trained multi-class model.</p>"},{"location":"03_convolutional_neural_networks_in_tensorflow/#making-a-prediction-with-our-trained-model","title":"Making a prediction with our trained model\u00b6","text":"<p>What good is a model if you can't make predictions with it?</p> <p>Let's first remind ourselves of the classes our multi-class model has been trained on and then we'll download some of own custom images to work with.</p>"},{"location":"03_convolutional_neural_networks_in_tensorflow/#saving-and-loading-our-model","title":"Saving and loading our model\u00b6","text":"<p>Once you've trained a model, you probably want to be able to save it and load it somewhere else.</p> <p>To do so, we can use the <code>save</code> and <code>load_model</code> functions.</p>"},{"location":"03_convolutional_neural_networks_in_tensorflow/#exercises","title":"\ud83d\udee0 Exercises\u00b6","text":"<ol> <li>Spend 20-minutes reading and interacting with the CNN explainer website.</li> </ol> <ul> <li>What are the key terms? e.g. explain convolution in your own words, pooling in your own words</li> </ul> <ol> <li>Play around with the \"understanding hyperparameters\" section in the CNN explainer website for 10-minutes.</li> </ol> <ul> <li>What is the kernel size?</li> <li>What is the stride?</li> <li>How could you adjust each of these in TensorFlow code?</li> </ul> <ol> <li><p>Take 10 photos of two different things and build your own CNN image classifier using the techniques we've built here.</p> </li> <li><p>Find an ideal learning rate for a simple convolutional neural network model on your the 10 class dataset.</p> </li> </ol>"},{"location":"03_convolutional_neural_networks_in_tensorflow/#extra-curriculum","title":"\ud83d\udcd6 Extra-curriculum\u00b6","text":"<ol> <li><p>Watch: MIT's Introduction to Deep Computer Vision lecture. This will give you a great intuition behind convolutional neural networks.</p> </li> <li><p>Watch: Deep dive on mini-batch gradient descent by deeplearning.ai. If you're still curious about why we use batches to train models, this technical overview covers many of the reasons why.</p> </li> <li><p>Read: CS231n Convolutional Neural Networks for Visual Recognition class notes. This will give a very deep understanding of what's going on behind the scenes of the convolutional neural network architectures we're writing.</p> </li> <li><p>Read: \"A guide to convolution arithmetic for deep learning\". This paper goes through all of the mathematics running behind the scenes of our convolutional layers.</p> </li> <li><p>Code practice: TensorFlow Data Augmentation Tutorial. For a more in-depth introduction on data augmentation with TensorFlow, spend an hour or two reading through this tutorial.</p> </li> </ol>"},{"location":"04_transfer_learning_in_tensorflow_part_1_feature_extraction/","title":"04. Transfer Learning with TensorFlow Part 1: Feature Extraction","text":"In\u00a0[23]: Copied! <pre># Add timestamp\nimport datetime\nprint(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\")\n</pre> # Add timestamp import datetime print(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\") <pre>Notebook last run (end-to-end): 2023-05-11 04:43:01.173110\n</pre> In\u00a0[1]: Copied! <pre># Are we using a GPU?\n!nvidia-smi\n</pre> # Are we using a GPU? !nvidia-smi <pre>Thu May 11 04:22:37 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   40C    P0    46W / 400W |      0MiB / 40960MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</pre> <p>If the cell above doesn't output something which looks like:</p> <pre><code>Fri Sep  4 03:35:21 2020       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                 ERR! |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre> <p>Go to Runtime -&gt; Change Runtime Type -&gt; Hardware Accelerator and select \"GPU\", then rerun the cell above.</p> In\u00a0[2]: Copied! <pre># Get data (10% of labels)\nimport zipfile\n\n# Download data\n!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n\n# Unzip the downloaded file\nzip_ref = zipfile.ZipFile(\"10_food_classes_10_percent.zip\", \"r\")\nzip_ref.extractall()\nzip_ref.close()\n</pre> # Get data (10% of labels) import zipfile  # Download data !wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip  # Unzip the downloaded file zip_ref = zipfile.ZipFile(\"10_food_classes_10_percent.zip\", \"r\") zip_ref.extractall() zip_ref.close() <pre>--2023-05-11 04:22:37--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\nResolving storage.googleapis.com (storage.googleapis.com)... 74.125.68.128, 74.125.24.128, 142.250.4.128, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|74.125.68.128|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 168546183 (161M) [application/zip]\nSaving to: \u201810_food_classes_10_percent.zip\u2019\n\n10_food_classes_10_ 100%[===================&gt;] 160.74M  19.2MB/s    in 9.7s    \n\n2023-05-11 04:22:48 (16.5 MB/s) - \u201810_food_classes_10_percent.zip\u2019 saved [168546183/168546183]\n\n</pre> In\u00a0[3]: Copied! <pre># How many images in each folder?\nimport os\n\n# Walk through 10 percent data directory and list number of files\nfor dirpath, dirnames, filenames in os.walk(\"10_food_classes_10_percent\"):\n  print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n</pre> # How many images in each folder? import os  # Walk through 10 percent data directory and list number of files for dirpath, dirnames, filenames in os.walk(\"10_food_classes_10_percent\"):   print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\") <pre>There are 2 directories and 0 images in '10_food_classes_10_percent'.\nThere are 10 directories and 0 images in '10_food_classes_10_percent/test'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_wings'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/fried_rice'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/ice_cream'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/grilled_salmon'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_curry'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/hamburger'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/steak'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/pizza'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/sushi'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/ramen'.\nThere are 10 directories and 0 images in '10_food_classes_10_percent/train'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_wings'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/fried_rice'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/ice_cream'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/grilled_salmon'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_curry'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/hamburger'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/steak'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/pizza'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/sushi'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/ramen'.\n</pre> <p>Notice how each of the training directories now has 75 images rather than 750 images. This is key to demonstrating how well transfer learning can perform with less labelled images.</p> <p>The test directories still have the same amount of images. This means we'll be training on less data but evaluating our models on the same amount of test data.</p> In\u00a0[4]: Copied! <pre># Setup data inputs\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nIMAGE_SHAPE = (224, 224)\nBATCH_SIZE = 32\n\ntrain_dir = \"10_food_classes_10_percent/train/\"\ntest_dir = \"10_food_classes_10_percent/test/\"\n\ntrain_datagen = ImageDataGenerator(rescale=1/255.)\ntest_datagen = ImageDataGenerator(rescale=1/255.)\n\nprint(\"Training images:\")\ntrain_data_10_percent = train_datagen.flow_from_directory(train_dir,\n                                               target_size=IMAGE_SHAPE,\n                                               batch_size=BATCH_SIZE,\n                                               class_mode=\"categorical\")\n\nprint(\"Testing images:\")\ntest_data = train_datagen.flow_from_directory(test_dir,\n                                              target_size=IMAGE_SHAPE,\n                                              batch_size=BATCH_SIZE,\n                                              class_mode=\"categorical\")\n</pre> # Setup data inputs from tensorflow.keras.preprocessing.image import ImageDataGenerator  IMAGE_SHAPE = (224, 224) BATCH_SIZE = 32  train_dir = \"10_food_classes_10_percent/train/\" test_dir = \"10_food_classes_10_percent/test/\"  train_datagen = ImageDataGenerator(rescale=1/255.) test_datagen = ImageDataGenerator(rescale=1/255.)  print(\"Training images:\") train_data_10_percent = train_datagen.flow_from_directory(train_dir,                                                target_size=IMAGE_SHAPE,                                                batch_size=BATCH_SIZE,                                                class_mode=\"categorical\")  print(\"Testing images:\") test_data = train_datagen.flow_from_directory(test_dir,                                               target_size=IMAGE_SHAPE,                                               batch_size=BATCH_SIZE,                                               class_mode=\"categorical\") <pre>Training images:\nFound 750 images belonging to 10 classes.\nTesting images:\nFound 2500 images belonging to 10 classes.\n</pre> <p>Excellent! Loading in the data we can see we've got 750 images in the training dataset belonging to 10 classes (75 per class) and 2500 images in the test set belonging to 10 classes (250 per class).</p> In\u00a0[5]: Copied! <pre># Create tensorboard callback (functionized because need to create a new one for each model)\nimport datetime\ndef create_tensorboard_callback(dir_name, experiment_name):\n  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n      log_dir=log_dir\n  )\n  print(f\"Saving TensorBoard log files to: {log_dir}\")\n  return tensorboard_callback\n</pre> # Create tensorboard callback (functionized because need to create a new one for each model) import datetime def create_tensorboard_callback(dir_name, experiment_name):   log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")   tensorboard_callback = tf.keras.callbacks.TensorBoard(       log_dir=log_dir   )   print(f\"Saving TensorBoard log files to: {log_dir}\")   return tensorboard_callback <p>Because you're likely to run multiple experiments, it's a good idea to be able to track them in some way.</p> <p>In our case, our function saves a model's performance logs to a directory named <code>[dir_name]/[experiment_name]/[current_timestamp]</code>, where:</p> <ul> <li><code>dir_name</code> is the overall logs directory</li> <li><code>experiment_name</code> is the particular experiment</li> <li><code>current_timestamp</code> is the time the experiment started based on Python's <code>datetime.datetime().now()</code></li> </ul> <p>\ud83d\udd11 Note: Depending on your use case, the above experimenting tracking naming method may work or you might require something more specific. The good news is, the TensorBoard callback makes it easy to track modelling logs as long as you specify where to track them. So you can get as creative as you like with how you name your experiments, just make sure you or your team can understand them.</p> In\u00a0[6]: Copied! <pre>import tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras import layers\n</pre> import tensorflow as tf import tensorflow_hub as hub from tensorflow.keras import layers <p>Now we'll get the feature vector URLs of two common computer vision architectures, EfficientNetB0 (2019) and ResNetV250 (2016) from TensorFlow Hub using the steps above.</p> <p>We're getting both of these because we're going to compare them to see which performs better on our data.</p> <p>\ud83d\udd11 Note: Comparing different model architecture performance on the same data is a very common practice. The simple reason is because you want to know which model performs best for your problem.</p> <p>Update: As of 14 August 2021, EfficientNet V2 pretrained models are available on TensorFlow Hub. The original code in this notebook uses EfficientNet V1, it has been left unchanged. In my experiments with this dataset, V1 outperforms V2. Best to experiment with your own data and see what suits you.</p> In\u00a0[7]: Copied! <pre># Resnet 50 V2 feature vector\nresnet_url = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"\n\n# Original: EfficientNetB0 feature vector (version 1)\nefficientnet_url = \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\"\n\n# # New: EfficientNetB0 feature vector (version 2)\n# efficientnet_url = \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\"\n</pre> # Resnet 50 V2 feature vector resnet_url = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"  # Original: EfficientNetB0 feature vector (version 1) efficientnet_url = \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\"  # # New: EfficientNetB0 feature vector (version 2) # efficientnet_url = \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\" <p>These URLs link to a saved pretrained model on TensorFlow Hub.</p> <p>When we use them in our model, the model will automatically be downloaded for us to use.</p> <p>To do this, we can use the <code>KerasLayer()</code> model inside the TensorFlow hub library.</p> <p>Since we're going to be comparing two models, to save ourselves code, we'll create a function <code>create_model()</code>. This function will take a model's TensorFlow Hub URL, instatiate a Keras Sequential model with the appropriate number of output layers and return the model.</p> In\u00a0[8]: Copied! <pre>def create_model(model_url, num_classes=10):\n  \"\"\"Takes a TensorFlow Hub URL and creates a Keras Sequential model with it.\n  \n  Args:\n    model_url (str): A TensorFlow Hub feature extraction URL.\n    num_classes (int): Number of output neurons in output layer,\n      should be equal to number of target classes, default 10.\n\n  Returns:\n    An uncompiled Keras Sequential model with model_url as feature\n    extractor layer and Dense output layer with num_classes outputs.\n  \"\"\"\n  # Download the pretrained model and save it as a Keras layer\n  feature_extractor_layer = hub.KerasLayer(model_url,\n                                           trainable=False, # freeze the underlying patterns\n                                           name='feature_extraction_layer',\n                                           input_shape=IMAGE_SHAPE+(3,)) # define the input image shape\n  \n  # Create our own model\n  model = tf.keras.Sequential([\n    feature_extractor_layer, # use the feature extraction layer as the base\n    layers.Dense(num_classes, activation='softmax', name='output_layer') # create our own output layer      \n  ])\n\n  return model\n</pre> def create_model(model_url, num_classes=10):   \"\"\"Takes a TensorFlow Hub URL and creates a Keras Sequential model with it.      Args:     model_url (str): A TensorFlow Hub feature extraction URL.     num_classes (int): Number of output neurons in output layer,       should be equal to number of target classes, default 10.    Returns:     An uncompiled Keras Sequential model with model_url as feature     extractor layer and Dense output layer with num_classes outputs.   \"\"\"   # Download the pretrained model and save it as a Keras layer   feature_extractor_layer = hub.KerasLayer(model_url,                                            trainable=False, # freeze the underlying patterns                                            name='feature_extraction_layer',                                            input_shape=IMAGE_SHAPE+(3,)) # define the input image shape      # Create our own model   model = tf.keras.Sequential([     feature_extractor_layer, # use the feature extraction layer as the base     layers.Dense(num_classes, activation='softmax', name='output_layer') # create our own output layer         ])    return model <p>Great! Now we've got a function for creating a model, we'll use it to first create a model using the ResNetV250 architecture as our feature extraction layer.</p> <p>Once the model is instantiated, we'll compile it using <code>categorical_crossentropy</code> as our loss function, the Adam optimizer and accuracy as our metric.</p> In\u00a0[9]: Copied! <pre># Create model\nresnet_model = create_model(resnet_url, num_classes=train_data_10_percent.num_classes)\n\n# Compile\nresnet_model.compile(loss='categorical_crossentropy',\n                     optimizer=tf.keras.optimizers.Adam(),\n                     metrics=['accuracy'])\n</pre> # Create model resnet_model = create_model(resnet_url, num_classes=train_data_10_percent.num_classes)  # Compile resnet_model.compile(loss='categorical_crossentropy',                      optimizer=tf.keras.optimizers.Adam(),                      metrics=['accuracy']) <p> What our current model looks like. A ResNet50V2 backbone with a custom dense layer on top (10 classes instead of 1000 ImageNet classes). Note: The Image shows ResNet34 instead of ResNet50. Image source: https://arxiv.org/abs/1512.03385.</p> <p>Beautiful. Time to fit the model.</p> <p>We've got the training data ready in <code>train_data_10_percent</code> as well as the test data saved as <code>test_data</code>.</p> <p>But before we call the fit function, there's one more thing we're going to add, a callback. More specifically, a TensorBoard callback so we can track the performance of our model on TensorBoard.</p> <p>We can add a callback to our model by using the <code>callbacks</code> parameter in the fit function.</p> <p>In our case, we'll pass the <code>callbacks</code> parameter the <code>create_tensorboard_callback()</code> we created earlier with some specific inputs so we know what experiments we're running.</p> <p>Let's keep this experiment short and train for 5 epochs.</p> In\u00a0[10]: Copied! <pre># Fit the model\nresnet_history = resnet_model.fit(train_data_10_percent,\n                                  epochs=5,\n                                  steps_per_epoch=len(train_data_10_percent),\n                                  validation_data=test_data,\n                                  validation_steps=len(test_data),\n                                  # Add TensorBoard callback to model (callbacks parameter takes a list)\n                                  callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub\", # save experiment logs here\n                                                                         experiment_name=\"resnet50V2\")]) # name of log files\n</pre> # Fit the model resnet_history = resnet_model.fit(train_data_10_percent,                                   epochs=5,                                   steps_per_epoch=len(train_data_10_percent),                                   validation_data=test_data,                                   validation_steps=len(test_data),                                   # Add TensorBoard callback to model (callbacks parameter takes a list)                                   callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub\", # save experiment logs here                                                                          experiment_name=\"resnet50V2\")]) # name of log files <pre>Saving TensorBoard log files to: tensorflow_hub/resnet50V2/20230511-042304\nEpoch 1/5\n24/24 [==============================] - 28s 680ms/step - loss: 1.7915 - accuracy: 0.3880 - val_loss: 1.1463 - val_accuracy: 0.6368\nEpoch 2/5\n24/24 [==============================] - 14s 620ms/step - loss: 0.8710 - accuracy: 0.7560 - val_loss: 0.8359 - val_accuracy: 0.7352\nEpoch 3/5\n24/24 [==============================] - 14s 618ms/step - loss: 0.6160 - accuracy: 0.8373 - val_loss: 0.7385 - val_accuracy: 0.7676\nEpoch 4/5\n24/24 [==============================] - 15s 624ms/step - loss: 0.4705 - accuracy: 0.8907 - val_loss: 0.7016 - val_accuracy: 0.7728\nEpoch 5/5\n24/24 [==============================] - 15s 640ms/step - loss: 0.3744 - accuracy: 0.9200 - val_loss: 0.6764 - val_accuracy: 0.7772\n</pre> <p>Wow!</p> <p>It seems that after only 5 epochs, the ResNetV250 feature extraction model was able to blow any of the architectures we made out of the water, achieving around 90% accuracy on the training set and nearly 80% accuracy on the test set...with only 10 percent of the training images!</p> <p>That goes to show the power of transfer learning. And it's one of the main reasons whenever you're trying to model your own datasets, you should look into what pretrained models already exist.</p> <p>Let's check out our model's training curves using our <code>plot_loss_curves</code> function.</p> In\u00a0[11]: Copied! <pre># If you wanted to, you could really turn this into a helper function to load in with a helper.py script...\nimport matplotlib.pyplot as plt\n\n# Plot the validation and training data separately\ndef plot_loss_curves(history):\n  \"\"\"\n  Returns separate loss curves for training and validation metrics.\n  \"\"\" \n  loss = history.history['loss']\n  val_loss = history.history['val_loss']\n\n  accuracy = history.history['accuracy']\n  val_accuracy = history.history['val_accuracy']\n\n  epochs = range(len(history.history['loss']))\n\n  # Plot loss\n  plt.plot(epochs, loss, label='training_loss')\n  plt.plot(epochs, val_loss, label='val_loss')\n  plt.title('Loss')\n  plt.xlabel('Epochs')\n  plt.legend()\n\n  # Plot accuracy\n  plt.figure()\n  plt.plot(epochs, accuracy, label='training_accuracy')\n  plt.plot(epochs, val_accuracy, label='val_accuracy')\n  plt.title('Accuracy')\n  plt.xlabel('Epochs')\n  plt.legend();\n</pre> # If you wanted to, you could really turn this into a helper function to load in with a helper.py script... import matplotlib.pyplot as plt  # Plot the validation and training data separately def plot_loss_curves(history):   \"\"\"   Returns separate loss curves for training and validation metrics.   \"\"\"    loss = history.history['loss']   val_loss = history.history['val_loss']    accuracy = history.history['accuracy']   val_accuracy = history.history['val_accuracy']    epochs = range(len(history.history['loss']))    # Plot loss   plt.plot(epochs, loss, label='training_loss')   plt.plot(epochs, val_loss, label='val_loss')   plt.title('Loss')   plt.xlabel('Epochs')   plt.legend()    # Plot accuracy   plt.figure()   plt.plot(epochs, accuracy, label='training_accuracy')   plt.plot(epochs, val_accuracy, label='val_accuracy')   plt.title('Accuracy')   plt.xlabel('Epochs')   plt.legend(); In\u00a0[12]: Copied! <pre>plot_loss_curves(resnet_history)\n</pre> plot_loss_curves(resnet_history) <p>And what about a summary of our model?</p> In\u00a0[13]: Copied! <pre># Resnet summary \nresnet_model.summary()\n</pre> # Resnet summary  resnet_model.summary() <pre>Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n feature_extraction_layer (K  (None, 2048)             23564800  \n erasLayer)                                                      \n                                                                 \n output_layer (Dense)        (None, 10)                20490     \n                                                                 \n=================================================================\nTotal params: 23,585,290\nTrainable params: 20,490\nNon-trainable params: 23,564,800\n_________________________________________________________________\n</pre> <p>You can see the power of TensorFlow Hub here. The feature extraction layer has 23,564,800 parameters which are prelearned patterns the model has already learned on the ImageNet dataset. Since we set <code>trainable=False</code>, these patterns remain frozen (non-trainable) during training.</p> <p>This means during training the model updates the 20,490 parameters in the output layer to suit our dataset.</p> <p>Okay, we've trained a ResNetV250 model, time to do the same with EfficientNetB0 model.</p> <p>The setup will be the exact same as before, except for the <code>model_url</code> parameter in the <code>create_model()</code> function and the <code>experiment_name</code> parameter in the <code>create_tensorboard_callback()</code> function.</p> In\u00a0[14]: Copied! <pre># Create model\nefficientnet_model = create_model(model_url=efficientnet_url, # use EfficientNetB0 TensorFlow Hub URL\n                                  num_classes=train_data_10_percent.num_classes)\n\n# Compile EfficientNet model\nefficientnet_model.compile(loss='categorical_crossentropy',\n                           optimizer=tf.keras.optimizers.Adam(),\n                           metrics=['accuracy'])\n\n# Fit EfficientNet model \nefficientnet_history = efficientnet_model.fit(train_data_10_percent, # only use 10% of training data\n                                              epochs=5, # train for 5 epochs\n                                              steps_per_epoch=len(train_data_10_percent),\n                                              validation_data=test_data,\n                                              validation_steps=len(test_data),\n                                              callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub\", \n                                                                                     # Track logs under different experiment name\n                                                                                     experiment_name=\"efficientnetB0\")])\n</pre> # Create model efficientnet_model = create_model(model_url=efficientnet_url, # use EfficientNetB0 TensorFlow Hub URL                                   num_classes=train_data_10_percent.num_classes)  # Compile EfficientNet model efficientnet_model.compile(loss='categorical_crossentropy',                            optimizer=tf.keras.optimizers.Adam(),                            metrics=['accuracy'])  # Fit EfficientNet model  efficientnet_history = efficientnet_model.fit(train_data_10_percent, # only use 10% of training data                                               epochs=5, # train for 5 epochs                                               steps_per_epoch=len(train_data_10_percent),                                               validation_data=test_data,                                               validation_steps=len(test_data),                                               callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub\",                                                                                       # Track logs under different experiment name                                                                                      experiment_name=\"efficientnetB0\")]) <pre>Saving TensorBoard log files to: tensorflow_hub/efficientnetB0/20230511-042443\nEpoch 1/5\n24/24 [==============================] - 26s 693ms/step - loss: 1.7622 - accuracy: 0.5053 - val_loss: 1.2434 - val_accuracy: 0.7268\nEpoch 2/5\n24/24 [==============================] - 15s 634ms/step - loss: 1.0120 - accuracy: 0.7693 - val_loss: 0.8459 - val_accuracy: 0.8240\nEpoch 3/5\n24/24 [==============================] - 15s 642ms/step - loss: 0.7289 - accuracy: 0.8347 - val_loss: 0.6823 - val_accuracy: 0.8460\nEpoch 4/5\n24/24 [==============================] - 15s 637ms/step - loss: 0.5871 - accuracy: 0.8667 - val_loss: 0.6004 - val_accuracy: 0.8592\nEpoch 5/5\n24/24 [==============================] - 14s 620ms/step - loss: 0.5013 - accuracy: 0.8947 - val_loss: 0.5504 - val_accuracy: 0.8636\n</pre> <p>Holy smokes! The EfficientNetB0 model does even better than the ResNetV250 model! Achieving over 85% accuracy on the test set...again with only 10% of the training data.</p> <p>How cool is that?</p> <p>With a couple of lines of code we're able to leverage state of the art models and adjust them to our own use case.</p> <p>Let's check out the loss curves.</p> In\u00a0[15]: Copied! <pre>plot_loss_curves(efficientnet_history)\n</pre> plot_loss_curves(efficientnet_history) <p>From the look of the EfficientNetB0 model's loss curves, it looks like if we kept training our model for longer, it might improve even further. Perhaps that's something you might want to try?</p> <p>Let's check out the model summary.</p> In\u00a0[16]: Copied! <pre>efficientnet_model.summary()\n</pre> efficientnet_model.summary() <pre>Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n feature_extraction_layer (K  (None, 1280)             4049564   \n erasLayer)                                                      \n                                                                 \n output_layer (Dense)        (None, 10)                12810     \n                                                                 \n=================================================================\nTotal params: 4,062,374\nTrainable params: 12,810\nNon-trainable params: 4,049,564\n_________________________________________________________________\n</pre> <p>It seems despite having over four times less parameters (4,049,564 vs. 23,564,800) than the ResNet50V2 extraction layer, the  EfficientNetB0 feature extraction layer yields better performance. Now it's clear where the \"efficient\" name came from.</p> In\u00a0[17]: Copied! <pre># Upload TensorBoard dev records\n!tensorboard dev upload --logdir ./tensorflow_hub/ \\\n  --name \"EfficientNetB0 vs. ResNet50V2\" \\\n  --description \"Comparing two different TF Hub feature extraction models architectures using 10% of training images\" \\\n  --one_shot\n</pre> # Upload TensorBoard dev records !tensorboard dev upload --logdir ./tensorflow_hub/ \\   --name \"EfficientNetB0 vs. ResNet50V2\" \\   --description \"Comparing two different TF Hub feature extraction models architectures using 10% of training images\" \\   --one_shot <pre>2023-05-11 04:26:10.912881: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n\n***** TensorBoard Uploader *****\n\nThis will upload your TensorBoard logs to https://tensorboard.dev/ from\nthe following directory:\n\n./tensorflow_hub/\n\nThis TensorBoard will be visible to everyone. Do not upload sensitive\ndata.\n\nYour use of this service is subject to Google's Terms of Service\n&lt;https://policies.google.com/terms&gt; and Privacy Policy\n&lt;https://policies.google.com/privacy&gt;, and TensorBoard.dev's Terms of Service\n&lt;https://tensorboard.dev/policy/terms/&gt;.\n\nThis notice will not be shown again while you are logged into the uploader.\nTo log out, run `tensorboard dev auth revoke`.\n\nContinue? (yes/NO) yes\n\nTo sign in with the TensorBoard uploader:\n\n1. On your computer or phone, visit:\n\n   https://www.google.com/device\n\n2. Sign in with your Google account, then enter:\n\n   PCB-DVW-YTS\n\n\n\nNew experiment created. View your TensorBoard at: https://tensorboard.dev/experiment/dIzMI7IkT7OHD1PmA4mMRQ/\n\n[2023-05-11T04:41:27] Started scanning logdir.\n[2023-05-11T04:41:32] Total uploaded: 60 scalars, 0 tensors, 2 binary objects (5.7 MB)\n[2023-05-11T04:41:32] Done scanning logdir.\n\n\nDone. View your TensorBoard at https://tensorboard.dev/experiment/dIzMI7IkT7OHD1PmA4mMRQ/\n</pre> <p>Every time you upload something to TensorBoad.dev you'll get a new experiment ID. The experiment ID will look something like this: https://tensorboard.dev/experiment/73taSKxXQeGPQsNBcVvY3g/ (this is the actual experiment from this notebook).</p> <p>If you upload the same directory again, you'll get a new experiment ID to go along with it.</p> <p>This means to track your experiments, you may want to look into how you name your uploads. That way when you find them on TensorBoard.dev you can tell what happened during each experiment (e.g. \"efficientnet0_10_percent_data\").</p> In\u00a0[21]: Copied! <pre># Check out experiments\n# !tensorboard dev list # uncomment to see\n</pre> # Check out experiments # !tensorboard dev list # uncomment to see In\u00a0[19]: Copied! <pre># Delete an experiment\n!tensorboard dev delete --experiment_id n6kd8XZ3Rdy1jSgSLH5WjA\n</pre> # Delete an experiment !tensorboard dev delete --experiment_id n6kd8XZ3Rdy1jSgSLH5WjA <pre>2023-05-11 04:41:41.121171: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nNo such experiment n6kd8XZ3Rdy1jSgSLH5WjA. Either it never existed or it has already been deleted.\n\n</pre> In\u00a0[22]: Copied! <pre># Check to see if experiments still exist\n# !tensorboard dev list # uncomment to see\n</pre> # Check to see if experiments still exist # !tensorboard dev list # uncomment to see"},{"location":"04_transfer_learning_in_tensorflow_part_1_feature_extraction/#04-transfer-learning-with-tensorflow-part-1-feature-extraction","title":"04. Transfer Learning with TensorFlow Part 1: Feature Extraction\u00b6","text":"<p>We've built a bunch of convolutional neural networks from scratch and they all seem to be learning, however, there is still plenty of room for improvement.</p> <p>To improve our model(s), we could spend a while trying different configurations, adding more layers, changing the learning rate, adjusting the number of neurons per layer and more.</p> <p>However, doing this is very time consuming.</p> <p>Luckily, there's a technique we can use to save time.</p> <p>It's called transfer learning, in other words, taking the patterns (also called weights) another model has learned from another problem and using them for our own problem.</p> <p>There are two main benefits to using transfer learning:</p> <ol> <li>Can leverage an existing neural network architecture proven to work on problems similar to our own.</li> <li>Can leverage a working neural network architecture which has already learned patterns on similar data to our own. This often results in achieving great results with less custom data.</li> </ol> <p>What this means is, instead of hand-crafting our own neural network architectures or building them from scratch, we can utilise models which have worked for others.</p> <p>And instead of training our own models from scratch on our own datasets, we can take the patterns a model has learned from datasets such as ImageNet (millions of images of different objects) and use them as the foundation of our own. Doing this often leads to getting great results with less data.</p> <p>Over the next few notebooks, we'll see the power of transfer learning in action.</p>"},{"location":"04_transfer_learning_in_tensorflow_part_1_feature_extraction/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>We're going to go through the following with TensorFlow:</p> <ul> <li>Introduce transfer learning (a way to beat all of our old self-built models)</li> <li>Using a smaller dataset to experiment faster (10% of training samples of 10 classes of food)</li> <li>Build a transfer learning feature extraction model using TensorFlow Hub</li> <li>Introduce the TensorBoard callback to track model training results</li> <li>Compare model results using TensorBoard</li> </ul>"},{"location":"04_transfer_learning_in_tensorflow_part_1_feature_extraction/#how-you-can-use-this-notebook","title":"How you can use this notebook\u00b6","text":"<p>You can read through the descriptions and the code (it should all run, except for the cells which error on purpose), but there's a better option.</p> <p>Write all of the code yourself.</p> <p>Yes. I'm serious. Create a new notebook, and rewrite each line by yourself. Investigate it, see if you can break it, why does it break?</p> <p>You don't have to write the text descriptions but writing the code yourself is a great way to get hands-on experience.</p> <p>Don't worry if you make mistakes, we all do. The way to get better and make less mistakes is to write more code.</p>"},{"location":"04_transfer_learning_in_tensorflow_part_1_feature_extraction/#using-a-gpu","title":"Using a GPU\u00b6","text":"<p>To begin, let's check to see if we're using a GPU. Using a GPU will make sure our model trains faster than using just a CPU.</p>"},{"location":"04_transfer_learning_in_tensorflow_part_1_feature_extraction/#transfer-leanring-with-tensorflow-hub-getting-great-results-with-10-of-the-data","title":"Transfer leanring with TensorFlow Hub: Getting great results with 10% of the data\u00b6","text":"<p>If you've been thinking, \"surely someone else has spent the time crafting the right model for the job...\" then you're in luck.</p> <p>For many of the problems you'll want to use deep learning for, chances are, a working model already exists.</p> <p>And the good news is, you can access many of them on TensorFlow Hub.</p> <p>TensorFlow Hub is a repository for existing model components. It makes it so you can import and use a fully trained model with as little as a URL.</p> <p>Now, I really want to demonstrate the power of transfer learning to you.</p> <p>To do so, what if I told you we could get much of the same results (or better) than our best model has gotten so far with only 10% of the original data, in other words, 10x less data.</p> <p>This seems counterintuitive right?</p> <p>Wouldn't you think more examples of what a picture of food looked like led to better results?</p> <p>And you'd be right if you thought so, generally, more data leads to better results.</p> <p>However, what if you didn't have more data? What if instead of 750 images per class, you had 75 images per class?</p> <p>Collecting 675 more images of a certain class could take a long time.</p> <p>So this is where another major benefit of transfer learning comes in.</p> <p>Transfer learning often allows you to get great results with less data.</p> <p>But don't just take my word for it. Let's download a subset of the data we've been using, namely 10% of the training data from the <code>10_food_classes</code> dataset and use it to train a food image classifier on.</p> <p> What we're working towards building. Taking a pre-trained model and adding our own custom layers on top, extracting all of the underlying patterns learned on another dataset our own images.</p>"},{"location":"04_transfer_learning_in_tensorflow_part_1_feature_extraction/#downloading-and-becoming-one-with-the-data","title":"Downloading and becoming one with the data\u00b6","text":""},{"location":"04_transfer_learning_in_tensorflow_part_1_feature_extraction/#creating-data-loaders-preparing-the-data","title":"Creating data loaders (preparing the data)\u00b6","text":"<p>Now we've downloaded the data, let's use the <code>ImageDataGenerator</code> class along with the <code>flow_from_directory</code> method to load in our images.</p>"},{"location":"04_transfer_learning_in_tensorflow_part_1_feature_extraction/#setting-up-callbacks-things-to-run-whilst-our-model-trains","title":"Setting up callbacks (things to run whilst our model trains)\u00b6","text":"<p>Before we build a model, there's an important concept we're going to get familiar with because it's going to play a key role in our future model building experiments.</p> <p>And that concept is callbacks.</p> <p>Callbacks are extra functionality you can add to your models to be performed during or after training. Some of the most popular callbacks include:</p> <ul> <li>Experiment tracking with TensorBoard - log the performance of multiple models and then view and compare these models in a visual way on TensorBoard (a dashboard for inspecting neural network parameters). Helpful to compare the results of different models on your data.</li> <li>Model checkpointing - save your model as it trains so you can stop training if needed and come back to continue off where you left. Helpful if training takes a long time and can't be done in one sitting.</li> <li>Early stopping - leave your model training for an arbitrary amount of time and have it stop training automatically when it ceases to improve. Helpful when you've got a large dataset and don't know how long training will take.</li> </ul> <p>We'll explore each of these overtime but for this notebook, we'll see how the TensorBoard callback can be used.</p> <p>The TensorBoard callback can be accessed using <code>tf.keras.callbacks.TensorBoard()</code>.</p> <p>Its main functionality is saving a model's training performance metrics to a specified <code>log_dir</code>.</p> <p>By default, logs are recorded every epoch using the <code>update_freq='epoch'</code> parameter. This is a good default since tracking model performance too often can slow down model training.</p> <p>To track our modelling experiments using TensorBoard, let's create a function which creates a TensorBoard callback for us.</p> <p>\ud83d\udd11 Note: We create a function for creating a TensorBoard callback because as we'll see later on, each model needs its own TensorBoard callback instance (so the function will create a new one each time it's run).</p>"},{"location":"04_transfer_learning_in_tensorflow_part_1_feature_extraction/#creating-models-using-tensorflow-hub","title":"Creating models using TensorFlow Hub\u00b6","text":"<p>In the past we've used TensorFlow to create our own models layer by layer from scratch.</p> <p>Now we're going to do a similar process, except the majority of our model's layers are going to come from TensorFlow Hub.</p> <p>In fact, we're going to use two models from TensorFlow Hub:</p> <ol> <li>ResNetV2 -  a state of the art computer vision model architecture from 2016.</li> <li>EfficientNet - a state of the art computer vision architecture from 2019.</li> </ol> <p>State of the art means that at some point, both of these models have achieved the lowest error rate on ImageNet (ILSVRC-2012-CLS), the gold standard of computer vision benchmarks.</p> <p>You might be wondering, how do you find these models on TensorFlow Hub?</p> <p>Here are the steps I took:</p> <ol> <li>Go to tfhub.dev.</li> <li>Choose your problem domain, e.g. \"Image\" (we're using food images).</li> <li>Select your TF version, which in our case is TF2.</li> <li>Remove all \"Problem domanin\" filters except for the problem you're working on.</li> </ol> <ul> <li>Note: \"Image feature vector\" can be used alongside almost any problem, we'll get to this soon.</li> </ul> <ol> <li>The models listed are all models which could potentially be used for your problem.</li> </ol> <p>\ud83e\udd14 Question: I see many options for image classification models, how do I know which is best?</p> <p>You can see a list of state of the art models on paperswithcode.com, a resource for collecting the latest in deep learning paper results which have code implementations for the findings they report.</p> <p>Since we're working with images, our target are the models which perform best on ImageNet.</p> <p>You'll probably find not all of the model architectures listed on paperswithcode appear on TensorFlow Hub. And this is okay, we can still use what's available.</p> <p>To find our models, let's narrow down our search using the Architecture tab.</p> <ol> <li>Select the Architecture tab on TensorFlow Hub and you'll see a dropdown menu of architecture names appear.</li> </ol> <ul> <li>The rule of thumb here is generally, names with larger numbers means better performing models. For example, EfficientNetB4 performs better than EfficientNetB0.<ul> <li>However, the tradeoff with larger numbers can mean they take longer to compute.</li> </ul> </li> </ul> <ol> <li>Select EfficientNetB0 and you should see something like the following: </li> <li>Clicking the one titled \"efficientnet/b0/feature-vector\" brings us to a page with a button that says \"Copy URL\". That URL is what we can use to harness the power of EfficientNetB0.</li> </ol> <ul> <li>Copying the URL should give you something like this: https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1</li> </ul> <p>\ud83e\udd14 Question: I thought we were doing image classification, why do we choose feature vector and not classification?</p> <p>Great observation. This is where the differnet types of transfer learning come into play, as is, feature extraction and fine-tuning.</p> <ol> <li>\"As is\" transfer learning is when you take a pretrained model as it is and apply it to your task without any changes.</li> </ol> <ul> <li><p>For example, many computer vision models are pretrained on the ImageNet dataset which contains 1000 different classes of images. This means passing a single image to this model will produce 1000 different prediction probability values (1 for each class).</p> <ul> <li>This is helpful if you have 1000 classes of image you'd like to classify and they're all the same as the ImageNet classes, however, it's not helpful if you want to classify only a small subset of classes (such as 10 different kinds of food). Model's with <code>\"/classification\"</code> in their name on TensorFlow Hub provide this kind of functionality.</li> </ul> </li> </ul> <ol> <li>Feature extraction transfer learning is when you take the underlying patterns (also called weights) a pretrained model has learned and adjust its outputs to be more suited to your problem.</li> </ol> <ul> <li><p>For example, say the pretrained model you were using had 236 different layers (EfficientNetB0 has 236 layers), but the top layer outputs 1000 classes because it was pretrained on ImageNet. To adjust this to your own problem, you might remove the original activation layer and replace it with your own but with the right number of output classes. The important part here is that only the top few layers become trainable, the rest remain frozen.</p> <ul> <li>This way all the underlying patterns remain in the rest of the layers and you can utilise them for your own problem. This kind of transfer learning is very helpful when your data is similar to the data a model has been pretrained on.</li> </ul> </li> </ul> <ol> <li><p>Fine-tuning transfer learning is when you take the underlying patterns (also called weights) of a pretrained model and adjust (fine-tune) them to your own problem.</p> <ul> <li>This usually means training some, many or all of the layers in the pretrained model. This is useful when you've got a large dataset (e.g. 100+ images per class) where your data is slightly different to the data the original model was trained on.</li> </ul> </li> </ol> <p>A common workflow is to \"freeze\" all of the learned patterns in the bottom layers of a pretrained model so they're untrainable. And then train the top 2-3 layers of so the pretrained model can adjust its outputs to your custom data (feature extraction).</p> <p>After you've trained the top 2-3 layers, you can then gradually \"unfreeze\" more and more layers and run the training process on your own data to further fine-tune the pretrained model.</p> <p>\ud83e\udd14 Question: Why train only the top 2-3 layers in feature extraction?</p> <p>The lower a layer is in a computer vision model as in, the closer it is to the input layer, the larger the features it learn. For example, a bottom layer in a computer vision model to identify images of cats or dogs might learn the outline of legs, where as, layers closer to the output might learn the shape of teeth. Often, you'll want the larger features (learned patterns are also called features) to remain, since these are similar for both animals, where as, the differences remain in the more fine-grained features.</p> <p> The different kinds of transfer learning. An original model, a feature extraction model (only top 2-3 layers change) and a fine-tuning model (many or all of original model get changed).</p> <p>Okay, enough talk, let's see this in action. Once we do, we'll explain what's happening.</p> <p>First we'll import TensorFlow and TensorFlow Hub.</p>"},{"location":"04_transfer_learning_in_tensorflow_part_1_feature_extraction/#comparing-models-using-tensorboard","title":"Comparing models using TensorBoard\u00b6","text":"<p>Alright, even though we've already compared the performance of our two models by looking at the accuracy scores. But what if you had more than two models?</p> <p>That's where an experiment tracking tool like TensorBoard (preinstalled in Google Colab) comes in.</p> <p>The good thing is, since we set up a TensorBoard callback, all of our model's training logs have been saved automatically. To visualize them, we can upload the results to TensorBoard.dev.</p> <p>Uploading your results to TensorBoard.dev enables you to track and share multiple different modelling experiments. So if you needed to show someone your results, you could send them a link to your TensorBoard.dev as well as the accompanying Colab notebook.</p> <p>\ud83d\udd11 Note: These experiments are public, do not upload sensitive data. You can delete experiments if needed.</p>"},{"location":"04_transfer_learning_in_tensorflow_part_1_feature_extraction/#uploading-experiments-to-tensorboard","title":"Uploading experiments to TensorBoard\u00b6","text":"<p>To upload a series of TensorFlow logs to TensorBoard, we can use the following command:</p> <pre><code>Upload TensorBoard dev records\n\n!tensorboard dev upload --logdir ./tensorflow_hub/ \\\n  --name \"EfficientNetB0 vs. ResNet50V2\" \\ \n  --description \"Comparing two different TF Hub feature extraction models architectures using 10% of training images\" \\ \n  --one_shot\n</code></pre> <p>Where:</p> <ul> <li><code>--logdir</code> is the target upload directory</li> <li><code>--name</code> is the name of the experiment</li> <li><code>--description</code> is a brief description of the experiment</li> <li><code>--one_shot</code> exits the TensorBoard uploader once uploading is finished</li> </ul> <p>Running the <code>tensorboard dev upload</code> command will first ask you to authorize the upload to TensorBoard.dev. After you've authorized the upload, your log files will be uploaded.</p>"},{"location":"04_transfer_learning_in_tensorflow_part_1_feature_extraction/#listing-experiments-youve-saved-to-tensorboard","title":"Listing experiments you've saved to TensorBoard\u00b6","text":"<p>To see all of the experiments you've uploaded you can use the command:</p> <p><code>tensorboard dev list</code></p>"},{"location":"04_transfer_learning_in_tensorflow_part_1_feature_extraction/#deleting-experiments-from-tensorboard","title":"Deleting experiments from TensorBoard\u00b6","text":"<p>Remember, all uploads to TensorBoard.dev are public, so to delete an experiment you can use the command:</p> <p><code>tensorboard dev delete --experiment_id [INSERT_EXPERIMENT_ID]</code></p>"},{"location":"04_transfer_learning_in_tensorflow_part_1_feature_extraction/#exercises","title":"\ud83d\udee0 Exercises\u00b6","text":"<ol> <li>Build and fit a model using the same data we have here but with the MobileNetV2 architecture feature extraction (<code>mobilenet_v2_100_224/feature_vector</code>) from TensorFlow Hub, how does it perform compared to our other models?</li> <li>Name 3 different image classification models on TensorFlow Hub that we haven't used.</li> <li>Build a model to classify images of two different things you've taken photos of.</li> </ol> <ul> <li>You can use any feature extraction layer from TensorFlow Hub you like for this.</li> <li>You should aim to have at least 10 images of each class, for example to build a fridge versus oven classifier, you'll want 10 images of fridges and 10 images of ovens.</li> </ul> <ol> <li>What is the current best performing model on ImageNet?</li> </ol> <ul> <li>Hint: you might want to check sotabench.com for this.</li> </ul>"},{"location":"04_transfer_learning_in_tensorflow_part_1_feature_extraction/#extra-curriculum","title":"\ud83d\udcd6 Extra-curriculum\u00b6","text":"<ul> <li>Read through the TensorFlow Transfer Learning Guide and define the main two types of transfer learning in your own words.</li> <li>Go through the Transfer Learning with TensorFlow Hub tutorial on the TensorFlow website and rewrite all of the code yourself into a new Google Colab notebook making comments about what each step does along the way.</li> <li>We haven't covered fine-tuning with TensorFlow Hub in this notebook, but if you'd like to know more, go through the fine-tuning a TensorFlow Hub model tutorial on the TensorFlow homepage.How to fine-tune a tensorflow hub model:</li> <li>Look into experiment tracking with Weights &amp; Biases, how could you integrate it with our existing TensorBoard logs?</li> </ul>"},{"location":"05_transfer_learning_in_tensorflow_part_2_fine_tuning/","title":"05. Transfer Learning with TensorFlow Part 2: Fine-tuning","text":"In\u00a0[1]: Copied! <pre>import datetime\nprint(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\")\n</pre> import datetime print(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\") <pre>Notebook last run (end-to-end): 2023-08-18 01:39:51.865865\n</pre> <p>\ud83d\udd11 Note: As of TensorFlow 2.10+ there seems to be issues with the <code>tf.keras.applications.efficientnet</code> models (used later on) when loading weights via the <code>load_weights()</code> methods.</p> <p>To fix this, I've updated the code to use <code>tf.keras.applications.efficientnet_v2</code>, this is a small change but results in far less errors.</p> <p>You can see the full write-up on the course GitHub.</p> In\u00a0[2]: Copied! <pre>import tensorflow as tf\n\nprint(f\"TensorFlow version: {tf.__version__}\")\n</pre> import tensorflow as tf  print(f\"TensorFlow version: {tf.__version__}\") <pre>TensorFlow version: 2.12.0\n</pre> In\u00a0[3]: Copied! <pre># Are we using a GPU? (if not &amp; you're using Google Colab, go to Runtime -&gt; Change Runtime Type -&gt; Harware Accelerator: GPU )\n!nvidia-smi\n</pre> # Are we using a GPU? (if not &amp; you're using Google Colab, go to Runtime -&gt; Change Runtime Type -&gt; Harware Accelerator: GPU ) !nvidia-smi <pre>Fri Aug 18 01:39:54 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   36C    P0    24W / 300W |      0MiB / 16384MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</pre> In\u00a0[4]: Copied! <pre># Get helper_functions.py script from course GitHub\n!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n\n# Import helper functions we're going to use\nfrom helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, walk_through_dir\n</pre> # Get helper_functions.py script from course GitHub !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py  # Import helper functions we're going to use from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, walk_through_dir <pre>--2023-08-18 01:39:54--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 10246 (10K) [text/plain]\nSaving to: \u2018helper_functions.py\u2019\n\n\rhelper_functions.py   0%[                    ]       0  --.-KB/s               \rhelper_functions.py 100%[===================&gt;]  10.01K  --.-KB/s    in 0s      \n\n2023-08-18 01:39:54 (41.3 MB/s) - \u2018helper_functions.py\u2019 saved [10246/10246]\n\n</pre> <p>Wonderful, now we've got a bunch of helper functions we can use throughout the notebook without having to rewrite them from scratch each time.</p> <p>\ud83d\udd11 Note: If you're running this notebook in Google Colab, when it times out Colab will delete the <code>helper_functions.py</code> file. So to use the functions imported above, you'll have to rerun the cell.</p> In\u00a0[5]: Copied! <pre># Get 10% of the data of the 10 classes\n!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n\nunzip_data(\"10_food_classes_10_percent.zip\")\n</pre> # Get 10% of the data of the 10 classes !wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip  unzip_data(\"10_food_classes_10_percent.zip\") <pre>--2023-08-18 01:39:55--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\nResolving storage.googleapis.com (storage.googleapis.com)... 173.194.203.128, 74.125.199.128, 74.125.195.128, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|173.194.203.128|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 168546183 (161M) [application/zip]\nSaving to: \u201810_food_classes_10_percent.zip\u2019\n\n10_food_classes_10_ 100%[===================&gt;] 160.74M   242MB/s    in 0.7s    \n\n2023-08-18 01:39:56 (242 MB/s) - \u201810_food_classes_10_percent.zip\u2019 saved [168546183/168546183]\n\n</pre> <p>The dataset we're downloading is the 10 food classes dataset (from Food 101) with 10% of the training images we used in the previous notebook.</p> <p>\ud83d\udd11 Note: You can see how this dataset was created in the image data modification notebook.</p> In\u00a0[6]: Copied! <pre># Walk through 10 percent data directory and list number of files\nwalk_through_dir(\"10_food_classes_10_percent\")\n</pre> # Walk through 10 percent data directory and list number of files walk_through_dir(\"10_food_classes_10_percent\") <pre>There are 2 directories and 0 images in '10_food_classes_10_percent'.\nThere are 10 directories and 0 images in '10_food_classes_10_percent/train'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/ramen'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_curry'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/pizza'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/ice_cream'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/grilled_salmon'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/steak'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_wings'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/hamburger'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/sushi'.\nThere are 0 directories and 75 images in '10_food_classes_10_percent/train/fried_rice'.\nThere are 10 directories and 0 images in '10_food_classes_10_percent/test'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/ramen'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_curry'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/pizza'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/ice_cream'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/grilled_salmon'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/steak'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_wings'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/hamburger'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/sushi'.\nThere are 0 directories and 250 images in '10_food_classes_10_percent/test/fried_rice'.\n</pre> <p>We can see that each of the training directories contain 75 images and each of the testing directories contain 250 images.</p> <p>Let's define our training and test filepaths.</p> In\u00a0[7]: Copied! <pre># Create training and test directories\ntrain_dir = \"10_food_classes_10_percent/train/\"\ntest_dir = \"10_food_classes_10_percent/test/\"\n</pre> # Create training and test directories train_dir = \"10_food_classes_10_percent/train/\" test_dir = \"10_food_classes_10_percent/test/\" <p>Now we've got some image data, we need a way of loading it into a TensorFlow compatible format.</p> <p>Previously, we've used the <code>ImageDataGenerator</code> class.</p> <p>However, as of August 2023, this class is deprecated and isn't recommended for future usage (it's too slow).</p> <p>Because of this, we'll move onto using <code>tf.keras.utils.image_dataset_from_directory()</code>.</p> <p>This method expects image data in the following file format:</p> <pre><code>Example of file structure\n\n10_food_classes_10_percent &lt;- top level folder\n\u2514\u2500\u2500\u2500train &lt;- training images\n\u2502   \u2514\u2500\u2500\u2500pizza\n\u2502   \u2502   \u2502   1008104.jpg\n\u2502   \u2502   \u2502   1638227.jpg\n\u2502   \u2502   \u2502   ...      \n\u2502   \u2514\u2500\u2500\u2500steak\n\u2502       \u2502   1000205.jpg\n\u2502       \u2502   1647351.jpg\n\u2502       \u2502   ...\n\u2502   \n\u2514\u2500\u2500\u2500test &lt;- testing images\n\u2502   \u2514\u2500\u2500\u2500pizza\n\u2502   \u2502   \u2502   1001116.jpg\n\u2502   \u2502   \u2502   1507019.jpg\n\u2502   \u2502   \u2502   ...      \n\u2502   \u2514\u2500\u2500\u2500steak\n\u2502       \u2502   100274.jpg\n\u2502       \u2502   1653815.jpg\n\u2502       \u2502   ...    \n</code></pre> <p>One of the main benefits of using <code>tf.keras.prepreprocessing.image_dataset_from_directory()</code> rather than <code>ImageDataGenerator</code> is that it creates a <code>tf.data.Dataset</code> object rather than a generator.</p> <p>The main advantage of this is the <code>tf.data.Dataset</code> API is much more efficient (faster) than the <code>ImageDataGenerator</code> API which is paramount for larger datasets.</p> <p>Let's see it in action.</p> In\u00a0[8]: Copied! <pre># Create data inputs\nimport tensorflow as tf\nIMG_SIZE = (224, 224) # define image size\ntrain_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=train_dir,\n                                                                            image_size=IMG_SIZE,\n                                                                            label_mode=\"categorical\", # what type are the labels?\n                                                                            batch_size=32) # batch_size is 32 by default, this is generally a good number\ntest_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=test_dir,\n                                                                           image_size=IMG_SIZE,\n                                                                           label_mode=\"categorical\")\n</pre> # Create data inputs import tensorflow as tf IMG_SIZE = (224, 224) # define image size train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=train_dir,                                                                             image_size=IMG_SIZE,                                                                             label_mode=\"categorical\", # what type are the labels?                                                                             batch_size=32) # batch_size is 32 by default, this is generally a good number test_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=test_dir,                                                                            image_size=IMG_SIZE,                                                                            label_mode=\"categorical\") <pre>Found 750 files belonging to 10 classes.\nFound 2500 files belonging to 10 classes.\n</pre> <p>Wonderful! Looks like our dataloaders have found the correct number of images for each dataset.</p> <p>For now, the main parameters we're concerned about in the <code>image_dataset_from_directory()</code> funtion are:</p> <ul> <li><code>directory</code> - the filepath of the target directory we're loading images in from.</li> <li><code>image_size</code> - the target size of the images we're going to load in (height, width).</li> <li><code>batch_size</code> - the batch size of the images we're going to load in. For example if the <code>batch_size</code> is 32 (the default), batches of 32 images and labels at a time will be passed to the model.</li> </ul> <p>There are more we could play around with if we needed to [in the documentation.</p> <p>If we check the training data datatype we should see it as a <code>BatchDataset</code> with shapes relating to our data.</p> In\u00a0[9]: Copied! <pre># Check the training data datatype\ntrain_data_10_percent\n</pre> # Check the training data datatype train_data_10_percent Out[9]: <pre>&lt;_BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))&gt;</pre> <p>In the above output:</p> <ul> <li><code>(None, 224, 224, 3)</code> refers to the tensor shape of our images where <code>None</code> is the batch size, <code>224</code> is the height (and width) and <code>3</code> is the color channels (red, green, blue).</li> <li><code>(None, 10)</code> refers to the tensor shape of the labels where <code>None</code> is the batch size and <code>10</code> is the number of possible labels (the 10 different food classes).</li> <li>Both image tensors and labels are of the datatype <code>tf.float32</code>.</li> </ul> <p>The <code>batch_size</code> is <code>None</code> due to it only being used during model training. You can think of <code>None</code> as a placeholder waiting to be filled with the <code>batch_size</code> parameter from <code>image_dataset_from_directory()</code>.</p> <p>Another benefit of using the <code>tf.data.Dataset</code> API are the assosciated methods which come with it.</p> <p>For example, if we want to find the name of the classes we were working with, we could use the <code>class_names</code> attribute.</p> In\u00a0[10]: Copied! <pre># Check out the class names of our dataset\ntrain_data_10_percent.class_names\n</pre> # Check out the class names of our dataset train_data_10_percent.class_names Out[10]: <pre>['chicken_curry',\n 'chicken_wings',\n 'fried_rice',\n 'grilled_salmon',\n 'hamburger',\n 'ice_cream',\n 'pizza',\n 'ramen',\n 'steak',\n 'sushi']</pre> <p>Or if we wanted to see an example batch of data, we could use the <code>take()</code> method.</p> In\u00a0[11]: Copied! <pre># See an example batch of data\nfor images, labels in train_data_10_percent.take(1):\n  print(images, labels)\n</pre> # See an example batch of data for images, labels in train_data_10_percent.take(1):   print(images, labels) <pre>tf.Tensor(\n[[[[1.18658157e+02 1.34658173e+02 1.34658173e+02]\n   [1.18117348e+02 1.34117340e+02 1.34117340e+02]\n   [1.19637756e+02 1.35637756e+02 1.35637756e+02]\n   ...\n   [5.63165398e+01 9.91634979e+01 9.18114929e+01]\n   [6.20816345e+01 1.09224518e+02 1.03224518e+02]\n   [6.39487991e+01 1.12260063e+02 1.08489639e+02]]\n\n  [[1.06357147e+02 1.22357147e+02 1.21357147e+02]\n   [1.09709190e+02 1.25709190e+02 1.24709190e+02]\n   [1.12872452e+02 1.28872452e+02 1.27872452e+02]\n   ...\n   [6.32701912e+01 1.05913071e+02 9.67702332e+01]\n   [6.22856750e+01 1.07270393e+02 1.02076508e+02]\n   [5.61224899e+01 1.01571533e+02 9.63112946e+01]]\n\n  [[9.16428604e+01 1.06071434e+02 1.05857147e+02]\n   [9.68418427e+01 1.11270416e+02 1.11056129e+02]\n   [1.00045921e+02 1.14474495e+02 1.14260208e+02]\n   ...\n   [8.43824387e+01 1.24550812e+02 1.16336548e+02]\n   [7.37497177e+01 1.15162979e+02 1.08994621e+02]\n   [2.96886349e+01 7.09029236e+01 6.69029236e+01]]\n\n  ...\n\n  [[7.94541016e+01 8.68878021e+01 8.90306473e+01]\n   [8.81582336e+01 9.64031525e+01 9.78163910e+01]\n   [9.55510254e+01 1.03428596e+02 1.04954079e+02]\n   ...\n   [1.24428589e+02 1.21428589e+02 1.16000000e+02]\n   [1.22801048e+02 1.19586754e+02 1.12158165e+02]\n   [1.22933716e+02 1.19719421e+02 1.11862244e+02]]\n\n  [[9.76941833e+01 1.07643173e+02 1.08668678e+02]\n   [1.03255264e+02 1.13250168e+02 1.12250168e+02]\n   [9.99133377e+01 1.10097023e+02 1.08500069e+02]\n   ...\n   [1.30443985e+02 1.27443985e+02 1.18586807e+02]\n   [1.29714355e+02 1.25714355e+02 1.14790840e+02]\n   [1.32857300e+02 1.27000122e+02 1.15071533e+02]]\n\n  [[9.17858047e+01 1.03785805e+02 1.01785805e+02]\n   [8.95970154e+01 1.01597015e+02 9.95970154e+01]\n   [8.95051575e+01 1.01505157e+02 9.75051575e+01]\n   ...\n   [1.35357208e+02 1.33505112e+02 1.22775513e+02]\n   [1.34025513e+02 1.30025513e+02 1.18357147e+02]\n   [1.33086792e+02 1.29857208e+02 1.15545952e+02]]]\n\n\n [[[1.00561228e+01 1.34846935e+01 1.30714283e+01]\n   [1.87397976e+01 1.10255098e+01 8.76530552e+00]\n   [2.44948978e+01 1.09336739e+01 4.22448969e+00]\n   ...\n   [1.84438972e+01 1.08571215e+01 8.43875980e+00]\n   [1.86428566e+01 1.16428576e+01 5.64285707e+00]\n   [1.95867577e+01 1.18724718e+01 4.22961426e+00]]\n\n  [[1.50255108e+01 1.14030609e+01 1.49285717e+01]\n   [1.97806129e+01 9.20408058e+00 1.28622456e+01]\n   [1.88571415e+01 7.15816259e+00 6.84183645e+00]\n   ...\n   [1.63265114e+01 1.22703876e+01 9.66830635e+00]\n   [1.59438534e+01 1.30000134e+01 7.72956896e+00]\n   [1.64080982e+01 1.43316650e+01 6.19381332e+00]]\n\n  [[1.38571434e+01 7.85714293e+00 9.42857170e+00]\n   [1.84438782e+01 1.00867357e+01 1.43010216e+01]\n   [1.81887741e+01 1.03367348e+01 1.41887751e+01]\n   ...\n   [1.38112078e+01 1.21683502e+01 8.71933651e+00]\n   [1.24285717e+01 1.30000000e+01 7.42857170e+00]\n   [1.30765381e+01 1.40765381e+01 8.29082394e+00]]\n\n  ...\n\n  [[1.65714722e+01 1.17857361e+01 8.50511646e+00]\n   [1.60561237e+01 1.00561237e+01 1.15153484e+01]\n   [1.53060913e+01 9.21426392e+00 1.06938648e+01]\n   ...\n   [1.58775539e+01 1.18316450e+01 8.78573608e+00]\n   [1.82703991e+01 1.32703981e+01 1.02703981e+01]\n   [1.81377335e+01 1.31377335e+01 9.35199738e+00]]\n\n  [[1.50000000e+01 1.00000000e+01 6.64285707e+00]\n   [1.59948969e+01 9.99489689e+00 1.19847174e+01]\n   [1.60714417e+01 1.00000000e+01 1.42143250e+01]\n   ...\n   [1.70561352e+01 1.30561342e+01 1.04846621e+01]\n   [1.68622208e+01 1.28622208e+01 9.86222076e+00]\n   [1.79540749e+01 1.29540758e+01 8.95407581e+00]]\n\n  [[1.63571777e+01 1.13571777e+01 8.00003529e+00]\n   [1.60000000e+01 1.00000000e+01 1.37142868e+01]\n   [1.56428223e+01 8.64282227e+00 1.59234619e+01]\n   ...\n   [1.78521194e+01 1.44949112e+01 1.32806473e+01]\n   [1.59285583e+01 1.19285583e+01 8.92855835e+00]\n   [1.73571777e+01 1.23571777e+01 8.35717773e+00]]]\n\n\n [[[9.96428604e+01 4.36428566e+01 2.66428566e+01]\n   [1.00974487e+02 4.49744911e+01 2.79744892e+01]\n   [1.00928566e+02 4.55714302e+01 2.83571434e+01]\n   ...\n   [1.59892273e+02 1.26616730e+02 9.66270294e+01]\n   [9.49591599e+01 5.68213806e+01 3.52959061e+01]\n   [8.11578674e+01 3.91578674e+01 2.31578693e+01]]\n\n  [[9.77397995e+01 4.17397957e+01 2.47397957e+01]\n   [9.90051041e+01 4.30051041e+01 2.60051022e+01]\n   [1.00770409e+02 4.54132690e+01 2.81989803e+01]\n   ...\n   [6.65968018e+01 3.01681900e+01 2.42336082e+00]\n   [6.98725586e+01 2.89388580e+01 9.08175468e+00]\n   [8.22501984e+01 3.64389687e+01 2.14440765e+01]]\n\n  [[1.03571426e+02 4.71428566e+01 3.23571434e+01]\n   [1.00071426e+02 4.40714264e+01 2.90714283e+01]\n   [9.82602081e+01 4.22602043e+01 2.72602043e+01]\n   ...\n   [8.19692001e+01 4.12548027e+01 1.43263178e+01]\n   [8.23265686e+01 3.61989899e+01 1.27245321e+01]\n   [8.15460205e+01 3.23317337e+01 1.40460205e+01]]\n\n  ...\n\n  [[7.69943161e+01 4.02750015e+01 7.95883179e+00]\n   [1.49617050e+02 1.07101807e+02 6.84590378e+01]\n   [1.72382751e+02 1.23142990e+02 7.40257111e+01]\n   ...\n   [1.24050819e+02 1.10096733e+02 6.00713196e+01]\n   [1.25897758e+02 1.11469231e+02 6.21835861e+01]\n   [1.36408173e+02 1.21979645e+02 7.45511169e+01]]\n\n  [[5.31785240e+01 2.17499924e+01 2.08678961e+00]\n   [8.83721466e+01 5.16629753e+01 2.31681156e+01]\n   [1.69887405e+02 1.21387421e+02 8.01731873e+01]\n   ...\n   [1.03188866e+02 9.80715027e+01 4.75154610e+01]\n   [1.07494804e+02 9.97652588e+01 5.28469086e+01]\n   [1.18214325e+02 1.07500092e+02 6.25715332e+01]]\n\n  [[4.53569336e+01 1.83263817e+01 7.04070187e+00]\n   [5.42549515e+01 2.05866871e+01 1.83662802e-01]\n   [1.09019783e+02 6.42289886e+01 2.82239151e+01]\n   ...\n   [9.87245026e+01 9.85969162e+01 4.91582108e+01]\n   [8.90457077e+01 8.59538651e+01 4.18059921e+01]\n   [9.70410385e+01 9.20410385e+01 5.07553978e+01]]]\n\n\n ...\n\n\n [[[2.54000000e+02 2.54000000e+02 2.54000000e+02]\n   [2.54000000e+02 2.54000000e+02 2.54000000e+02]\n   [2.54000000e+02 2.54000000e+02 2.54000000e+02]\n   ...\n   [1.03785736e+02 1.04785736e+02 9.07857361e+01]\n   [1.04071442e+02 1.05071442e+02 9.10714417e+01]\n   [1.05000000e+02 1.06000000e+02 9.20000000e+01]]\n\n  [[2.54000000e+02 2.54000000e+02 2.54000000e+02]\n   [2.54000000e+02 2.54000000e+02 2.54000000e+02]\n   [2.54000000e+02 2.54000000e+02 2.54000000e+02]\n   ...\n   [1.04729614e+02 1.05729614e+02 9.17296143e+01]\n   [1.04933678e+02 1.05933678e+02 9.19336777e+01]\n   [1.05000000e+02 1.06000000e+02 9.20000000e+01]]\n\n  [[2.54000000e+02 2.54000000e+02 2.54000000e+02]\n   [2.54000000e+02 2.54000000e+02 2.54000000e+02]\n   [2.54000000e+02 2.54000000e+02 2.54000000e+02]\n   ...\n   [1.05382637e+02 1.06382637e+02 9.23826370e+01]\n   [1.05000000e+02 1.06000000e+02 9.20000000e+01]\n   [1.05000000e+02 1.06000000e+02 9.20000000e+01]]\n\n  ...\n\n  [[1.51000366e+02 1.23148277e+02 8.75717087e+01]\n   [1.81531052e+02 1.54173843e+02 1.14031006e+02]\n   [2.01418442e+02 1.77464355e+02 1.29372528e+02]\n   ...\n   [1.56698837e+02 1.34270309e+02 9.72243958e+01]\n   [1.67561020e+02 1.44703964e+02 1.07703964e+02]\n   [1.80540848e+02 1.55683792e+02 1.19469528e+02]]\n\n  [[1.66280731e+02 1.41760361e+02 1.06020538e+02]\n   [1.74851913e+02 1.51122360e+02 1.13341789e+02]\n   [1.67749680e+02 1.45892563e+02 1.07321213e+02]\n   ...\n   [1.67688797e+02 1.49688797e+02 1.11688797e+02]\n   [1.50270462e+02 1.32270462e+02 9.61377792e+01]\n   [1.44224365e+02 1.26224358e+02 9.02243576e+01]]\n\n  [[1.86229233e+02 1.67770065e+02 1.31999649e+02]\n   [1.52626785e+02 1.34698318e+02 1.02295227e+02]\n   [1.28321167e+02 1.11397705e+02 8.26171722e+01]\n   ...\n   [1.54229584e+02 1.36306122e+02 9.60765305e+01]\n   [1.69928619e+02 1.51928619e+02 1.12020470e+02]\n   [1.49765457e+02 1.34765457e+02 9.57654572e+01]]]\n\n\n [[[1.60000000e+01 9.00000000e+00 0.00000000e+00]\n   [1.80373096e+01 8.03730869e+00 0.00000000e+00]\n   [2.04422836e+01 8.15210438e+00 1.52678585e+00]\n   ...\n   [1.41512177e+02 8.60389099e+01 5.67488060e+01]\n   [1.31497604e+02 7.44976120e+01 4.74976120e+01]\n   [9.87061234e+01 4.07061234e+01 1.82527027e+01]]\n\n  [[1.72560596e+01 9.60108471e+00 1.92857170e+00]\n   [1.90000000e+01 8.00000000e+00 2.00000000e+00]\n   [2.05267868e+01 7.23660707e+00 3.52678585e+00]\n   ...\n   [1.37288544e+02 7.97384033e+01 5.13768768e+01]\n   [1.32715057e+02 7.09780884e+01 4.58486443e+01]\n   [1.25001762e+02 6.13413200e+01 3.88709908e+01]]\n\n  [[1.80000000e+01 8.78571415e+00 4.42857170e+00]\n   [2.12598858e+01 7.25988531e+00 4.25988531e+00]\n   [2.27633934e+01 7.23660707e+00 6.52678585e+00]\n   ...\n   [1.35598724e+02 7.42236252e+01 4.85731277e+01]\n   [1.30317886e+02 6.42071915e+01 3.97330399e+01]\n   [1.44314301e+02 7.44399185e+01 5.00696678e+01]]\n\n  ...\n\n  [[5.50862360e+00 4.50862360e+00 5.08623779e-01]\n   [4.74013519e+00 3.74013519e+00 0.00000000e+00]\n   [4.00000000e+00 3.00000000e+00 0.00000000e+00]\n   ...\n   [2.16505070e+01 1.66505070e+01 1.26505070e+01]\n   [2.12142639e+01 1.62142639e+01 1.22142639e+01]\n   [2.12142639e+01 1.62142639e+01 1.22142639e+01]]\n\n  [[5.60107565e+00 4.60107565e+00 6.01075709e-01]\n   [5.00000000e+00 4.00000000e+00 0.00000000e+00]\n   [5.00000000e+00 4.00000000e+00 0.00000000e+00]\n   ...\n   [2.64329834e+01 2.14329834e+01 1.74329834e+01]\n   [2.13169250e+01 1.63169250e+01 1.23169250e+01]\n   [1.90251942e+01 1.40251951e+01 1.00251951e+01]]\n\n  [[3.76879120e+00 2.76879120e+00 0.00000000e+00]\n   [5.02072906e+00 4.02072906e+00 2.07291134e-02]\n   [5.84790373e+00 4.84790373e+00 8.47903728e-01]\n   ...\n   [2.81019630e+01 2.31019630e+01 1.91019630e+01]\n   [2.44732056e+01 1.94732056e+01 1.54732056e+01]\n   [2.17098389e+01 1.67098389e+01 1.27098389e+01]]]\n\n\n [[[1.38484695e+02 1.24229591e+02 1.24357140e+02]\n   [1.37551025e+02 1.25474495e+02 1.27500008e+02]\n   [1.41642853e+02 1.30642853e+02 1.37071426e+02]\n   ...\n   [1.36806229e+02 7.66633682e+01 7.10919342e+01]\n   [1.23571205e+02 7.72651825e+01 6.48824997e+01]\n   [8.62853622e+01 5.75712547e+01 3.92854691e+01]]\n\n  [[1.33163269e+02 1.16306122e+02 1.16331627e+02]\n   [1.33005096e+02 1.17852043e+02 1.20714287e+02]\n   [1.35816330e+02 1.23244896e+02 1.30459183e+02]\n   ...\n   [9.15152588e+01 5.30867348e+01 5.65152817e+01]\n   [7.96835403e+01 5.30356674e+01 5.03111496e+01]\n   [5.42649994e+01 4.33110542e+01 3.53110161e+01]]\n\n  [[1.42214294e+02 1.21928574e+02 1.23071426e+02]\n   [1.40397949e+02 1.24183670e+02 1.27112244e+02]\n   [1.40928558e+02 1.28188782e+02 1.35739792e+02]\n   ...\n   [5.83366356e+01 4.21683273e+01 5.07397346e+01]\n   [5.45663376e+01 4.67398643e+01 5.04082108e+01]\n   [5.12038918e+01 5.11376419e+01 5.01376076e+01]]\n\n  ...\n\n  [[7.41377640e+01 3.99948845e+01 2.08571262e+01]\n   [6.82295914e+01 3.51989784e+01 1.61989784e+01]\n   [6.40000000e+01 3.34285736e+01 1.58571434e+01]\n   ...\n   [1.67765320e+02 1.39551056e+02 1.03193848e+02]\n   [1.69755112e+02 1.43755112e+02 1.06755119e+02]\n   [1.64994873e+02 1.39209167e+02 1.01566284e+02]]\n\n  [[7.61020660e+01 3.91938820e+01 2.01479759e+01]\n   [7.46531754e+01 3.96531448e+01 2.07194729e+01]\n   [6.97041855e+01 3.91327591e+01 2.15613327e+01]\n   ...\n   [1.71642914e+02 1.43428650e+02 1.07071442e+02]\n   [1.69999908e+02 1.43999908e+02 1.06999908e+02]\n   [1.64494888e+02 1.39494888e+02 9.94948883e+01]]\n\n  [[7.87857056e+01 4.10153008e+01 2.20153008e+01]\n   [7.00408401e+01 3.48979874e+01 1.59694157e+01]\n   [7.56938705e+01 4.48469353e+01 2.68469372e+01]\n   ...\n   [1.64714294e+02 1.36500031e+02 1.00142822e+02]\n   [1.55663269e+02 1.30663269e+02 9.06632614e+01]\n   [1.59474701e+02 1.34474701e+02 9.44746933e+01]]]], shape=(32, 224, 224, 3), dtype=float32) tf.Tensor(\n[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(32, 10), dtype=float32)\n</pre> <p>Notice how the image arrays come out as tensors of pixel values where as the labels come out as one-hot encodings (e.g. <code>[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]</code> for <code>hamburger</code>).</p> In\u00a0[12]: Copied! <pre># 1. Create base model with tf.keras.applications\nbase_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False)\n\n# OLD\n# base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n\n# 2. Freeze the base model (so the pre-learned patterns remain)\nbase_model.trainable = False\n\n# 3. Create inputs into the base model\ninputs = tf.keras.layers.Input(shape=(224, 224, 3), name=\"input_layer\")\n\n# 4. If using ResNet50V2, add this to speed up convergence, remove for EfficientNetV2\n# x = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)(inputs)\n\n# 5. Pass the inputs to the base_model (note: using tf.keras.applications, EfficientNetV2 inputs don't have to be normalized)\nx = base_model(inputs)\n# Check data shape after passing it to base_model\nprint(f\"Shape after base_model: {x.shape}\")\n\n# 6. Average pool the outputs of the base model (aggregate all the most important information, reduce number of computations)\nx = tf.keras.layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\nprint(f\"After GlobalAveragePooling2D(): {x.shape}\")\n\n# 7. Create the output activation layer\noutputs = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"output_layer\")(x)\n\n# 8. Combine the inputs with the outputs into a model\nmodel_0 = tf.keras.Model(inputs, outputs)\n\n# 9. Compile the model\nmodel_0.compile(loss='categorical_crossentropy',\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=[\"accuracy\"])\n\n# 10. Fit the model (we use less steps for validation so it's faster)\nhistory_10_percent = model_0.fit(train_data_10_percent,\n                                 epochs=5,\n                                 steps_per_epoch=len(train_data_10_percent),\n                                 validation_data=test_data_10_percent,\n                                 # Go through less of the validation data so epochs are faster (we want faster experiments!)\n                                 validation_steps=int(0.25 * len(test_data_10_percent)),\n                                 # Track our model's training logs for visualization later\n                                 callbacks=[create_tensorboard_callback(\"transfer_learning\", \"10_percent_feature_extract\")])\n</pre> # 1. Create base model with tf.keras.applications base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False)  # OLD # base_model = tf.keras.applications.EfficientNetB0(include_top=False)  # 2. Freeze the base model (so the pre-learned patterns remain) base_model.trainable = False  # 3. Create inputs into the base model inputs = tf.keras.layers.Input(shape=(224, 224, 3), name=\"input_layer\")  # 4. If using ResNet50V2, add this to speed up convergence, remove for EfficientNetV2 # x = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)(inputs)  # 5. Pass the inputs to the base_model (note: using tf.keras.applications, EfficientNetV2 inputs don't have to be normalized) x = base_model(inputs) # Check data shape after passing it to base_model print(f\"Shape after base_model: {x.shape}\")  # 6. Average pool the outputs of the base model (aggregate all the most important information, reduce number of computations) x = tf.keras.layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x) print(f\"After GlobalAveragePooling2D(): {x.shape}\")  # 7. Create the output activation layer outputs = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"output_layer\")(x)  # 8. Combine the inputs with the outputs into a model model_0 = tf.keras.Model(inputs, outputs)  # 9. Compile the model model_0.compile(loss='categorical_crossentropy',               optimizer=tf.keras.optimizers.Adam(),               metrics=[\"accuracy\"])  # 10. Fit the model (we use less steps for validation so it's faster) history_10_percent = model_0.fit(train_data_10_percent,                                  epochs=5,                                  steps_per_epoch=len(train_data_10_percent),                                  validation_data=test_data_10_percent,                                  # Go through less of the validation data so epochs are faster (we want faster experiments!)                                  validation_steps=int(0.25 * len(test_data_10_percent)),                                  # Track our model's training logs for visualization later                                  callbacks=[create_tensorboard_callback(\"transfer_learning\", \"10_percent_feature_extract\")]) <pre>Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b0_notop.h5\n24274472/24274472 [==============================] - 0s 0us/step\nShape after base_model: (None, 7, 7, 1280)\nAfter GlobalAveragePooling2D(): (None, 1280)\nSaving TensorBoard log files to: transfer_learning/10_percent_feature_extract/20230818-014007\nEpoch 1/5\n24/24 [==============================] - 20s 156ms/step - loss: 1.8704 - accuracy: 0.4387 - val_loss: 1.2824 - val_accuracy: 0.7418\nEpoch 2/5\n24/24 [==============================] - 2s 85ms/step - loss: 1.1395 - accuracy: 0.7533 - val_loss: 0.8783 - val_accuracy: 0.8010\nEpoch 3/5\n24/24 [==============================] - 2s 67ms/step - loss: 0.8327 - accuracy: 0.8147 - val_loss: 0.7168 - val_accuracy: 0.8322\nEpoch 4/5\n24/24 [==============================] - 2s 85ms/step - loss: 0.6915 - accuracy: 0.8453 - val_loss: 0.6149 - val_accuracy: 0.8520\nEpoch 5/5\n24/24 [==============================] - 2s 86ms/step - loss: 0.5813 - accuracy: 0.8720 - val_loss: 0.5555 - val_accuracy: 0.8569\n</pre> <p>Nice! After a minute or so of training our model performs incredibly well on both the training and test sets.</p> <p>This is incredible.</p> <p>All thanks to the power of transfer learning!</p> <p>It's important to note the kind of transfer learning we used here is called feature extraction transfer learning, similar to what we did with the TensorFlow Hub models.</p> <p>In other words, we passed our custom data to an already pre-trained model (<code>EfficientNetV2B0</code>), asked it \"what patterns do you see?\" and then put our own output layer on top to make sure the outputs were tailored to our desired number of classes.</p> <p>We also used the Keras Functional API to build our model rather than the Sequential API. For now, the benefits of this main not seem clear but when you start to build more sophisticated models, you'll probably want to use the Functional API. So it's important to have exposure to this way of building models.</p> <p>\ud83d\udcd6 Resource: To see the benefits and use cases of the Functional API versus the Sequential API, check out the TensorFlow Functional API documentation.</p> <p>Let's inspect the layers in our model, we'll start with the base.</p> In\u00a0[13]: Copied! <pre># Check layers in our base model\nfor layer_number, layer in enumerate(base_model.layers):\n  print(layer_number, layer.name)\n</pre> # Check layers in our base model for layer_number, layer in enumerate(base_model.layers):   print(layer_number, layer.name) <pre>0 input_1\n1 rescaling\n2 normalization\n3 stem_conv\n4 stem_bn\n5 stem_activation\n6 block1a_project_conv\n7 block1a_project_bn\n8 block1a_project_activation\n9 block2a_expand_conv\n10 block2a_expand_bn\n11 block2a_expand_activation\n12 block2a_project_conv\n13 block2a_project_bn\n14 block2b_expand_conv\n15 block2b_expand_bn\n16 block2b_expand_activation\n17 block2b_project_conv\n18 block2b_project_bn\n19 block2b_drop\n20 block2b_add\n21 block3a_expand_conv\n22 block3a_expand_bn\n23 block3a_expand_activation\n24 block3a_project_conv\n25 block3a_project_bn\n26 block3b_expand_conv\n27 block3b_expand_bn\n28 block3b_expand_activation\n29 block3b_project_conv\n30 block3b_project_bn\n31 block3b_drop\n32 block3b_add\n33 block4a_expand_conv\n34 block4a_expand_bn\n35 block4a_expand_activation\n36 block4a_dwconv2\n37 block4a_bn\n38 block4a_activation\n39 block4a_se_squeeze\n40 block4a_se_reshape\n41 block4a_se_reduce\n42 block4a_se_expand\n43 block4a_se_excite\n44 block4a_project_conv\n45 block4a_project_bn\n46 block4b_expand_conv\n47 block4b_expand_bn\n48 block4b_expand_activation\n49 block4b_dwconv2\n50 block4b_bn\n51 block4b_activation\n52 block4b_se_squeeze\n53 block4b_se_reshape\n54 block4b_se_reduce\n55 block4b_se_expand\n56 block4b_se_excite\n57 block4b_project_conv\n58 block4b_project_bn\n59 block4b_drop\n60 block4b_add\n61 block4c_expand_conv\n62 block4c_expand_bn\n63 block4c_expand_activation\n64 block4c_dwconv2\n65 block4c_bn\n66 block4c_activation\n67 block4c_se_squeeze\n68 block4c_se_reshape\n69 block4c_se_reduce\n70 block4c_se_expand\n71 block4c_se_excite\n72 block4c_project_conv\n73 block4c_project_bn\n74 block4c_drop\n75 block4c_add\n76 block5a_expand_conv\n77 block5a_expand_bn\n78 block5a_expand_activation\n79 block5a_dwconv2\n80 block5a_bn\n81 block5a_activation\n82 block5a_se_squeeze\n83 block5a_se_reshape\n84 block5a_se_reduce\n85 block5a_se_expand\n86 block5a_se_excite\n87 block5a_project_conv\n88 block5a_project_bn\n89 block5b_expand_conv\n90 block5b_expand_bn\n91 block5b_expand_activation\n92 block5b_dwconv2\n93 block5b_bn\n94 block5b_activation\n95 block5b_se_squeeze\n96 block5b_se_reshape\n97 block5b_se_reduce\n98 block5b_se_expand\n99 block5b_se_excite\n100 block5b_project_conv\n101 block5b_project_bn\n102 block5b_drop\n103 block5b_add\n104 block5c_expand_conv\n105 block5c_expand_bn\n106 block5c_expand_activation\n107 block5c_dwconv2\n108 block5c_bn\n109 block5c_activation\n110 block5c_se_squeeze\n111 block5c_se_reshape\n112 block5c_se_reduce\n113 block5c_se_expand\n114 block5c_se_excite\n115 block5c_project_conv\n116 block5c_project_bn\n117 block5c_drop\n118 block5c_add\n119 block5d_expand_conv\n120 block5d_expand_bn\n121 block5d_expand_activation\n122 block5d_dwconv2\n123 block5d_bn\n124 block5d_activation\n125 block5d_se_squeeze\n126 block5d_se_reshape\n127 block5d_se_reduce\n128 block5d_se_expand\n129 block5d_se_excite\n130 block5d_project_conv\n131 block5d_project_bn\n132 block5d_drop\n133 block5d_add\n134 block5e_expand_conv\n135 block5e_expand_bn\n136 block5e_expand_activation\n137 block5e_dwconv2\n138 block5e_bn\n139 block5e_activation\n140 block5e_se_squeeze\n141 block5e_se_reshape\n142 block5e_se_reduce\n143 block5e_se_expand\n144 block5e_se_excite\n145 block5e_project_conv\n146 block5e_project_bn\n147 block5e_drop\n148 block5e_add\n149 block6a_expand_conv\n150 block6a_expand_bn\n151 block6a_expand_activation\n152 block6a_dwconv2\n153 block6a_bn\n154 block6a_activation\n155 block6a_se_squeeze\n156 block6a_se_reshape\n157 block6a_se_reduce\n158 block6a_se_expand\n159 block6a_se_excite\n160 block6a_project_conv\n161 block6a_project_bn\n162 block6b_expand_conv\n163 block6b_expand_bn\n164 block6b_expand_activation\n165 block6b_dwconv2\n166 block6b_bn\n167 block6b_activation\n168 block6b_se_squeeze\n169 block6b_se_reshape\n170 block6b_se_reduce\n171 block6b_se_expand\n172 block6b_se_excite\n173 block6b_project_conv\n174 block6b_project_bn\n175 block6b_drop\n176 block6b_add\n177 block6c_expand_conv\n178 block6c_expand_bn\n179 block6c_expand_activation\n180 block6c_dwconv2\n181 block6c_bn\n182 block6c_activation\n183 block6c_se_squeeze\n184 block6c_se_reshape\n185 block6c_se_reduce\n186 block6c_se_expand\n187 block6c_se_excite\n188 block6c_project_conv\n189 block6c_project_bn\n190 block6c_drop\n191 block6c_add\n192 block6d_expand_conv\n193 block6d_expand_bn\n194 block6d_expand_activation\n195 block6d_dwconv2\n196 block6d_bn\n197 block6d_activation\n198 block6d_se_squeeze\n199 block6d_se_reshape\n200 block6d_se_reduce\n201 block6d_se_expand\n202 block6d_se_excite\n203 block6d_project_conv\n204 block6d_project_bn\n205 block6d_drop\n206 block6d_add\n207 block6e_expand_conv\n208 block6e_expand_bn\n209 block6e_expand_activation\n210 block6e_dwconv2\n211 block6e_bn\n212 block6e_activation\n213 block6e_se_squeeze\n214 block6e_se_reshape\n215 block6e_se_reduce\n216 block6e_se_expand\n217 block6e_se_excite\n218 block6e_project_conv\n219 block6e_project_bn\n220 block6e_drop\n221 block6e_add\n222 block6f_expand_conv\n223 block6f_expand_bn\n224 block6f_expand_activation\n225 block6f_dwconv2\n226 block6f_bn\n227 block6f_activation\n228 block6f_se_squeeze\n229 block6f_se_reshape\n230 block6f_se_reduce\n231 block6f_se_expand\n232 block6f_se_excite\n233 block6f_project_conv\n234 block6f_project_bn\n235 block6f_drop\n236 block6f_add\n237 block6g_expand_conv\n238 block6g_expand_bn\n239 block6g_expand_activation\n240 block6g_dwconv2\n241 block6g_bn\n242 block6g_activation\n243 block6g_se_squeeze\n244 block6g_se_reshape\n245 block6g_se_reduce\n246 block6g_se_expand\n247 block6g_se_excite\n248 block6g_project_conv\n249 block6g_project_bn\n250 block6g_drop\n251 block6g_add\n252 block6h_expand_conv\n253 block6h_expand_bn\n254 block6h_expand_activation\n255 block6h_dwconv2\n256 block6h_bn\n257 block6h_activation\n258 block6h_se_squeeze\n259 block6h_se_reshape\n260 block6h_se_reduce\n261 block6h_se_expand\n262 block6h_se_excite\n263 block6h_project_conv\n264 block6h_project_bn\n265 block6h_drop\n266 block6h_add\n267 top_conv\n268 top_bn\n269 top_activation\n</pre> <p>Wow, that's a lot of layers... to handcode all of those would've taken a fairly long time to do, yet we can still take advatange of them thanks to the power of transfer learning.</p> <p>How about a summary of the base model?</p> In\u00a0[14]: Copied! <pre>base_model.summary()\n</pre> base_model.summary() <pre>Model: \"efficientnetv2-b0\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, None, None,  0           []                               \n                                 3)]                                                              \n                                                                                                  \n rescaling (Rescaling)          (None, None, None,   0           ['input_1[0][0]']                \n                                3)                                                                \n                                                                                                  \n normalization (Normalization)  (None, None, None,   0           ['rescaling[0][0]']              \n                                3)                                                                \n                                                                                                  \n stem_conv (Conv2D)             (None, None, None,   864         ['normalization[0][0]']          \n                                32)                                                               \n                                                                                                  \n stem_bn (BatchNormalization)   (None, None, None,   128         ['stem_conv[0][0]']              \n                                32)                                                               \n                                                                                                  \n stem_activation (Activation)   (None, None, None,   0           ['stem_bn[0][0]']                \n                                32)                                                               \n                                                                                                  \n block1a_project_conv (Conv2D)  (None, None, None,   4608        ['stem_activation[0][0]']        \n                                16)                                                               \n                                                                                                  \n block1a_project_bn (BatchNorma  (None, None, None,   64         ['block1a_project_conv[0][0]']   \n lization)                      16)                                                               \n                                                                                                  \n block1a_project_activation (Ac  (None, None, None,   0          ['block1a_project_bn[0][0]']     \n tivation)                      16)                                                               \n                                                                                                  \n block2a_expand_conv (Conv2D)   (None, None, None,   9216        ['block1a_project_activation[0][0\n                                64)                              ]']                              \n                                                                                                  \n block2a_expand_bn (BatchNormal  (None, None, None,   256        ['block2a_expand_conv[0][0]']    \n ization)                       64)                                                               \n                                                                                                  \n block2a_expand_activation (Act  (None, None, None,   0          ['block2a_expand_bn[0][0]']      \n ivation)                       64)                                                               \n                                                                                                  \n block2a_project_conv (Conv2D)  (None, None, None,   2048        ['block2a_expand_activation[0][0]\n                                32)                              ']                               \n                                                                                                  \n block2a_project_bn (BatchNorma  (None, None, None,   128        ['block2a_project_conv[0][0]']   \n lization)                      32)                                                               \n                                                                                                  \n block2b_expand_conv (Conv2D)   (None, None, None,   36864       ['block2a_project_bn[0][0]']     \n                                128)                                                              \n                                                                                                  \n block2b_expand_bn (BatchNormal  (None, None, None,   512        ['block2b_expand_conv[0][0]']    \n ization)                       128)                                                              \n                                                                                                  \n block2b_expand_activation (Act  (None, None, None,   0          ['block2b_expand_bn[0][0]']      \n ivation)                       128)                                                              \n                                                                                                  \n block2b_project_conv (Conv2D)  (None, None, None,   4096        ['block2b_expand_activation[0][0]\n                                32)                              ']                               \n                                                                                                  \n block2b_project_bn (BatchNorma  (None, None, None,   128        ['block2b_project_conv[0][0]']   \n lization)                      32)                                                               \n                                                                                                  \n block2b_drop (Dropout)         (None, None, None,   0           ['block2b_project_bn[0][0]']     \n                                32)                                                               \n                                                                                                  \n block2b_add (Add)              (None, None, None,   0           ['block2b_drop[0][0]',           \n                                32)                               'block2a_project_bn[0][0]']     \n                                                                                                  \n block3a_expand_conv (Conv2D)   (None, None, None,   36864       ['block2b_add[0][0]']            \n                                128)                                                              \n                                                                                                  \n block3a_expand_bn (BatchNormal  (None, None, None,   512        ['block3a_expand_conv[0][0]']    \n ization)                       128)                                                              \n                                                                                                  \n block3a_expand_activation (Act  (None, None, None,   0          ['block3a_expand_bn[0][0]']      \n ivation)                       128)                                                              \n                                                                                                  \n block3a_project_conv (Conv2D)  (None, None, None,   6144        ['block3a_expand_activation[0][0]\n                                48)                              ']                               \n                                                                                                  \n block3a_project_bn (BatchNorma  (None, None, None,   192        ['block3a_project_conv[0][0]']   \n lization)                      48)                                                               \n                                                                                                  \n block3b_expand_conv (Conv2D)   (None, None, None,   82944       ['block3a_project_bn[0][0]']     \n                                192)                                                              \n                                                                                                  \n block3b_expand_bn (BatchNormal  (None, None, None,   768        ['block3b_expand_conv[0][0]']    \n ization)                       192)                                                              \n                                                                                                  \n block3b_expand_activation (Act  (None, None, None,   0          ['block3b_expand_bn[0][0]']      \n ivation)                       192)                                                              \n                                                                                                  \n block3b_project_conv (Conv2D)  (None, None, None,   9216        ['block3b_expand_activation[0][0]\n                                48)                              ']                               \n                                                                                                  \n block3b_project_bn (BatchNorma  (None, None, None,   192        ['block3b_project_conv[0][0]']   \n lization)                      48)                                                               \n                                                                                                  \n block3b_drop (Dropout)         (None, None, None,   0           ['block3b_project_bn[0][0]']     \n                                48)                                                               \n                                                                                                  \n block3b_add (Add)              (None, None, None,   0           ['block3b_drop[0][0]',           \n                                48)                               'block3a_project_bn[0][0]']     \n                                                                                                  \n block4a_expand_conv (Conv2D)   (None, None, None,   9216        ['block3b_add[0][0]']            \n                                192)                                                              \n                                                                                                  \n block4a_expand_bn (BatchNormal  (None, None, None,   768        ['block4a_expand_conv[0][0]']    \n ization)                       192)                                                              \n                                                                                                  \n block4a_expand_activation (Act  (None, None, None,   0          ['block4a_expand_bn[0][0]']      \n ivation)                       192)                                                              \n                                                                                                  \n block4a_dwconv2 (DepthwiseConv  (None, None, None,   1728       ['block4a_expand_activation[0][0]\n 2D)                            192)                             ']                               \n                                                                                                  \n block4a_bn (BatchNormalization  (None, None, None,   768        ['block4a_dwconv2[0][0]']        \n )                              192)                                                              \n                                                                                                  \n block4a_activation (Activation  (None, None, None,   0          ['block4a_bn[0][0]']             \n )                              192)                                                              \n                                                                                                  \n block4a_se_squeeze (GlobalAver  (None, 192)         0           ['block4a_activation[0][0]']     \n agePooling2D)                                                                                    \n                                                                                                  \n block4a_se_reshape (Reshape)   (None, 1, 1, 192)    0           ['block4a_se_squeeze[0][0]']     \n                                                                                                  \n block4a_se_reduce (Conv2D)     (None, 1, 1, 12)     2316        ['block4a_se_reshape[0][0]']     \n                                                                                                  \n block4a_se_expand (Conv2D)     (None, 1, 1, 192)    2496        ['block4a_se_reduce[0][0]']      \n                                                                                                  \n block4a_se_excite (Multiply)   (None, None, None,   0           ['block4a_activation[0][0]',     \n                                192)                              'block4a_se_expand[0][0]']      \n                                                                                                  \n block4a_project_conv (Conv2D)  (None, None, None,   18432       ['block4a_se_excite[0][0]']      \n                                96)                                                               \n                                                                                                  \n block4a_project_bn (BatchNorma  (None, None, None,   384        ['block4a_project_conv[0][0]']   \n lization)                      96)                                                               \n                                                                                                  \n block4b_expand_conv (Conv2D)   (None, None, None,   36864       ['block4a_project_bn[0][0]']     \n                                384)                                                              \n                                                                                                  \n block4b_expand_bn (BatchNormal  (None, None, None,   1536       ['block4b_expand_conv[0][0]']    \n ization)                       384)                                                              \n                                                                                                  \n block4b_expand_activation (Act  (None, None, None,   0          ['block4b_expand_bn[0][0]']      \n ivation)                       384)                                                              \n                                                                                                  \n block4b_dwconv2 (DepthwiseConv  (None, None, None,   3456       ['block4b_expand_activation[0][0]\n 2D)                            384)                             ']                               \n                                                                                                  \n block4b_bn (BatchNormalization  (None, None, None,   1536       ['block4b_dwconv2[0][0]']        \n )                              384)                                                              \n                                                                                                  \n block4b_activation (Activation  (None, None, None,   0          ['block4b_bn[0][0]']             \n )                              384)                                                              \n                                                                                                  \n block4b_se_squeeze (GlobalAver  (None, 384)         0           ['block4b_activation[0][0]']     \n agePooling2D)                                                                                    \n                                                                                                  \n block4b_se_reshape (Reshape)   (None, 1, 1, 384)    0           ['block4b_se_squeeze[0][0]']     \n                                                                                                  \n block4b_se_reduce (Conv2D)     (None, 1, 1, 24)     9240        ['block4b_se_reshape[0][0]']     \n                                                                                                  \n block4b_se_expand (Conv2D)     (None, 1, 1, 384)    9600        ['block4b_se_reduce[0][0]']      \n                                                                                                  \n block4b_se_excite (Multiply)   (None, None, None,   0           ['block4b_activation[0][0]',     \n                                384)                              'block4b_se_expand[0][0]']      \n                                                                                                  \n block4b_project_conv (Conv2D)  (None, None, None,   36864       ['block4b_se_excite[0][0]']      \n                                96)                                                               \n                                                                                                  \n block4b_project_bn (BatchNorma  (None, None, None,   384        ['block4b_project_conv[0][0]']   \n lization)                      96)                                                               \n                                                                                                  \n block4b_drop (Dropout)         (None, None, None,   0           ['block4b_project_bn[0][0]']     \n                                96)                                                               \n                                                                                                  \n block4b_add (Add)              (None, None, None,   0           ['block4b_drop[0][0]',           \n                                96)                               'block4a_project_bn[0][0]']     \n                                                                                                  \n block4c_expand_conv (Conv2D)   (None, None, None,   36864       ['block4b_add[0][0]']            \n                                384)                                                              \n                                                                                                  \n block4c_expand_bn (BatchNormal  (None, None, None,   1536       ['block4c_expand_conv[0][0]']    \n ization)                       384)                                                              \n                                                                                                  \n block4c_expand_activation (Act  (None, None, None,   0          ['block4c_expand_bn[0][0]']      \n ivation)                       384)                                                              \n                                                                                                  \n block4c_dwconv2 (DepthwiseConv  (None, None, None,   3456       ['block4c_expand_activation[0][0]\n 2D)                            384)                             ']                               \n                                                                                                  \n block4c_bn (BatchNormalization  (None, None, None,   1536       ['block4c_dwconv2[0][0]']        \n )                              384)                                                              \n                                                                                                  \n block4c_activation (Activation  (None, None, None,   0          ['block4c_bn[0][0]']             \n )                              384)                                                              \n                                                                                                  \n block4c_se_squeeze (GlobalAver  (None, 384)         0           ['block4c_activation[0][0]']     \n agePooling2D)                                                                                    \n                                                                                                  \n block4c_se_reshape (Reshape)   (None, 1, 1, 384)    0           ['block4c_se_squeeze[0][0]']     \n                                                                                                  \n block4c_se_reduce (Conv2D)     (None, 1, 1, 24)     9240        ['block4c_se_reshape[0][0]']     \n                                                                                                  \n block4c_se_expand (Conv2D)     (None, 1, 1, 384)    9600        ['block4c_se_reduce[0][0]']      \n                                                                                                  \n block4c_se_excite (Multiply)   (None, None, None,   0           ['block4c_activation[0][0]',     \n                                384)                              'block4c_se_expand[0][0]']      \n                                                                                                  \n block4c_project_conv (Conv2D)  (None, None, None,   36864       ['block4c_se_excite[0][0]']      \n                                96)                                                               \n                                                                                                  \n block4c_project_bn (BatchNorma  (None, None, None,   384        ['block4c_project_conv[0][0]']   \n lization)                      96)                                                               \n                                                                                                  \n block4c_drop (Dropout)         (None, None, None,   0           ['block4c_project_bn[0][0]']     \n                                96)                                                               \n                                                                                                  \n block4c_add (Add)              (None, None, None,   0           ['block4c_drop[0][0]',           \n                                96)                               'block4b_add[0][0]']            \n                                                                                                  \n block5a_expand_conv (Conv2D)   (None, None, None,   55296       ['block4c_add[0][0]']            \n                                576)                                                              \n                                                                                                  \n block5a_expand_bn (BatchNormal  (None, None, None,   2304       ['block5a_expand_conv[0][0]']    \n ization)                       576)                                                              \n                                                                                                  \n block5a_expand_activation (Act  (None, None, None,   0          ['block5a_expand_bn[0][0]']      \n ivation)                       576)                                                              \n                                                                                                  \n block5a_dwconv2 (DepthwiseConv  (None, None, None,   5184       ['block5a_expand_activation[0][0]\n 2D)                            576)                             ']                               \n                                                                                                  \n block5a_bn (BatchNormalization  (None, None, None,   2304       ['block5a_dwconv2[0][0]']        \n )                              576)                                                              \n                                                                                                  \n block5a_activation (Activation  (None, None, None,   0          ['block5a_bn[0][0]']             \n )                              576)                                                              \n                                                                                                  \n block5a_se_squeeze (GlobalAver  (None, 576)         0           ['block5a_activation[0][0]']     \n agePooling2D)                                                                                    \n                                                                                                  \n block5a_se_reshape (Reshape)   (None, 1, 1, 576)    0           ['block5a_se_squeeze[0][0]']     \n                                                                                                  \n block5a_se_reduce (Conv2D)     (None, 1, 1, 24)     13848       ['block5a_se_reshape[0][0]']     \n                                                                                                  \n block5a_se_expand (Conv2D)     (None, 1, 1, 576)    14400       ['block5a_se_reduce[0][0]']      \n                                                                                                  \n block5a_se_excite (Multiply)   (None, None, None,   0           ['block5a_activation[0][0]',     \n                                576)                              'block5a_se_expand[0][0]']      \n                                                                                                  \n block5a_project_conv (Conv2D)  (None, None, None,   64512       ['block5a_se_excite[0][0]']      \n                                112)                                                              \n                                                                                                  \n block5a_project_bn (BatchNorma  (None, None, None,   448        ['block5a_project_conv[0][0]']   \n lization)                      112)                                                              \n                                                                                                  \n block5b_expand_conv (Conv2D)   (None, None, None,   75264       ['block5a_project_bn[0][0]']     \n                                672)                                                              \n                                                                                                  \n block5b_expand_bn (BatchNormal  (None, None, None,   2688       ['block5b_expand_conv[0][0]']    \n ization)                       672)                                                              \n                                                                                                  \n block5b_expand_activation (Act  (None, None, None,   0          ['block5b_expand_bn[0][0]']      \n ivation)                       672)                                                              \n                                                                                                  \n block5b_dwconv2 (DepthwiseConv  (None, None, None,   6048       ['block5b_expand_activation[0][0]\n 2D)                            672)                             ']                               \n                                                                                                  \n block5b_bn (BatchNormalization  (None, None, None,   2688       ['block5b_dwconv2[0][0]']        \n )                              672)                                                              \n                                                                                                  \n block5b_activation (Activation  (None, None, None,   0          ['block5b_bn[0][0]']             \n )                              672)                                                              \n                                                                                                  \n block5b_se_squeeze (GlobalAver  (None, 672)         0           ['block5b_activation[0][0]']     \n agePooling2D)                                                                                    \n                                                                                                  \n block5b_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5b_se_squeeze[0][0]']     \n                                                                                                  \n block5b_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5b_se_reshape[0][0]']     \n                                                                                                  \n block5b_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5b_se_reduce[0][0]']      \n                                                                                                  \n block5b_se_excite (Multiply)   (None, None, None,   0           ['block5b_activation[0][0]',     \n                                672)                              'block5b_se_expand[0][0]']      \n                                                                                                  \n block5b_project_conv (Conv2D)  (None, None, None,   75264       ['block5b_se_excite[0][0]']      \n                                112)                                                              \n                                                                                                  \n block5b_project_bn (BatchNorma  (None, None, None,   448        ['block5b_project_conv[0][0]']   \n lization)                      112)                                                              \n                                                                                                  \n block5b_drop (Dropout)         (None, None, None,   0           ['block5b_project_bn[0][0]']     \n                                112)                                                              \n                                                                                                  \n block5b_add (Add)              (None, None, None,   0           ['block5b_drop[0][0]',           \n                                112)                              'block5a_project_bn[0][0]']     \n                                                                                                  \n block5c_expand_conv (Conv2D)   (None, None, None,   75264       ['block5b_add[0][0]']            \n                                672)                                                              \n                                                                                                  \n block5c_expand_bn (BatchNormal  (None, None, None,   2688       ['block5c_expand_conv[0][0]']    \n ization)                       672)                                                              \n                                                                                                  \n block5c_expand_activation (Act  (None, None, None,   0          ['block5c_expand_bn[0][0]']      \n ivation)                       672)                                                              \n                                                                                                  \n block5c_dwconv2 (DepthwiseConv  (None, None, None,   6048       ['block5c_expand_activation[0][0]\n 2D)                            672)                             ']                               \n                                                                                                  \n block5c_bn (BatchNormalization  (None, None, None,   2688       ['block5c_dwconv2[0][0]']        \n )                              672)                                                              \n                                                                                                  \n block5c_activation (Activation  (None, None, None,   0          ['block5c_bn[0][0]']             \n )                              672)                                                              \n                                                                                                  \n block5c_se_squeeze (GlobalAver  (None, 672)         0           ['block5c_activation[0][0]']     \n agePooling2D)                                                                                    \n                                                                                                  \n block5c_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5c_se_squeeze[0][0]']     \n                                                                                                  \n block5c_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5c_se_reshape[0][0]']     \n                                                                                                  \n block5c_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5c_se_reduce[0][0]']      \n                                                                                                  \n block5c_se_excite (Multiply)   (None, None, None,   0           ['block5c_activation[0][0]',     \n                                672)                              'block5c_se_expand[0][0]']      \n                                                                                                  \n block5c_project_conv (Conv2D)  (None, None, None,   75264       ['block5c_se_excite[0][0]']      \n                                112)                                                              \n                                                                                                  \n block5c_project_bn (BatchNorma  (None, None, None,   448        ['block5c_project_conv[0][0]']   \n lization)                      112)                                                              \n                                                                                                  \n block5c_drop (Dropout)         (None, None, None,   0           ['block5c_project_bn[0][0]']     \n                                112)                                                              \n                                                                                                  \n block5c_add (Add)              (None, None, None,   0           ['block5c_drop[0][0]',           \n                                112)                              'block5b_add[0][0]']            \n                                                                                                  \n block5d_expand_conv (Conv2D)   (None, None, None,   75264       ['block5c_add[0][0]']            \n                                672)                                                              \n                                                                                                  \n block5d_expand_bn (BatchNormal  (None, None, None,   2688       ['block5d_expand_conv[0][0]']    \n ization)                       672)                                                              \n                                                                                                  \n block5d_expand_activation (Act  (None, None, None,   0          ['block5d_expand_bn[0][0]']      \n ivation)                       672)                                                              \n                                                                                                  \n block5d_dwconv2 (DepthwiseConv  (None, None, None,   6048       ['block5d_expand_activation[0][0]\n 2D)                            672)                             ']                               \n                                                                                                  \n block5d_bn (BatchNormalization  (None, None, None,   2688       ['block5d_dwconv2[0][0]']        \n )                              672)                                                              \n                                                                                                  \n block5d_activation (Activation  (None, None, None,   0          ['block5d_bn[0][0]']             \n )                              672)                                                              \n                                                                                                  \n block5d_se_squeeze (GlobalAver  (None, 672)         0           ['block5d_activation[0][0]']     \n agePooling2D)                                                                                    \n                                                                                                  \n block5d_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5d_se_squeeze[0][0]']     \n                                                                                                  \n block5d_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5d_se_reshape[0][0]']     \n                                                                                                  \n block5d_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5d_se_reduce[0][0]']      \n                                                                                                  \n block5d_se_excite (Multiply)   (None, None, None,   0           ['block5d_activation[0][0]',     \n                                672)                              'block5d_se_expand[0][0]']      \n                                                                                                  \n block5d_project_conv (Conv2D)  (None, None, None,   75264       ['block5d_se_excite[0][0]']      \n                                112)                                                              \n                                                                                                  \n block5d_project_bn (BatchNorma  (None, None, None,   448        ['block5d_project_conv[0][0]']   \n lization)                      112)                                                              \n                                                                                                  \n block5d_drop (Dropout)         (None, None, None,   0           ['block5d_project_bn[0][0]']     \n                                112)                                                              \n                                                                                                  \n block5d_add (Add)              (None, None, None,   0           ['block5d_drop[0][0]',           \n                                112)                              'block5c_add[0][0]']            \n                                                                                                  \n block5e_expand_conv (Conv2D)   (None, None, None,   75264       ['block5d_add[0][0]']            \n                                672)                                                              \n                                                                                                  \n block5e_expand_bn (BatchNormal  (None, None, None,   2688       ['block5e_expand_conv[0][0]']    \n ization)                       672)                                                              \n                                                                                                  \n block5e_expand_activation (Act  (None, None, None,   0          ['block5e_expand_bn[0][0]']      \n ivation)                       672)                                                              \n                                                                                                  \n block5e_dwconv2 (DepthwiseConv  (None, None, None,   6048       ['block5e_expand_activation[0][0]\n 2D)                            672)                             ']                               \n                                                                                                  \n block5e_bn (BatchNormalization  (None, None, None,   2688       ['block5e_dwconv2[0][0]']        \n )                              672)                                                              \n                                                                                                  \n block5e_activation (Activation  (None, None, None,   0          ['block5e_bn[0][0]']             \n )                              672)                                                              \n                                                                                                  \n block5e_se_squeeze (GlobalAver  (None, 672)         0           ['block5e_activation[0][0]']     \n agePooling2D)                                                                                    \n                                                                                                  \n block5e_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block5e_se_squeeze[0][0]']     \n                                                                                                  \n block5e_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block5e_se_reshape[0][0]']     \n                                                                                                  \n block5e_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block5e_se_reduce[0][0]']      \n                                                                                                  \n block5e_se_excite (Multiply)   (None, None, None,   0           ['block5e_activation[0][0]',     \n                                672)                              'block5e_se_expand[0][0]']      \n                                                                                                  \n block5e_project_conv (Conv2D)  (None, None, None,   75264       ['block5e_se_excite[0][0]']      \n                                112)                                                              \n                                                                                                  \n block5e_project_bn (BatchNorma  (None, None, None,   448        ['block5e_project_conv[0][0]']   \n lization)                      112)                                                              \n                                                                                                  \n block5e_drop (Dropout)         (None, None, None,   0           ['block5e_project_bn[0][0]']     \n                                112)                                                              \n                                                                                                  \n block5e_add (Add)              (None, None, None,   0           ['block5e_drop[0][0]',           \n                                112)                              'block5d_add[0][0]']            \n                                                                                                  \n block6a_expand_conv (Conv2D)   (None, None, None,   75264       ['block5e_add[0][0]']            \n                                672)                                                              \n                                                                                                  \n block6a_expand_bn (BatchNormal  (None, None, None,   2688       ['block6a_expand_conv[0][0]']    \n ization)                       672)                                                              \n                                                                                                  \n block6a_expand_activation (Act  (None, None, None,   0          ['block6a_expand_bn[0][0]']      \n ivation)                       672)                                                              \n                                                                                                  \n block6a_dwconv2 (DepthwiseConv  (None, None, None,   6048       ['block6a_expand_activation[0][0]\n 2D)                            672)                             ']                               \n                                                                                                  \n block6a_bn (BatchNormalization  (None, None, None,   2688       ['block6a_dwconv2[0][0]']        \n )                              672)                                                              \n                                                                                                  \n block6a_activation (Activation  (None, None, None,   0          ['block6a_bn[0][0]']             \n )                              672)                                                              \n                                                                                                  \n block6a_se_squeeze (GlobalAver  (None, 672)         0           ['block6a_activation[0][0]']     \n agePooling2D)                                                                                    \n                                                                                                  \n block6a_se_reshape (Reshape)   (None, 1, 1, 672)    0           ['block6a_se_squeeze[0][0]']     \n                                                                                                  \n block6a_se_reduce (Conv2D)     (None, 1, 1, 28)     18844       ['block6a_se_reshape[0][0]']     \n                                                                                                  \n block6a_se_expand (Conv2D)     (None, 1, 1, 672)    19488       ['block6a_se_reduce[0][0]']      \n                                                                                                  \n block6a_se_excite (Multiply)   (None, None, None,   0           ['block6a_activation[0][0]',     \n                                672)                              'block6a_se_expand[0][0]']      \n                                                                                                  \n block6a_project_conv (Conv2D)  (None, None, None,   129024      ['block6a_se_excite[0][0]']      \n                                192)                                                              \n                                                                                                  \n block6a_project_bn (BatchNorma  (None, None, None,   768        ['block6a_project_conv[0][0]']   \n lization)                      192)                                                              \n                                                                                                  \n block6b_expand_conv (Conv2D)   (None, None, None,   221184      ['block6a_project_bn[0][0]']     \n                                1152)                                                             \n                                                                                                  \n block6b_expand_bn (BatchNormal  (None, None, None,   4608       ['block6b_expand_conv[0][0]']    \n ization)                       1152)                                                             \n                                                                                                  \n block6b_expand_activation (Act  (None, None, None,   0          ['block6b_expand_bn[0][0]']      \n ivation)                       1152)                                                             \n                                                                                                  \n block6b_dwconv2 (DepthwiseConv  (None, None, None,   10368      ['block6b_expand_activation[0][0]\n 2D)                            1152)                            ']                               \n                                                                                                  \n block6b_bn (BatchNormalization  (None, None, None,   4608       ['block6b_dwconv2[0][0]']        \n )                              1152)                                                             \n                                                                                                  \n block6b_activation (Activation  (None, None, None,   0          ['block6b_bn[0][0]']             \n )                              1152)                                                             \n                                                                                                  \n block6b_se_squeeze (GlobalAver  (None, 1152)        0           ['block6b_activation[0][0]']     \n agePooling2D)                                                                                    \n                                                                                                  \n block6b_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6b_se_squeeze[0][0]']     \n                                                                                                  \n block6b_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6b_se_reshape[0][0]']     \n                                                                                                  \n block6b_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6b_se_reduce[0][0]']      \n                                                                                                  \n block6b_se_excite (Multiply)   (None, None, None,   0           ['block6b_activation[0][0]',     \n                                1152)                             'block6b_se_expand[0][0]']      \n                                                                                                  \n block6b_project_conv (Conv2D)  (None, None, None,   221184      ['block6b_se_excite[0][0]']      \n                                192)                                                              \n                                                                                                  \n block6b_project_bn (BatchNorma  (None, None, None,   768        ['block6b_project_conv[0][0]']   \n lization)                      192)                                                              \n                                                                                                  \n block6b_drop (Dropout)         (None, None, None,   0           ['block6b_project_bn[0][0]']     \n                                192)                                                              \n                                                                                                  \n block6b_add (Add)              (None, None, None,   0           ['block6b_drop[0][0]',           \n                                192)                              'block6a_project_bn[0][0]']     \n                                                                                                  \n block6c_expand_conv (Conv2D)   (None, None, None,   221184      ['block6b_add[0][0]']            \n                                1152)                                                             \n                                                                                                  \n block6c_expand_bn (BatchNormal  (None, None, None,   4608       ['block6c_expand_conv[0][0]']    \n ization)                       1152)                                                             \n                                                                                                  \n block6c_expand_activation (Act  (None, None, None,   0          ['block6c_expand_bn[0][0]']      \n ivation)                       1152)                                                             \n                                                                                                  \n block6c_dwconv2 (DepthwiseConv  (None, None, None,   10368      ['block6c_expand_activation[0][0]\n 2D)                            1152)                            ']                               \n                                                                                                  \n block6c_bn (BatchNormalization  (None, None, None,   4608       ['block6c_dwconv2[0][0]']        \n )                              1152)                                                             \n                                                                                                  \n block6c_activation (Activation  (None, None, None,   0          ['block6c_bn[0][0]']             \n )                              1152)                                                             \n                                                                                                  \n block6c_se_squeeze (GlobalAver  (None, 1152)        0           ['block6c_activation[0][0]']     \n agePooling2D)                                                                                    \n                                                                                                  \n block6c_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6c_se_squeeze[0][0]']     \n                                                                                                  \n block6c_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6c_se_reshape[0][0]']     \n                                                                                                  \n block6c_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6c_se_reduce[0][0]']      \n                                                                                                  \n block6c_se_excite (Multiply)   (None, None, None,   0           ['block6c_activation[0][0]',     \n                                1152)                             'block6c_se_expand[0][0]']      \n                                                                                                  \n block6c_project_conv (Conv2D)  (None, None, None,   221184      ['block6c_se_excite[0][0]']      \n                                192)                                                              \n                                                                                                  \n block6c_project_bn (BatchNorma  (None, None, None,   768        ['block6c_project_conv[0][0]']   \n lization)                      192)                                                              \n                                                                                                  \n block6c_drop (Dropout)         (None, None, None,   0           ['block6c_project_bn[0][0]']     \n                                192)                                                              \n                                                                                                  \n block6c_add (Add)              (None, None, None,   0           ['block6c_drop[0][0]',           \n                                192)                              'block6b_add[0][0]']            \n                                                                                                  \n block6d_expand_conv (Conv2D)   (None, None, None,   221184      ['block6c_add[0][0]']            \n                                1152)                                                             \n                                                                                                  \n block6d_expand_bn (BatchNormal  (None, None, None,   4608       ['block6d_expand_conv[0][0]']    \n ization)                       1152)                                                             \n                                                                                                  \n block6d_expand_activation (Act  (None, None, None,   0          ['block6d_expand_bn[0][0]']      \n ivation)                       1152)                                                             \n                                                                                                  \n block6d_dwconv2 (DepthwiseConv  (None, None, None,   10368      ['block6d_expand_activation[0][0]\n 2D)                            1152)                            ']                               \n                                                                                                  \n block6d_bn (BatchNormalization  (None, None, None,   4608       ['block6d_dwconv2[0][0]']        \n )                              1152)                                                             \n                                                                                                  \n block6d_activation (Activation  (None, None, None,   0          ['block6d_bn[0][0]']             \n )                              1152)                                                             \n                                                                                                  \n block6d_se_squeeze (GlobalAver  (None, 1152)        0           ['block6d_activation[0][0]']     \n agePooling2D)                                                                                    \n                                                                                                  \n block6d_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6d_se_squeeze[0][0]']     \n                                                                                                  \n block6d_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6d_se_reshape[0][0]']     \n                                                                                                  \n block6d_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6d_se_reduce[0][0]']      \n                                                                                                  \n block6d_se_excite (Multiply)   (None, None, None,   0           ['block6d_activation[0][0]',     \n                                1152)                             'block6d_se_expand[0][0]']      \n                                                                                                  \n block6d_project_conv (Conv2D)  (None, None, None,   221184      ['block6d_se_excite[0][0]']      \n                                192)                                                              \n                                                                                                  \n block6d_project_bn (BatchNorma  (None, None, None,   768        ['block6d_project_conv[0][0]']   \n lization)                      192)                                                              \n                                                                                                  \n block6d_drop (Dropout)         (None, None, None,   0           ['block6d_project_bn[0][0]']     \n                                192)                                                              \n                                                                                                  \n block6d_add (Add)              (None, None, None,   0           ['block6d_drop[0][0]',           \n                                192)                              'block6c_add[0][0]']            \n                                                                                                  \n block6e_expand_conv (Conv2D)   (None, None, None,   221184      ['block6d_add[0][0]']            \n                                1152)                                                             \n                                                                                                  \n block6e_expand_bn (BatchNormal  (None, None, None,   4608       ['block6e_expand_conv[0][0]']    \n ization)                       1152)                                                             \n                                                                                                  \n block6e_expand_activation (Act  (None, None, None,   0          ['block6e_expand_bn[0][0]']      \n ivation)                       1152)                                                             \n                                                                                                  \n block6e_dwconv2 (DepthwiseConv  (None, None, None,   10368      ['block6e_expand_activation[0][0]\n 2D)                            1152)                            ']                               \n                                                                                                  \n block6e_bn (BatchNormalization  (None, None, None,   4608       ['block6e_dwconv2[0][0]']        \n )                              1152)                                                             \n                                                                                                  \n block6e_activation (Activation  (None, None, None,   0          ['block6e_bn[0][0]']             \n )                              1152)                                                             \n                                                                                                  \n block6e_se_squeeze (GlobalAver  (None, 1152)        0           ['block6e_activation[0][0]']     \n agePooling2D)                                                                                    \n                                                                                                  \n block6e_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6e_se_squeeze[0][0]']     \n                                                                                                  \n block6e_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6e_se_reshape[0][0]']     \n                                                                                                  \n block6e_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6e_se_reduce[0][0]']      \n                                                                                                  \n block6e_se_excite (Multiply)   (None, None, None,   0           ['block6e_activation[0][0]',     \n                                1152)                             'block6e_se_expand[0][0]']      \n                                                                                                  \n block6e_project_conv (Conv2D)  (None, None, None,   221184      ['block6e_se_excite[0][0]']      \n                                192)                                                              \n                                                                                                  \n block6e_project_bn (BatchNorma  (None, None, None,   768        ['block6e_project_conv[0][0]']   \n lization)                      192)                                                              \n                                                                                                  \n block6e_drop (Dropout)         (None, None, None,   0           ['block6e_project_bn[0][0]']     \n                                192)                                                              \n                                                                                                  \n block6e_add (Add)              (None, None, None,   0           ['block6e_drop[0][0]',           \n                                192)                              'block6d_add[0][0]']            \n                                                                                                  \n block6f_expand_conv (Conv2D)   (None, None, None,   221184      ['block6e_add[0][0]']            \n                                1152)                                                             \n                                                                                                  \n block6f_expand_bn (BatchNormal  (None, None, None,   4608       ['block6f_expand_conv[0][0]']    \n ization)                       1152)                                                             \n                                                                                                  \n block6f_expand_activation (Act  (None, None, None,   0          ['block6f_expand_bn[0][0]']      \n ivation)                       1152)                                                             \n                                                                                                  \n block6f_dwconv2 (DepthwiseConv  (None, None, None,   10368      ['block6f_expand_activation[0][0]\n 2D)                            1152)                            ']                               \n                                                                                                  \n block6f_bn (BatchNormalization  (None, None, None,   4608       ['block6f_dwconv2[0][0]']        \n )                              1152)                                                             \n                                                                                                  \n block6f_activation (Activation  (None, None, None,   0          ['block6f_bn[0][0]']             \n )                              1152)                                                             \n                                                                                                  \n block6f_se_squeeze (GlobalAver  (None, 1152)        0           ['block6f_activation[0][0]']     \n agePooling2D)                                                                                    \n                                                                                                  \n block6f_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6f_se_squeeze[0][0]']     \n                                                                                                  \n block6f_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6f_se_reshape[0][0]']     \n                                                                                                  \n block6f_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6f_se_reduce[0][0]']      \n                                                                                                  \n block6f_se_excite (Multiply)   (None, None, None,   0           ['block6f_activation[0][0]',     \n                                1152)                             'block6f_se_expand[0][0]']      \n                                                                                                  \n block6f_project_conv (Conv2D)  (None, None, None,   221184      ['block6f_se_excite[0][0]']      \n                                192)                                                              \n                                                                                                  \n block6f_project_bn (BatchNorma  (None, None, None,   768        ['block6f_project_conv[0][0]']   \n lization)                      192)                                                              \n                                                                                                  \n block6f_drop (Dropout)         (None, None, None,   0           ['block6f_project_bn[0][0]']     \n                                192)                                                              \n                                                                                                  \n block6f_add (Add)              (None, None, None,   0           ['block6f_drop[0][0]',           \n                                192)                              'block6e_add[0][0]']            \n                                                                                                  \n block6g_expand_conv (Conv2D)   (None, None, None,   221184      ['block6f_add[0][0]']            \n                                1152)                                                             \n                                                                                                  \n block6g_expand_bn (BatchNormal  (None, None, None,   4608       ['block6g_expand_conv[0][0]']    \n ization)                       1152)                                                             \n                                                                                                  \n block6g_expand_activation (Act  (None, None, None,   0          ['block6g_expand_bn[0][0]']      \n ivation)                       1152)                                                             \n                                                                                                  \n block6g_dwconv2 (DepthwiseConv  (None, None, None,   10368      ['block6g_expand_activation[0][0]\n 2D)                            1152)                            ']                               \n                                                                                                  \n block6g_bn (BatchNormalization  (None, None, None,   4608       ['block6g_dwconv2[0][0]']        \n )                              1152)                                                             \n                                                                                                  \n block6g_activation (Activation  (None, None, None,   0          ['block6g_bn[0][0]']             \n )                              1152)                                                             \n                                                                                                  \n block6g_se_squeeze (GlobalAver  (None, 1152)        0           ['block6g_activation[0][0]']     \n agePooling2D)                                                                                    \n                                                                                                  \n block6g_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6g_se_squeeze[0][0]']     \n                                                                                                  \n block6g_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6g_se_reshape[0][0]']     \n                                                                                                  \n block6g_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6g_se_reduce[0][0]']      \n                                                                                                  \n block6g_se_excite (Multiply)   (None, None, None,   0           ['block6g_activation[0][0]',     \n                                1152)                             'block6g_se_expand[0][0]']      \n                                                                                                  \n block6g_project_conv (Conv2D)  (None, None, None,   221184      ['block6g_se_excite[0][0]']      \n                                192)                                                              \n                                                                                                  \n block6g_project_bn (BatchNorma  (None, None, None,   768        ['block6g_project_conv[0][0]']   \n lization)                      192)                                                              \n                                                                                                  \n block6g_drop (Dropout)         (None, None, None,   0           ['block6g_project_bn[0][0]']     \n                                192)                                                              \n                                                                                                  \n block6g_add (Add)              (None, None, None,   0           ['block6g_drop[0][0]',           \n                                192)                              'block6f_add[0][0]']            \n                                                                                                  \n block6h_expand_conv (Conv2D)   (None, None, None,   221184      ['block6g_add[0][0]']            \n                                1152)                                                             \n                                                                                                  \n block6h_expand_bn (BatchNormal  (None, None, None,   4608       ['block6h_expand_conv[0][0]']    \n ization)                       1152)                                                             \n                                                                                                  \n block6h_expand_activation (Act  (None, None, None,   0          ['block6h_expand_bn[0][0]']      \n ivation)                       1152)                                                             \n                                                                                                  \n block6h_dwconv2 (DepthwiseConv  (None, None, None,   10368      ['block6h_expand_activation[0][0]\n 2D)                            1152)                            ']                               \n                                                                                                  \n block6h_bn (BatchNormalization  (None, None, None,   4608       ['block6h_dwconv2[0][0]']        \n )                              1152)                                                             \n                                                                                                  \n block6h_activation (Activation  (None, None, None,   0          ['block6h_bn[0][0]']             \n )                              1152)                                                             \n                                                                                                  \n block6h_se_squeeze (GlobalAver  (None, 1152)        0           ['block6h_activation[0][0]']     \n agePooling2D)                                                                                    \n                                                                                                  \n block6h_se_reshape (Reshape)   (None, 1, 1, 1152)   0           ['block6h_se_squeeze[0][0]']     \n                                                                                                  \n block6h_se_reduce (Conv2D)     (None, 1, 1, 48)     55344       ['block6h_se_reshape[0][0]']     \n                                                                                                  \n block6h_se_expand (Conv2D)     (None, 1, 1, 1152)   56448       ['block6h_se_reduce[0][0]']      \n                                                                                                  \n block6h_se_excite (Multiply)   (None, None, None,   0           ['block6h_activation[0][0]',     \n                                1152)                             'block6h_se_expand[0][0]']      \n                                                                                                  \n block6h_project_conv (Conv2D)  (None, None, None,   221184      ['block6h_se_excite[0][0]']      \n                                192)                                                              \n                                                                                                  \n block6h_project_bn (BatchNorma  (None, None, None,   768        ['block6h_project_conv[0][0]']   \n lization)                      192)                                                              \n                                                                                                  \n block6h_drop (Dropout)         (None, None, None,   0           ['block6h_project_bn[0][0]']     \n                                192)                                                              \n                                                                                                  \n block6h_add (Add)              (None, None, None,   0           ['block6h_drop[0][0]',           \n                                192)                              'block6g_add[0][0]']            \n                                                                                                  \n top_conv (Conv2D)              (None, None, None,   245760      ['block6h_add[0][0]']            \n                                1280)                                                             \n                                                                                                  \n top_bn (BatchNormalization)    (None, None, None,   5120        ['top_conv[0][0]']               \n                                1280)                                                             \n                                                                                                  \n top_activation (Activation)    (None, None, None,   0           ['top_bn[0][0]']                 \n                                1280)                                                             \n                                                                                                  \n==================================================================================================\nTotal params: 5,919,312\nTrainable params: 0\nNon-trainable params: 5,919,312\n__________________________________________________________________________________________________\n</pre> <p>You can see how each of the different layers have a certain number of parameters each. Since we are using a pre-trained model, you can think of all of these parameters are patterns the base model has learned on another dataset. And because we set <code>base_model.trainable = False</code>, these patterns remain as they are during training (they're frozen and don't get updated).</p> <p>Alright that was the base model, let's see the summary of our overall model.</p> In\u00a0[15]: Copied! <pre># Check summary of model constructed with Functional API\nmodel_0.summary()\n</pre> # Check summary of model constructed with Functional API model_0.summary() <pre>Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n                                                                 \n efficientnetv2-b0 (Function  (None, None, None, 1280)  5919312  \n al)                                                             \n                                                                 \n global_average_pooling_laye  (None, 1280)             0         \n r (GlobalAveragePooling2D)                                      \n                                                                 \n output_layer (Dense)        (None, 10)                12810     \n                                                                 \n=================================================================\nTotal params: 5,932,122\nTrainable params: 12,810\nNon-trainable params: 5,919,312\n_________________________________________________________________\n</pre> <p>Our overall model has five layers but really, one of those layers (<code>efficientnetv2-b0</code>) has 269 layers.</p> <p>You can see how the output shape started out as <code>(None, 224, 224, 3)</code> for the input layer (the shape of our images) but was transformed to be <code>(None, 10)</code> by the output layer (the shape of our labels), where <code>None</code> is the placeholder for the batch size.</p> <p>Notice too, the only trainable parameters in the model are those in the output layer.</p> <p>How do our model's training curves look?</p> In\u00a0[16]: Copied! <pre># Check out our model's training curves\nplot_loss_curves(history_10_percent)\n</pre> # Check out our model's training curves plot_loss_curves(history_10_percent) In\u00a0[17]: Copied! <pre># Define input tensor shape (same number of dimensions as the output of efficientnetv2-b0)\ninput_shape = (1, 4, 4, 3)\n\n# Create a random tensor\ntf.random.set_seed(42)\ninput_tensor = tf.random.normal(input_shape)\nprint(f\"Random input tensor:\\n {input_tensor}\\n\")\n\n# Pass the random tensor through a global average pooling 2D layer\nglobal_average_pooled_tensor = tf.keras.layers.GlobalAveragePooling2D()(input_tensor)\nprint(f\"2D global average pooled random tensor:\\n {global_average_pooled_tensor}\\n\")\n\n# Check the shapes of the different tensors\nprint(f\"Shape of input tensor: {input_tensor.shape}\")\nprint(f\"Shape of 2D global averaged pooled input tensor: {global_average_pooled_tensor.shape}\")\n</pre> # Define input tensor shape (same number of dimensions as the output of efficientnetv2-b0) input_shape = (1, 4, 4, 3)  # Create a random tensor tf.random.set_seed(42) input_tensor = tf.random.normal(input_shape) print(f\"Random input tensor:\\n {input_tensor}\\n\")  # Pass the random tensor through a global average pooling 2D layer global_average_pooled_tensor = tf.keras.layers.GlobalAveragePooling2D()(input_tensor) print(f\"2D global average pooled random tensor:\\n {global_average_pooled_tensor}\\n\")  # Check the shapes of the different tensors print(f\"Shape of input tensor: {input_tensor.shape}\") print(f\"Shape of 2D global averaged pooled input tensor: {global_average_pooled_tensor.shape}\") <pre>Random input tensor:\n [[[[ 0.3274685  -0.8426258   0.3194337 ]\n   [-1.4075519  -2.3880599  -1.0392479 ]\n   [-0.5573232   0.539707    1.6994323 ]\n   [ 0.28893656 -1.5066116  -0.2645474 ]]\n\n  [[-0.59722406 -1.9171132  -0.62044144]\n   [ 0.8504023  -0.40604794 -3.0258412 ]\n   [ 0.9058464   0.29855987 -0.22561555]\n   [-0.7616443  -1.8917141  -0.93847126]]\n\n  [[ 0.77852213 -0.47338897  0.97772694]\n   [ 0.24694404  0.20573747 -0.5256233 ]\n   [ 0.32410017  0.02545409 -0.10638497]\n   [-0.6369475   1.1603122   0.2507359 ]]\n\n  [[-0.41728503  0.4012578  -1.4145443 ]\n   [-0.5931857  -1.6617213   0.33567193]\n   [ 0.10815629  0.23479682 -0.56668764]\n   [-0.35819843  0.88698614  0.52744764]]]]\n\n2D global average pooled random tensor:\n [[-0.09368646 -0.45840448 -0.2885598 ]]\n\nShape of input tensor: (1, 4, 4, 3)\nShape of 2D global averaged pooled input tensor: (1, 3)\n</pre> <p>You can see the <code>tf.keras.layers.GlobalAveragePooling2D()</code> layer condensed the input tensor from shape <code>(1, 4, 4, 3)</code> to <code>(1, 3)</code>. It did so by averaging the <code>input_tensor</code> across the middle two axes.</p> <p>We can replicate this operation using the <code>tf.reduce_mean()</code> operation and specifying the appropriate axes.</p> In\u00a0[18]: Copied! <pre># This is the same as GlobalAveragePooling2D()\ntf.reduce_mean(input_tensor, axis=[1, 2]) # average across the middle axes\n</pre> # This is the same as GlobalAveragePooling2D() tf.reduce_mean(input_tensor, axis=[1, 2]) # average across the middle axes Out[18]: <pre>&lt;tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[-0.09368646, -0.45840448, -0.2885598 ]], dtype=float32)&gt;</pre> <p>Doing this not only makes the output of the base model compatible with the input shape requirement of our output layer (<code>tf.keras.layers.Dense()</code>), it also condenses the information found by the base model into a lower dimension feature vector.</p> <p>\ud83d\udd11 Note: One of the reasons feature extraction transfer learning is named how it is is because what often happens is a pretrained model outputs a feature vector (a long tensor of numbers, in our case, this is the output of the <code>tf.keras.layers.GlobalAveragePooling2D()</code> layer) which can then be used to extract patterns out of.</p> <p>\ud83d\udee0 Practice: Do the same as the above cell but for <code>tf.keras.layers.GlobalMaxPool2D()</code>.</p> In\u00a0[19]: Copied! <pre># Download and unzip data\n!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_1_percent.zip\nunzip_data(\"10_food_classes_1_percent.zip\")\n\n# Create training and test dirs\ntrain_dir_1_percent = \"10_food_classes_1_percent/train/\"\ntest_dir = \"10_food_classes_1_percent/test/\"\n</pre> # Download and unzip data !wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_1_percent.zip unzip_data(\"10_food_classes_1_percent.zip\")  # Create training and test dirs train_dir_1_percent = \"10_food_classes_1_percent/train/\" test_dir = \"10_food_classes_1_percent/test/\" <pre>--2023-08-18 01:40:38--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_1_percent.zip\nResolving storage.googleapis.com (storage.googleapis.com)... 74.125.20.128, 108.177.98.128, 74.125.197.128, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|74.125.20.128|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 133612354 (127M) [application/zip]\nSaving to: \u201810_food_classes_1_percent.zip\u2019\n\n10_food_classes_1_p 100%[===================&gt;] 127.42M   222MB/s    in 0.6s    \n\n2023-08-18 01:40:39 (222 MB/s) - \u201810_food_classes_1_percent.zip\u2019 saved [133612354/133612354]\n\n</pre> <p>How many images are we working with?</p> In\u00a0[20]: Copied! <pre># Walk through 1 percent data directory and list number of files\nwalk_through_dir(\"10_food_classes_1_percent\")\n</pre> # Walk through 1 percent data directory and list number of files walk_through_dir(\"10_food_classes_1_percent\") <pre>There are 2 directories and 0 images in '10_food_classes_1_percent'.\nThere are 10 directories and 0 images in '10_food_classes_1_percent/train'.\nThere are 0 directories and 7 images in '10_food_classes_1_percent/train/ramen'.\nThere are 0 directories and 7 images in '10_food_classes_1_percent/train/chicken_curry'.\nThere are 0 directories and 7 images in '10_food_classes_1_percent/train/pizza'.\nThere are 0 directories and 7 images in '10_food_classes_1_percent/train/ice_cream'.\nThere are 0 directories and 7 images in '10_food_classes_1_percent/train/grilled_salmon'.\nThere are 0 directories and 7 images in '10_food_classes_1_percent/train/steak'.\nThere are 0 directories and 7 images in '10_food_classes_1_percent/train/chicken_wings'.\nThere are 0 directories and 7 images in '10_food_classes_1_percent/train/hamburger'.\nThere are 0 directories and 7 images in '10_food_classes_1_percent/train/sushi'.\nThere are 0 directories and 7 images in '10_food_classes_1_percent/train/fried_rice'.\nThere are 10 directories and 0 images in '10_food_classes_1_percent/test'.\nThere are 0 directories and 250 images in '10_food_classes_1_percent/test/ramen'.\nThere are 0 directories and 250 images in '10_food_classes_1_percent/test/chicken_curry'.\nThere are 0 directories and 250 images in '10_food_classes_1_percent/test/pizza'.\nThere are 0 directories and 250 images in '10_food_classes_1_percent/test/ice_cream'.\nThere are 0 directories and 250 images in '10_food_classes_1_percent/test/grilled_salmon'.\nThere are 0 directories and 250 images in '10_food_classes_1_percent/test/steak'.\nThere are 0 directories and 250 images in '10_food_classes_1_percent/test/chicken_wings'.\nThere are 0 directories and 250 images in '10_food_classes_1_percent/test/hamburger'.\nThere are 0 directories and 250 images in '10_food_classes_1_percent/test/sushi'.\nThere are 0 directories and 250 images in '10_food_classes_1_percent/test/fried_rice'.\n</pre> <p>Alright, looks like we've only got seven images of each class, this should be a bit of a challenge for our model.</p> <p>\ud83d\udd11 Note: As with the 10% of data subset, the 1% of images were chosen at random from the original full training dataset. The test images are the same as the ones which have previously been used. If you want to see how this data was preprocessed, check out the Food Vision Image Preprocessing notebook.</p> <p>Time to load our images in as <code>tf.data.Dataset</code> objects, to do so, we'll use the <code>image_dataset_from_directory()</code> method.</p> In\u00a0[21]: Copied! <pre>import tensorflow as tf\nIMG_SIZE = (224, 224)\ntrain_data_1_percent = tf.keras.preprocessing.image_dataset_from_directory(train_dir_1_percent,\n                                                                           label_mode=\"categorical\",\n                                                                           batch_size=32, # default\n                                                                           image_size=IMG_SIZE)\ntest_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n                                                                label_mode=\"categorical\",\n                                                                image_size=IMG_SIZE)\n</pre> import tensorflow as tf IMG_SIZE = (224, 224) train_data_1_percent = tf.keras.preprocessing.image_dataset_from_directory(train_dir_1_percent,                                                                            label_mode=\"categorical\",                                                                            batch_size=32, # default                                                                            image_size=IMG_SIZE) test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,                                                                 label_mode=\"categorical\",                                                                 image_size=IMG_SIZE) <pre>Found 70 files belonging to 10 classes.\nFound 2500 files belonging to 10 classes.\n</pre> <p>Data loaded. Time to augment it.</p> In\u00a0[22]: Copied! <pre>import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n# from tensorflow.keras.layers.experimental import preprocessing\n\n# NEW: Newer versions of TensorFlow (2.10+) can use the tensorflow.keras.layers API directly for data augmentation\ndata_augmentation = keras.Sequential([\n  layers.RandomFlip(\"horizontal\"),\n  layers.RandomRotation(0.2),\n  layers.RandomZoom(0.2),\n  layers.RandomHeight(0.2),\n  layers.RandomWidth(0.2),\n  # preprocessing.Rescaling(1./255) # keep for ResNet50V2, remove for EfficientNetV2B0\n], name =\"data_augmentation\")\n\n# # UPDATE: Previous versions of TensorFlow (e.g. 2.4 and below used the tensorflow.keras.layers.experimental.processing API)\n# # Create a data augmentation stage with horizontal flipping, rotations, zooms\n# data_augmentation = keras.Sequential([\n#   preprocessing.RandomFlip(\"horizontal\"),\n#   preprocessing.RandomRotation(0.2),\n#   preprocessing.RandomZoom(0.2),\n#   preprocessing.RandomHeight(0.2),\n#   preprocessing.RandomWidth(0.2),\n#   # preprocessing.Rescaling(1./255) # keep for ResNet50V2, remove for EfficientNetV2B0\n# ], name =\"data_augmentation\")\n</pre> import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers # from tensorflow.keras.layers.experimental import preprocessing  # NEW: Newer versions of TensorFlow (2.10+) can use the tensorflow.keras.layers API directly for data augmentation data_augmentation = keras.Sequential([   layers.RandomFlip(\"horizontal\"),   layers.RandomRotation(0.2),   layers.RandomZoom(0.2),   layers.RandomHeight(0.2),   layers.RandomWidth(0.2),   # preprocessing.Rescaling(1./255) # keep for ResNet50V2, remove for EfficientNetV2B0 ], name =\"data_augmentation\")  # # UPDATE: Previous versions of TensorFlow (e.g. 2.4 and below used the tensorflow.keras.layers.experimental.processing API) # # Create a data augmentation stage with horizontal flipping, rotations, zooms # data_augmentation = keras.Sequential([ #   preprocessing.RandomFlip(\"horizontal\"), #   preprocessing.RandomRotation(0.2), #   preprocessing.RandomZoom(0.2), #   preprocessing.RandomHeight(0.2), #   preprocessing.RandomWidth(0.2), #   # preprocessing.Rescaling(1./255) # keep for ResNet50V2, remove for EfficientNetV2B0 # ], name =\"data_augmentation\") <p>And that's it! Our data augmentation Sequential model is ready to go. As you'll see shortly, we'll be able to slot this \"model\" as a layer into our transfer learning model later on.</p> <p>But before we do that, let's test it out by passing random images through it.</p> In\u00a0[23]: Copied! <pre># View a random image\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport os\nimport random\ntarget_class = random.choice(train_data_1_percent.class_names) # choose a random class\ntarget_dir = \"10_food_classes_1_percent/train/\" + target_class # create the target directory\nrandom_image = random.choice(os.listdir(target_dir)) # choose a random image from target directory\nrandom_image_path = target_dir + \"/\" + random_image # create the choosen random image path\nimg = mpimg.imread(random_image_path) # read in the chosen target image\nplt.imshow(img) # plot the target image\nplt.title(f\"Original random image from class: {target_class}\")\nplt.axis(False); # turn off the axes\n\n# Augment the image\naugmented_img = data_augmentation(tf.expand_dims(img, axis=0)) # data augmentation model requires shape (None, height, width, 3)\nplt.figure()\nplt.imshow(tf.squeeze(augmented_img)/255.) # requires normalization after augmentation\nplt.title(f\"Augmented random image from class: {target_class}\")\nplt.axis(False);\n</pre> # View a random image import matplotlib.pyplot as plt import matplotlib.image as mpimg import os import random target_class = random.choice(train_data_1_percent.class_names) # choose a random class target_dir = \"10_food_classes_1_percent/train/\" + target_class # create the target directory random_image = random.choice(os.listdir(target_dir)) # choose a random image from target directory random_image_path = target_dir + \"/\" + random_image # create the choosen random image path img = mpimg.imread(random_image_path) # read in the chosen target image plt.imshow(img) # plot the target image plt.title(f\"Original random image from class: {target_class}\") plt.axis(False); # turn off the axes  # Augment the image augmented_img = data_augmentation(tf.expand_dims(img, axis=0)) # data augmentation model requires shape (None, height, width, 3) plt.figure() plt.imshow(tf.squeeze(augmented_img)/255.) # requires normalization after augmentation plt.title(f\"Augmented random image from class: {target_class}\") plt.axis(False); <p>Run the cell above a few times and you can see the different random augmentations on different classes of images. Because we're going to add the data augmentation model as a layer in our upcoming transfer learning model, it'll apply these kind of random augmentations to each of the training images which passes through it.</p> <p>Doing this will make our training dataset a little more varied. You can think of it as if you were taking a photo of food in real-life, not all of the images are going to be perfect, some of them are going to be orientated in strange ways. These are the kind of images we want our model to be able to handle.</p> <p>Speaking of model, let's build one with the Functional API. We'll run through all of the same steps as before except for one difference, we'll add our data augmentation Sequential model as a layer immediately after the input layer.</p> In\u00a0[24]: Copied! <pre># Setup input shape and base model, freezing the base model layers\ninput_shape = (224, 224, 3)\nbase_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False)\nbase_model.trainable = False\n\n# Create input layer\ninputs = layers.Input(shape=input_shape, name=\"input_layer\")\n\n# Add in data augmentation Sequential model as a layer\nx = data_augmentation(inputs)\n\n# Give base_model inputs (after augmentation) and don't train it\nx = base_model(x, training=False)\n\n# Pool output features of base model\nx = layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n\n# Put a dense layer on as the output\noutputs = layers.Dense(10, activation=\"softmax\", name=\"output_layer\")(x)\n\n# Make a model with inputs and outputs\nmodel_1 = keras.Model(inputs, outputs)\n\n# Compile the model\nmodel_1.compile(loss=\"categorical_crossentropy\",\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=[\"accuracy\"])\n\n# Fit the model\nhistory_1_percent = model_1.fit(train_data_1_percent,\n                    epochs=5,\n                    steps_per_epoch=len(train_data_1_percent),\n                    validation_data=test_data,\n                    validation_steps=int(0.25* len(test_data)), # validate for less steps\n                    # Track model training logs\n                    callbacks=[create_tensorboard_callback(\"transfer_learning\", \"1_percent_data_aug\")])\n</pre> # Setup input shape and base model, freezing the base model layers input_shape = (224, 224, 3) base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False) base_model.trainable = False  # Create input layer inputs = layers.Input(shape=input_shape, name=\"input_layer\")  # Add in data augmentation Sequential model as a layer x = data_augmentation(inputs)  # Give base_model inputs (after augmentation) and don't train it x = base_model(x, training=False)  # Pool output features of base model x = layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)  # Put a dense layer on as the output outputs = layers.Dense(10, activation=\"softmax\", name=\"output_layer\")(x)  # Make a model with inputs and outputs model_1 = keras.Model(inputs, outputs)  # Compile the model model_1.compile(loss=\"categorical_crossentropy\",               optimizer=tf.keras.optimizers.Adam(),               metrics=[\"accuracy\"])  # Fit the model history_1_percent = model_1.fit(train_data_1_percent,                     epochs=5,                     steps_per_epoch=len(train_data_1_percent),                     validation_data=test_data,                     validation_steps=int(0.25* len(test_data)), # validate for less steps                     # Track model training logs                     callbacks=[create_tensorboard_callback(\"transfer_learning\", \"1_percent_data_aug\")]) <pre>Saving TensorBoard log files to: transfer_learning/1_percent_data_aug/20230818-014045\nEpoch 1/5\n3/3 [==============================] - 12s 2s/step - loss: 2.3484 - accuracy: 0.1000 - val_loss: 2.1935 - val_accuracy: 0.1908\nEpoch 2/5\n3/3 [==============================] - 2s 965ms/step - loss: 2.1591 - accuracy: 0.2000 - val_loss: 2.1027 - val_accuracy: 0.2418\nEpoch 3/5\n3/3 [==============================] - 2s 894ms/step - loss: 1.9803 - accuracy: 0.3571 - val_loss: 2.0100 - val_accuracy: 0.3076\nEpoch 4/5\n3/3 [==============================] - 2s 916ms/step - loss: 1.8378 - accuracy: 0.5000 - val_loss: 1.9392 - val_accuracy: 0.3372\nEpoch 5/5\n3/3 [==============================] - 2s 866ms/step - loss: 1.7660 - accuracy: 0.5429 - val_loss: 1.8595 - val_accuracy: 0.3931\n</pre> <p>Wow!</p> <p>How cool is that? Using only 7 training images per class, using transfer learning our model was able to get ~45%+ accuracy on the validation set.</p> <p>This result is pretty amazing since the original Food-101 paper achieved 50.67% accuracy with all the data, namely, 750 training images per class (note: this metric was across 101 classes, not 10, we'll get to 101 classes soon).</p> <p>If we check out a summary of our model, we should see the data augmentation layer just after the input layer.</p> In\u00a0[25]: Copied! <pre># Check out model summary\nmodel_1.summary()\n</pre> # Check out model summary model_1.summary() <pre>Model: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n                                                                 \n data_augmentation (Sequenti  (None, None, None, 3)    0         \n al)                                                             \n                                                                 \n efficientnetv2-b0 (Function  (None, None, None, 1280)  5919312  \n al)                                                             \n                                                                 \n global_average_pooling_laye  (None, 1280)             0         \n r (GlobalAveragePooling2D)                                      \n                                                                 \n output_layer (Dense)        (None, 10)                12810     \n                                                                 \n=================================================================\nTotal params: 5,932,122\nTrainable params: 12,810\nNon-trainable params: 5,919,312\n_________________________________________________________________\n</pre> <p>There it is. We've now got data augmentation built right into the our model. This means if we saved it and reloaded it somewhere else, the data augmentation layers would come with it.</p> <p>The important thing to remember is data augmentation only runs during training. So if we were to evaluate or use our model for inference (predicting the class of an image) the data augmentation layers will be automatically turned off.</p> <p>To see this in action, let's evaluate our model on the test data.</p> In\u00a0[26]: Copied! <pre># Evaluate on the test data\nresults_1_percent_data_aug = model_1.evaluate(test_data)\nresults_1_percent_data_aug\n</pre> # Evaluate on the test data results_1_percent_data_aug = model_1.evaluate(test_data) results_1_percent_data_aug <pre>79/79 [==============================] - 3s 36ms/step - loss: 1.8197 - accuracy: 0.4188\n</pre> Out[26]: <pre>[1.819704294204712, 0.4187999963760376]</pre> <p>The results here may be slightly better/worse than the log outputs of our model during training because during training we only evaluate our model on 25% of the test data using the line <code>validation_steps=int(0.25 * len(test_data))</code>. Doing this speeds up our epochs but still gives us enough of an idea of how our model is going.</p> <p>Let's stay consistent and check out our model's loss curves.</p> In\u00a0[27]: Copied! <pre># How does the model go with a data augmentation layer with 1% of data\nplot_loss_curves(history_1_percent)\n</pre> # How does the model go with a data augmentation layer with 1% of data plot_loss_curves(history_1_percent) <p>It looks like the metrics on both datasets would improve if we kept training for more epochs. But we'll leave that for now, we've got more experiments to do!</p> In\u00a0[28]: Copied! <pre># Get 10% of the data of the 10 classes (uncomment if you haven't gotten \"10_food_classes_10_percent.zip\" already)\n# !wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n# unzip_data(\"10_food_classes_10_percent.zip\")\n\ntrain_dir_10_percent = \"10_food_classes_10_percent/train/\"\ntest_dir = \"10_food_classes_10_percent/test/\"\n</pre> # Get 10% of the data of the 10 classes (uncomment if you haven't gotten \"10_food_classes_10_percent.zip\" already) # !wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip # unzip_data(\"10_food_classes_10_percent.zip\")  train_dir_10_percent = \"10_food_classes_10_percent/train/\" test_dir = \"10_food_classes_10_percent/test/\" <p>Data downloaded. Let's create the dataloaders.</p> In\u00a0[29]: Copied! <pre># Setup data inputs\nimport tensorflow as tf\nIMG_SIZE = (224, 224)\ntrain_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(train_dir_10_percent,\n                                                                            label_mode=\"categorical\",\n                                                                            image_size=IMG_SIZE)\n# Note: the test data is the same as the previous experiment, we could\n# skip creating this, but we'll leave this here to practice.\ntest_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n                                                                label_mode=\"categorical\",\n                                                                image_size=IMG_SIZE)\n</pre> # Setup data inputs import tensorflow as tf IMG_SIZE = (224, 224) train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(train_dir_10_percent,                                                                             label_mode=\"categorical\",                                                                             image_size=IMG_SIZE) # Note: the test data is the same as the previous experiment, we could # skip creating this, but we'll leave this here to practice. test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,                                                                 label_mode=\"categorical\",                                                                 image_size=IMG_SIZE) <pre>Found 750 files belonging to 10 classes.\nFound 2500 files belonging to 10 classes.\n</pre> <p>Awesome!</p> <p>We've got 10x more images to work with, 75 per class instead of 7 per class.</p> <p>Let's build a model with data augmentation built in. We could reuse the data augmentation Sequential model we created before but we'll recreate it to practice.</p> In\u00a0[30]: Copied! <pre># Create a functional model with data augmentation\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers.experimental import preprocessing # OLD\n\n# NEW: Newer versions of TensorFlow (2.10+) can use the tensorflow.keras.layers API directly for data augmentation\ndata_augmentation = keras.Sequential([\n  layers.RandomFlip(\"horizontal\"),\n  layers.RandomRotation(0.2),\n  layers.RandomZoom(0.2),\n  layers.RandomHeight(0.2),\n  layers.RandomWidth(0.2),\n  # preprocessing.Rescaling(1./255) # keep for ResNet50V2, remove for EfficientNet\n], name =\"data_augmentation\")\n\n## OLD\n# # Build data augmentation layer\n# data_augmentation = Sequential([\n#   preprocessing.RandomFlip('horizontal'),\n#   preprocessing.RandomHeight(0.2),\n#   preprocessing.RandomWidth(0.2),\n#   preprocessing.RandomZoom(0.2),\n#   preprocessing.RandomRotation(0.2),\n#   # preprocessing.Rescaling(1./255) # keep for ResNet50V2, remove for EfficientNet\n# ], name=\"data_augmentation\")\n\n# Setup the input shape to our model\ninput_shape = (224, 224, 3)\n\n# Create a frozen base model\n# base_model = tf.keras.applications.EfficientNetB0(include_top=False)\nbase_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False)\nbase_model.trainable = False\n\n# Create input and output layers\ninputs = layers.Input(shape=input_shape, name=\"input_layer\") # create input layer\nx = data_augmentation(inputs) # augment our training images\nx = base_model(x, training=False) # pass augmented images to base model but keep it in inference mode, so batchnorm layers don't get updated: https://keras.io/guides/transfer_learning/#build-a-model\nx = layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\noutputs = layers.Dense(10, activation=\"softmax\", name=\"output_layer\")(x)\nmodel_2 = tf.keras.Model(inputs, outputs)\n\n# Compile\nmodel_2.compile(loss=\"categorical_crossentropy\",\n              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), # use Adam optimizer with base learning rate\n              metrics=[\"accuracy\"])\n</pre> # Create a functional model with data augmentation import tensorflow as tf from tensorflow.keras import layers from tensorflow.keras.models import Sequential # from tensorflow.keras.layers.experimental import preprocessing # OLD  # NEW: Newer versions of TensorFlow (2.10+) can use the tensorflow.keras.layers API directly for data augmentation data_augmentation = keras.Sequential([   layers.RandomFlip(\"horizontal\"),   layers.RandomRotation(0.2),   layers.RandomZoom(0.2),   layers.RandomHeight(0.2),   layers.RandomWidth(0.2),   # preprocessing.Rescaling(1./255) # keep for ResNet50V2, remove for EfficientNet ], name =\"data_augmentation\")  ## OLD # # Build data augmentation layer # data_augmentation = Sequential([ #   preprocessing.RandomFlip('horizontal'), #   preprocessing.RandomHeight(0.2), #   preprocessing.RandomWidth(0.2), #   preprocessing.RandomZoom(0.2), #   preprocessing.RandomRotation(0.2), #   # preprocessing.Rescaling(1./255) # keep for ResNet50V2, remove for EfficientNet # ], name=\"data_augmentation\")  # Setup the input shape to our model input_shape = (224, 224, 3)  # Create a frozen base model # base_model = tf.keras.applications.EfficientNetB0(include_top=False) base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False) base_model.trainable = False  # Create input and output layers inputs = layers.Input(shape=input_shape, name=\"input_layer\") # create input layer x = data_augmentation(inputs) # augment our training images x = base_model(x, training=False) # pass augmented images to base model but keep it in inference mode, so batchnorm layers don't get updated: https://keras.io/guides/transfer_learning/#build-a-model x = layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x) outputs = layers.Dense(10, activation=\"softmax\", name=\"output_layer\")(x) model_2 = tf.keras.Model(inputs, outputs)  # Compile model_2.compile(loss=\"categorical_crossentropy\",               optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), # use Adam optimizer with base learning rate               metrics=[\"accuracy\"]) <p>To save time for later (when we want to perform multiple experiments with <code>model_2</code>), let's put the code above into a function we can resuse.</p> In\u00a0[31]: Copied! <pre>def create_base_model(input_shape: tuple[int, int, int] = (224, 224, 3),\n                      output_shape: int = 10,\n                      learning_rate: float = 0.001,\n                      training: bool = False) -&gt; tf.keras.Model:\n    \"\"\"\n    Create a model based on EfficientNetV2B0 with built-in data augmentation.\n\n    Parameters:\n    - input_shape (tuple): Expected shape of input images. Default is (224, 224, 3).\n    - output_shape (int): Number of classes for the output layer. Default is 10.\n    - learning_rate (float): Learning rate for the Adam optimizer. Default is 0.001.\n    - training (bool): Whether the base model is trainable. Default is False.\n\n    Returns:\n    - tf.keras.Model: The compiled model with specified input and output settings.\n    \"\"\"\n\n    # Create base model\n    base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False)\n    base_model.trainable = training\n\n    # Setup model input and outputs with data augmentation built-in\n    inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n    x = data_augmentation(inputs)\n    x = base_model(x, training=False)  # pass augmented images to base model but keep it in inference mode\n    x = layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n    outputs = layers.Dense(units=output_shape, activation=\"softmax\", name=\"output_layer\")(x)\n    model = tf.keras.Model(inputs, outputs)\n\n    # Compile model\n    model.compile(loss=\"categorical_crossentropy\",\n                  optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n                  metrics=[\"accuracy\"])\n\n    return model\n\n# Create an instance of model_2 with our new function\nmodel_2 = create_base_model()\n</pre> def create_base_model(input_shape: tuple[int, int, int] = (224, 224, 3),                       output_shape: int = 10,                       learning_rate: float = 0.001,                       training: bool = False) -&gt; tf.keras.Model:     \"\"\"     Create a model based on EfficientNetV2B0 with built-in data augmentation.      Parameters:     - input_shape (tuple): Expected shape of input images. Default is (224, 224, 3).     - output_shape (int): Number of classes for the output layer. Default is 10.     - learning_rate (float): Learning rate for the Adam optimizer. Default is 0.001.     - training (bool): Whether the base model is trainable. Default is False.      Returns:     - tf.keras.Model: The compiled model with specified input and output settings.     \"\"\"      # Create base model     base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False)     base_model.trainable = training      # Setup model input and outputs with data augmentation built-in     inputs = layers.Input(shape=input_shape, name=\"input_layer\")     x = data_augmentation(inputs)     x = base_model(x, training=False)  # pass augmented images to base model but keep it in inference mode     x = layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)     outputs = layers.Dense(units=output_shape, activation=\"softmax\", name=\"output_layer\")(x)     model = tf.keras.Model(inputs, outputs)      # Compile model     model.compile(loss=\"categorical_crossentropy\",                   optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),                   metrics=[\"accuracy\"])      return model  # Create an instance of model_2 with our new function model_2 = create_base_model() In\u00a0[32]: Copied! <pre># Setup checkpoint path\ncheckpoint_path = \"ten_percent_model_checkpoints_weights/checkpoint.ckpt\" # note: remember saving directly to Colab is temporary\n\n# Create a ModelCheckpoint callback that saves the model's weights only\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                         save_weights_only=True, # set to False to save the entire model\n                                                         save_best_only=True, # save only the best model weights instead of a model every epoch\n                                                         save_freq=\"epoch\", # save every epoch\n                                                         verbose=1)\n</pre> # Setup checkpoint path checkpoint_path = \"ten_percent_model_checkpoints_weights/checkpoint.ckpt\" # note: remember saving directly to Colab is temporary  # Create a ModelCheckpoint callback that saves the model's weights only checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,                                                          save_weights_only=True, # set to False to save the entire model                                                          save_best_only=True, # save only the best model weights instead of a model every epoch                                                          save_freq=\"epoch\", # save every epoch                                                          verbose=1) <p>\ud83e\udd14 Question: What's the difference between saving the entire model (SavedModel format) and saving the weights only?</p> <p>The <code>SavedModel</code> format saves a model's architecture, weights and training configuration all in one folder. It makes it very easy to reload your model exactly how it is elsewhere. However, if you do not want to share all of these details with others, you may want to save and share the weights only (these will just be large tensors of non-human interpretable numbers). If disk space is an issue, saving the weights only is faster and takes up less space than saving the whole model.</p> <p>Time to fit the model.</p> <p>Because we're going to be fine-tuning it later, we'll create a variable <code>initial_epochs</code> and set it to 5 to use later.</p> <p>We'll also add in our <code>checkpoint_callback</code> in our list of <code>callbacks</code>.</p> In\u00a0[33]: Copied! <pre># Fit the model saving checkpoints every epoch\ninitial_epochs = 5\nhistory_10_percent_data_aug = model_2.fit(train_data_10_percent,\n                                          epochs=initial_epochs,\n                                          validation_data=test_data,\n                                          validation_steps=int(0.25 * len(test_data)), # do less steps per validation (quicker)\n                                          callbacks=[create_tensorboard_callback(\"transfer_learning\", \"10_percent_data_aug\"),\n                                                     checkpoint_callback])\n</pre> # Fit the model saving checkpoints every epoch initial_epochs = 5 history_10_percent_data_aug = model_2.fit(train_data_10_percent,                                           epochs=initial_epochs,                                           validation_data=test_data,                                           validation_steps=int(0.25 * len(test_data)), # do less steps per validation (quicker)                                           callbacks=[create_tensorboard_callback(\"transfer_learning\", \"10_percent_data_aug\"),                                                      checkpoint_callback]) <pre>Saving TensorBoard log files to: transfer_learning/10_percent_data_aug/20230818-014119\nEpoch 1/5\n24/24 [==============================] - ETA: 0s - loss: 2.0057 - accuracy: 0.3360\nEpoch 1: val_loss improved from inf to 1.50455, saving model to ten_percent_model_checkpoints_weights/checkpoint.ckpt\n24/24 [==============================] - 18s 411ms/step - loss: 2.0057 - accuracy: 0.3360 - val_loss: 1.5045 - val_accuracy: 0.6332\nEpoch 2/5\n24/24 [==============================] - ETA: 0s - loss: 1.3970 - accuracy: 0.6413\nEpoch 2: val_loss improved from 1.50455 to 1.04085, saving model to ten_percent_model_checkpoints_weights/checkpoint.ckpt\n24/24 [==============================] - 7s 291ms/step - loss: 1.3970 - accuracy: 0.6413 - val_loss: 1.0408 - val_accuracy: 0.7615\nEpoch 3/5\n24/24 [==============================] - ETA: 0s - loss: 1.1097 - accuracy: 0.7253\nEpoch 3: val_loss improved from 1.04085 to 0.86102, saving model to ten_percent_model_checkpoints_weights/checkpoint.ckpt\n24/24 [==============================] - 7s 285ms/step - loss: 1.1097 - accuracy: 0.7253 - val_loss: 0.8610 - val_accuracy: 0.7895\nEpoch 4/5\n24/24 [==============================] - ETA: 0s - loss: 0.9359 - accuracy: 0.7707\nEpoch 4: val_loss improved from 0.86102 to 0.72648, saving model to ten_percent_model_checkpoints_weights/checkpoint.ckpt\n24/24 [==============================] - 6s 247ms/step - loss: 0.9359 - accuracy: 0.7707 - val_loss: 0.7265 - val_accuracy: 0.8306\nEpoch 5/5\n24/24 [==============================] - ETA: 0s - loss: 0.8332 - accuracy: 0.7840\nEpoch 5: val_loss improved from 0.72648 to 0.68383, saving model to ten_percent_model_checkpoints_weights/checkpoint.ckpt\n24/24 [==============================] - 6s 249ms/step - loss: 0.8332 - accuracy: 0.7840 - val_loss: 0.6838 - val_accuracy: 0.8240\n</pre> <p>Would you look at that! Looks like our <code>ModelCheckpoint</code> callback worked and our model saved its weights every epoch without too much overhead (saving the whole model takes longer than just the weights).</p> <p>Let's evaluate our model and check its loss curves.</p> In\u00a0[34]: Copied! <pre># Evaluate on the test data\nresults_10_percent_data_aug = model_2.evaluate(test_data)\nresults_10_percent_data_aug\n</pre> # Evaluate on the test data results_10_percent_data_aug = model_2.evaluate(test_data) results_10_percent_data_aug <pre>79/79 [==============================] - 3s 34ms/step - loss: 0.6795 - accuracy: 0.8216\n</pre> Out[34]: <pre>[0.6794611215591431, 0.8216000199317932]</pre> In\u00a0[35]: Copied! <pre># Plot model loss curves\nplot_loss_curves(history_10_percent_data_aug)\n</pre> # Plot model loss curves plot_loss_curves(history_10_percent_data_aug) <p>Looking at these, our model's performance with 10% of the data and data augmentation isn't as good as the model with 10% of the data without data augmentation (see <code>model_0</code> results above), however the curves are trending in the right direction, meaning if we decided to train for longer, its metrics would likely improve.</p> <p>Since we checkpointed (is that a word?) our model's weights, we might as well see what it's like to load it back in. We'll be able to test if it saved correctly by evaluting it on the test data.</p> <p>To load saved model weights you can use the the <code>load_weights()</code> method, passing it the path where your saved weights are stored.</p> In\u00a0[36]: Copied! <pre># Load in saved model weights and evaluate model\nmodel_2.load_weights(checkpoint_path)\nloaded_weights_model_results = model_2.evaluate(test_data)\n</pre> # Load in saved model weights and evaluate model model_2.load_weights(checkpoint_path) loaded_weights_model_results = model_2.evaluate(test_data) <pre>79/79 [==============================] - 3s 38ms/step - loss: 0.6795 - accuracy: 0.8216\n</pre> <p>Now let's compare the results of our previously trained model and the loaded model. These results should very close if not exactly the same. The reason for minor differences comes down to the precision level of numbers calculated.</p> In\u00a0[37]: Copied! <pre># If the results from our native model and the loaded weights are the same, this should output True\nresults_10_percent_data_aug == loaded_weights_model_results\n</pre> # If the results from our native model and the loaded weights are the same, this should output True results_10_percent_data_aug == loaded_weights_model_results Out[37]: <pre>True</pre> <p>If the above cell doesn't output <code>True</code>, it's because the numbers are close but not the exact same (due to how computers store numbers with degrees of precision).</p> <p>However, they should be very close...</p> In\u00a0[38]: Copied! <pre>import numpy as np\n# Check to see if loaded model results are very close to native model results (should output True)\nnp.isclose(np.array(results_10_percent_data_aug), np.array(loaded_weights_model_results))\n</pre> import numpy as np # Check to see if loaded model results are very close to native model results (should output True) np.isclose(np.array(results_10_percent_data_aug), np.array(loaded_weights_model_results)) Out[38]: <pre>array([ True,  True])</pre> In\u00a0[39]: Copied! <pre># Check the difference between the two results (small values)\nprint(np.array(results_10_percent_data_aug) - np.array(loaded_weights_model_results))\n</pre> # Check the difference between the two results (small values) print(np.array(results_10_percent_data_aug) - np.array(loaded_weights_model_results)) <pre>[0. 0.]\n</pre> In\u00a0[40]: Copied! <pre># Layers in loaded model\nmodel_2.layers\n</pre> # Layers in loaded model model_2.layers Out[40]: <pre>[&lt;keras.engine.input_layer.InputLayer at 0x7c62e86157b0&gt;,\n &lt;keras.engine.sequential.Sequential at 0x7c62e8188280&gt;,\n &lt;keras.engine.functional.Functional at 0x7c62a3daecb0&gt;,\n &lt;keras.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D at 0x7c62a3c0a890&gt;,\n &lt;keras.layers.core.dense.Dense at 0x7c62a3ddbf70&gt;]</pre> <p>How about we check their names, numbers and if they're trainable?</p> In\u00a0[41]: Copied! <pre>for layer_number, layer in enumerate(model_2.layers):\n  print(f\"Layer number: {layer_number} | Layer name: {layer.name} | Layer type: {layer} | Trainable? {layer.trainable}\")\n</pre> for layer_number, layer in enumerate(model_2.layers):   print(f\"Layer number: {layer_number} | Layer name: {layer.name} | Layer type: {layer} | Trainable? {layer.trainable}\") <pre>Layer number: 0 | Layer name: input_layer | Layer type: &lt;keras.engine.input_layer.InputLayer object at 0x7c62e86157b0&gt; | Trainable? True\nLayer number: 1 | Layer name: data_augmentation | Layer type: &lt;keras.engine.sequential.Sequential object at 0x7c62e8188280&gt; | Trainable? True\nLayer number: 2 | Layer name: efficientnetv2-b0 | Layer type: &lt;keras.engine.functional.Functional object at 0x7c62a3daecb0&gt; | Trainable? False\nLayer number: 3 | Layer name: global_average_pooling_layer | Layer type: &lt;keras.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D object at 0x7c62a3c0a890&gt; | Trainable? True\nLayer number: 4 | Layer name: output_layer | Layer type: &lt;keras.layers.core.dense.Dense object at 0x7c62a3ddbf70&gt; | Trainable? True\n</pre> <p>Looking good.</p> <p>We've got an input layer, a Sequential layer (the data augmentation model), a Functional layer (EfficientNetV2B0), a pooling layer and a Dense layer (the output layer).</p> <p>How about a summary?</p> In\u00a0[42]: Copied! <pre>model_2.summary()\n</pre> model_2.summary() <pre>Model: \"model_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n                                                                 \n data_augmentation (Sequenti  (None, None, None, 3)    0         \n al)                                                             \n                                                                 \n efficientnetv2-b0 (Function  (None, None, None, 1280)  5919312  \n al)                                                             \n                                                                 \n global_average_pooling_laye  (None, 1280)             0         \n r (GlobalAveragePooling2D)                                      \n                                                                 \n output_layer (Dense)        (None, 10)                12810     \n                                                                 \n=================================================================\nTotal params: 5,932,122\nTrainable params: 12,810\nNon-trainable params: 5,919,312\n_________________________________________________________________\n</pre> <p>Alright, it looks like all of the layers in the <code>efficientnetv2-b0</code> layer are frozen. We can confirm this using the <code>trainable_variables</code> attribute.</p> <p>Note: The layers of <code>base_model</code> (our <code>efficientnetv2-b0</code> feature extractor) of <code>model_2</code> is accessible by referencing <code>model_2.layers[2]</code>.</p> In\u00a0[43]: Copied! <pre># Access the base_model layers of model_2\nmodel_2_base_model = model_2.layers[2]\nmodel_2_base_model.name\n</pre> # Access the base_model layers of model_2 model_2_base_model = model_2.layers[2] model_2_base_model.name Out[43]: <pre>'efficientnetv2-b0'</pre> In\u00a0[44]: Copied! <pre># How many layers are trainable in our model_2_base_model?\nprint(len(model_2_base_model.trainable_variables)) # layer at index 2 is the EfficientNetV2B0 layer (the base model)\n</pre> # How many layers are trainable in our model_2_base_model? print(len(model_2_base_model.trainable_variables)) # layer at index 2 is the EfficientNetV2B0 layer (the base model) <pre>0\n</pre> <p>We can even check layer by layer to see if the they're trainable.</p> <p>To access the layers in <code>model_2_base_model</code>, we can use the <code>layers</code> attribute.</p> In\u00a0[45]: Copied! <pre># Check which layers are tuneable (trainable)\nfor layer_number, layer in enumerate(model_2_base_model.layers):\n  print(layer_number, layer.name, layer.trainable)\n</pre> # Check which layers are tuneable (trainable) for layer_number, layer in enumerate(model_2_base_model.layers):   print(layer_number, layer.name, layer.trainable) <pre>0 input_4 False\n1 rescaling_3 False\n2 normalization_3 False\n3 stem_conv False\n4 stem_bn False\n5 stem_activation False\n6 block1a_project_conv False\n7 block1a_project_bn False\n8 block1a_project_activation False\n9 block2a_expand_conv False\n10 block2a_expand_bn False\n11 block2a_expand_activation False\n12 block2a_project_conv False\n13 block2a_project_bn False\n14 block2b_expand_conv False\n15 block2b_expand_bn False\n16 block2b_expand_activation False\n17 block2b_project_conv False\n18 block2b_project_bn False\n19 block2b_drop False\n20 block2b_add False\n21 block3a_expand_conv False\n22 block3a_expand_bn False\n23 block3a_expand_activation False\n24 block3a_project_conv False\n25 block3a_project_bn False\n26 block3b_expand_conv False\n27 block3b_expand_bn False\n28 block3b_expand_activation False\n29 block3b_project_conv False\n30 block3b_project_bn False\n31 block3b_drop False\n32 block3b_add False\n33 block4a_expand_conv False\n34 block4a_expand_bn False\n35 block4a_expand_activation False\n36 block4a_dwconv2 False\n37 block4a_bn False\n38 block4a_activation False\n39 block4a_se_squeeze False\n40 block4a_se_reshape False\n41 block4a_se_reduce False\n42 block4a_se_expand False\n43 block4a_se_excite False\n44 block4a_project_conv False\n45 block4a_project_bn False\n46 block4b_expand_conv False\n47 block4b_expand_bn False\n48 block4b_expand_activation False\n49 block4b_dwconv2 False\n50 block4b_bn False\n51 block4b_activation False\n52 block4b_se_squeeze False\n53 block4b_se_reshape False\n54 block4b_se_reduce False\n55 block4b_se_expand False\n56 block4b_se_excite False\n57 block4b_project_conv False\n58 block4b_project_bn False\n59 block4b_drop False\n60 block4b_add False\n61 block4c_expand_conv False\n62 block4c_expand_bn False\n63 block4c_expand_activation False\n64 block4c_dwconv2 False\n65 block4c_bn False\n66 block4c_activation False\n67 block4c_se_squeeze False\n68 block4c_se_reshape False\n69 block4c_se_reduce False\n70 block4c_se_expand False\n71 block4c_se_excite False\n72 block4c_project_conv False\n73 block4c_project_bn False\n74 block4c_drop False\n75 block4c_add False\n76 block5a_expand_conv False\n77 block5a_expand_bn False\n78 block5a_expand_activation False\n79 block5a_dwconv2 False\n80 block5a_bn False\n81 block5a_activation False\n82 block5a_se_squeeze False\n83 block5a_se_reshape False\n84 block5a_se_reduce False\n85 block5a_se_expand False\n86 block5a_se_excite False\n87 block5a_project_conv False\n88 block5a_project_bn False\n89 block5b_expand_conv False\n90 block5b_expand_bn False\n91 block5b_expand_activation False\n92 block5b_dwconv2 False\n93 block5b_bn False\n94 block5b_activation False\n95 block5b_se_squeeze False\n96 block5b_se_reshape False\n97 block5b_se_reduce False\n98 block5b_se_expand False\n99 block5b_se_excite False\n100 block5b_project_conv False\n101 block5b_project_bn False\n102 block5b_drop False\n103 block5b_add False\n104 block5c_expand_conv False\n105 block5c_expand_bn False\n106 block5c_expand_activation False\n107 block5c_dwconv2 False\n108 block5c_bn False\n109 block5c_activation False\n110 block5c_se_squeeze False\n111 block5c_se_reshape False\n112 block5c_se_reduce False\n113 block5c_se_expand False\n114 block5c_se_excite False\n115 block5c_project_conv False\n116 block5c_project_bn False\n117 block5c_drop False\n118 block5c_add False\n119 block5d_expand_conv False\n120 block5d_expand_bn False\n121 block5d_expand_activation False\n122 block5d_dwconv2 False\n123 block5d_bn False\n124 block5d_activation False\n125 block5d_se_squeeze False\n126 block5d_se_reshape False\n127 block5d_se_reduce False\n128 block5d_se_expand False\n129 block5d_se_excite False\n130 block5d_project_conv False\n131 block5d_project_bn False\n132 block5d_drop False\n133 block5d_add False\n134 block5e_expand_conv False\n135 block5e_expand_bn False\n136 block5e_expand_activation False\n137 block5e_dwconv2 False\n138 block5e_bn False\n139 block5e_activation False\n140 block5e_se_squeeze False\n141 block5e_se_reshape False\n142 block5e_se_reduce False\n143 block5e_se_expand False\n144 block5e_se_excite False\n145 block5e_project_conv False\n146 block5e_project_bn False\n147 block5e_drop False\n148 block5e_add False\n149 block6a_expand_conv False\n150 block6a_expand_bn False\n151 block6a_expand_activation False\n152 block6a_dwconv2 False\n153 block6a_bn False\n154 block6a_activation False\n155 block6a_se_squeeze False\n156 block6a_se_reshape False\n157 block6a_se_reduce False\n158 block6a_se_expand False\n159 block6a_se_excite False\n160 block6a_project_conv False\n161 block6a_project_bn False\n162 block6b_expand_conv False\n163 block6b_expand_bn False\n164 block6b_expand_activation False\n165 block6b_dwconv2 False\n166 block6b_bn False\n167 block6b_activation False\n168 block6b_se_squeeze False\n169 block6b_se_reshape False\n170 block6b_se_reduce False\n171 block6b_se_expand False\n172 block6b_se_excite False\n173 block6b_project_conv False\n174 block6b_project_bn False\n175 block6b_drop False\n176 block6b_add False\n177 block6c_expand_conv False\n178 block6c_expand_bn False\n179 block6c_expand_activation False\n180 block6c_dwconv2 False\n181 block6c_bn False\n182 block6c_activation False\n183 block6c_se_squeeze False\n184 block6c_se_reshape False\n185 block6c_se_reduce False\n186 block6c_se_expand False\n187 block6c_se_excite False\n188 block6c_project_conv False\n189 block6c_project_bn False\n190 block6c_drop False\n191 block6c_add False\n192 block6d_expand_conv False\n193 block6d_expand_bn False\n194 block6d_expand_activation False\n195 block6d_dwconv2 False\n196 block6d_bn False\n197 block6d_activation False\n198 block6d_se_squeeze False\n199 block6d_se_reshape False\n200 block6d_se_reduce False\n201 block6d_se_expand False\n202 block6d_se_excite False\n203 block6d_project_conv False\n204 block6d_project_bn False\n205 block6d_drop False\n206 block6d_add False\n207 block6e_expand_conv False\n208 block6e_expand_bn False\n209 block6e_expand_activation False\n210 block6e_dwconv2 False\n211 block6e_bn False\n212 block6e_activation False\n213 block6e_se_squeeze False\n214 block6e_se_reshape False\n215 block6e_se_reduce False\n216 block6e_se_expand False\n217 block6e_se_excite False\n218 block6e_project_conv False\n219 block6e_project_bn False\n220 block6e_drop False\n221 block6e_add False\n222 block6f_expand_conv False\n223 block6f_expand_bn False\n224 block6f_expand_activation False\n225 block6f_dwconv2 False\n226 block6f_bn False\n227 block6f_activation False\n228 block6f_se_squeeze False\n229 block6f_se_reshape False\n230 block6f_se_reduce False\n231 block6f_se_expand False\n232 block6f_se_excite False\n233 block6f_project_conv False\n234 block6f_project_bn False\n235 block6f_drop False\n236 block6f_add False\n237 block6g_expand_conv False\n238 block6g_expand_bn False\n239 block6g_expand_activation False\n240 block6g_dwconv2 False\n241 block6g_bn False\n242 block6g_activation False\n243 block6g_se_squeeze False\n244 block6g_se_reshape False\n245 block6g_se_reduce False\n246 block6g_se_expand False\n247 block6g_se_excite False\n248 block6g_project_conv False\n249 block6g_project_bn False\n250 block6g_drop False\n251 block6g_add False\n252 block6h_expand_conv False\n253 block6h_expand_bn False\n254 block6h_expand_activation False\n255 block6h_dwconv2 False\n256 block6h_bn False\n257 block6h_activation False\n258 block6h_se_squeeze False\n259 block6h_se_reshape False\n260 block6h_se_reduce False\n261 block6h_se_expand False\n262 block6h_se_excite False\n263 block6h_project_conv False\n264 block6h_project_bn False\n265 block6h_drop False\n266 block6h_add False\n267 top_conv False\n268 top_bn False\n269 top_activation False\n</pre> <p>Beautiful. This is exactly what we're after.</p> <p>Now to fine-tune the base model to our own data, we're going to unfreeze the top 10 layers and continue training our model for another 5 epochs.</p> <p>This means all of the base model's layers except for the last 10 will remain frozen and untrainable. And the weights in the remaining unfrozen layers will be updated during training.</p> <p>Ideally, we should see the model's performance improve.</p> <p>\ud83e\udd14 Question: How many layers should you unfreeze when training?</p> <p>There's no set rule for this. You could unfreeze every layer in the pretrained model or you could try unfreezing one layer at a time. Best to experiment with different amounts of unfreezing and fine-tuning to see what happens. Generally, the less data you have, the less layers you want to unfreeze and the more gradually you want to fine-tune.</p> <p>\ud83d\udcd6 Resource: The ULMFiT (Universal Language Model Fine-tuning for Text Classification) paper has a great series of experiments on fine-tuning models.</p> <p>To begin fine-tuning, we'll unfreeze the entire <code>model_2_base_model</code> by setting its <code>trainable</code> attribute to <code>True</code>.</p> <p>Then we'll refreeze every layer in <code>model_2_base_model</code> except for the last 10 by looping through them and setting their <code>trainable</code> attribute to <code>False</code>.</p> <p>Finally, we'll recompile the whole model.</p> In\u00a0[46]: Copied! <pre># Make all the layers in model_2_base_model trainable\nmodel_2_base_model.trainable = True\n\n# Freeze all layers except for the last 10\nfor layer in model_2_base_model.layers[:-10]:\n  layer.trainable = False\n\n# Recompile the whole model (always recompile after any adjustments to a model)\nmodel_2.compile(loss=\"categorical_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # lr is 10x lower than before for fine-tuning\n                metrics=[\"accuracy\"])\n</pre> # Make all the layers in model_2_base_model trainable model_2_base_model.trainable = True  # Freeze all layers except for the last 10 for layer in model_2_base_model.layers[:-10]:   layer.trainable = False  # Recompile the whole model (always recompile after any adjustments to a model) model_2.compile(loss=\"categorical_crossentropy\",                 optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # lr is 10x lower than before for fine-tuning                 metrics=[\"accuracy\"]) <p>Wonderful, now let's check which layers of the pretrained model are trainable.</p> In\u00a0[47]: Copied! <pre># Check which layers are tuneable (trainable)\nfor layer_number, layer in enumerate(model_2_base_model.layers):\n  print(layer_number, layer.name, layer.trainable)\n</pre> # Check which layers are tuneable (trainable) for layer_number, layer in enumerate(model_2_base_model.layers):   print(layer_number, layer.name, layer.trainable) <pre>0 input_4 False\n1 rescaling_3 False\n2 normalization_3 False\n3 stem_conv False\n4 stem_bn False\n5 stem_activation False\n6 block1a_project_conv False\n7 block1a_project_bn False\n8 block1a_project_activation False\n9 block2a_expand_conv False\n10 block2a_expand_bn False\n11 block2a_expand_activation False\n12 block2a_project_conv False\n13 block2a_project_bn False\n14 block2b_expand_conv False\n15 block2b_expand_bn False\n16 block2b_expand_activation False\n17 block2b_project_conv False\n18 block2b_project_bn False\n19 block2b_drop False\n20 block2b_add False\n21 block3a_expand_conv False\n22 block3a_expand_bn False\n23 block3a_expand_activation False\n24 block3a_project_conv False\n25 block3a_project_bn False\n26 block3b_expand_conv False\n27 block3b_expand_bn False\n28 block3b_expand_activation False\n29 block3b_project_conv False\n30 block3b_project_bn False\n31 block3b_drop False\n32 block3b_add False\n33 block4a_expand_conv False\n34 block4a_expand_bn False\n35 block4a_expand_activation False\n36 block4a_dwconv2 False\n37 block4a_bn False\n38 block4a_activation False\n39 block4a_se_squeeze False\n40 block4a_se_reshape False\n41 block4a_se_reduce False\n42 block4a_se_expand False\n43 block4a_se_excite False\n44 block4a_project_conv False\n45 block4a_project_bn False\n46 block4b_expand_conv False\n47 block4b_expand_bn False\n48 block4b_expand_activation False\n49 block4b_dwconv2 False\n50 block4b_bn False\n51 block4b_activation False\n52 block4b_se_squeeze False\n53 block4b_se_reshape False\n54 block4b_se_reduce False\n55 block4b_se_expand False\n56 block4b_se_excite False\n57 block4b_project_conv False\n58 block4b_project_bn False\n59 block4b_drop False\n60 block4b_add False\n61 block4c_expand_conv False\n62 block4c_expand_bn False\n63 block4c_expand_activation False\n64 block4c_dwconv2 False\n65 block4c_bn False\n66 block4c_activation False\n67 block4c_se_squeeze False\n68 block4c_se_reshape False\n69 block4c_se_reduce False\n70 block4c_se_expand False\n71 block4c_se_excite False\n72 block4c_project_conv False\n73 block4c_project_bn False\n74 block4c_drop False\n75 block4c_add False\n76 block5a_expand_conv False\n77 block5a_expand_bn False\n78 block5a_expand_activation False\n79 block5a_dwconv2 False\n80 block5a_bn False\n81 block5a_activation False\n82 block5a_se_squeeze False\n83 block5a_se_reshape False\n84 block5a_se_reduce False\n85 block5a_se_expand False\n86 block5a_se_excite False\n87 block5a_project_conv False\n88 block5a_project_bn False\n89 block5b_expand_conv False\n90 block5b_expand_bn False\n91 block5b_expand_activation False\n92 block5b_dwconv2 False\n93 block5b_bn False\n94 block5b_activation False\n95 block5b_se_squeeze False\n96 block5b_se_reshape False\n97 block5b_se_reduce False\n98 block5b_se_expand False\n99 block5b_se_excite False\n100 block5b_project_conv False\n101 block5b_project_bn False\n102 block5b_drop False\n103 block5b_add False\n104 block5c_expand_conv False\n105 block5c_expand_bn False\n106 block5c_expand_activation False\n107 block5c_dwconv2 False\n108 block5c_bn False\n109 block5c_activation False\n110 block5c_se_squeeze False\n111 block5c_se_reshape False\n112 block5c_se_reduce False\n113 block5c_se_expand False\n114 block5c_se_excite False\n115 block5c_project_conv False\n116 block5c_project_bn False\n117 block5c_drop False\n118 block5c_add False\n119 block5d_expand_conv False\n120 block5d_expand_bn False\n121 block5d_expand_activation False\n122 block5d_dwconv2 False\n123 block5d_bn False\n124 block5d_activation False\n125 block5d_se_squeeze False\n126 block5d_se_reshape False\n127 block5d_se_reduce False\n128 block5d_se_expand False\n129 block5d_se_excite False\n130 block5d_project_conv False\n131 block5d_project_bn False\n132 block5d_drop False\n133 block5d_add False\n134 block5e_expand_conv False\n135 block5e_expand_bn False\n136 block5e_expand_activation False\n137 block5e_dwconv2 False\n138 block5e_bn False\n139 block5e_activation False\n140 block5e_se_squeeze False\n141 block5e_se_reshape False\n142 block5e_se_reduce False\n143 block5e_se_expand False\n144 block5e_se_excite False\n145 block5e_project_conv False\n146 block5e_project_bn False\n147 block5e_drop False\n148 block5e_add False\n149 block6a_expand_conv False\n150 block6a_expand_bn False\n151 block6a_expand_activation False\n152 block6a_dwconv2 False\n153 block6a_bn False\n154 block6a_activation False\n155 block6a_se_squeeze False\n156 block6a_se_reshape False\n157 block6a_se_reduce False\n158 block6a_se_expand False\n159 block6a_se_excite False\n160 block6a_project_conv False\n161 block6a_project_bn False\n162 block6b_expand_conv False\n163 block6b_expand_bn False\n164 block6b_expand_activation False\n165 block6b_dwconv2 False\n166 block6b_bn False\n167 block6b_activation False\n168 block6b_se_squeeze False\n169 block6b_se_reshape False\n170 block6b_se_reduce False\n171 block6b_se_expand False\n172 block6b_se_excite False\n173 block6b_project_conv False\n174 block6b_project_bn False\n175 block6b_drop False\n176 block6b_add False\n177 block6c_expand_conv False\n178 block6c_expand_bn False\n179 block6c_expand_activation False\n180 block6c_dwconv2 False\n181 block6c_bn False\n182 block6c_activation False\n183 block6c_se_squeeze False\n184 block6c_se_reshape False\n185 block6c_se_reduce False\n186 block6c_se_expand False\n187 block6c_se_excite False\n188 block6c_project_conv False\n189 block6c_project_bn False\n190 block6c_drop False\n191 block6c_add False\n192 block6d_expand_conv False\n193 block6d_expand_bn False\n194 block6d_expand_activation False\n195 block6d_dwconv2 False\n196 block6d_bn False\n197 block6d_activation False\n198 block6d_se_squeeze False\n199 block6d_se_reshape False\n200 block6d_se_reduce False\n201 block6d_se_expand False\n202 block6d_se_excite False\n203 block6d_project_conv False\n204 block6d_project_bn False\n205 block6d_drop False\n206 block6d_add False\n207 block6e_expand_conv False\n208 block6e_expand_bn False\n209 block6e_expand_activation False\n210 block6e_dwconv2 False\n211 block6e_bn False\n212 block6e_activation False\n213 block6e_se_squeeze False\n214 block6e_se_reshape False\n215 block6e_se_reduce False\n216 block6e_se_expand False\n217 block6e_se_excite False\n218 block6e_project_conv False\n219 block6e_project_bn False\n220 block6e_drop False\n221 block6e_add False\n222 block6f_expand_conv False\n223 block6f_expand_bn False\n224 block6f_expand_activation False\n225 block6f_dwconv2 False\n226 block6f_bn False\n227 block6f_activation False\n228 block6f_se_squeeze False\n229 block6f_se_reshape False\n230 block6f_se_reduce False\n231 block6f_se_expand False\n232 block6f_se_excite False\n233 block6f_project_conv False\n234 block6f_project_bn False\n235 block6f_drop False\n236 block6f_add False\n237 block6g_expand_conv False\n238 block6g_expand_bn False\n239 block6g_expand_activation False\n240 block6g_dwconv2 False\n241 block6g_bn False\n242 block6g_activation False\n243 block6g_se_squeeze False\n244 block6g_se_reshape False\n245 block6g_se_reduce False\n246 block6g_se_expand False\n247 block6g_se_excite False\n248 block6g_project_conv False\n249 block6g_project_bn False\n250 block6g_drop False\n251 block6g_add False\n252 block6h_expand_conv False\n253 block6h_expand_bn False\n254 block6h_expand_activation False\n255 block6h_dwconv2 False\n256 block6h_bn False\n257 block6h_activation False\n258 block6h_se_squeeze False\n259 block6h_se_reshape False\n260 block6h_se_reduce True\n261 block6h_se_expand True\n262 block6h_se_excite True\n263 block6h_project_conv True\n264 block6h_project_bn True\n265 block6h_drop True\n266 block6h_add True\n267 top_conv True\n268 top_bn True\n269 top_activation True\n</pre> <p>Nice! It seems all layers except for the last 10 are frozen and untrainable. This means only the last 10 layers of the base model along with the output layer will have their weights updated during training.</p> <p>\ud83e\udd14 Question: Why did we recompile the model?</p> <p>Every time you make a change to your models, you need to recompile them.</p> <p>In our case, we're using the exact same loss, optimizer and metrics as before, except this time the learning rate for our optimizer will be 10x smaller than before (<code>0.0001</code> instead of Adam's default of <code>0.001</code>).</p> <p>We do this so the model doesn't try to overwrite the existing weights in the pretrained model too fast. In other words, we want learning to be more gradual.</p> <p>\ud83d\udd11 Note: There's no set standard for setting the learning rate during fine-tuning, though reductions of 2.6x-10x+ seem to work well in practice.</p> <p>How many trainable variables do we have now?</p> In\u00a0[48]: Copied! <pre>print(len(model_2.trainable_variables))\n</pre> print(len(model_2.trainable_variables)) <pre>12\n</pre> <p>Wonderful, it looks like our model has a total of 12 trainable variables, the last 10 layers of the base model and the weight and bias parameters of the Dense output layer.</p> <p>Time to fine-tune!</p> <p>We're going to continue training on from where our previous model finished. Since it trained for 5 epochs, our fine-tuning will begin on the epoch 5 and continue for another 5 epochs.</p> <p>To do this, we can use the <code>initial_epoch</code> parameter of the <code>fit()</code> method. We'll pass it the last epoch of the previous model's training history (<code>history_10_percent_data_aug.epoch[-1]</code>).</p> In\u00a0[49]: Copied! <pre># Fine tune for another 5 epochs\nfine_tune_epochs = initial_epochs + 5\n\n# Refit the model (same as model_2 except with more trainable layers)\nhistory_fine_10_percent_data_aug = model_2.fit(train_data_10_percent,\n                                               epochs=fine_tune_epochs,\n                                               validation_data=test_data,\n                                               initial_epoch=history_10_percent_data_aug.epoch[-1], # start from previous last epoch\n                                               validation_steps=int(0.25 * len(test_data)),\n                                               callbacks=[create_tensorboard_callback(\"transfer_learning\", \"10_percent_fine_tune_last_10\")]) # name experiment appropriately\n</pre> # Fine tune for another 5 epochs fine_tune_epochs = initial_epochs + 5  # Refit the model (same as model_2 except with more trainable layers) history_fine_10_percent_data_aug = model_2.fit(train_data_10_percent,                                                epochs=fine_tune_epochs,                                                validation_data=test_data,                                                initial_epoch=history_10_percent_data_aug.epoch[-1], # start from previous last epoch                                                validation_steps=int(0.25 * len(test_data)),                                                callbacks=[create_tensorboard_callback(\"transfer_learning\", \"10_percent_fine_tune_last_10\")]) # name experiment appropriately <pre>Saving TensorBoard log files to: transfer_learning/10_percent_fine_tune_last_10/20230818-014212\nEpoch 5/10\n24/24 [==============================] - 19s 333ms/step - loss: 0.7145 - accuracy: 0.8080 - val_loss: 0.5455 - val_accuracy: 0.8355\nEpoch 6/10\n24/24 [==============================] - 6s 228ms/step - loss: 0.6243 - accuracy: 0.8133 - val_loss: 0.5008 - val_accuracy: 0.8372\nEpoch 7/10\n24/24 [==============================] - 5s 199ms/step - loss: 0.5388 - accuracy: 0.8333 - val_loss: 0.4581 - val_accuracy: 0.8520\nEpoch 8/10\n24/24 [==============================] - 5s 215ms/step - loss: 0.5132 - accuracy: 0.8427 - val_loss: 0.4846 - val_accuracy: 0.8487\nEpoch 9/10\n24/24 [==============================] - 5s 191ms/step - loss: 0.4753 - accuracy: 0.8467 - val_loss: 0.4536 - val_accuracy: 0.8569\nEpoch 10/10\n24/24 [==============================] - 6s 225ms/step - loss: 0.4245 - accuracy: 0.8653 - val_loss: 0.4656 - val_accuracy: 0.8470\n</pre> <p>\ud83d\udd11 Note: Fine-tuning usually takes far longer per epoch than feature extraction (due to updating more weights throughout a network).</p> <p>Ho ho, looks like our model has gained a few percentage points of accuracy! Let's evalaute it.</p> In\u00a0[50]: Copied! <pre># Evaluate the model on the test data\nresults_fine_tune_10_percent = model_2.evaluate(test_data)\n</pre> # Evaluate the model on the test data results_fine_tune_10_percent = model_2.evaluate(test_data) <pre>79/79 [==============================] - 3s 31ms/step - loss: 0.4526 - accuracy: 0.8504\n</pre> <p>Remember, the results from evaluating the model might be slightly different to the outputs from training since during training we only evaluate on 25% of the test data.</p> <p>Alright, we need a way to evaluate our model's performance before and after fine-tuning. How about we write a function to compare the before and after?</p> In\u00a0[51]: Copied! <pre>def compare_historys(original_history, new_history, initial_epochs=5):\n    \"\"\"\n    Compares two model history objects.\n    \"\"\"\n    # Get original history measurements\n    acc = original_history.history[\"accuracy\"]\n    loss = original_history.history[\"loss\"]\n\n    print(len(acc))\n\n    val_acc = original_history.history[\"val_accuracy\"]\n    val_loss = original_history.history[\"val_loss\"]\n\n    # Combine original history with new history\n    total_acc = acc + new_history.history[\"accuracy\"]\n    total_loss = loss + new_history.history[\"loss\"]\n\n    total_val_acc = val_acc + new_history.history[\"val_accuracy\"]\n    total_val_loss = val_loss + new_history.history[\"val_loss\"]\n\n    print(len(total_acc))\n    print(total_acc)\n\n    # Make plots\n    plt.figure(figsize=(8, 8))\n    plt.subplot(2, 1, 1)\n    plt.plot(total_acc, label='Training Accuracy')\n    plt.plot(total_val_acc, label='Validation Accuracy')\n    plt.plot([initial_epochs-1, initial_epochs-1],\n              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs\n    plt.legend(loc='lower right')\n    plt.title('Training and Validation Accuracy')\n\n    plt.subplot(2, 1, 2)\n    plt.plot(total_loss, label='Training Loss')\n    plt.plot(total_val_loss, label='Validation Loss')\n    plt.plot([initial_epochs-1, initial_epochs-1],\n              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs\n    plt.legend(loc='upper right')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('epoch')\n    plt.show()\n</pre> def compare_historys(original_history, new_history, initial_epochs=5):     \"\"\"     Compares two model history objects.     \"\"\"     # Get original history measurements     acc = original_history.history[\"accuracy\"]     loss = original_history.history[\"loss\"]      print(len(acc))      val_acc = original_history.history[\"val_accuracy\"]     val_loss = original_history.history[\"val_loss\"]      # Combine original history with new history     total_acc = acc + new_history.history[\"accuracy\"]     total_loss = loss + new_history.history[\"loss\"]      total_val_acc = val_acc + new_history.history[\"val_accuracy\"]     total_val_loss = val_loss + new_history.history[\"val_loss\"]      print(len(total_acc))     print(total_acc)      # Make plots     plt.figure(figsize=(8, 8))     plt.subplot(2, 1, 1)     plt.plot(total_acc, label='Training Accuracy')     plt.plot(total_val_acc, label='Validation Accuracy')     plt.plot([initial_epochs-1, initial_epochs-1],               plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs     plt.legend(loc='lower right')     plt.title('Training and Validation Accuracy')      plt.subplot(2, 1, 2)     plt.plot(total_loss, label='Training Loss')     plt.plot(total_val_loss, label='Validation Loss')     plt.plot([initial_epochs-1, initial_epochs-1],               plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs     plt.legend(loc='upper right')     plt.title('Training and Validation Loss')     plt.xlabel('epoch')     plt.show() <p>This is where saving the history variables of our model training comes in handy. Let's see what happened after fine-tuning the last 10 layers of our model.</p> In\u00a0[52]: Copied! <pre>compare_historys(original_history=history_10_percent_data_aug,\n                 new_history=history_fine_10_percent_data_aug,\n                 initial_epochs=5)\n</pre> compare_historys(original_history=history_10_percent_data_aug,                  new_history=history_fine_10_percent_data_aug,                  initial_epochs=5) <pre>5\n11\n[0.335999995470047, 0.6413333415985107, 0.7253333330154419, 0.7706666588783264, 0.7839999794960022, 0.8080000281333923, 0.8133333325386047, 0.8333333134651184, 0.8426666855812073, 0.846666693687439, 0.8653333187103271]\n</pre> <p>Alright, alright, seems like the curves are heading in the right direction after fine-tuning. But remember, it should be noted that fine-tuning usually works best with larger amounts of data.</p> In\u00a0[53]: Copied! <pre># Download and unzip 10 classes of data with all images\n!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip\nunzip_data(\"10_food_classes_all_data.zip\")\n\n# Setup data directories\ntrain_dir = \"10_food_classes_all_data/train/\"\ntest_dir = \"10_food_classes_all_data/test/\"\n</pre> # Download and unzip 10 classes of data with all images !wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip unzip_data(\"10_food_classes_all_data.zip\")  # Setup data directories train_dir = \"10_food_classes_all_data/train/\" test_dir = \"10_food_classes_all_data/test/\" <pre>--2023-08-18 01:43:01--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip\nResolving storage.googleapis.com (storage.googleapis.com)... 173.194.202.128, 173.194.203.128, 74.125.199.128, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|173.194.202.128|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 519183241 (495M) [application/zip]\nSaving to: \u201810_food_classes_all_data.zip\u2019\n\n10_food_classes_all 100%[===================&gt;] 495.13M   148MB/s    in 3.5s    \n\n2023-08-18 01:43:04 (143 MB/s) - \u201810_food_classes_all_data.zip\u2019 saved [519183241/519183241]\n\n</pre> In\u00a0[54]: Copied! <pre># How many images are we working with now?\nwalk_through_dir(\"10_food_classes_all_data\")\n</pre> # How many images are we working with now? walk_through_dir(\"10_food_classes_all_data\") <pre>There are 2 directories and 0 images in '10_food_classes_all_data'.\nThere are 10 directories and 0 images in '10_food_classes_all_data/train'.\nThere are 0 directories and 750 images in '10_food_classes_all_data/train/ramen'.\nThere are 0 directories and 750 images in '10_food_classes_all_data/train/chicken_curry'.\nThere are 0 directories and 750 images in '10_food_classes_all_data/train/pizza'.\nThere are 0 directories and 750 images in '10_food_classes_all_data/train/ice_cream'.\nThere are 0 directories and 750 images in '10_food_classes_all_data/train/grilled_salmon'.\nThere are 0 directories and 750 images in '10_food_classes_all_data/train/steak'.\nThere are 0 directories and 750 images in '10_food_classes_all_data/train/chicken_wings'.\nThere are 0 directories and 750 images in '10_food_classes_all_data/train/hamburger'.\nThere are 0 directories and 750 images in '10_food_classes_all_data/train/sushi'.\nThere are 0 directories and 750 images in '10_food_classes_all_data/train/fried_rice'.\nThere are 10 directories and 0 images in '10_food_classes_all_data/test'.\nThere are 0 directories and 250 images in '10_food_classes_all_data/test/ramen'.\nThere are 0 directories and 250 images in '10_food_classes_all_data/test/chicken_curry'.\nThere are 0 directories and 250 images in '10_food_classes_all_data/test/pizza'.\nThere are 0 directories and 250 images in '10_food_classes_all_data/test/ice_cream'.\nThere are 0 directories and 250 images in '10_food_classes_all_data/test/grilled_salmon'.\nThere are 0 directories and 250 images in '10_food_classes_all_data/test/steak'.\nThere are 0 directories and 250 images in '10_food_classes_all_data/test/chicken_wings'.\nThere are 0 directories and 250 images in '10_food_classes_all_data/test/hamburger'.\nThere are 0 directories and 250 images in '10_food_classes_all_data/test/sushi'.\nThere are 0 directories and 250 images in '10_food_classes_all_data/test/fried_rice'.\n</pre> <p>And now we'll turn the images into tensors datasets.</p> In\u00a0[55]: Copied! <pre># Setup data inputs\nimport tensorflow as tf\nIMG_SIZE = (224, 224)\ntrain_data_10_classes_full = tf.keras.preprocessing.image_dataset_from_directory(train_dir,\n                                                                                 label_mode=\"categorical\",\n                                                                                 image_size=IMG_SIZE)\n\n# Note: this is the same test dataset we've been using for the previous modelling experiments\ntest_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n                                                                label_mode=\"categorical\",\n                                                                image_size=IMG_SIZE)\n</pre> # Setup data inputs import tensorflow as tf IMG_SIZE = (224, 224) train_data_10_classes_full = tf.keras.preprocessing.image_dataset_from_directory(train_dir,                                                                                  label_mode=\"categorical\",                                                                                  image_size=IMG_SIZE)  # Note: this is the same test dataset we've been using for the previous modelling experiments test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,                                                                 label_mode=\"categorical\",                                                                 image_size=IMG_SIZE) <pre>Found 7500 files belonging to 10 classes.\nFound 2500 files belonging to 10 classes.\n</pre> <p>Oh this is looking good. We've got 10x more images in of the training classes to work with.</p> <p>The test dataset is the same we've been using for our previous experiments.</p> <p>As it is now, our <code>model_2</code> has been fine-tuned on 10 percent of the data, so to begin fine-tuning on all of the data and keep our experiments consistent, we need to revert it back to the weights we checkpointed after 5 epochs of feature-extraction.</p> <p>To demonstrate this, we'll first evaluate the current <code>model_2</code>.</p> In\u00a0[56]: Copied! <pre># Evaluate model (this is the fine-tuned 10 percent of data version)\nmodel_2.evaluate(test_data)\n</pre> # Evaluate model (this is the fine-tuned 10 percent of data version) model_2.evaluate(test_data) <pre>79/79 [==============================] - 3s 39ms/step - loss: 0.4526 - accuracy: 0.8504\n</pre> Out[56]: <pre>[0.452595591545105, 0.8503999710083008]</pre> <p>These are the same values as <code>results_fine_tune_10_percent</code>.</p> In\u00a0[57]: Copied! <pre>results_fine_tune_10_percent\n</pre> results_fine_tune_10_percent Out[57]: <pre>[0.4525955617427826, 0.8503999710083008]</pre> <p>To keep our experiments clean, we'll load a create a new instance of <code>model_2</code> using our <code>create_base_model()</code> function.</p> <p>More specifically, we're trying to measure:</p> <ul> <li>Experiment 3 (previous one) - <code>model_2</code> with 10 layers fine-tuned for 5 more epochs on 10% of the data.</li> <li>Experiment 4 (this one) - <code>model_2</code> with layers fined-tuned for 5 more epochs on 100% on the data.</li> </ul> <p>Importantly, both experiments should use the same test data (to keep evaluation the same).</p> <p>And they should also start from the same checkpoint (<code>model_2</code> feature extractor trained for 5 epochs on 10% of the data).</p> <p>Let's first create new instance of <code>model_2</code>.</p> In\u00a0[58]: Copied! <pre># Create a new instance of model_2 for Experiment 4\nmodel_2 = create_base_model(learning_rate=0.0001) # 10x lower learning rate for fine-tuning\n</pre> # Create a new instance of model_2 for Experiment 4 model_2 = create_base_model(learning_rate=0.0001) # 10x lower learning rate for fine-tuning <p>And now to make sure it starts at the same checkpoint, we can load the checkpointed weights from <code>checkpoint_path</code>.</p> In\u00a0[59]: Copied! <pre># Load previously checkpointed weights\nmodel_2.load_weights(checkpoint_path)\n</pre> # Load previously checkpointed weights model_2.load_weights(checkpoint_path) Out[59]: <pre>&lt;tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7c626c0f0e20&gt;</pre> <p>Let's now get a summary and check how many trainable variables there are.</p> In\u00a0[60]: Copied! <pre>model_2.summary()\n</pre> model_2.summary() <pre>Model: \"model_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n                                                                 \n data_augmentation (Sequenti  (None, None, None, 3)    0         \n al)                                                             \n                                                                 \n efficientnetv2-b0 (Function  (None, None, None, 1280)  5919312  \n al)                                                             \n                                                                 \n global_average_pooling_laye  (None, 1280)             0         \n r (GlobalAveragePooling2D)                                      \n                                                                 \n output_layer (Dense)        (None, 10)                12810     \n                                                                 \n=================================================================\nTotal params: 5,932,122\nTrainable params: 12,810\nNon-trainable params: 5,919,312\n_________________________________________________________________\n</pre> In\u00a0[61]: Copied! <pre>print(len(model_2.trainable_variables))\n</pre> print(len(model_2.trainable_variables)) <pre>2\n</pre> <p>Nice! This is the same as our original checkpoint.</p> <p>And the results should be the same as <code>results_10_percent_data_aug</code>.</p> In\u00a0[62]: Copied! <pre># After loading the weights, this should have gone down (no fine-tuning)\nmodel_2.evaluate(test_data)\n</pre> # After loading the weights, this should have gone down (no fine-tuning) model_2.evaluate(test_data) <pre>79/79 [==============================] - 6s 33ms/step - loss: 0.6795 - accuracy: 0.8216\n</pre> Out[62]: <pre>[0.6794609427452087, 0.8216000199317932]</pre> <p>Alright, the previous steps might seem quite confusing but all we've done is:</p> <ol> <li>Trained a feature extraction transfer learning model for 5 epochs on 10% of the data (with all base model layers frozen) and saved the model's weights using <code>ModelCheckpoint</code> (Model 2).</li> <li>Fine-tuned the same model on the same 10% of the data for a further 5 epochs with the top 10 layers of the base model unfrozen (Model 3).</li> <li>Saved the results and training logs each time.</li> <li>Reloaded the model from 1 to do the same steps as 2 but with all (100%) of the data (Model 4).</li> </ol> <p>The same steps as 2?</p> <p>Yeah, we're going to fine-tune the last 10 layers of the base model with the full dataset for another 5 epochs but first let's remind ourselves which layers are trainable.</p> In\u00a0[63]: Copied! <pre># Check which layers are tuneable in the whole model\nfor layer_number, layer in enumerate(model_2.layers):\n  print(layer_number, layer.name, layer.trainable)\n</pre> # Check which layers are tuneable in the whole model for layer_number, layer in enumerate(model_2.layers):   print(layer_number, layer.name, layer.trainable) <pre>0 input_layer True\n1 data_augmentation True\n2 efficientnetv2-b0 False\n3 global_average_pooling_layer True\n4 output_layer True\n</pre> <p>Remember, the <code>base_model</code> of <code>model_2</code> (<code>efficientnetv2-b0</code>) can be referenced by <code>model_2.layers[2]</code>.</p> <p>So let's unfreeze the last 10 layers of the <code>base_model</code> to make them trainable (for fine-tuning).</p> In\u00a0[64]: Copied! <pre># Unfreeze the top 10 layers in model_2's base_model\nmodel_2_base_model = model_2.layers[2]\nmodel_2_base_model.trainable = True\n\n# Freeze all layers except for the last 10\nfor layer in model_2_base_model.layers[:-10]:\n  layer.trainable = False\n</pre> # Unfreeze the top 10 layers in model_2's base_model model_2_base_model = model_2.layers[2] model_2_base_model.trainable = True  # Freeze all layers except for the last 10 for layer in model_2_base_model.layers[:-10]:   layer.trainable = False <p>Now let's make sure the right layers are trainable (we only want the last 10 to be trainable).</p> <p>Note: You could experiment which number of layers should be trainable here. Generally, the more data you have, the more layers that can be fine-tuned.</p> In\u00a0[65]: Copied! <pre># Check which layers are tuneable in the base model\nfor layer_number, layer in enumerate(model_2_base_model.layers):\n  print(layer_number, layer.name, layer.trainable)\n</pre> # Check which layers are tuneable in the base model for layer_number, layer in enumerate(model_2_base_model.layers):   print(layer_number, layer.name, layer.trainable) <pre>0 input_5 False\n1 rescaling_4 False\n2 normalization_4 False\n3 stem_conv False\n4 stem_bn False\n5 stem_activation False\n6 block1a_project_conv False\n7 block1a_project_bn False\n8 block1a_project_activation False\n9 block2a_expand_conv False\n10 block2a_expand_bn False\n11 block2a_expand_activation False\n12 block2a_project_conv False\n13 block2a_project_bn False\n14 block2b_expand_conv False\n15 block2b_expand_bn False\n16 block2b_expand_activation False\n17 block2b_project_conv False\n18 block2b_project_bn False\n19 block2b_drop False\n20 block2b_add False\n21 block3a_expand_conv False\n22 block3a_expand_bn False\n23 block3a_expand_activation False\n24 block3a_project_conv False\n25 block3a_project_bn False\n26 block3b_expand_conv False\n27 block3b_expand_bn False\n28 block3b_expand_activation False\n29 block3b_project_conv False\n30 block3b_project_bn False\n31 block3b_drop False\n32 block3b_add False\n33 block4a_expand_conv False\n34 block4a_expand_bn False\n35 block4a_expand_activation False\n36 block4a_dwconv2 False\n37 block4a_bn False\n38 block4a_activation False\n39 block4a_se_squeeze False\n40 block4a_se_reshape False\n41 block4a_se_reduce False\n42 block4a_se_expand False\n43 block4a_se_excite False\n44 block4a_project_conv False\n45 block4a_project_bn False\n46 block4b_expand_conv False\n47 block4b_expand_bn False\n48 block4b_expand_activation False\n49 block4b_dwconv2 False\n50 block4b_bn False\n51 block4b_activation False\n52 block4b_se_squeeze False\n53 block4b_se_reshape False\n54 block4b_se_reduce False\n55 block4b_se_expand False\n56 block4b_se_excite False\n57 block4b_project_conv False\n58 block4b_project_bn False\n59 block4b_drop False\n60 block4b_add False\n61 block4c_expand_conv False\n62 block4c_expand_bn False\n63 block4c_expand_activation False\n64 block4c_dwconv2 False\n65 block4c_bn False\n66 block4c_activation False\n67 block4c_se_squeeze False\n68 block4c_se_reshape False\n69 block4c_se_reduce False\n70 block4c_se_expand False\n71 block4c_se_excite False\n72 block4c_project_conv False\n73 block4c_project_bn False\n74 block4c_drop False\n75 block4c_add False\n76 block5a_expand_conv False\n77 block5a_expand_bn False\n78 block5a_expand_activation False\n79 block5a_dwconv2 False\n80 block5a_bn False\n81 block5a_activation False\n82 block5a_se_squeeze False\n83 block5a_se_reshape False\n84 block5a_se_reduce False\n85 block5a_se_expand False\n86 block5a_se_excite False\n87 block5a_project_conv False\n88 block5a_project_bn False\n89 block5b_expand_conv False\n90 block5b_expand_bn False\n91 block5b_expand_activation False\n92 block5b_dwconv2 False\n93 block5b_bn False\n94 block5b_activation False\n95 block5b_se_squeeze False\n96 block5b_se_reshape False\n97 block5b_se_reduce False\n98 block5b_se_expand False\n99 block5b_se_excite False\n100 block5b_project_conv False\n101 block5b_project_bn False\n102 block5b_drop False\n103 block5b_add False\n104 block5c_expand_conv False\n105 block5c_expand_bn False\n106 block5c_expand_activation False\n107 block5c_dwconv2 False\n108 block5c_bn False\n109 block5c_activation False\n110 block5c_se_squeeze False\n111 block5c_se_reshape False\n112 block5c_se_reduce False\n113 block5c_se_expand False\n114 block5c_se_excite False\n115 block5c_project_conv False\n116 block5c_project_bn False\n117 block5c_drop False\n118 block5c_add False\n119 block5d_expand_conv False\n120 block5d_expand_bn False\n121 block5d_expand_activation False\n122 block5d_dwconv2 False\n123 block5d_bn False\n124 block5d_activation False\n125 block5d_se_squeeze False\n126 block5d_se_reshape False\n127 block5d_se_reduce False\n128 block5d_se_expand False\n129 block5d_se_excite False\n130 block5d_project_conv False\n131 block5d_project_bn False\n132 block5d_drop False\n133 block5d_add False\n134 block5e_expand_conv False\n135 block5e_expand_bn False\n136 block5e_expand_activation False\n137 block5e_dwconv2 False\n138 block5e_bn False\n139 block5e_activation False\n140 block5e_se_squeeze False\n141 block5e_se_reshape False\n142 block5e_se_reduce False\n143 block5e_se_expand False\n144 block5e_se_excite False\n145 block5e_project_conv False\n146 block5e_project_bn False\n147 block5e_drop False\n148 block5e_add False\n149 block6a_expand_conv False\n150 block6a_expand_bn False\n151 block6a_expand_activation False\n152 block6a_dwconv2 False\n153 block6a_bn False\n154 block6a_activation False\n155 block6a_se_squeeze False\n156 block6a_se_reshape False\n157 block6a_se_reduce False\n158 block6a_se_expand False\n159 block6a_se_excite False\n160 block6a_project_conv False\n161 block6a_project_bn False\n162 block6b_expand_conv False\n163 block6b_expand_bn False\n164 block6b_expand_activation False\n165 block6b_dwconv2 False\n166 block6b_bn False\n167 block6b_activation False\n168 block6b_se_squeeze False\n169 block6b_se_reshape False\n170 block6b_se_reduce False\n171 block6b_se_expand False\n172 block6b_se_excite False\n173 block6b_project_conv False\n174 block6b_project_bn False\n175 block6b_drop False\n176 block6b_add False\n177 block6c_expand_conv False\n178 block6c_expand_bn False\n179 block6c_expand_activation False\n180 block6c_dwconv2 False\n181 block6c_bn False\n182 block6c_activation False\n183 block6c_se_squeeze False\n184 block6c_se_reshape False\n185 block6c_se_reduce False\n186 block6c_se_expand False\n187 block6c_se_excite False\n188 block6c_project_conv False\n189 block6c_project_bn False\n190 block6c_drop False\n191 block6c_add False\n192 block6d_expand_conv False\n193 block6d_expand_bn False\n194 block6d_expand_activation False\n195 block6d_dwconv2 False\n196 block6d_bn False\n197 block6d_activation False\n198 block6d_se_squeeze False\n199 block6d_se_reshape False\n200 block6d_se_reduce False\n201 block6d_se_expand False\n202 block6d_se_excite False\n203 block6d_project_conv False\n204 block6d_project_bn False\n205 block6d_drop False\n206 block6d_add False\n207 block6e_expand_conv False\n208 block6e_expand_bn False\n209 block6e_expand_activation False\n210 block6e_dwconv2 False\n211 block6e_bn False\n212 block6e_activation False\n213 block6e_se_squeeze False\n214 block6e_se_reshape False\n215 block6e_se_reduce False\n216 block6e_se_expand False\n217 block6e_se_excite False\n218 block6e_project_conv False\n219 block6e_project_bn False\n220 block6e_drop False\n221 block6e_add False\n222 block6f_expand_conv False\n223 block6f_expand_bn False\n224 block6f_expand_activation False\n225 block6f_dwconv2 False\n226 block6f_bn False\n227 block6f_activation False\n228 block6f_se_squeeze False\n229 block6f_se_reshape False\n230 block6f_se_reduce False\n231 block6f_se_expand False\n232 block6f_se_excite False\n233 block6f_project_conv False\n234 block6f_project_bn False\n235 block6f_drop False\n236 block6f_add False\n237 block6g_expand_conv False\n238 block6g_expand_bn False\n239 block6g_expand_activation False\n240 block6g_dwconv2 False\n241 block6g_bn False\n242 block6g_activation False\n243 block6g_se_squeeze False\n244 block6g_se_reshape False\n245 block6g_se_reduce False\n246 block6g_se_expand False\n247 block6g_se_excite False\n248 block6g_project_conv False\n249 block6g_project_bn False\n250 block6g_drop False\n251 block6g_add False\n252 block6h_expand_conv False\n253 block6h_expand_bn False\n254 block6h_expand_activation False\n255 block6h_dwconv2 False\n256 block6h_bn False\n257 block6h_activation False\n258 block6h_se_squeeze False\n259 block6h_se_reshape False\n260 block6h_se_reduce True\n261 block6h_se_expand True\n262 block6h_se_excite True\n263 block6h_project_conv True\n264 block6h_project_bn True\n265 block6h_drop True\n266 block6h_add True\n267 top_conv True\n268 top_bn True\n269 top_activation True\n</pre> <p>Looking good! The last 10 layers are trainable (unfrozen).</p> <p>We've got one more step to do before we can begin fine-tuning.</p> <p>Do you remember what it is?</p> <p>I'll give you a hint. We just reloaded the weights to our model and what do we need to do every time we make a change to our models?</p> <p>Recompile them!</p> <p>This will be just as before.</p> In\u00a0[66]: Copied! <pre># Recompile the model (always recompile after any adjustments to a model)\nmodel_2.compile(loss=\"categorical_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # lr is 10x lower than before for fine-tuning\n                metrics=[\"accuracy\"])\n</pre> # Recompile the model (always recompile after any adjustments to a model) model_2.compile(loss=\"categorical_crossentropy\",                 optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # lr is 10x lower than before for fine-tuning                 metrics=[\"accuracy\"]) <p>Alright, time to fine-tune on all of the data!</p> In\u00a0[67]: Copied! <pre># Continue to train and fine-tune the model to our data\nfine_tune_epochs = initial_epochs + 5\n\nhistory_fine_10_classes_full = model_2.fit(train_data_10_classes_full,\n                                           epochs=fine_tune_epochs,\n                                           initial_epoch=history_10_percent_data_aug.epoch[-1],\n                                           validation_data=test_data,\n                                           validation_steps=int(0.25 * len(test_data)),\n                                           callbacks=[create_tensorboard_callback(\"transfer_learning\", \"full_10_classes_fine_tune_last_10\")])\n</pre> # Continue to train and fine-tune the model to our data fine_tune_epochs = initial_epochs + 5  history_fine_10_classes_full = model_2.fit(train_data_10_classes_full,                                            epochs=fine_tune_epochs,                                            initial_epoch=history_10_percent_data_aug.epoch[-1],                                            validation_data=test_data,                                            validation_steps=int(0.25 * len(test_data)),                                            callbacks=[create_tensorboard_callback(\"transfer_learning\", \"full_10_classes_fine_tune_last_10\")]) <pre>Saving TensorBoard log files to: transfer_learning/full_10_classes_fine_tune_last_10/20230818-014323\nEpoch 5/10\n235/235 [==============================] - 43s 144ms/step - loss: 0.7247 - accuracy: 0.7724 - val_loss: 0.3794 - val_accuracy: 0.8832\nEpoch 6/10\n235/235 [==============================] - 29s 122ms/step - loss: 0.5906 - accuracy: 0.8093 - val_loss: 0.3624 - val_accuracy: 0.8783\nEpoch 7/10\n235/235 [==============================] - 26s 108ms/step - loss: 0.5465 - accuracy: 0.8220 - val_loss: 0.3211 - val_accuracy: 0.9046\nEpoch 8/10\n235/235 [==============================] - 24s 102ms/step - loss: 0.5178 - accuracy: 0.8332 - val_loss: 0.3015 - val_accuracy: 0.9095\nEpoch 9/10\n235/235 [==============================] - 21s 89ms/step - loss: 0.4782 - accuracy: 0.8425 - val_loss: 0.2541 - val_accuracy: 0.9227\nEpoch 10/10\n235/235 [==============================] - 19s 81ms/step - loss: 0.4562 - accuracy: 0.8501 - val_loss: 0.2632 - val_accuracy: 0.9227\n</pre> <p>\ud83d\udd11 Note: Training took longer per epoch, but that makes sense because we're using 10x more training data than before.</p> <p>Let's evaluate on all of the test data.</p> In\u00a0[68]: Copied! <pre>results_fine_tune_full_data = model_2.evaluate(test_data)\nresults_fine_tune_full_data\n</pre> results_fine_tune_full_data = model_2.evaluate(test_data) results_fine_tune_full_data <pre>79/79 [==============================] - 3s 35ms/step - loss: 0.2658 - accuracy: 0.9156\n</pre> Out[68]: <pre>[0.2658187747001648, 0.9156000018119812]</pre> In\u00a0[69]: Copied! <pre>results_fine_tune_10_percent\n</pre> results_fine_tune_10_percent Out[69]: <pre>[0.4525955617427826, 0.8503999710083008]</pre> <p>Nice! It looks like fine-tuning with all of the data has given our model a boost, how do the training curves look?</p> In\u00a0[70]: Copied! <pre># How did fine-tuning go with more data?\ncompare_historys(original_history=history_10_percent_data_aug,\n                 new_history=history_fine_10_classes_full,\n                 initial_epochs=5)\n</pre> # How did fine-tuning go with more data? compare_historys(original_history=history_10_percent_data_aug,                  new_history=history_fine_10_classes_full,                  initial_epochs=5) <pre>5\n11\n[0.335999995470047, 0.6413333415985107, 0.7253333330154419, 0.7706666588783264, 0.7839999794960022, 0.7724000215530396, 0.809333324432373, 0.8220000267028809, 0.8331999778747559, 0.8425333499908447, 0.8501333594322205]\n</pre> <p>Looks like that extra data helped! Those curves are looking great. And if we trained for longer, they might even keep improving.</p> In\u00a0[71]: Copied! <pre># View tensorboard logs of transfer learning modelling experiments (should be 4 models)\n# Upload TensorBoard dev records\n# !tensorboard dev upload --logdir ./transfer_learning \\\n#   --name \"Transfer learning experiments\" \\\n#   --description \"A series of different transfer learning experiments with varying amounts of data and fine-tuning\" \\\n#   --one_shot # exits the uploader when upload has finished\n</pre> # View tensorboard logs of transfer learning modelling experiments (should be 4 models) # Upload TensorBoard dev records # !tensorboard dev upload --logdir ./transfer_learning \\ #   --name \"Transfer learning experiments\" \\ #   --description \"A series of different transfer learning experiments with varying amounts of data and fine-tuning\" \\ #   --one_shot # exits the uploader when upload has finished <p>Once we've uploaded the results to TensorBoard.dev we get a shareable link we can use to view and compare our experiments and share our results with others if needed.</p> <p>You can view the original versions of the experiments we ran in this notebook here: https://tensorboard.dev/experiment/2O76kw3PQbKl0lByfg5B4w/</p> <p>\ud83e\udd14 Question: Which model performed the best? Why do you think this is? How did fine-tuning go?</p> <p>To find all of your previous TensorBoard.dev experiments using the command <code>tensorboard dev list</code>.</p> In\u00a0[72]: Copied! <pre># View previous experiments\n# !tensorboard dev list\n</pre> # View previous experiments # !tensorboard dev list <p>And if you want to remove a previous experiment (and delete it from public viewing) you can use the command:</p> <pre><code>tensorboard dev delete --experiment_id [INSERT_EXPERIMENT_ID_TO_DELETE]```\n</code></pre> In\u00a0[73]: Copied! <pre># Remove previous experiments\n# !tensorboard dev delete --experiment_id OUbW0O3pRqqQgAphVBxi8Q\n</pre> # Remove previous experiments # !tensorboard dev delete --experiment_id OUbW0O3pRqqQgAphVBxi8Q"},{"location":"05_transfer_learning_in_tensorflow_part_2_fine_tuning/#05-transfer-learning-with-tensorflow-part-2-fine-tuning","title":"05. Transfer Learning with TensorFlow Part 2: Fine-tuning\u00b6","text":"<p>In the previous section, we saw how we could leverage feature extraction transfer learning to get far better results on our Food Vision project than building our own models (even with less data).</p> <p>Now we're going to cover another type of transfer learning: fine-tuning.</p> <p>In fine-tuning transfer learning the pre-trained model weights from another model are unfrozen and tweaked during to better suit your own data.</p> <p>For feature extraction transfer learning, you may only train the top 1-3 layers of a pre-trained model with your own data, in fine-tuning transfer learning, you might train 1-3+ layers of a pre-trained model (where the '+' indicates that many or all of the layers could be trained).</p> <p> Feature extraction transfer learning vs. fine-tuning transfer learning. The main difference between the two is that in fine-tuning, more layers of the pre-trained model get unfrozen and tuned on custom data. This fine-tuning usually takes more data than feature extraction to be effective.</p>"},{"location":"05_transfer_learning_in_tensorflow_part_2_fine_tuning/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>We're going to go through the follow with TensorFlow:</p> <ul> <li>Introduce fine-tuning, a type of transfer learning to modify a pre-trained model to be more suited to your data</li> <li>Using the Keras Functional API (a differnt way to build models in Keras)</li> <li>Using a smaller dataset to experiment faster (e.g. 1-10% of training samples of 10 classes of food)</li> <li>Data augmentation (how to make your training dataset more diverse without adding more data)</li> <li>Running a series of modelling experiments on our Food Vision data<ul> <li>Model 0: a transfer learning model using the Keras Functional API</li> <li>Model 1: a feature extraction transfer learning model on 1% of the data with data augmentation</li> <li>Model 2: a feature extraction transfer learning model on 10% of the data with data augmentation</li> <li>Model 3: a fine-tuned transfer learning model on 10% of the data</li> <li>Model 4: a fine-tuned transfer learning model on 100% of the data</li> </ul> </li> <li>Introduce the ModelCheckpoint callback to save intermediate training results</li> <li>Compare model experiments results using TensorBoard</li> </ul>"},{"location":"05_transfer_learning_in_tensorflow_part_2_fine_tuning/#how-you-can-use-this-notebook","title":"How you can use this notebook\u00b6","text":"<p>You can read through the descriptions and the code (it should all run, except for the cells which error on purpose), but there's a better option.</p> <p>Write all of the code yourself.</p> <p>Yes. I'm serious. Create a new notebook, and rewrite each line by yourself. Investigate it, see if you can break it, why does it break?</p> <p>You don't have to write the text descriptions but writing the code yourself is a great way to get hands-on experience.</p> <p>Don't worry if you make mistakes, we all do. The way to get better and make less mistakes is to write more code.</p>"},{"location":"05_transfer_learning_in_tensorflow_part_2_fine_tuning/#creating-helper-functions","title":"Creating helper functions\u00b6","text":"<p>Throughout your machine learning experiments, you'll likely come across snippets of code you want to use over and over again.</p> <p>For example, a plotting function which plots a model's <code>history</code> object (see <code>plot_loss_curves()</code> below).</p> <p>You could recreate these functions over and over again.</p> <p>But as you might've guessed, rewritting the same functions becomes tedious.</p> <p>One of the solutions is to store them in a helper script such as <code>helper_functions.py</code>. And then import the necesary functionality when you need it.</p> <p>For example, you might write:</p> <pre><code>from helper_functions import plot_loss_curves\n\n...\n\nplot_loss_curves(history)\n</code></pre> <p>Let's see what this looks like.</p>"},{"location":"05_transfer_learning_in_tensorflow_part_2_fine_tuning/#10-food-classes-working-with-less-data","title":"10 Food Classes: Working with less data\u00b6","text":"<p>We saw in the previous notebook that we could get great results with only 10% of the training data using transfer learning with TensorFlow Hub.</p> <p>In this notebook, we're going to continue to work with smaller subsets of the data, except this time we'll have a look at how we can use the in-built pretrained models within the <code>tf.keras.applications</code> module as well as how to fine-tune them to our own custom dataset.</p> <p>We'll also practice using a new but similar dataloader function to what we've used before, <code>image_dataset_from_directory()</code> which is part of the <code>tf.keras.utils</code> module.</p> <p>Finally, we'll also be practicing using the Keras Functional API for building deep learning models. The Functional API is a more flexible way to create models than the tf.keras.Sequential API.</p> <p>We'll explore each of these in more detail as we go.</p> <p>Let's start by downloading some data.</p>"},{"location":"05_transfer_learning_in_tensorflow_part_2_fine_tuning/#model-0-building-a-transfer-learning-model-using-the-keras-functional-api","title":"Model 0: Building a transfer learning model using the Keras Functional API\u00b6","text":"<p>Alright, our data is tensor-ified, let's build a model.</p> <p>To do so we're going to be using the <code>tf.keras.applications</code> module as it contains a series of already trained (on ImageNet) computer vision models as well as the Keras Functional API to construct our model.</p> <p>We're going to go through the following steps:</p> <ol> <li>Instantiate a pre-trained base model object by choosing a target model such as <code>EfficientNetV2B0</code> from <code>tf.keras.applications.efficientnet_v2</code>, setting the <code>include_top</code> parameter to <code>False</code> (we do this because we're going to create our own top, which are the output layers for the model).</li> <li>Set the base model's <code>trainable</code> attribute to <code>False</code> to freeze all of the weights in the pre-trained model.</li> <li>Define an input layer for our model, for example, what shape of data should our model expect?</li> <li>[Optional] Normalize the inputs to our model if it requires. Some computer vision models such as <code>ResNetV250</code> require their inputs to be between 0 &amp; 1.</li> </ol> <p>\ud83e\udd14 Note: As of writing, the <code>EfficientNet</code> (and <code>EfficientNetV2</code>) models in the <code>tf.keras.applications</code> module do not require images to be normalized (pixel values between 0 and 1) on input, where as many of the other models do. I posted an issue to the TensorFlow GitHub about this and they confirmed this.</p> <ol> <li>Pass the inputs to the base model.</li> <li>Pool the outputs of the base model into a shape compatible with the output activation layer (turn base model output tensors into same shape as label tensors). This can be done using <code>tf.keras.layers.GlobalAveragePooling2D()</code> or <code>tf.keras.layers.GlobalMaxPooling2D()</code> though the former is more common in practice.</li> <li>Create an output activation layer using <code>tf.keras.layers.Dense()</code> with the appropriate activation function and number of neurons.</li> <li>Combine the inputs and outputs layer into a model using <code>tf.keras.Model()</code>.</li> <li>Compile the model using the appropriate loss function and choose of optimizer.</li> <li>Fit the model for desired number of epochs and with necessary callbacks (in our case, we'll start off with the TensorBoard callback).</li> </ol> <p>Woah... that sounds like a lot. Before we get ahead of ourselves, let's see it in practice.</p>"},{"location":"05_transfer_learning_in_tensorflow_part_2_fine_tuning/#getting-a-feature-vector-from-a-trained-model","title":"Getting a feature vector from a trained model\u00b6","text":"<p>\ud83e\udd14 Question: What happens with the <code>tf.keras.layers.GlobalAveragePooling2D()</code> layer? I haven't seen it before.</p> <p>The <code>tf.keras.layers.GlobalAveragePooling2D()</code> layer transforms a 4D tensor into a 2D tensor by averaging the values across the inner-axes.</p> <p>The previous sentence is a bit of a mouthful, so let's see an example.</p>"},{"location":"05_transfer_learning_in_tensorflow_part_2_fine_tuning/#running-a-series-of-transfer-learning-experiments","title":"Running a series of transfer learning experiments\u00b6","text":"<p>We've seen the incredible results of transfer learning on 10% of the training data, what about 1% of the training data?</p> <p>What kind of results do you think we can get using 100x less data than the original CNN models we built ourselves?</p> <p>Why don't we answer that question while running the following modelling experiments:</p> <ol> <li>Model 1: Use feature extraction transfer learning on 1% of the training data with data augmentation.</li> <li>Model 2: Use feature extraction transfer learning on 10% of the training data with data augmentation and save the results to a checkpoint.</li> <li>Model 3: Fine-tune the Model 2 checkpoint on 10% of the training data with data augmentation.</li> <li>Model 4: Fine-tune the Model 2 checkpoint on 100% of the training data with data augmentation.</li> </ol> <p>While all of the experiments will be run on different versions of the training data, they will all be evaluated on the same test dataset, this ensures the results of each experiment are as comparable as possible.</p> <p>All experiments will be done using the <code>EfficientNetV2B0</code> model within the <code>tf.keras.applications.efficientnet_v2</code> module.</p> <p>To make sure we're keeping track of our experiments, we'll use our <code>create_tensorboard_callback()</code> function to log all of the model training logs.</p> <p>We'll construct each model using the Keras Functional API and instead of implementing data augmentation in the <code>ImageDataGenerator</code> class as we have previously, we're going to build it right into the model using the <code>tf.keras.layers</code> module.</p> <p>Let's begin by downloading the data for experiment 1, using feature extraction transfer learning on 1% of the training data with data augmentation.</p>"},{"location":"05_transfer_learning_in_tensorflow_part_2_fine_tuning/#adding-data-augmentation-right-into-the-model","title":"Adding data augmentation right into the model\u00b6","text":"<p>Previously we've used the different parameters of the <code>ImageDataGenerator</code> class to augment our training images, this time we're going to build data augmentation right into the model.</p> <p>How?</p> <p>Using the <code>tf.keras.layers</code> module and creating a dedicated data augmentation layer.</p> <p>This a relatively new feature added to TensorFlow 2.10+ but it's very powerful. Adding a data augmentation layer to the model has the following benefits:</p> <ul> <li>Preprocessing of the images (augmenting them) happens on the GPU rather than on the CPU (much faster).<ul> <li>Images are best preprocessed on the GPU where as text and structured data are more suited to be preprocessed on the CPU.</li> </ul> </li> <li>Image data augmentation only happens during training so we can still export our whole model and use it elsewhere. And if someone else wanted to train the same model as us, including the same kind of data augmentation, they could.</li> </ul> <p> Example of using data augmentation as the first layer within a model (EfficientNetB0).</p> <p>\ud83d\udcda Resource: For more information on different methods of data augmentation, check out the the TensorFlow data augmentation guide.</p> <p>To use data augmentation right within our model we'll create a Keras Sequential model consisting of only data preprocessing layers, we can then use this Sequential model within another Functional model.</p> <p>If that sounds confusing, it'll make sense once we create it in code.</p> <p>The data augmentation transformations we're going to use are:</p> <ul> <li><code>tf.keras.layers.RandomFlip</code> - flips image on horizontal or vertical axis.</li> <li><code>tf.keras.layersRandomRotation</code> - randomly rotates image by a specified amount.</li> <li><code>tf.keras.layers.RandomZoom</code> - randomly zooms into an image by specified amount.</li> <li><code>tf.keras.layers.RandomHeight</code> - randomly shifts image height by a specified amount.</li> <li><code>tf.keras.layers.RandomWidth</code> - randomly shifts image width by a specified amount.</li> <li><code>tf.keras.layers.Rescaling</code> - normalizes the image pixel values to be between 0 and 1, this is worth mentioning because it is required for some image models but since we're using <code>tf.keras.applications.efficientnet_v2.EfficientNetV2B0</code>, it's not required (the model pretrained model implements rescaling itself).</li> </ul> <p>There are more option but these will do for now.</p>"},{"location":"05_transfer_learning_in_tensorflow_part_2_fine_tuning/#model-1-feature-extraction-transfer-learning-on-1-of-the-data-with-data-augmentation","title":"Model 1: Feature extraction transfer learning on 1% of the data with data augmentation\u00b6","text":""},{"location":"05_transfer_learning_in_tensorflow_part_2_fine_tuning/#model-2-feature-extraction-transfer-learning-with-10-of-data-and-data-augmentation","title":"Model 2: Feature extraction transfer learning with 10% of data and data augmentation\u00b6","text":"<p>Alright, we've tested 1% of the training data with data augmentation, how about we try 10% of the data with data augmentation?</p> <p>But wait...</p> <p>\ud83e\udd14 Question: How do you know what experiments to run?</p> <p>Great question.</p> <p>The truth here is you often won't. Machine learning is still a very experimental practice. It's only after trying a fair few things that you'll start to develop an intuition of what to try.</p> <p>My advice is to follow your curiosity as tenaciously as possible. If you feel like you want to try something, write the code for it and run it. See how it goes. The worst thing that'll happen is you'll figure out what doesn't work, the most valuable kind of knowledge.</p> <p>From a practical standpoint, as we've talked about before, you'll want to reduce the amount of time between your initial experiments as much as possible. In other words, run a plethora of smaller experiments, using less data and less training iterations before you find something promising and then scale it up.</p> <p>In the theme of scale, let's scale our 1% training data augmentation experiment up to 10% training data augmentation. That sentence doesn't really make sense but you get what I mean.</p> <p>We're going to run through the exact same steps as the previous model, the only difference being using 10% of the training data instead of 1%.</p>"},{"location":"05_transfer_learning_in_tensorflow_part_2_fine_tuning/#creating-a-modelcheckpoint-callback","title":"Creating a ModelCheckpoint callback\u00b6","text":"<p>Our model is compiled and ready to be fit, so why haven't we fit it yet?</p> <p>Well, for this experiment we're going to introduce a new callback, the <code>ModelCheckpoint</code> callback.</p> <p>The <code>ModelCheckpoint</code> callback gives you the ability to save your model, as a whole in the <code>SavedModel</code> format or the weights (patterns) only to a specified directory as it trains.</p> <p>This is helpful if you think your model is going to be training for a long time and you want to make backups of it as it trains. It also means if you think your model could benefit from being trained for longer, you can reload it from a specific checkpoint and continue training from there.</p> <p>For example, say you fit a feature extraction transfer learning model for 5 epochs and you check the training curves and see it was still improving and you want to see if fine-tuning for another 5 epochs could help, you can load the checkpoint, unfreeze some (or all) of the base model layers and then continue training.</p> <p>In fact, that's exactly what we're going to do.</p> <p>But first, let's create a <code>ModelCheckpoint</code> callback. To do so, we have to specifcy a directory we'd like to save to.</p>"},{"location":"05_transfer_learning_in_tensorflow_part_2_fine_tuning/#model-3-fine-tuning-an-existing-model-on-10-of-the-data","title":"Model 3: Fine-tuning an existing model on 10% of the data\u00b6","text":"<p> High-level example of fine-tuning an EfficientNet model. Bottom layers (layers closer to the input data) stay frozen where as top layers (layers closer to the output data) are updated during training.</p> <p>So far our saved model has been trained using feature extraction transfer learning for 5 epochs on 10% of the training data and data augmentation.</p> <p>This means all of the layers in the base model (<code>EfficientNetV2B0</code>) were frozen during training.</p> <p>For our next experiment we're going to switch to fine-tuning transfer learning. This means we'll be using the same base model except we'll be unfreezing some of its layers (ones closest to the top) and running the model for a few more epochs.</p> <p>The idea with fine-tuning is to start customizing the pre-trained model more to our own data.</p> <p>\ud83d\udd11 Note: Fine-tuning usually works best after training a feature extraction model for a few epochs and with large amounts of data. For more on this, check out Keras' guide on Transfer learning &amp; fine-tuning.</p> <p>We've verified our loaded model's performance, let's check out its layers.</p>"},{"location":"05_transfer_learning_in_tensorflow_part_2_fine_tuning/#model-4-fine-tuning-an-existing-model-all-of-the-data","title":"Model 4: Fine-tuning an existing model all of the data\u00b6","text":"<p>Enough talk about how fine-tuning a model usually works with more data, let's try it out.</p> <p>We'll start by downloading the full version of our 10 food classes dataset.</p>"},{"location":"05_transfer_learning_in_tensorflow_part_2_fine_tuning/#viewing-our-experiment-data-on-tensorboard","title":"Viewing our experiment data on TensorBoard\u00b6","text":"<p>Right now our experimental results are scattered all throughout our notebook. If we want to share them with someone, they'd be getting a bunch of different graphs and metrics... not a fun time.</p> <p>But guess what?</p> <p>Thanks to the TensorBoard callback we made with our helper function <code>create_tensorflow_callback()</code>, we've been tracking our modelling experiments the whole time.</p> <p>How about we upload them to TensorBoard.dev and check them out?</p> <p>We can do with the <code>tensorboard dev upload</code> command and passing it the directory where our experiments have been logged.</p> <p>\ud83d\udd11 Note: Remember, whatever you upload to TensorBoard.dev becomes public. If there are training logs you don't want to share, don't upload them.</p>"},{"location":"05_transfer_learning_in_tensorflow_part_2_fine_tuning/#exercises","title":"\ud83d\udee0 Exercises\u00b6","text":"<ol> <li>Write a function to visualize an image from any dataset (train or test file) and any class (e.g. \"steak\", \"pizza\"... etc), visualize it and make a prediction on it using a trained model.</li> <li>Use feature-extraction to train a transfer learning model on 10% of the Food Vision data for 10 epochs using <code>tf.keras.applications.efficientnet_v2.EfficientNetV2B0</code> as the base model. Use the <code>ModelCheckpoint</code> callback to save the weights to file.</li> <li>Fine-tune the last 20 layers of the base model you trained in 2 for another 10 epochs. How did it go?</li> <li>Fine-tune the last 30 layers of the base model you trained in 2 for another 10 epochs. How did it go?</li> </ol>"},{"location":"05_transfer_learning_in_tensorflow_part_2_fine_tuning/#extra-curriculum","title":"\ud83d\udcd6 Extra-curriculum\u00b6","text":"<ul> <li>Read the documentation on data augmentation in TensorFlow.</li> <li>Read the ULMFit paper (technical) for an introduction to the concept of freezing and unfreezing different layers.</li> <li>Read up on learning rate scheduling (there's a TensorFlow callback for this), how could this influence our model training?<ul> <li>If you're training for longer, you probably want to reduce the learning rate as you go... the closer you get to the bottom of the hill, the smaller steps you want to take. Imagine it like finding a coin at the bottom of your couch. In the beginning your arm movements are going to be large and the closer you get, the smaller your movements become.</li> </ul> </li> </ul>"},{"location":"06_transfer_learning_in_tensorflow_part_3_scaling_up/","title":"06. Transfer Learning with TensorFlow Part 3: Scaling up (\ud83c\udf54\ud83d\udc41 Food Vision mini)","text":"In\u00a0[1]: Copied! <pre># Are we using a GPU? \n# If not, and you're in Google Colab, go to Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU\n!nvidia-smi\n</pre> # Are we using a GPU?  # If not, and you're in Google Colab, go to Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU !nvidia-smi <pre>Thu May 18 02:18:58 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   45C    P0    49W / 400W |      0MiB / 40960MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</pre> In\u00a0[2]: Copied! <pre>import datetime\nprint(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\")\n</pre> import datetime print(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\") <pre>Notebook last run (end-to-end): 2023-05-18 02:18:58.717388\n</pre> In\u00a0[3]: Copied! <pre># Get helper functions file\n!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n</pre> # Get helper functions file !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py <pre>--2023-05-18 02:18:58--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 10246 (10K) [text/plain]\nSaving to: \u2018helper_functions.py\u2019\n\n\rhelper_functions.py   0%[                    ]       0  --.-KB/s               \rhelper_functions.py 100%[===================&gt;]  10.01K  --.-KB/s    in 0s      \n\n2023-05-18 02:18:58 (109 MB/s) - \u2018helper_functions.py\u2019 saved [10246/10246]\n\n</pre> In\u00a0[4]: Copied! <pre># Import series of helper functions for the notebook (we've created/used these in previous notebooks)\nfrom helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, compare_historys, walk_through_dir\n</pre> # Import series of helper functions for the notebook (we've created/used these in previous notebooks) from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, compare_historys, walk_through_dir In\u00a0[5]: Copied! <pre># Download data from Google Storage (already preformatted)\n!wget https://storage.googleapis.com/ztm_tf_course/food_vision/101_food_classes_10_percent.zip \n\nunzip_data(\"101_food_classes_10_percent.zip\")\n\ntrain_dir = \"101_food_classes_10_percent/train/\"\ntest_dir = \"101_food_classes_10_percent/test/\"\n</pre> # Download data from Google Storage (already preformatted) !wget https://storage.googleapis.com/ztm_tf_course/food_vision/101_food_classes_10_percent.zip   unzip_data(\"101_food_classes_10_percent.zip\")  train_dir = \"101_food_classes_10_percent/train/\" test_dir = \"101_food_classes_10_percent/test/\" <pre>--2023-05-18 02:19:02--  https://storage.googleapis.com/ztm_tf_course/food_vision/101_food_classes_10_percent.zip\nResolving storage.googleapis.com (storage.googleapis.com)... 142.250.141.128, 142.251.2.128, 2607:f8b0:4023:c03::80, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|142.250.141.128|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1625420029 (1.5G) [application/zip]\nSaving to: \u2018101_food_classes_10_percent.zip\u2019\n\n101_food_classes_10 100%[===================&gt;]   1.51G   196MB/s    in 7.9s    \n\n2023-05-18 02:19:10 (197 MB/s) - \u2018101_food_classes_10_percent.zip\u2019 saved [1625420029/1625420029]\n\n</pre> In\u00a0[6]: Copied! <pre># How many images/classes are there?\nwalk_through_dir(\"101_food_classes_10_percent\")\n</pre> # How many images/classes are there? walk_through_dir(\"101_food_classes_10_percent\") <pre>There are 2 directories and 0 images in '101_food_classes_10_percent'.\nThere are 101 directories and 0 images in '101_food_classes_10_percent/train'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/dumplings'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/ceviche'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/cheese_plate'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/spring_rolls'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/hot_dog'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/tacos'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/cup_cakes'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/samosa'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/gnocchi'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/pad_thai'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/french_fries'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/spaghetti_carbonara'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/huevos_rancheros'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/frozen_yogurt'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/gyoza'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/spaghetti_bolognese'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/eggs_benedict'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/miso_soup'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/hummus'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/lasagna'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/mussels'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/ice_cream'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/foie_gras'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/chocolate_mousse'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/beet_salad'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/poutine'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/garlic_bread'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/waffles'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/peking_duck'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/caprese_salad'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/fried_calamari'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/fish_and_chips'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/macaroni_and_cheese'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/crab_cakes'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/french_onion_soup'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/caesar_salad'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/baklava'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/pizza'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/escargots'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/carrot_cake'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/lobster_bisque'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/beignets'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/apple_pie'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/donuts'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/prime_rib'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/cheesecake'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/scallops'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/bruschetta'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/pulled_pork_sandwich'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/chocolate_cake'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/pork_chop'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/croque_madame'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/sushi'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/paella'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/ramen'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/edamame'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/macarons'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/deviled_eggs'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/oysters'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/chicken_wings'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/hot_and_sour_soup'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/onion_rings'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/churros'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/french_toast'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/risotto'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/chicken_curry'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/club_sandwich'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/chicken_quesadilla'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/hamburger'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/steak'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/beef_carpaccio'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/tuna_tartare'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/greek_salad'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/omelette'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/shrimp_and_grits'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/panna_cotta'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/grilled_cheese_sandwich'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/clam_chowder'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/sashimi'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/cannoli'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/creme_brulee'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/seaweed_salad'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/strawberry_shortcake'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/guacamole'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/breakfast_burrito'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/ravioli'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/pho'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/takoyaki'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/grilled_salmon'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/pancakes'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/falafel'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/filet_mignon'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/beef_tartare'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/lobster_roll_sandwich'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/fried_rice'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/bread_pudding'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/red_velvet_cake'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/nachos'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/tiramisu'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/baby_back_ribs'.\nThere are 0 directories and 75 images in '101_food_classes_10_percent/train/bibimbap'.\nThere are 101 directories and 0 images in '101_food_classes_10_percent/test'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/dumplings'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/ceviche'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/cheese_plate'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/spring_rolls'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/hot_dog'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/tacos'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/cup_cakes'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/samosa'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/gnocchi'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/pad_thai'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/french_fries'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/spaghetti_carbonara'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/huevos_rancheros'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/frozen_yogurt'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/gyoza'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/spaghetti_bolognese'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/eggs_benedict'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/miso_soup'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/hummus'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/lasagna'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/mussels'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/ice_cream'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/foie_gras'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/chocolate_mousse'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/beet_salad'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/poutine'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/garlic_bread'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/waffles'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/peking_duck'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/caprese_salad'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/fried_calamari'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/fish_and_chips'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/macaroni_and_cheese'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/crab_cakes'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/french_onion_soup'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/caesar_salad'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/baklava'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/pizza'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/escargots'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/carrot_cake'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/lobster_bisque'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/beignets'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/apple_pie'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/donuts'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/prime_rib'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/cheesecake'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/scallops'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/bruschetta'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/pulled_pork_sandwich'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/chocolate_cake'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/pork_chop'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/croque_madame'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/sushi'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/paella'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/ramen'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/edamame'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/macarons'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/deviled_eggs'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/oysters'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/chicken_wings'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/hot_and_sour_soup'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/onion_rings'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/churros'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/french_toast'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/risotto'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/chicken_curry'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/club_sandwich'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/chicken_quesadilla'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/hamburger'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/steak'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/beef_carpaccio'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/tuna_tartare'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/greek_salad'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/omelette'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/shrimp_and_grits'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/panna_cotta'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/grilled_cheese_sandwich'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/clam_chowder'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/sashimi'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/cannoli'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/creme_brulee'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/seaweed_salad'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/strawberry_shortcake'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/guacamole'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/breakfast_burrito'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/ravioli'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/pho'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/takoyaki'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/grilled_salmon'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/pancakes'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/falafel'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/filet_mignon'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/beef_tartare'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/lobster_roll_sandwich'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/fried_rice'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/bread_pudding'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/red_velvet_cake'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/nachos'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/tiramisu'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/baby_back_ribs'.\nThere are 0 directories and 250 images in '101_food_classes_10_percent/test/bibimbap'.\n</pre> <p>As before our data comes in the common image classification data format of:</p> <pre><code>Example of file structure\n\n10_food_classes_10_percent &lt;- top level folder\n\u2514\u2500\u2500\u2500train &lt;- training images\n\u2502   \u2514\u2500\u2500\u2500pizza\n\u2502   \u2502   \u2502   1008104.jpg\n\u2502   \u2502   \u2502   1638227.jpg\n\u2502   \u2502   \u2502   ...      \n\u2502   \u2514\u2500\u2500\u2500steak\n\u2502       \u2502   1000205.jpg\n\u2502       \u2502   1647351.jpg\n\u2502       \u2502   ...\n\u2502   \n\u2514\u2500\u2500\u2500test &lt;- testing images\n\u2502   \u2514\u2500\u2500\u2500pizza\n\u2502   \u2502   \u2502   1001116.jpg\n\u2502   \u2502   \u2502   1507019.jpg\n\u2502   \u2502   \u2502   ...      \n\u2502   \u2514\u2500\u2500\u2500steak\n\u2502       \u2502   100274.jpg\n\u2502       \u2502   1653815.jpg\n\u2502       \u2502   ...    \n</code></pre> <p>Let's use the <code>image_dataset_from_directory()</code> function to turn our images and labels into a <code>tf.data.Dataset</code>, a TensorFlow datatype which allows for us to pass it directory to our model.</p> <p>For the test dataset, we're going to set <code>shuffle=False</code> so we can perform repeatable evaluation and visualization on it later.</p> In\u00a0[7]: Copied! <pre># Setup data inputs\nimport tensorflow as tf\nIMG_SIZE = (224, 224)\ntrain_data_all_10_percent = tf.keras.preprocessing.image_dataset_from_directory(train_dir,\n                                                                                label_mode=\"categorical\",\n                                                                                image_size=IMG_SIZE)\n                                                                                \ntest_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n                                                                label_mode=\"categorical\",\n                                                                image_size=IMG_SIZE,\n                                                                shuffle=False) # don't shuffle test data for prediction analysis\n</pre> # Setup data inputs import tensorflow as tf IMG_SIZE = (224, 224) train_data_all_10_percent = tf.keras.preprocessing.image_dataset_from_directory(train_dir,                                                                                 label_mode=\"categorical\",                                                                                 image_size=IMG_SIZE)                                                                                  test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,                                                                 label_mode=\"categorical\",                                                                 image_size=IMG_SIZE,                                                                 shuffle=False) # don't shuffle test data for prediction analysis <pre>Found 7575 files belonging to 101 classes.\nFound 25250 files belonging to 101 classes.\n</pre> <p>Wonderful! It looks like our data has been imported as expected with 75 images per class in the training set (75 images * 101 classes = 7575 images) and 25250 images in the test set (250 images * 101 classes = 25250 images).</p> In\u00a0[8]: Copied! <pre># Create checkpoint callback to save model for later use\ncheckpoint_path = \"101_classes_10_percent_data_model_checkpoint\"\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n                                                         save_weights_only=True, # save only the model weights\n                                                         monitor=\"val_accuracy\", # save the model weights which score the best validation accuracy\n                                                         save_best_only=True) # only keep the best model weights on file (delete the rest)\n</pre> # Create checkpoint callback to save model for later use checkpoint_path = \"101_classes_10_percent_data_model_checkpoint\" checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,                                                          save_weights_only=True, # save only the model weights                                                          monitor=\"val_accuracy\", # save the model weights which score the best validation accuracy                                                          save_best_only=True) # only keep the best model weights on file (delete the rest) <p>Checkpoint ready. Now let's create a small data augmentation model with the Sequential API. Because we're working with a reduced sized training set, this will help prevent our model from overfitting on the training data.</p> In\u00a0[9]: Copied! <pre># Import the required modules for model creation\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\n\n## NEW: Newer versions of TensorFlow (2.10+) can use the tensorflow.keras.layers API directly for data augmentation\ndata_augmentation = Sequential([\n  layers.RandomFlip(\"horizontal\"),\n  layers.RandomRotation(0.2),\n  layers.RandomZoom(0.2),\n  layers.RandomHeight(0.2),\n  layers.RandomWidth(0.2),\n  # preprocessing.Rescaling(1./255) # keep for ResNet50V2, remove for EfficientNetB0\n], name =\"data_augmentation\")\n\n## OLD\n# # Setup data augmentation\n# from tensorflow.keras.layers.experimental import preprocessing\n# data_augmentation = Sequential([\n#   preprocessing.RandomFlip(\"horizontal\"), # randomly flip images on horizontal edge\n#   preprocessing.RandomRotation(0.2), # randomly rotate images by a specific amount\n#   preprocessing.RandomHeight(0.2), # randomly adjust the height of an image by a specific amount\n#   preprocessing.RandomWidth(0.2), # randomly adjust the width of an image by a specific amount\n#   preprocessing.RandomZoom(0.2), # randomly zoom into an image\n#   # preprocessing.Rescaling(1./255) # keep for models like ResNet50V2, remove for EfficientNet\n# ], name=\"data_augmentation\")\n</pre> # Import the required modules for model creation from tensorflow.keras import layers from tensorflow.keras.models import Sequential  ## NEW: Newer versions of TensorFlow (2.10+) can use the tensorflow.keras.layers API directly for data augmentation data_augmentation = Sequential([   layers.RandomFlip(\"horizontal\"),   layers.RandomRotation(0.2),   layers.RandomZoom(0.2),   layers.RandomHeight(0.2),   layers.RandomWidth(0.2),   # preprocessing.Rescaling(1./255) # keep for ResNet50V2, remove for EfficientNetB0 ], name =\"data_augmentation\")  ## OLD # # Setup data augmentation # from tensorflow.keras.layers.experimental import preprocessing # data_augmentation = Sequential([ #   preprocessing.RandomFlip(\"horizontal\"), # randomly flip images on horizontal edge #   preprocessing.RandomRotation(0.2), # randomly rotate images by a specific amount #   preprocessing.RandomHeight(0.2), # randomly adjust the height of an image by a specific amount #   preprocessing.RandomWidth(0.2), # randomly adjust the width of an image by a specific amount #   preprocessing.RandomZoom(0.2), # randomly zoom into an image #   # preprocessing.Rescaling(1./255) # keep for models like ResNet50V2, remove for EfficientNet # ], name=\"data_augmentation\") <p>Beautiful! We'll be able to insert the <code>data_augmentation</code> Sequential model as a layer in our Functional API model. That way if we want to continue training our model at a later time, the data augmentation is already built right in.</p> <p>Speaking of Functional API model's, time to put together a feature extraction transfer learning model using <code>tf.keras.applications.efficientnet.EfficientNetB0</code> as our base model.</p> <p>We'll import the base model using the parameter <code>include_top=False</code> so we can add on our own output layers, notably <code>GlobalAveragePooling2D()</code> (condense the outputs of the base model into a shape usable by the output layer) followed by a <code>Dense</code> layer.</p> In\u00a0[10]: Copied! <pre># Setup base model and freeze its layers (this will extract features)\nbase_model = tf.keras.applications.efficientnet.EfficientNetB0(include_top=False)\nbase_model.trainable = False\n\n# Setup model architecture with trainable top layers\ninputs = layers.Input(shape=(224, 224, 3), name=\"input_layer\") # shape of input image\nx = data_augmentation(inputs) # augment images (only happens during training)\nx = base_model(x, training=False) # put the base model in inference mode so we can use it to extract features without updating the weights\nx = layers.GlobalAveragePooling2D(name=\"global_average_pooling\")(x) # pool the outputs of the base model\noutputs = layers.Dense(len(train_data_all_10_percent.class_names), activation=\"softmax\", name=\"output_layer\")(x) # same number of outputs as classes\nmodel = tf.keras.Model(inputs, outputs)\n</pre> # Setup base model and freeze its layers (this will extract features) base_model = tf.keras.applications.efficientnet.EfficientNetB0(include_top=False) base_model.trainable = False  # Setup model architecture with trainable top layers inputs = layers.Input(shape=(224, 224, 3), name=\"input_layer\") # shape of input image x = data_augmentation(inputs) # augment images (only happens during training) x = base_model(x, training=False) # put the base model in inference mode so we can use it to extract features without updating the weights x = layers.GlobalAveragePooling2D(name=\"global_average_pooling\")(x) # pool the outputs of the base model outputs = layers.Dense(len(train_data_all_10_percent.class_names), activation=\"softmax\", name=\"output_layer\")(x) # same number of outputs as classes model = tf.keras.Model(inputs, outputs) <pre>Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n16705208/16705208 [==============================] - 0s 0us/step\n</pre> <p> A colourful figure of the model we've created with: 224x224 images as input, data augmentation as a layer, EfficientNetB0 as a backbone, an averaging pooling layer as well as dense layer with 10 neurons (same as number of classes we're working with) as output.</p> <p>Model created. Let's inspect it.</p> In\u00a0[11]: Copied! <pre># Get a summary of our model\nmodel.summary()\n</pre> # Get a summary of our model model.summary() <pre>Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n                                                                 \n data_augmentation (Sequenti  (None, None, None, 3)    0         \n al)                                                             \n                                                                 \n efficientnetb0 (Functional)  (None, None, None, 1280)  4049571  \n                                                                 \n global_average_pooling (Glo  (None, 1280)             0         \n balAveragePooling2D)                                            \n                                                                 \n output_layer (Dense)        (None, 101)               129381    \n                                                                 \n=================================================================\nTotal params: 4,178,952\nTrainable params: 129,381\nNon-trainable params: 4,049,571\n_________________________________________________________________\n</pre> <p>Looking good! Our Functional model has 5 layers but each of those layers have varying amounts of layers within them.</p> <p>Notice the number of trainable and non-trainable parameters. It seems the only trainable parameters are within the <code>output_layer</code> which is exactly what we're after with this first run of feature extraction; keep all the learned patterns in the base model (<code>EfficientNetb0</code>) frozen whilst allowing the model to tune its outputs to our custom data.</p> <p>Time to compile and fit.</p> In\u00a0[12]: Copied! <pre># Compile\nmodel.compile(loss=\"categorical_crossentropy\",\n              optimizer=tf.keras.optimizers.Adam(), # use Adam with default settings\n              metrics=[\"accuracy\"])\n\n# Fit\nhistory_all_classes_10_percent = model.fit(train_data_all_10_percent,\n                                           epochs=5, # fit for 5 epochs to keep experiments quick\n                                           validation_data=test_data,\n                                           validation_steps=int(0.15 * len(test_data)), # evaluate on smaller portion of test data\n                                           callbacks=[checkpoint_callback]) # save best model weights to file\n</pre> # Compile model.compile(loss=\"categorical_crossentropy\",               optimizer=tf.keras.optimizers.Adam(), # use Adam with default settings               metrics=[\"accuracy\"])  # Fit history_all_classes_10_percent = model.fit(train_data_all_10_percent,                                            epochs=5, # fit for 5 epochs to keep experiments quick                                            validation_data=test_data,                                            validation_steps=int(0.15 * len(test_data)), # evaluate on smaller portion of test data                                            callbacks=[checkpoint_callback]) # save best model weights to file <pre>Epoch 1/5\n237/237 [==============================] - 29s 59ms/step - loss: 3.3881 - accuracy: 0.2796 - val_loss: 2.4697 - val_accuracy: 0.4661\nEpoch 2/5\n237/237 [==============================] - 12s 51ms/step - loss: 2.1996 - accuracy: 0.4937 - val_loss: 2.0252 - val_accuracy: 0.5188\nEpoch 3/5\n237/237 [==============================] - 12s 51ms/step - loss: 1.8245 - accuracy: 0.5675 - val_loss: 1.8783 - val_accuracy: 0.5344\nEpoch 4/5\n237/237 [==============================] - 12s 50ms/step - loss: 1.6061 - accuracy: 0.6055 - val_loss: 1.8206 - val_accuracy: 0.5355\nEpoch 5/5\n237/237 [==============================] - 11s 48ms/step - loss: 1.4493 - accuracy: 0.6440 - val_loss: 1.7824 - val_accuracy: 0.5310\n</pre> <p>Woah! It looks like our model is getting some impressive results, but remember, during training our model only evaluated on 15% of the test data. Let's see how it did on the whole test dataset.</p> In\u00a0[13]: Copied! <pre># Evaluate model \nresults_feature_extraction_model = model.evaluate(test_data)\nresults_feature_extraction_model\n</pre> # Evaluate model  results_feature_extraction_model = model.evaluate(test_data) results_feature_extraction_model <pre>790/790 [==============================] - 16s 21ms/step - loss: 1.5888 - accuracy: 0.5797\n</pre> Out[13]: <pre>[1.5887829065322876, 0.5797227621078491]</pre> <p>Well it looks like we just beat our baseline (the results from the original Food101 paper) with 10% of the data! In under 5-minutes... that's the power of deep learning and more precisely, transfer learning: leveraging what one model has learned on another dataset for our own dataset.</p> <p>How do the loss curves look?</p> In\u00a0[14]: Copied! <pre>plot_loss_curves(history_all_classes_10_percent)\n</pre> plot_loss_curves(history_all_classes_10_percent) <p>\ud83e\udd14 Question: What do these curves suggest? Hint: ideally, the two curves should be very similar to each other, if not, there may be some overfitting or underfitting.</p> In\u00a0[15]: Copied! <pre># Unfreeze all of the layers in the base model\nbase_model.trainable = True\n\n# Refreeze every layer except for the last 5\nfor layer in base_model.layers[:-5]:\n  layer.trainable = False\n</pre> # Unfreeze all of the layers in the base model base_model.trainable = True  # Refreeze every layer except for the last 5 for layer in base_model.layers[:-5]:   layer.trainable = False <p>We just made a change to the layers in our model and what do we have to do every time we make a change to our model?</p> <p>Recompile it.</p> <p>Because we're fine-tuning, we'll use a 10x lower learning rate to ensure the updates to the previous trained weights aren't too large.</p> <p> When fine-tuning and unfreezing layers of your pre-trained model, it's common practice to lower the learning rate you used for your feature extraction model. How much by? A 10x lower learning rate is usually a good place to to start.</p> In\u00a0[16]: Copied! <pre># Recompile model with lower learning rate\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=tf.keras.optimizers.Adam(1e-4), # 10x lower learning rate than default\n              metrics=['accuracy'])\n</pre> # Recompile model with lower learning rate model.compile(loss='categorical_crossentropy',               optimizer=tf.keras.optimizers.Adam(1e-4), # 10x lower learning rate than default               metrics=['accuracy']) <p>Model recompiled, how about we make sure the layers we want are trainable?</p> In\u00a0[17]: Copied! <pre># What layers in the model are trainable?\nfor layer in model.layers:\n  print(layer.name, layer.trainable)\n</pre> # What layers in the model are trainable? for layer in model.layers:   print(layer.name, layer.trainable) <pre>input_layer True\ndata_augmentation True\nefficientnetb0 True\nglobal_average_pooling True\noutput_layer True\n</pre> In\u00a0[18]: Copied! <pre># Check which layers are trainable\nfor layer_number, layer in enumerate(base_model.layers):\n  print(layer_number, layer.name, layer.trainable)\n</pre> # Check which layers are trainable for layer_number, layer in enumerate(base_model.layers):   print(layer_number, layer.name, layer.trainable) <pre>0 input_1 False\n1 rescaling False\n2 normalization False\n3 rescaling_1 False\n4 stem_conv_pad False\n5 stem_conv False\n6 stem_bn False\n7 stem_activation False\n8 block1a_dwconv False\n9 block1a_bn False\n10 block1a_activation False\n11 block1a_se_squeeze False\n12 block1a_se_reshape False\n13 block1a_se_reduce False\n14 block1a_se_expand False\n15 block1a_se_excite False\n16 block1a_project_conv False\n17 block1a_project_bn False\n18 block2a_expand_conv False\n19 block2a_expand_bn False\n20 block2a_expand_activation False\n21 block2a_dwconv_pad False\n22 block2a_dwconv False\n23 block2a_bn False\n24 block2a_activation False\n25 block2a_se_squeeze False\n26 block2a_se_reshape False\n27 block2a_se_reduce False\n28 block2a_se_expand False\n29 block2a_se_excite False\n30 block2a_project_conv False\n31 block2a_project_bn False\n32 block2b_expand_conv False\n33 block2b_expand_bn False\n34 block2b_expand_activation False\n35 block2b_dwconv False\n36 block2b_bn False\n37 block2b_activation False\n38 block2b_se_squeeze False\n39 block2b_se_reshape False\n40 block2b_se_reduce False\n41 block2b_se_expand False\n42 block2b_se_excite False\n43 block2b_project_conv False\n44 block2b_project_bn False\n45 block2b_drop False\n46 block2b_add False\n47 block3a_expand_conv False\n48 block3a_expand_bn False\n49 block3a_expand_activation False\n50 block3a_dwconv_pad False\n51 block3a_dwconv False\n52 block3a_bn False\n53 block3a_activation False\n54 block3a_se_squeeze False\n55 block3a_se_reshape False\n56 block3a_se_reduce False\n57 block3a_se_expand False\n58 block3a_se_excite False\n59 block3a_project_conv False\n60 block3a_project_bn False\n61 block3b_expand_conv False\n62 block3b_expand_bn False\n63 block3b_expand_activation False\n64 block3b_dwconv False\n65 block3b_bn False\n66 block3b_activation False\n67 block3b_se_squeeze False\n68 block3b_se_reshape False\n69 block3b_se_reduce False\n70 block3b_se_expand False\n71 block3b_se_excite False\n72 block3b_project_conv False\n73 block3b_project_bn False\n74 block3b_drop False\n75 block3b_add False\n76 block4a_expand_conv False\n77 block4a_expand_bn False\n78 block4a_expand_activation False\n79 block4a_dwconv_pad False\n80 block4a_dwconv False\n81 block4a_bn False\n82 block4a_activation False\n83 block4a_se_squeeze False\n84 block4a_se_reshape False\n85 block4a_se_reduce False\n86 block4a_se_expand False\n87 block4a_se_excite False\n88 block4a_project_conv False\n89 block4a_project_bn False\n90 block4b_expand_conv False\n91 block4b_expand_bn False\n92 block4b_expand_activation False\n93 block4b_dwconv False\n94 block4b_bn False\n95 block4b_activation False\n96 block4b_se_squeeze False\n97 block4b_se_reshape False\n98 block4b_se_reduce False\n99 block4b_se_expand False\n100 block4b_se_excite False\n101 block4b_project_conv False\n102 block4b_project_bn False\n103 block4b_drop False\n104 block4b_add False\n105 block4c_expand_conv False\n106 block4c_expand_bn False\n107 block4c_expand_activation False\n108 block4c_dwconv False\n109 block4c_bn False\n110 block4c_activation False\n111 block4c_se_squeeze False\n112 block4c_se_reshape False\n113 block4c_se_reduce False\n114 block4c_se_expand False\n115 block4c_se_excite False\n116 block4c_project_conv False\n117 block4c_project_bn False\n118 block4c_drop False\n119 block4c_add False\n120 block5a_expand_conv False\n121 block5a_expand_bn False\n122 block5a_expand_activation False\n123 block5a_dwconv False\n124 block5a_bn False\n125 block5a_activation False\n126 block5a_se_squeeze False\n127 block5a_se_reshape False\n128 block5a_se_reduce False\n129 block5a_se_expand False\n130 block5a_se_excite False\n131 block5a_project_conv False\n132 block5a_project_bn False\n133 block5b_expand_conv False\n134 block5b_expand_bn False\n135 block5b_expand_activation False\n136 block5b_dwconv False\n137 block5b_bn False\n138 block5b_activation False\n139 block5b_se_squeeze False\n140 block5b_se_reshape False\n141 block5b_se_reduce False\n142 block5b_se_expand False\n143 block5b_se_excite False\n144 block5b_project_conv False\n145 block5b_project_bn False\n146 block5b_drop False\n147 block5b_add False\n148 block5c_expand_conv False\n149 block5c_expand_bn False\n150 block5c_expand_activation False\n151 block5c_dwconv False\n152 block5c_bn False\n153 block5c_activation False\n154 block5c_se_squeeze False\n155 block5c_se_reshape False\n156 block5c_se_reduce False\n157 block5c_se_expand False\n158 block5c_se_excite False\n159 block5c_project_conv False\n160 block5c_project_bn False\n161 block5c_drop False\n162 block5c_add False\n163 block6a_expand_conv False\n164 block6a_expand_bn False\n165 block6a_expand_activation False\n166 block6a_dwconv_pad False\n167 block6a_dwconv False\n168 block6a_bn False\n169 block6a_activation False\n170 block6a_se_squeeze False\n171 block6a_se_reshape False\n172 block6a_se_reduce False\n173 block6a_se_expand False\n174 block6a_se_excite False\n175 block6a_project_conv False\n176 block6a_project_bn False\n177 block6b_expand_conv False\n178 block6b_expand_bn False\n179 block6b_expand_activation False\n180 block6b_dwconv False\n181 block6b_bn False\n182 block6b_activation False\n183 block6b_se_squeeze False\n184 block6b_se_reshape False\n185 block6b_se_reduce False\n186 block6b_se_expand False\n187 block6b_se_excite False\n188 block6b_project_conv False\n189 block6b_project_bn False\n190 block6b_drop False\n191 block6b_add False\n192 block6c_expand_conv False\n193 block6c_expand_bn False\n194 block6c_expand_activation False\n195 block6c_dwconv False\n196 block6c_bn False\n197 block6c_activation False\n198 block6c_se_squeeze False\n199 block6c_se_reshape False\n200 block6c_se_reduce False\n201 block6c_se_expand False\n202 block6c_se_excite False\n203 block6c_project_conv False\n204 block6c_project_bn False\n205 block6c_drop False\n206 block6c_add False\n207 block6d_expand_conv False\n208 block6d_expand_bn False\n209 block6d_expand_activation False\n210 block6d_dwconv False\n211 block6d_bn False\n212 block6d_activation False\n213 block6d_se_squeeze False\n214 block6d_se_reshape False\n215 block6d_se_reduce False\n216 block6d_se_expand False\n217 block6d_se_excite False\n218 block6d_project_conv False\n219 block6d_project_bn False\n220 block6d_drop False\n221 block6d_add False\n222 block7a_expand_conv False\n223 block7a_expand_bn False\n224 block7a_expand_activation False\n225 block7a_dwconv False\n226 block7a_bn False\n227 block7a_activation False\n228 block7a_se_squeeze False\n229 block7a_se_reshape False\n230 block7a_se_reduce False\n231 block7a_se_expand False\n232 block7a_se_excite False\n233 block7a_project_conv True\n234 block7a_project_bn True\n235 top_conv True\n236 top_bn True\n237 top_activation True\n</pre> <p>Excellent! Time to fine-tune our model.</p> <p>Another 5 epochs should be enough to see whether any benefits come about (though we could always try more).</p> <p>We'll start the training off where the feature extraction model left off using the <code>initial_epoch</code> parameter in the <code>fit()</code> function.</p> In\u00a0[19]: Copied! <pre># Fine-tune for 5 more epochs\nfine_tune_epochs = 10 # model has already done 5 epochs, this is the total number of epochs we're after (5+5=10)\n\nhistory_all_classes_10_percent_fine_tune = model.fit(train_data_all_10_percent,\n                                                     epochs=fine_tune_epochs,\n                                                     validation_data=test_data,\n                                                     validation_steps=int(0.15 * len(test_data)), # validate on 15% of the test data\n                                                     initial_epoch=history_all_classes_10_percent.epoch[-1]) # start from previous last epoch\n</pre> # Fine-tune for 5 more epochs fine_tune_epochs = 10 # model has already done 5 epochs, this is the total number of epochs we're after (5+5=10)  history_all_classes_10_percent_fine_tune = model.fit(train_data_all_10_percent,                                                      epochs=fine_tune_epochs,                                                      validation_data=test_data,                                                      validation_steps=int(0.15 * len(test_data)), # validate on 15% of the test data                                                      initial_epoch=history_all_classes_10_percent.epoch[-1]) # start from previous last epoch <pre>Epoch 5/10\n237/237 [==============================] - 51s 173ms/step - loss: 1.2195 - accuracy: 0.6764 - val_loss: 1.7268 - val_accuracy: 0.5392\nEpoch 6/10\n237/237 [==============================] - 25s 104ms/step - loss: 1.1019 - accuracy: 0.7006 - val_loss: 1.7007 - val_accuracy: 0.5493\nEpoch 7/10\n237/237 [==============================] - 22s 91ms/step - loss: 1.0089 - accuracy: 0.7291 - val_loss: 1.7452 - val_accuracy: 0.5426\nEpoch 8/10\n237/237 [==============================] - 20s 84ms/step - loss: 0.9453 - accuracy: 0.7480 - val_loss: 1.7133 - val_accuracy: 0.5506\nEpoch 9/10\n237/237 [==============================] - 19s 79ms/step - loss: 0.8790 - accuracy: 0.7673 - val_loss: 1.7370 - val_accuracy: 0.5471\nEpoch 10/10\n237/237 [==============================] - 17s 72ms/step - loss: 0.8351 - accuracy: 0.7768 - val_loss: 1.7431 - val_accuracy: 0.5498\n</pre> <p>Once again, during training we were only evaluating on a small portion of the test data, let's find out how our model went on all of the test data.</p> In\u00a0[20]: Copied! <pre># Evaluate fine-tuned model on the whole test dataset\nresults_all_classes_10_percent_fine_tune = model.evaluate(test_data)\nresults_all_classes_10_percent_fine_tune\n</pre> # Evaluate fine-tuned model on the whole test dataset results_all_classes_10_percent_fine_tune = model.evaluate(test_data) results_all_classes_10_percent_fine_tune <pre>790/790 [==============================] - 16s 21ms/step - loss: 1.5078 - accuracy: 0.6011\n</pre> Out[20]: <pre>[1.5077574253082275, 0.6010693311691284]</pre> <p>Hmm... it seems like our model got a slight boost from fine-tuning.</p> <p>We might get a better picture by using our <code>compare_historys()</code> function and seeing what the training curves say.</p> In\u00a0[21]: Copied! <pre>compare_historys(original_history=history_all_classes_10_percent,\n                 new_history=history_all_classes_10_percent_fine_tune,\n                 initial_epochs=5)\n</pre> compare_historys(original_history=history_all_classes_10_percent,                  new_history=history_all_classes_10_percent_fine_tune,                  initial_epochs=5) <p>It seems that after fine-tuning, our model's training metrics improved significantly but validation, not so much. Looks like our model is starting to overfit.</p> <p>This is okay though, its very often the case that fine-tuning leads to overfitting when the data a pre-trained model has been trained on is similar to your custom data.</p> <p>In our case, our pre-trained model, <code>EfficientNetB0</code> was trained on ImageNet which contains many real life pictures of food just like our food dataset.</p> <p>If feautre extraction already works well, the improvements you see from fine-tuning may not be as great as if your dataset was significantly different from the data your base model was pre-trained on.</p> In\u00a0[22]: Copied! <pre># # Save model to drive so it can be used later \n# model.save(\"drive/My Drive/tensorflow_course/101_food_class_10_percent_saved_big_dog_model\")\n</pre> # # Save model to drive so it can be used later  # model.save(\"drive/My Drive/tensorflow_course/101_food_class_10_percent_saved_big_dog_model\") In\u00a0[\u00a0]: Copied! <pre>import tensorflow as tf\n\n# Download pre-trained model from Google Storage (like a cooking show, I trained this model earlier, so the results may be different than above)\n!wget https://storage.googleapis.com/ztm_tf_course/food_vision/06_101_food_class_10_percent_saved_big_dog_model.zip\nsaved_model_path = \"06_101_food_class_10_percent_saved_big_dog_model.zip\"\nunzip_data(saved_model_path)\n\n# Note: loading a model will output a lot of 'WARNINGS', these can be ignored: https://www.tensorflow.org/tutorials/keras/save_and_load#save_checkpoints_during_training\n# There's also a thread on GitHub trying to fix these warnings: https://github.com/tensorflow/tensorflow/issues/40166\n# model = tf.keras.models.load_model(\"drive/My Drive/tensorflow_course/101_food_class_10_percent_saved_big_dog_model/\") # path to drive model\nmodel = tf.keras.models.load_model(saved_model_path.split(\".\")[0]) # don't include \".zip\" in loaded model path\n</pre> import tensorflow as tf  # Download pre-trained model from Google Storage (like a cooking show, I trained this model earlier, so the results may be different than above) !wget https://storage.googleapis.com/ztm_tf_course/food_vision/06_101_food_class_10_percent_saved_big_dog_model.zip saved_model_path = \"06_101_food_class_10_percent_saved_big_dog_model.zip\" unzip_data(saved_model_path)  # Note: loading a model will output a lot of 'WARNINGS', these can be ignored: https://www.tensorflow.org/tutorials/keras/save_and_load#save_checkpoints_during_training # There's also a thread on GitHub trying to fix these warnings: https://github.com/tensorflow/tensorflow/issues/40166 # model = tf.keras.models.load_model(\"drive/My Drive/tensorflow_course/101_food_class_10_percent_saved_big_dog_model/\") # path to drive model model = tf.keras.models.load_model(saved_model_path.split(\".\")[0]) # don't include \".zip\" in loaded model path <p>To make sure our loaded model is indead a trained model, let's evaluate its performance on the test dataset.</p> In\u00a0[24]: Copied! <pre># Check to see if loaded model is a trained model\nloaded_loss, loaded_accuracy = model.evaluate(test_data)\nloaded_loss, loaded_accuracy\n</pre> # Check to see if loaded model is a trained model loaded_loss, loaded_accuracy = model.evaluate(test_data) loaded_loss, loaded_accuracy <pre>790/790 [==============================] - 19s 22ms/step - loss: 1.8022 - accuracy: 0.6078\n</pre> Out[24]: <pre>(1.8021684885025024, 0.6078019738197327)</pre> <p>Wonderful! It looks like our loaded model is performing just as well as it was before we saved it. Let's make some predictions.</p> In\u00a0[25]: Copied! <pre># Make predictions with model\npred_probs = model.predict(test_data, verbose=1) # set verbosity to see how long it will take\n</pre> # Make predictions with model pred_probs = model.predict(test_data, verbose=1) # set verbosity to see how long it will take  <pre>790/790 [==============================] - 15s 18ms/step\n</pre> <p>We just passed all of the test images to our model and asked it to make a prediction on what food it thinks is in each.</p> <p>So if we had 25250 images in the test dataset, how many predictions do you think we should have?</p> In\u00a0[26]: Copied! <pre># How many predictions are there?\nlen(pred_probs)\n</pre> # How many predictions are there? len(pred_probs) Out[26]: <pre>25250</pre> <p>And if each image could be one of 101 classes, how many predictions do you think we'll have for each image?</p> In\u00a0[27]: Copied! <pre># What's the shape of our predictions?\npred_probs.shape\n</pre> # What's the shape of our predictions? pred_probs.shape Out[27]: <pre>(25250, 101)</pre> <p>What we've got is often referred to as a predictions probability tensor (or array).</p> <p>Let's see what the first 10 look like.</p> In\u00a0[28]: Copied! <pre># How do they look?\npred_probs[:10]\n</pre> # How do they look? pred_probs[:10] Out[28]: <pre>array([[5.9555572e-02, 3.5662119e-06, 4.1279349e-02, ..., 1.4194699e-09,\n        8.4039129e-05, 3.0820314e-03],\n       [9.6338320e-01, 1.3765826e-09, 8.5042708e-04, ..., 5.4804143e-05,\n        7.8341188e-12, 9.7811304e-10],\n       [9.5942634e-01, 3.2432759e-05, 1.4769287e-03, ..., 7.1438910e-07,\n        5.5323352e-07, 4.0179562e-05],\n       ...,\n       [4.7279873e-01, 1.2954312e-07, 1.4748254e-03, ..., 5.9630687e-04,\n        6.7163957e-05, 2.3532135e-05],\n       [4.4502247e-02, 4.7261000e-07, 1.2174438e-01, ..., 6.2917384e-06,\n        7.5576504e-06, 3.6633476e-03],\n       [7.2373080e-01, 1.9256416e-09, 5.2089705e-05, ..., 1.2218992e-03,\n        1.5755526e-09, 9.6206924e-05]], dtype=float32)</pre> <p>Alright, it seems like we've got a bunch of tensors of really small numbers, how about we zoom into one of them?</p> In\u00a0[29]: Copied! <pre># We get one prediction probability per class\nprint(f\"Number of prediction probabilities for sample 0: {len(pred_probs[0])}\")\nprint(f\"What prediction probability sample 0 looks like:\\n {pred_probs[0]}\")\nprint(f\"The class with the highest predicted probability by the model for sample 0: {pred_probs[0].argmax()}\")\n</pre> # We get one prediction probability per class print(f\"Number of prediction probabilities for sample 0: {len(pred_probs[0])}\") print(f\"What prediction probability sample 0 looks like:\\n {pred_probs[0]}\") print(f\"The class with the highest predicted probability by the model for sample 0: {pred_probs[0].argmax()}\") <pre>Number of prediction probabilities for sample 0: 101\nWhat prediction probability sample 0 looks like:\n [5.95555715e-02 3.56621194e-06 4.12793495e-02 1.06392506e-09\n 8.19964274e-09 8.61560245e-09 8.10409119e-07 8.49006710e-07\n 1.98095986e-05 7.99297993e-07 3.17695403e-09 9.83702989e-07\n 2.83578265e-04 7.78589082e-10 7.43659853e-04 3.87942600e-05\n 6.44463080e-06 2.50333073e-06 3.77276956e-05 2.05599179e-07\n 1.55429479e-05 8.10427650e-07 2.60885736e-06 2.03088433e-07\n 8.30327224e-07 5.42988437e-06 3.74273668e-06 1.32360203e-08\n 2.74555851e-03 2.79426695e-05 6.86718571e-10 2.53517483e-05\n 1.66382728e-04 7.57455043e-10 4.02367179e-04 1.30578828e-08\n 1.79280721e-06 1.43956686e-06 2.30789818e-02 8.24019480e-07\n 8.61712351e-07 1.69789450e-06 7.03542946e-06 1.85722051e-08\n 2.87577478e-07 7.99586633e-06 2.07466110e-06 1.86462174e-07\n 3.34909487e-08 3.17168073e-04 1.04948031e-05 8.55388123e-07\n 8.47566068e-01 1.05432355e-05 4.34703423e-07 3.72825380e-05\n 3.50601949e-05 3.25856490e-05 6.74493349e-05 1.27383810e-08\n 2.62344230e-10 1.03191360e-05 8.54040845e-05 1.05053311e-06\n 2.11897259e-06 3.72938448e-05 7.52453388e-08 2.50746176e-04\n 9.31413410e-07 1.24124577e-04 6.20197443e-06 1.24211716e-08\n 4.04207458e-05 6.82277772e-08 1.24867688e-06 5.15563556e-08\n 7.48909628e-08 7.54529829e-05 7.75354492e-05 6.31509749e-07\n 9.79340939e-07 2.18733803e-05 1.49467369e-05 1.39609512e-07\n 1.22257961e-05 1.90126449e-02 4.97255787e-05 4.59980038e-06\n 1.51860661e-07 3.38210441e-07 3.89491328e-09 1.64673807e-07\n 8.08345867e-05 4.90067578e-06 2.41164742e-07 2.32299317e-05\n 3.09399824e-04 3.10968826e-05 1.41946987e-09 8.40391294e-05\n 3.08203138e-03]\nThe class with the highest predicted probability by the model for sample 0: 52\n</pre> <p>As we discussed before, for each image tensor we pass to our model, because of the number of output neurons and activation function in the last layer (<code>layers.Dense(len(train_data_all_10_percent.class_names), activation=\"softmax\"</code>), it outputs a prediction probability between 0 and 1 for all each of the 101 classes.</p> <p>And the index of the highest prediction probability can be considered what the model thinks is the most likely label. Similarly, the lower prediction probaiblity value, the less the model thinks that the target image is that specific class.</p> <p>\ud83d\udd11 Note: Due to the nature of the softmax activation function, the sum of each of the prediction probabilities for a single sample will be 1 (or at least very close to 1). E.g. <code>pred_probs[0].sum() = 1</code>.</p> <p>We can find the index of the maximum value in each prediction probability tensor using the <code>argmax()</code> method.</p> In\u00a0[30]: Copied! <pre># Get the class predicitons of each label\npred_classes = pred_probs.argmax(axis=1)\n\n# How do they look?\npred_classes[:10]\n</pre> # Get the class predicitons of each label pred_classes = pred_probs.argmax(axis=1)  # How do they look? pred_classes[:10] Out[30]: <pre>array([52,  0,  0, 80, 79, 61, 29,  0, 85,  0])</pre> <p>Beautiful! We've now got the predicted class index for each of the samples in our test dataset.</p> <p>We'll be able to compare these to the test dataset labels to further evaluate our model.</p> <p>To get the test dataset labels we can unravel our <code>test_data</code> object (which is in the form of a <code>tf.data.Dataset</code>) using the <code>unbatch()</code> method.</p> <p>Doing this will give us access to the images and labels in the test dataset. Since the labels are in one-hot encoded format, we'll take use the <code>argmax()</code> method to return the index of the label.</p> <p>\ud83d\udd11 Note: This unravelling is why we <code>shuffle=False</code> when creating the test data object. Otherwise, whenever we loaded the test dataset (like when making predictions), it would be shuffled every time, meaning if we tried to compare our predictions to the labels, they would be in different orders.</p> In\u00a0[31]: Copied! <pre># Note: This might take a minute or so due to unravelling 790 batches\ny_labels = []\nfor images, labels in test_data.unbatch(): # unbatch the test data and get images and labels\n  y_labels.append(labels.numpy().argmax()) # append the index which has the largest value (labels are one-hot)\ny_labels[:10] # check what they look like (unshuffled)\n</pre> # Note: This might take a minute or so due to unravelling 790 batches y_labels = [] for images, labels in test_data.unbatch(): # unbatch the test data and get images and labels   y_labels.append(labels.numpy().argmax()) # append the index which has the largest value (labels are one-hot) y_labels[:10] # check what they look like (unshuffled) Out[31]: <pre>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</pre> <p>Nice! Since <code>test_data</code> isn't shuffled, the <code>y_labels</code> array comes back in the same order as the <code>pred_classes</code> array.</p> <p>The final check is to see how many labels we've got.</p> In\u00a0[32]: Copied! <pre># How many labels are there? (should be the same as how many prediction probabilities we have)\nlen(y_labels)\n</pre> # How many labels are there? (should be the same as how many prediction probabilities we have) len(y_labels) Out[32]: <pre>25250</pre> <p>As expected, the number of labels matches the number of images we've got. Time to compare our model's predictions with the ground truth labels.</p> In\u00a0[33]: Copied! <pre># Get accuracy score by comparing predicted classes to ground truth labels\nfrom sklearn.metrics import accuracy_score\nsklearn_accuracy = accuracy_score(y_labels, pred_classes)\nsklearn_accuracy\n</pre> # Get accuracy score by comparing predicted classes to ground truth labels from sklearn.metrics import accuracy_score sklearn_accuracy = accuracy_score(y_labels, pred_classes) sklearn_accuracy Out[33]: <pre>0.6078019801980198</pre> In\u00a0[34]: Copied! <pre># Does the evaluate method compare to the Scikit-Learn measured accuracy?\nimport numpy as np\nprint(f\"Close? {np.isclose(loaded_accuracy, sklearn_accuracy)} | Difference: {loaded_accuracy - sklearn_accuracy}\")\n</pre> # Does the evaluate method compare to the Scikit-Learn measured accuracy? import numpy as np print(f\"Close? {np.isclose(loaded_accuracy, sklearn_accuracy)} | Difference: {loaded_accuracy - sklearn_accuracy}\") <pre>Close? True | Difference: -6.378287120689663e-09\n</pre> <p>Okay, it looks like our <code>pred_classes</code> array and <code>y_labels</code> arrays are in the right orders.</p> <p>How about we get a little bit more visual with a confusion matrix?</p> <p>To do so, we'll use our <code>make_confusion_matrix</code> function we created in a previous notebook.</p> In\u00a0[35]: Copied! <pre># We'll import our make_confusion_matrix function from https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/helper_functions.py \n# But if you run it out of the box, it doesn't really work for 101 classes...\n# the cell below adds a little functionality to make it readable.\nfrom helper_functions import make_confusion_matrix\n</pre> # We'll import our make_confusion_matrix function from https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/helper_functions.py  # But if you run it out of the box, it doesn't really work for 101 classes... # the cell below adds a little functionality to make it readable. from helper_functions import make_confusion_matrix In\u00a0[36]: Copied! <pre># Note: The following confusion matrix code is a remix of Scikit-Learn's \n# plot_confusion_matrix function - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html\nimport itertools\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\n\n# Our function needs a different name to sklearn's plot_confusion_matrix\ndef make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15, norm=False, savefig=False): \n  \"\"\"Makes a labelled confusion matrix comparing predictions and ground truth labels.\n\n  If classes is passed, confusion matrix will be labelled, if not, integer class values\n  will be used.\n\n  Args:\n    y_true: Array of truth labels (must be same shape as y_pred).\n    y_pred: Array of predicted labels (must be same shape as y_true).\n    classes: Array of class labels (e.g. string form). If `None`, integer labels are used.\n    figsize: Size of output figure (default=(10, 10)).\n    text_size: Size of output figure text (default=15).\n    norm: normalize values or not (default=False).\n    savefig: save confusion matrix to file (default=False).\n  \n  Returns:\n    A labelled confusion matrix plot comparing y_true and y_pred.\n\n  Example usage:\n    make_confusion_matrix(y_true=test_labels, # ground truth test labels\n                          y_pred=y_preds, # predicted labels\n                          classes=class_names, # array of class label names\n                          figsize=(15, 15),\n                          text_size=10)\n  \"\"\"  \n  # Create the confustion matrix\n  cm = confusion_matrix(y_true, y_pred)\n  cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it\n  n_classes = cm.shape[0] # find the number of classes we're dealing with\n\n  # Plot the figure and make it pretty\n  fig, ax = plt.subplots(figsize=figsize)\n  cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better\n  fig.colorbar(cax)\n\n  # Are there a list of classes?\n  if classes:\n    labels = classes\n  else:\n    labels = np.arange(cm.shape[0])\n  \n  # Label the axes\n  ax.set(title=\"Confusion Matrix\",\n         xlabel=\"Predicted label\",\n         ylabel=\"True label\",\n         xticks=np.arange(n_classes), # create enough axis slots for each class\n         yticks=np.arange(n_classes), \n         xticklabels=labels, # axes will labeled with class names (if they exist) or ints\n         yticklabels=labels)\n  \n  # Make x-axis labels appear on bottom\n  ax.xaxis.set_label_position(\"bottom\")\n  ax.xaxis.tick_bottom()\n\n  ### Added: Rotate xticks for readability &amp; increase font size (required due to such a large confusion matrix)\n  plt.xticks(rotation=70, fontsize=text_size)\n  plt.yticks(fontsize=text_size)\n\n  # Set the threshold for different colors\n  threshold = (cm.max() + cm.min()) / 2.\n\n  # Plot the text on each cell\n  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n    if norm:\n      plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",\n              horizontalalignment=\"center\",\n              color=\"white\" if cm[i, j] &gt; threshold else \"black\",\n              size=text_size)\n    else:\n      plt.text(j, i, f\"{cm[i, j]}\",\n              horizontalalignment=\"center\",\n              color=\"white\" if cm[i, j] &gt; threshold else \"black\",\n              size=text_size)\n\n  # Save the figure to the current working directory\n  if savefig:\n    fig.savefig(\"confusion_matrix.png\")\n</pre> # Note: The following confusion matrix code is a remix of Scikit-Learn's  # plot_confusion_matrix function - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html import itertools import matplotlib.pyplot as plt import numpy as np from sklearn.metrics import confusion_matrix  # Our function needs a different name to sklearn's plot_confusion_matrix def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15, norm=False, savefig=False):    \"\"\"Makes a labelled confusion matrix comparing predictions and ground truth labels.    If classes is passed, confusion matrix will be labelled, if not, integer class values   will be used.    Args:     y_true: Array of truth labels (must be same shape as y_pred).     y_pred: Array of predicted labels (must be same shape as y_true).     classes: Array of class labels (e.g. string form). If `None`, integer labels are used.     figsize: Size of output figure (default=(10, 10)).     text_size: Size of output figure text (default=15).     norm: normalize values or not (default=False).     savefig: save confusion matrix to file (default=False).      Returns:     A labelled confusion matrix plot comparing y_true and y_pred.    Example usage:     make_confusion_matrix(y_true=test_labels, # ground truth test labels                           y_pred=y_preds, # predicted labels                           classes=class_names, # array of class label names                           figsize=(15, 15),                           text_size=10)   \"\"\"     # Create the confustion matrix   cm = confusion_matrix(y_true, y_pred)   cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it   n_classes = cm.shape[0] # find the number of classes we're dealing with    # Plot the figure and make it pretty   fig, ax = plt.subplots(figsize=figsize)   cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better   fig.colorbar(cax)    # Are there a list of classes?   if classes:     labels = classes   else:     labels = np.arange(cm.shape[0])      # Label the axes   ax.set(title=\"Confusion Matrix\",          xlabel=\"Predicted label\",          ylabel=\"True label\",          xticks=np.arange(n_classes), # create enough axis slots for each class          yticks=np.arange(n_classes),           xticklabels=labels, # axes will labeled with class names (if they exist) or ints          yticklabels=labels)      # Make x-axis labels appear on bottom   ax.xaxis.set_label_position(\"bottom\")   ax.xaxis.tick_bottom()    ### Added: Rotate xticks for readability &amp; increase font size (required due to such a large confusion matrix)   plt.xticks(rotation=70, fontsize=text_size)   plt.yticks(fontsize=text_size)    # Set the threshold for different colors   threshold = (cm.max() + cm.min()) / 2.    # Plot the text on each cell   for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):     if norm:       plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",               horizontalalignment=\"center\",               color=\"white\" if cm[i, j] &gt; threshold else \"black\",               size=text_size)     else:       plt.text(j, i, f\"{cm[i, j]}\",               horizontalalignment=\"center\",               color=\"white\" if cm[i, j] &gt; threshold else \"black\",               size=text_size)    # Save the figure to the current working directory   if savefig:     fig.savefig(\"confusion_matrix.png\") <p>Right now our predictions and truth labels are in the form of integers, however, they'll be much easier to understand if we get their actual names. We can do so using the <code>class_names</code> attribute on our <code>test_data</code> object.</p> In\u00a0[54]: Copied! <pre># Get the class names\nclass_names = test_data.class_names\nclass_names[:10]\n</pre> # Get the class names class_names = test_data.class_names class_names[:10] Out[54]: <pre>['apple_pie',\n 'baby_back_ribs',\n 'baklava',\n 'beef_carpaccio',\n 'beef_tartare',\n 'beet_salad',\n 'beignets',\n 'bibimbap',\n 'bread_pudding',\n 'breakfast_burrito']</pre> <p>101 class names and 25250 predictions and ground truth labels ready to go! Looks like our confusion matrix is going to be a big one!</p> In\u00a0[38]: Copied! <pre># Plot a confusion matrix with all 25250 predictions, ground truth labels and 101 classes\nmake_confusion_matrix(y_true=y_labels,\n                      y_pred=pred_classes,\n                      classes=class_names,\n                      figsize=(100, 100),\n                      text_size=20,\n                      norm=False,\n                      savefig=True)\n</pre> # Plot a confusion matrix with all 25250 predictions, ground truth labels and 101 classes make_confusion_matrix(y_true=y_labels,                       y_pred=pred_classes,                       classes=class_names,                       figsize=(100, 100),                       text_size=20,                       norm=False,                       savefig=True) <p>Woah! Now that's a big confusion matrix. It may look a little daunting at first but after zooming in a little, we can see how it gives us insight into which classes its getting \"confused\" on.</p> <p>The good news is, the majority of the predictions are right down the top left to bottom right diagonal, meaning they're correct.</p> <p>It looks like the model gets most confused on classes which look visualually similar, such as predicting <code>filet_mignon</code> for instances of <code>pork_chop</code> and <code>chocolate_cake</code> for instances of <code>tiramisu</code>.</p> <p>Since we're working on a classification problem, we can further evaluate our model's predictions using Scikit-Learn's <code>classification_report()</code> function.</p> In\u00a0[39]: Copied! <pre>from sklearn.metrics import classification_report\nprint(classification_report(y_labels, pred_classes))\n</pre> from sklearn.metrics import classification_report print(classification_report(y_labels, pred_classes)) <pre>              precision    recall  f1-score   support\n\n           0       0.29      0.20      0.24       250\n           1       0.51      0.69      0.59       250\n           2       0.56      0.65      0.60       250\n           3       0.74      0.53      0.62       250\n           4       0.73      0.44      0.55       250\n           5       0.34      0.54      0.42       250\n           6       0.67      0.79      0.72       250\n           7       0.82      0.76      0.79       250\n           8       0.40      0.37      0.39       250\n           9       0.62      0.44      0.52       250\n          10       0.62      0.42      0.50       250\n          11       0.83      0.48      0.61       250\n          12       0.52      0.74      0.61       250\n          13       0.56      0.60      0.58       250\n          14       0.56      0.59      0.57       250\n          15       0.44      0.32      0.37       250\n          16       0.45      0.75      0.57       250\n          17       0.37      0.51      0.43       250\n          18       0.43      0.60      0.50       250\n          19       0.68      0.60      0.64       250\n          20       0.68      0.75      0.71       250\n          21       0.35      0.64      0.45       250\n          22       0.29      0.36      0.33       250\n          23       0.66      0.77      0.71       250\n          24       0.83      0.72      0.77       250\n          25       0.75      0.71      0.73       250\n          26       0.51      0.42      0.46       250\n          27       0.78      0.72      0.75       250\n          28       0.70      0.69      0.69       250\n          29       0.70      0.68      0.69       250\n          30       0.92      0.63      0.75       250\n          31       0.78      0.70      0.73       250\n          32       0.75      0.83      0.79       250\n          33       0.89      0.98      0.94       250\n          34       0.68      0.78      0.72       250\n          35       0.78      0.66      0.72       250\n          36       0.53      0.56      0.55       250\n          37       0.30      0.55      0.39       250\n          38       0.78      0.63      0.69       250\n          39       0.27      0.33      0.30       250\n          40       0.72      0.81      0.76       250\n          41       0.81      0.62      0.70       250\n          42       0.50      0.58      0.54       250\n          43       0.75      0.60      0.67       250\n          44       0.74      0.45      0.56       250\n          45       0.77      0.85      0.81       250\n          46       0.80      0.46      0.58       250\n          47       0.44      0.49      0.46       250\n          48       0.45      0.82      0.58       250\n          49       0.50      0.44      0.47       250\n          50       0.54      0.39      0.46       250\n          51       0.71      0.86      0.78       250\n          52       0.51      0.77      0.61       250\n          53       0.67      0.68      0.68       250\n          54       0.88      0.75      0.81       250\n          55       0.86      0.69      0.76       250\n          56       0.56      0.24      0.34       250\n          57       0.62      0.45      0.52       250\n          58       0.68      0.58      0.62       250\n          59       0.70      0.37      0.49       250\n          60       0.83      0.59      0.69       250\n          61       0.54      0.81      0.65       250\n          62       0.72      0.49      0.58       250\n          63       0.94      0.86      0.90       250\n          64       0.78      0.85      0.81       250\n          65       0.82      0.82      0.82       250\n          66       0.69      0.33      0.45       250\n          67       0.41      0.57      0.48       250\n          68       0.90      0.78      0.83       250\n          69       0.84      0.82      0.83       250\n          70       0.62      0.83      0.71       250\n          71       0.81      0.46      0.59       250\n          72       0.64      0.65      0.64       250\n          73       0.51      0.44      0.47       250\n          74       0.72      0.61      0.66       250\n          75       0.85      0.90      0.87       250\n          76       0.78      0.79      0.78       250\n          77       0.36      0.27      0.31       250\n          78       0.79      0.74      0.76       250\n          79       0.44      0.81      0.57       250\n          80       0.57      0.60      0.59       250\n          81       0.65      0.70      0.68       250\n          82       0.38      0.31      0.34       250\n          83       0.58      0.80      0.67       250\n          84       0.61      0.38      0.47       250\n          85       0.44      0.74      0.55       250\n          86       0.71      0.86      0.78       250\n          87       0.41      0.39      0.40       250\n          88       0.83      0.80      0.81       250\n          89       0.71      0.31      0.43       250\n          90       0.92      0.69      0.79       250\n          91       0.83      0.87      0.85       250\n          92       0.68      0.65      0.67       250\n          93       0.31      0.38      0.34       250\n          94       0.61      0.54      0.57       250\n          95       0.74      0.61      0.67       250\n          96       0.56      0.29      0.38       250\n          97       0.46      0.74      0.57       250\n          98       0.47      0.33      0.38       250\n          99       0.52      0.27      0.35       250\n         100       0.59      0.70      0.64       250\n\n    accuracy                           0.61     25250\n   macro avg       0.63      0.61      0.61     25250\nweighted avg       0.63      0.61      0.61     25250\n\n</pre> <p>The <code>classification_report()</code> outputs the precision, recall and f1-score's per class.</p> <p>A reminder:</p> <ul> <li>Precision - Proportion of true positives over total number of samples. Higher precision leads to less false positives (model predicts 1 when it should've been 0).</li> <li>Recall - Proportion of true positives over total number of true positives and false negatives (model predicts 0 when it should've been 1). Higher recall leads to less false negatives.</li> <li>F1 score - Combines precision and recall into one metric. 1 is best, 0 is worst.</li> </ul> <p>The above output is helpful but with so many classes, it's a bit hard to understand.</p> <p>Let's see if we make it easier with the help of a visualization.</p> <p>First, we'll get the output of <code>classification_report()</code> as a dictionary by setting <code>output_dict=True</code>.</p> In\u00a0[58]: Copied! <pre># Get a dictionary of the classification report\nclassification_report_dict = classification_report(y_labels, pred_classes, output_dict=True)\nclassification_report_dict\n</pre> # Get a dictionary of the classification report classification_report_dict = classification_report(y_labels, pred_classes, output_dict=True) classification_report_dict Out[58]: <pre>{'0': {'precision': 0.29310344827586204,\n  'recall': 0.204,\n  'f1-score': 0.24056603773584903,\n  'support': 250},\n '1': {'precision': 0.5088235294117647,\n  'recall': 0.692,\n  'f1-score': 0.5864406779661017,\n  'support': 250},\n '2': {'precision': 0.5625,\n  'recall': 0.648,\n  'f1-score': 0.6022304832713754,\n  'support': 250},\n '3': {'precision': 0.7415730337078652,\n  'recall': 0.528,\n  'f1-score': 0.616822429906542,\n  'support': 250},\n '4': {'precision': 0.7315436241610739,\n  'recall': 0.436,\n  'f1-score': 0.5463659147869674,\n  'support': 250},\n '5': {'precision': 0.3426395939086294,\n  'recall': 0.54,\n  'f1-score': 0.4192546583850932,\n  'support': 250},\n '6': {'precision': 0.6700680272108843,\n  'recall': 0.788,\n  'f1-score': 0.724264705882353,\n  'support': 250},\n '7': {'precision': 0.8197424892703863,\n  'recall': 0.764,\n  'f1-score': 0.7908902691511386,\n  'support': 250},\n '8': {'precision': 0.4025974025974026,\n  'recall': 0.372,\n  'f1-score': 0.3866943866943867,\n  'support': 250},\n '9': {'precision': 0.6214689265536724,\n  'recall': 0.44,\n  'f1-score': 0.5152224824355972,\n  'support': 250},\n '10': {'precision': 0.6235294117647059,\n  'recall': 0.424,\n  'f1-score': 0.5047619047619047,\n  'support': 250},\n '11': {'precision': 0.8344827586206897,\n  'recall': 0.484,\n  'f1-score': 0.6126582278481012,\n  'support': 250},\n '12': {'precision': 0.5211267605633803,\n  'recall': 0.74,\n  'f1-score': 0.6115702479338843,\n  'support': 250},\n '13': {'precision': 0.5601503759398496,\n  'recall': 0.596,\n  'f1-score': 0.5775193798449612,\n  'support': 250},\n '14': {'precision': 0.5584905660377358,\n  'recall': 0.592,\n  'f1-score': 0.574757281553398,\n  'support': 250},\n '15': {'precision': 0.4388888888888889,\n  'recall': 0.316,\n  'f1-score': 0.36744186046511623,\n  'support': 250},\n '16': {'precision': 0.4530120481927711,\n  'recall': 0.752,\n  'f1-score': 0.5654135338345864,\n  'support': 250},\n '17': {'precision': 0.3681159420289855,\n  'recall': 0.508,\n  'f1-score': 0.42689075630252105,\n  'support': 250},\n '18': {'precision': 0.4318840579710145,\n  'recall': 0.596,\n  'f1-score': 0.5008403361344538,\n  'support': 250},\n '19': {'precision': 0.6832579185520362,\n  'recall': 0.604,\n  'f1-score': 0.6411889596602972,\n  'support': 250},\n '20': {'precision': 0.68,\n  'recall': 0.748,\n  'f1-score': 0.7123809523809523,\n  'support': 250},\n '21': {'precision': 0.350109409190372,\n  'recall': 0.64,\n  'f1-score': 0.45261669024045265,\n  'support': 250},\n '22': {'precision': 0.29449838187702265,\n  'recall': 0.364,\n  'f1-score': 0.3255813953488372,\n  'support': 250},\n '23': {'precision': 0.6632302405498282,\n  'recall': 0.772,\n  'f1-score': 0.7134935304990757,\n  'support': 250},\n '24': {'precision': 0.8294930875576036,\n  'recall': 0.72,\n  'f1-score': 0.7708779443254817,\n  'support': 250},\n '25': {'precision': 0.7542372881355932,\n  'recall': 0.712,\n  'f1-score': 0.7325102880658436,\n  'support': 250},\n '26': {'precision': 0.5121951219512195,\n  'recall': 0.42,\n  'f1-score': 0.46153846153846156,\n  'support': 250},\n '27': {'precision': 0.776824034334764,\n  'recall': 0.724,\n  'f1-score': 0.7494824016563146,\n  'support': 250},\n '28': {'precision': 0.7020408163265306,\n  'recall': 0.688,\n  'f1-score': 0.6949494949494949,\n  'support': 250},\n '29': {'precision': 0.7024793388429752,\n  'recall': 0.68,\n  'f1-score': 0.6910569105691057,\n  'support': 250},\n '30': {'precision': 0.9235294117647059,\n  'recall': 0.628,\n  'f1-score': 0.7476190476190476,\n  'support': 250},\n '31': {'precision': 0.7767857142857143,\n  'recall': 0.696,\n  'f1-score': 0.7341772151898734,\n  'support': 250},\n '32': {'precision': 0.7472924187725631,\n  'recall': 0.828,\n  'f1-score': 0.7855787476280836,\n  'support': 250},\n '33': {'precision': 0.8913043478260869,\n  'recall': 0.984,\n  'f1-score': 0.9353612167300379,\n  'support': 250},\n '34': {'precision': 0.6783216783216783,\n  'recall': 0.776,\n  'f1-score': 0.7238805970149255,\n  'support': 250},\n '35': {'precision': 0.7819905213270142,\n  'recall': 0.66,\n  'f1-score': 0.715835140997831,\n  'support': 250},\n '36': {'precision': 0.5340909090909091,\n  'recall': 0.564,\n  'f1-score': 0.5486381322957198,\n  'support': 250},\n '37': {'precision': 0.29782608695652174,\n  'recall': 0.548,\n  'f1-score': 0.38591549295774646,\n  'support': 250},\n '38': {'precision': 0.7772277227722773,\n  'recall': 0.628,\n  'f1-score': 0.6946902654867257,\n  'support': 250},\n '39': {'precision': 0.2703583061889251,\n  'recall': 0.332,\n  'f1-score': 0.29802513464991026,\n  'support': 250},\n '40': {'precision': 0.7214285714285714,\n  'recall': 0.808,\n  'f1-score': 0.7622641509433963,\n  'support': 250},\n '41': {'precision': 0.8105263157894737,\n  'recall': 0.616,\n  'f1-score': 0.7,\n  'support': 250},\n '42': {'precision': 0.5017182130584192,\n  'recall': 0.584,\n  'f1-score': 0.5397412199630314,\n  'support': 250},\n '43': {'precision': 0.746268656716418,\n  'recall': 0.6,\n  'f1-score': 0.6651884700665188,\n  'support': 250},\n '44': {'precision': 0.7417218543046358,\n  'recall': 0.448,\n  'f1-score': 0.5586034912718205,\n  'support': 250},\n '45': {'precision': 0.7717391304347826,\n  'recall': 0.852,\n  'f1-score': 0.8098859315589354,\n  'support': 250},\n '46': {'precision': 0.8028169014084507,\n  'recall': 0.456,\n  'f1-score': 0.5816326530612245,\n  'support': 250},\n '47': {'precision': 0.4392857142857143,\n  'recall': 0.492,\n  'f1-score': 0.4641509433962264,\n  'support': 250},\n '48': {'precision': 0.44835164835164837,\n  'recall': 0.816,\n  'f1-score': 0.5787234042553191,\n  'support': 250},\n '49': {'precision': 0.5045454545454545,\n  'recall': 0.444,\n  'f1-score': 0.47234042553191485,\n  'support': 250},\n '50': {'precision': 0.5444444444444444,\n  'recall': 0.392,\n  'f1-score': 0.45581395348837206,\n  'support': 250},\n '51': {'precision': 0.7081967213114754,\n  'recall': 0.864,\n  'f1-score': 0.7783783783783783,\n  'support': 250},\n '52': {'precision': 0.5092838196286472,\n  'recall': 0.768,\n  'f1-score': 0.6124401913875598,\n  'support': 250},\n '53': {'precision': 0.6719367588932806,\n  'recall': 0.68,\n  'f1-score': 0.6759443339960238,\n  'support': 250},\n '54': {'precision': 0.8785046728971962,\n  'recall': 0.752,\n  'f1-score': 0.8103448275862069,\n  'support': 250},\n '55': {'precision': 0.86,\n  'recall': 0.688,\n  'f1-score': 0.7644444444444444,\n  'support': 250},\n '56': {'precision': 0.5607476635514018,\n  'recall': 0.24,\n  'f1-score': 0.3361344537815126,\n  'support': 250},\n '57': {'precision': 0.6187845303867403,\n  'recall': 0.448,\n  'f1-score': 0.5197215777262181,\n  'support': 250},\n '58': {'precision': 0.6792452830188679,\n  'recall': 0.576,\n  'f1-score': 0.6233766233766233,\n  'support': 250},\n '59': {'precision': 0.7045454545454546,\n  'recall': 0.372,\n  'f1-score': 0.486910994764398,\n  'support': 250},\n '60': {'precision': 0.8305084745762712,\n  'recall': 0.588,\n  'f1-score': 0.6885245901639344,\n  'support': 250},\n '61': {'precision': 0.543010752688172,\n  'recall': 0.808,\n  'f1-score': 0.6495176848874598,\n  'support': 250},\n '62': {'precision': 0.7218934911242604,\n  'recall': 0.488,\n  'f1-score': 0.5823389021479712,\n  'support': 250},\n '63': {'precision': 0.9385964912280702,\n  'recall': 0.856,\n  'f1-score': 0.895397489539749,\n  'support': 250},\n '64': {'precision': 0.7773722627737226,\n  'recall': 0.852,\n  'f1-score': 0.8129770992366412,\n  'support': 250},\n '65': {'precision': 0.82, 'recall': 0.82, 'f1-score': 0.82, 'support': 250},\n '66': {'precision': 0.6949152542372882,\n  'recall': 0.328,\n  'f1-score': 0.4456521739130435,\n  'support': 250},\n '67': {'precision': 0.4074074074074074,\n  'recall': 0.572,\n  'f1-score': 0.47587354409317806,\n  'support': 250},\n '68': {'precision': 0.8981481481481481,\n  'recall': 0.776,\n  'f1-score': 0.832618025751073,\n  'support': 250},\n '69': {'precision': 0.8442622950819673,\n  'recall': 0.824,\n  'f1-score': 0.8340080971659919,\n  'support': 250},\n '70': {'precision': 0.6216216216216216,\n  'recall': 0.828,\n  'f1-score': 0.7101200686106347,\n  'support': 250},\n '71': {'precision': 0.8111888111888111,\n  'recall': 0.464,\n  'f1-score': 0.5903307888040712,\n  'support': 250},\n '72': {'precision': 0.6403162055335968,\n  'recall': 0.648,\n  'f1-score': 0.6441351888667992,\n  'support': 250},\n '73': {'precision': 0.5091743119266054,\n  'recall': 0.444,\n  'f1-score': 0.4743589743589744,\n  'support': 250},\n '74': {'precision': 0.7169811320754716,\n  'recall': 0.608,\n  'f1-score': 0.658008658008658,\n  'support': 250},\n '75': {'precision': 0.8452830188679246,\n  'recall': 0.896,\n  'f1-score': 0.8699029126213592,\n  'support': 250},\n '76': {'precision': 0.7786561264822134,\n  'recall': 0.788,\n  'f1-score': 0.7833001988071571,\n  'support': 250},\n '77': {'precision': 0.3641304347826087,\n  'recall': 0.268,\n  'f1-score': 0.30875576036866365,\n  'support': 250},\n '78': {'precision': 0.7863247863247863,\n  'recall': 0.736,\n  'f1-score': 0.7603305785123966,\n  'support': 250},\n '79': {'precision': 0.44130434782608696,\n  'recall': 0.812,\n  'f1-score': 0.571830985915493,\n  'support': 250},\n '80': {'precision': 0.5747126436781609,\n  'recall': 0.6,\n  'f1-score': 0.5870841487279843,\n  'support': 250},\n '81': {'precision': 0.654275092936803,\n  'recall': 0.704,\n  'f1-score': 0.6782273603082851,\n  'support': 250},\n '82': {'precision': 0.3804878048780488,\n  'recall': 0.312,\n  'f1-score': 0.34285714285714286,\n  'support': 250},\n '83': {'precision': 0.5763688760806917,\n  'recall': 0.8,\n  'f1-score': 0.6700167504187604,\n  'support': 250},\n '84': {'precision': 0.6103896103896104,\n  'recall': 0.376,\n  'f1-score': 0.4653465346534653,\n  'support': 250},\n '85': {'precision': 0.4423076923076923,\n  'recall': 0.736,\n  'f1-score': 0.5525525525525525,\n  'support': 250},\n '86': {'precision': 0.7105263157894737,\n  'recall': 0.864,\n  'f1-score': 0.779783393501805,\n  'support': 250},\n '87': {'precision': 0.40756302521008403,\n  'recall': 0.388,\n  'f1-score': 0.3975409836065574,\n  'support': 250},\n '88': {'precision': 0.8298755186721992,\n  'recall': 0.8,\n  'f1-score': 0.8146639511201631,\n  'support': 250},\n '89': {'precision': 0.7129629629629629,\n  'recall': 0.308,\n  'f1-score': 0.4301675977653631,\n  'support': 250},\n '90': {'precision': 0.9153439153439153,\n  'recall': 0.692,\n  'f1-score': 0.7881548974943051,\n  'support': 250},\n '91': {'precision': 0.8282442748091603,\n  'recall': 0.868,\n  'f1-score': 0.84765625,\n  'support': 250},\n '92': {'precision': 0.6835443037974683,\n  'recall': 0.648,\n  'f1-score': 0.6652977412731006,\n  'support': 250},\n '93': {'precision': 0.3125,\n  'recall': 0.38,\n  'f1-score': 0.34296028880866425,\n  'support': 250},\n '94': {'precision': 0.6118721461187214,\n  'recall': 0.536,\n  'f1-score': 0.5714285714285714,\n  'support': 250},\n '95': {'precision': 0.7427184466019418,\n  'recall': 0.612,\n  'f1-score': 0.6710526315789473,\n  'support': 250},\n '96': {'precision': 0.5625,\n  'recall': 0.288,\n  'f1-score': 0.3809523809523809,\n  'support': 250},\n '97': {'precision': 0.45588235294117646,\n  'recall': 0.744,\n  'f1-score': 0.5653495440729484,\n  'support': 250},\n '98': {'precision': 0.4659090909090909,\n  'recall': 0.328,\n  'f1-score': 0.38497652582159625,\n  'support': 250},\n '99': {'precision': 0.5193798449612403,\n  'recall': 0.268,\n  'f1-score': 0.35356200527704484,\n  'support': 250},\n '100': {'precision': 0.5912162162162162,\n  'recall': 0.7,\n  'f1-score': 0.641025641025641,\n  'support': 250},\n 'accuracy': 0.6078019801980198,\n 'macro avg': {'precision': 0.6328178312597097,\n  'recall': 0.6078019801980199,\n  'f1-score': 0.6061453730563883,\n  'support': 25250},\n 'weighted avg': {'precision': 0.6328178312597095,\n  'recall': 0.6078019801980198,\n  'f1-score': 0.6061453730563882,\n  'support': 25250}}</pre> <p>Alright, there's still a fair few values here, how about we narrow down?</p> <p>Since the f1-score combines precision and recall in one metric, let's focus on that.</p> <p>To extract it, we'll create an empty dictionary called <code>class_f1_scores</code> and then loop through each item in <code>classification_report_dict</code>, appending the class name and f1-score as the key, value pairs in <code>class_f1_scores</code>.</p> In\u00a0[41]: Copied! <pre># Create empty dictionary\nclass_f1_scores = {}\n# Loop through classification report items\nfor k, v in classification_report_dict.items():\n  if k == \"accuracy\": # stop once we get to accuracy key\n    break\n  else:\n    # Append class names and f1-scores to new dictionary\n    class_f1_scores[class_names[int(k)]] = v[\"f1-score\"]\nclass_f1_scores\n</pre> # Create empty dictionary class_f1_scores = {} # Loop through classification report items for k, v in classification_report_dict.items():   if k == \"accuracy\": # stop once we get to accuracy key     break   else:     # Append class names and f1-scores to new dictionary     class_f1_scores[class_names[int(k)]] = v[\"f1-score\"] class_f1_scores Out[41]: <pre>{'apple_pie': 0.24056603773584903,\n 'baby_back_ribs': 0.5864406779661017,\n 'baklava': 0.6022304832713754,\n 'beef_carpaccio': 0.616822429906542,\n 'beef_tartare': 0.5463659147869674,\n 'beet_salad': 0.4192546583850932,\n 'beignets': 0.724264705882353,\n 'bibimbap': 0.7908902691511386,\n 'bread_pudding': 0.3866943866943867,\n 'breakfast_burrito': 0.5152224824355972,\n 'bruschetta': 0.5047619047619047,\n 'caesar_salad': 0.6126582278481012,\n 'cannoli': 0.6115702479338843,\n 'caprese_salad': 0.5775193798449612,\n 'carrot_cake': 0.574757281553398,\n 'ceviche': 0.36744186046511623,\n 'cheese_plate': 0.5654135338345864,\n 'cheesecake': 0.42689075630252105,\n 'chicken_curry': 0.5008403361344538,\n 'chicken_quesadilla': 0.6411889596602972,\n 'chicken_wings': 0.7123809523809523,\n 'chocolate_cake': 0.45261669024045265,\n 'chocolate_mousse': 0.3255813953488372,\n 'churros': 0.7134935304990757,\n 'clam_chowder': 0.7708779443254817,\n 'club_sandwich': 0.7325102880658436,\n 'crab_cakes': 0.46153846153846156,\n 'creme_brulee': 0.7494824016563146,\n 'croque_madame': 0.6949494949494949,\n 'cup_cakes': 0.6910569105691057,\n 'deviled_eggs': 0.7476190476190476,\n 'donuts': 0.7341772151898734,\n 'dumplings': 0.7855787476280836,\n 'edamame': 0.9353612167300379,\n 'eggs_benedict': 0.7238805970149255,\n 'escargots': 0.715835140997831,\n 'falafel': 0.5486381322957198,\n 'filet_mignon': 0.38591549295774646,\n 'fish_and_chips': 0.6946902654867257,\n 'foie_gras': 0.29802513464991026,\n 'french_fries': 0.7622641509433963,\n 'french_onion_soup': 0.7,\n 'french_toast': 0.5397412199630314,\n 'fried_calamari': 0.6651884700665188,\n 'fried_rice': 0.5586034912718205,\n 'frozen_yogurt': 0.8098859315589354,\n 'garlic_bread': 0.5816326530612245,\n 'gnocchi': 0.4641509433962264,\n 'greek_salad': 0.5787234042553191,\n 'grilled_cheese_sandwich': 0.47234042553191485,\n 'grilled_salmon': 0.45581395348837206,\n 'guacamole': 0.7783783783783783,\n 'gyoza': 0.6124401913875598,\n 'hamburger': 0.6759443339960238,\n 'hot_and_sour_soup': 0.8103448275862069,\n 'hot_dog': 0.7644444444444444,\n 'huevos_rancheros': 0.3361344537815126,\n 'hummus': 0.5197215777262181,\n 'ice_cream': 0.6233766233766233,\n 'lasagna': 0.486910994764398,\n 'lobster_bisque': 0.6885245901639344,\n 'lobster_roll_sandwich': 0.6495176848874598,\n 'macaroni_and_cheese': 0.5823389021479712,\n 'macarons': 0.895397489539749,\n 'miso_soup': 0.8129770992366412,\n 'mussels': 0.82,\n 'nachos': 0.4456521739130435,\n 'omelette': 0.47587354409317806,\n 'onion_rings': 0.832618025751073,\n 'oysters': 0.8340080971659919,\n 'pad_thai': 0.7101200686106347,\n 'paella': 0.5903307888040712,\n 'pancakes': 0.6441351888667992,\n 'panna_cotta': 0.4743589743589744,\n 'peking_duck': 0.658008658008658,\n 'pho': 0.8699029126213592,\n 'pizza': 0.7833001988071571,\n 'pork_chop': 0.30875576036866365,\n 'poutine': 0.7603305785123966,\n 'prime_rib': 0.571830985915493,\n 'pulled_pork_sandwich': 0.5870841487279843,\n 'ramen': 0.6782273603082851,\n 'ravioli': 0.34285714285714286,\n 'red_velvet_cake': 0.6700167504187604,\n 'risotto': 0.4653465346534653,\n 'samosa': 0.5525525525525525,\n 'sashimi': 0.779783393501805,\n 'scallops': 0.3975409836065574,\n 'seaweed_salad': 0.8146639511201631,\n 'shrimp_and_grits': 0.4301675977653631,\n 'spaghetti_bolognese': 0.7881548974943051,\n 'spaghetti_carbonara': 0.84765625,\n 'spring_rolls': 0.6652977412731006,\n 'steak': 0.34296028880866425,\n 'strawberry_shortcake': 0.5714285714285714,\n 'sushi': 0.6710526315789473,\n 'tacos': 0.3809523809523809,\n 'takoyaki': 0.5653495440729484,\n 'tiramisu': 0.38497652582159625,\n 'tuna_tartare': 0.35356200527704484,\n 'waffles': 0.641025641025641}</pre> <p>Looking good!</p> <p>It seems like our dictionary is ordered by the class names. However, I think if we're trying to visualize different scores, it might look nicer if they were in some kind of order.</p> <p>How about we turn our <code>class_f1_scores</code> dictionary into a pandas DataFrame and sort it in ascending fashion?</p> In\u00a0[59]: Copied! <pre># Turn f1-scores into dataframe for visualization\nimport pandas as pd\nf1_scores = pd.DataFrame({\"class_name\": list(class_f1_scores.keys()),\n                          \"f1-score\": list(class_f1_scores.values())}).sort_values(\"f1-score\", ascending=False)\nf1_scores.head()\n</pre> # Turn f1-scores into dataframe for visualization import pandas as pd f1_scores = pd.DataFrame({\"class_name\": list(class_f1_scores.keys()),                           \"f1-score\": list(class_f1_scores.values())}).sort_values(\"f1-score\", ascending=False) f1_scores.head() Out[59]: class_name f1-score 33 edamame 0.935361 63 macarons 0.895397 75 pho 0.869903 91 spaghetti_carbonara 0.847656 69 oysters 0.834008 <p>Now we're talking! Let's finish it off with a nice horizontal bar chart.</p> In\u00a0[43]: Copied! <pre>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(12, 25))\nscores = ax.barh(range(len(f1_scores)), f1_scores[\"f1-score\"].values)\nax.set_yticks(range(len(f1_scores)))\nax.set_yticklabels(list(f1_scores[\"class_name\"]))\nax.set_xlabel(\"f1-score\")\nax.set_title(\"F1-Scores for 10 Different Classes\")\nax.invert_yaxis(); # reverse the order\n\ndef autolabel(rects): # Modified version of: https://matplotlib.org/examples/api/barchart_demo.html\n  \"\"\"\n  Attach a text label above each bar displaying its height (it's value).\n  \"\"\"\n  for rect in rects:\n    width = rect.get_width()\n    ax.text(1.03*width, rect.get_y() + rect.get_height()/1.5,\n            f\"{width:.2f}\",\n            ha='center', va='bottom')\n\nautolabel(scores)\n</pre> import matplotlib.pyplot as plt  fig, ax = plt.subplots(figsize=(12, 25)) scores = ax.barh(range(len(f1_scores)), f1_scores[\"f1-score\"].values) ax.set_yticks(range(len(f1_scores))) ax.set_yticklabels(list(f1_scores[\"class_name\"])) ax.set_xlabel(\"f1-score\") ax.set_title(\"F1-Scores for 10 Different Classes\") ax.invert_yaxis(); # reverse the order  def autolabel(rects): # Modified version of: https://matplotlib.org/examples/api/barchart_demo.html   \"\"\"   Attach a text label above each bar displaying its height (it's value).   \"\"\"   for rect in rects:     width = rect.get_width()     ax.text(1.03*width, rect.get_y() + rect.get_height()/1.5,             f\"{width:.2f}\",             ha='center', va='bottom')  autolabel(scores) <p>Now that's a good looking graph! I mean, the text positioning could be improved a little but it'll do for now.</p> <p>Can you see how visualizing our model's predictions gives us a completely new insight into its performance?</p> <p>A few moments ago we only had an accuracy score but now we've got an indiciation of how well our model is performing on a class by class basis.</p> <p>It seems like our model performs fairly poorly on classes like <code>apple_pie</code> and <code>ravioli</code> while for classes like <code>edamame</code> and <code>pho</code> the performance is quite outstanding.</p> <p>Findings like these give us clues into where we could go next with our experiments. Perhaps we may have to collect more data on poor performing classes or perhaps the worst performing classes are just hard to make predictions on.</p> <p>\ud83d\udee0 Exercise: Visualize some of the most poor performing classes, do you notice any trends among them?</p> In\u00a0[44]: Copied! <pre>def load_and_prep_image(filename, img_shape=224, scale=True):\n  \"\"\"\n  Reads in an image from filename, turns it into a tensor and reshapes into\n  (224, 224, 3).\n\n  Parameters\n  ----------\n  filename (str): string filename of target image\n  img_shape (int): size to resize target image to, default 224\n  scale (bool): whether to scale pixel values to range(0, 1), default True\n  \"\"\"\n  # Read in the image\n  img = tf.io.read_file(filename)\n  # Decode it into a tensor\n  img = tf.io.decode_image(img)\n  # Resize the image\n  img = tf.image.resize(img, [img_shape, img_shape])\n  if scale:\n    # Rescale the image (get all values between 0 and 1)\n    return img/255.\n  else:\n    return img\n</pre> def load_and_prep_image(filename, img_shape=224, scale=True):   \"\"\"   Reads in an image from filename, turns it into a tensor and reshapes into   (224, 224, 3).    Parameters   ----------   filename (str): string filename of target image   img_shape (int): size to resize target image to, default 224   scale (bool): whether to scale pixel values to range(0, 1), default True   \"\"\"   # Read in the image   img = tf.io.read_file(filename)   # Decode it into a tensor   img = tf.io.decode_image(img)   # Resize the image   img = tf.image.resize(img, [img_shape, img_shape])   if scale:     # Rescale the image (get all values between 0 and 1)     return img/255.   else:     return img <p>Image loading and preprocessing function ready.</p> <p>Now let's write some code to:</p> <ol> <li>Load a few random images from the test dataset.</li> <li>Make predictions on them.</li> <li>Plot the original image(s) along with the model's predicted label, prediction probability and ground truth label.</li> </ol> In\u00a0[45]: Copied! <pre># Make preds on a series of random images\nimport os\nimport random\n\nplt.figure(figsize=(17, 10))\nfor i in range(3):\n  # Choose a random image from a random class \n  class_name = random.choice(class_names)\n  filename = random.choice(os.listdir(test_dir + \"/\" + class_name))\n  filepath = test_dir + class_name + \"/\" + filename\n\n  # Load the image and make predictions\n  img = load_and_prep_image(filepath, scale=False) # don't scale images for EfficientNet predictions\n  pred_prob = model.predict(tf.expand_dims(img, axis=0)) # model accepts tensors of shape [None, 224, 224, 3]\n  pred_class = class_names[pred_prob.argmax()] # find the predicted class \n\n  # Plot the image(s)\n  plt.subplot(1, 3, i+1)\n  plt.imshow(img/255.)\n  if class_name == pred_class: # Change the color of text based on whether prediction is right or wrong\n    title_color = \"g\"\n  else:\n    title_color = \"r\"\n  plt.title(f\"actual: {class_name}, pred: {pred_class}, prob: {pred_prob.max():.2f}\", c=title_color)\n  plt.axis(False);\n</pre> # Make preds on a series of random images import os import random  plt.figure(figsize=(17, 10)) for i in range(3):   # Choose a random image from a random class    class_name = random.choice(class_names)   filename = random.choice(os.listdir(test_dir + \"/\" + class_name))   filepath = test_dir + class_name + \"/\" + filename    # Load the image and make predictions   img = load_and_prep_image(filepath, scale=False) # don't scale images for EfficientNet predictions   pred_prob = model.predict(tf.expand_dims(img, axis=0)) # model accepts tensors of shape [None, 224, 224, 3]   pred_class = class_names[pred_prob.argmax()] # find the predicted class     # Plot the image(s)   plt.subplot(1, 3, i+1)   plt.imshow(img/255.)   if class_name == pred_class: # Change the color of text based on whether prediction is right or wrong     title_color = \"g\"   else:     title_color = \"r\"   plt.title(f\"actual: {class_name}, pred: {pred_class}, prob: {pred_prob.max():.2f}\", c=title_color)   plt.axis(False); <pre>1/1 [==============================] - 1s 1s/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 28ms/step\n</pre> <p>After going through enough random samples, it starts to become clear that the model tends to make far worse predictions on classes which are visually similar such as <code>baby_back_ribs</code> getting mistaken as <code>steak</code> and vice versa.</p> In\u00a0[46]: Copied! <pre># 1. Get the filenames of all of our test data\nfilepaths = []\nfor filepath in test_data.list_files(\"101_food_classes_10_percent/test/*/*.jpg\", \n                                     shuffle=False):\n  filepaths.append(filepath.numpy())\nfilepaths[:10]\n</pre> # 1. Get the filenames of all of our test data filepaths = [] for filepath in test_data.list_files(\"101_food_classes_10_percent/test/*/*.jpg\",                                       shuffle=False):   filepaths.append(filepath.numpy()) filepaths[:10] Out[46]: <pre>[b'101_food_classes_10_percent/test/apple_pie/1011328.jpg',\n b'101_food_classes_10_percent/test/apple_pie/101251.jpg',\n b'101_food_classes_10_percent/test/apple_pie/1034399.jpg',\n b'101_food_classes_10_percent/test/apple_pie/103801.jpg',\n b'101_food_classes_10_percent/test/apple_pie/1038694.jpg',\n b'101_food_classes_10_percent/test/apple_pie/1047447.jpg',\n b'101_food_classes_10_percent/test/apple_pie/1068632.jpg',\n b'101_food_classes_10_percent/test/apple_pie/110043.jpg',\n b'101_food_classes_10_percent/test/apple_pie/1106961.jpg',\n b'101_food_classes_10_percent/test/apple_pie/1113017.jpg']</pre> <p>Now we've got all of the test image filepaths, let's combine them into a DataFrame along with:</p> <ul> <li>Their ground truth labels (<code>y_labels</code>).</li> <li>The class the model predicted (<code>pred_classes</code>).</li> <li>The maximum prediction probabilitity value (<code>pred_probs.max(axis=1)</code>).</li> <li>The ground truth class names.</li> <li>The predicted class names.</li> </ul> In\u00a0[47]: Copied! <pre># 2. Create a dataframe out of current prediction data for analysis\nimport pandas as pd\npred_df = pd.DataFrame({\"img_path\": filepaths,\n                        \"y_true\": y_labels,\n                        \"y_pred\": pred_classes,\n                        \"pred_conf\": pred_probs.max(axis=1), # get the maximum prediction probability value\n                        \"y_true_classname\": [class_names[i] for i in y_labels],\n                        \"y_pred_classname\": [class_names[i] for i in pred_classes]}) \npred_df.head()\n</pre> # 2. Create a dataframe out of current prediction data for analysis import pandas as pd pred_df = pd.DataFrame({\"img_path\": filepaths,                         \"y_true\": y_labels,                         \"y_pred\": pred_classes,                         \"pred_conf\": pred_probs.max(axis=1), # get the maximum prediction probability value                         \"y_true_classname\": [class_names[i] for i in y_labels],                         \"y_pred_classname\": [class_names[i] for i in pred_classes]})  pred_df.head() Out[47]: img_path y_true y_pred pred_conf y_true_classname y_pred_classname 0 b'101_food_classes_10_percent/test/apple_pie/1... 0 52 0.847566 apple_pie gyoza 1 b'101_food_classes_10_percent/test/apple_pie/1... 0 0 0.963383 apple_pie apple_pie 2 b'101_food_classes_10_percent/test/apple_pie/1... 0 0 0.959426 apple_pie apple_pie 3 b'101_food_classes_10_percent/test/apple_pie/1... 0 80 0.656333 apple_pie pulled_pork_sandwich 4 b'101_food_classes_10_percent/test/apple_pie/1... 0 79 0.364707 apple_pie prime_rib <p>Nice! How about we make a simple column telling us whether or not the prediction is right or wrong?</p> In\u00a0[48]: Copied! <pre># 3. Is the prediction correct?\npred_df[\"pred_correct\"] = pred_df[\"y_true\"] == pred_df[\"y_pred\"]\npred_df.head()\n</pre> # 3. Is the prediction correct? pred_df[\"pred_correct\"] = pred_df[\"y_true\"] == pred_df[\"y_pred\"] pred_df.head() Out[48]: img_path y_true y_pred pred_conf y_true_classname y_pred_classname pred_correct 0 b'101_food_classes_10_percent/test/apple_pie/1... 0 52 0.847566 apple_pie gyoza False 1 b'101_food_classes_10_percent/test/apple_pie/1... 0 0 0.963383 apple_pie apple_pie True 2 b'101_food_classes_10_percent/test/apple_pie/1... 0 0 0.959426 apple_pie apple_pie True 3 b'101_food_classes_10_percent/test/apple_pie/1... 0 80 0.656333 apple_pie pulled_pork_sandwich False 4 b'101_food_classes_10_percent/test/apple_pie/1... 0 79 0.364707 apple_pie prime_rib False <p>And now since we know which predictions were right or wrong and along with their prediction probabilities, how about we get the 100 \"most wrong\" predictions by sorting for wrong predictions and descending prediction probabilties?</p> In\u00a0[49]: Copied! <pre># 4. Get the top 100 wrong examples\ntop_100_wrong = pred_df[pred_df[\"pred_correct\"] == False].sort_values(\"pred_conf\", ascending=False)[:100]\ntop_100_wrong.head(20)\n</pre> # 4. Get the top 100 wrong examples top_100_wrong = pred_df[pred_df[\"pred_correct\"] == False].sort_values(\"pred_conf\", ascending=False)[:100] top_100_wrong.head(20) Out[49]: img_path y_true y_pred pred_conf y_true_classname y_pred_classname pred_correct 21810 b'101_food_classes_10_percent/test/scallops/17... 87 29 0.999997 scallops cup_cakes False 231 b'101_food_classes_10_percent/test/apple_pie/8... 0 100 0.999995 apple_pie waffles False 15359 b'101_food_classes_10_percent/test/lobster_rol... 61 53 0.999988 lobster_roll_sandwich hamburger False 23539 b'101_food_classes_10_percent/test/strawberry_... 94 83 0.999987 strawberry_shortcake red_velvet_cake False 21400 b'101_food_classes_10_percent/test/samosa/3140... 85 92 0.999982 samosa spring_rolls False 24540 b'101_food_classes_10_percent/test/tiramisu/16... 98 83 0.999946 tiramisu red_velvet_cake False 2511 b'101_food_classes_10_percent/test/bruschetta/... 10 61 0.999945 bruschetta lobster_roll_sandwich False 5574 b'101_food_classes_10_percent/test/chocolate_m... 22 21 0.999939 chocolate_mousse chocolate_cake False 17855 b'101_food_classes_10_percent/test/paella/2314... 71 65 0.999930 paella mussels False 23797 b'101_food_classes_10_percent/test/sushi/16593... 95 86 0.999904 sushi sashimi False 18001 b'101_food_classes_10_percent/test/pancakes/10... 72 67 0.999902 pancakes omelette False 11642 b'101_food_classes_10_percent/test/garlic_brea... 46 10 0.999879 garlic_bread bruschetta False 10847 b'101_food_classes_10_percent/test/fried_calam... 43 68 0.999870 fried_calamari onion_rings False 23631 b'101_food_classes_10_percent/test/strawberry_... 94 83 0.999859 strawberry_shortcake red_velvet_cake False 1155 b'101_food_classes_10_percent/test/beef_tartar... 4 5 0.999856 beef_tartare beet_salad False 10854 b'101_food_classes_10_percent/test/fried_calam... 43 68 0.999854 fried_calamari onion_rings False 23904 b'101_food_classes_10_percent/test/sushi/33652... 95 86 0.999821 sushi sashimi False 7316 b'101_food_classes_10_percent/test/cup_cakes/1... 29 83 0.999817 cup_cakes red_velvet_cake False 13144 b'101_food_classes_10_percent/test/gyoza/31214... 52 92 0.999799 gyoza spring_rolls False 10880 b'101_food_classes_10_percent/test/fried_calam... 43 68 0.999780 fried_calamari onion_rings False <p>Very interesting... just by comparing the ground truth classname (<code>y_true_classname</code>) and the prediction classname column (<code>y_pred_classname</code>), do you notice any trends?</p> <p>It might be easier if we visualize them.</p> In\u00a0[50]: Copied! <pre># 5. Visualize some of the most wrong examples\nimages_to_view = 9\nstart_index = 10 # change the start index to view more\nplt.figure(figsize=(15, 10))\nfor i, row in enumerate(top_100_wrong[start_index:start_index+images_to_view].itertuples()): \n  plt.subplot(3, 3, i+1)\n  img = load_and_prep_image(row[1], scale=True)\n  _, _, _, _, pred_prob, y_true, y_pred, _ = row # only interested in a few parameters of each row\n  plt.imshow(img)\n  plt.title(f\"actual: {y_true}, pred: {y_pred} \\nprob: {pred_prob:.2f}\")\n  plt.axis(False)\n</pre> # 5. Visualize some of the most wrong examples images_to_view = 9 start_index = 10 # change the start index to view more plt.figure(figsize=(15, 10)) for i, row in enumerate(top_100_wrong[start_index:start_index+images_to_view].itertuples()):    plt.subplot(3, 3, i+1)   img = load_and_prep_image(row[1], scale=True)   _, _, _, _, pred_prob, y_true, y_pred, _ = row # only interested in a few parameters of each row   plt.imshow(img)   plt.title(f\"actual: {y_true}, pred: {y_pred} \\nprob: {pred_prob:.2f}\")   plt.axis(False) <p>Going through the model's most wrong predictions can usually help figure out a couple of things:</p> <ul> <li>Some of the labels might be wrong - If our model ends up being good enough, it may actually learning to predict very well on certain classes. This means some images which the model predicts the right label may show up as wrong if the ground truth label is wrong. If this is the case, we can often use our model to help us improve the labels in our dataset(s) and in turn, potentially making future models better. This process of using the model to help improve labels is often referred to as active learning.</li> <li>Could more samples be collected? - If there's a recurring pattern for a certain class being poorly predicted on, perhaps it's a good idea to collect more samples of that particular class in different scenarios to improve further models.</li> </ul> In\u00a0[51]: Copied! <pre># Download some custom images from Google Storage\n# Note: you can upload your own custom images to Google Colab using the \"upload\" button in the Files tab\n!wget https://storage.googleapis.com/ztm_tf_course/food_vision/custom_food_images.zip\n\nunzip_data(\"custom_food_images.zip\")\n</pre> # Download some custom images from Google Storage # Note: you can upload your own custom images to Google Colab using the \"upload\" button in the Files tab !wget https://storage.googleapis.com/ztm_tf_course/food_vision/custom_food_images.zip  unzip_data(\"custom_food_images.zip\")  <pre>--2023-05-18 02:25:41--  https://storage.googleapis.com/ztm_tf_course/food_vision/custom_food_images.zip\nResolving storage.googleapis.com (storage.googleapis.com)... 142.250.141.128, 142.251.2.128, 74.125.137.128, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|142.250.141.128|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 13192985 (13M) [application/zip]\nSaving to: \u2018custom_food_images.zip\u2019\n\ncustom_food_images. 100%[===================&gt;]  12.58M  57.4MB/s    in 0.2s    \n\n2023-05-18 02:25:42 (57.4 MB/s) - \u2018custom_food_images.zip\u2019 saved [13192985/13192985]\n\n</pre> <p>Wonderful, we can load these in and turn them into tensors using our <code>load_and_prep_image()</code> function but first we need a list of image filepaths.</p> In\u00a0[52]: Copied! <pre># Get custom food images filepaths\ncustom_food_images = [\"custom_food_images/\" + img_path for img_path in os.listdir(\"custom_food_images\")]\ncustom_food_images\n</pre> # Get custom food images filepaths custom_food_images = [\"custom_food_images/\" + img_path for img_path in os.listdir(\"custom_food_images\")] custom_food_images Out[52]: <pre>['custom_food_images/steak.jpeg',\n 'custom_food_images/sushi.jpeg',\n 'custom_food_images/ramen.jpeg',\n 'custom_food_images/pizza-dad.jpeg',\n 'custom_food_images/hamburger.jpeg',\n 'custom_food_images/chicken_wings.jpeg']</pre> <p>Now we can use similar code to what we used previously to load in our images, make a prediction on each using our trained model and then plot the image along with the predicted class.</p> In\u00a0[53]: Copied! <pre># Make predictions on custom food images\nfor img in custom_food_images:\n  img = load_and_prep_image(img, scale=False) # load in target image and turn it into tensor\n  pred_prob = model.predict(tf.expand_dims(img, axis=0)) # make prediction on image with shape [None, 224, 224, 3]\n  pred_class = class_names[pred_prob.argmax()] # find the predicted class label\n  # Plot the image with appropriate annotations\n  plt.figure()\n  plt.imshow(img/255.) # imshow() requires float inputs to be normalized\n  plt.title(f\"pred: {pred_class}, prob: {pred_prob.max():.2f}\")\n  plt.axis(False)\n</pre> # Make predictions on custom food images for img in custom_food_images:   img = load_and_prep_image(img, scale=False) # load in target image and turn it into tensor   pred_prob = model.predict(tf.expand_dims(img, axis=0)) # make prediction on image with shape [None, 224, 224, 3]   pred_class = class_names[pred_prob.argmax()] # find the predicted class label   # Plot the image with appropriate annotations   plt.figure()   plt.imshow(img/255.) # imshow() requires float inputs to be normalized   plt.title(f\"pred: {pred_class}, prob: {pred_prob.max():.2f}\")   plt.axis(False) <pre>1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 28ms/step\n</pre> <p>Two thumbs up! How cool is that?! Our Food Vision model has come to life!</p> <p>Seeing a machine learning model work on a premade test dataset is cool but seeing it work on your own data is mind blowing.</p> <p>And guess what... our model got these incredible results (10%+ better than the baseline) with only 10% of the training images.</p> <p>I wonder what would happen if we trained a model with all of the data (100% of the training data from Food101 instead of 10%)? Hint: that's your task in the next notebook.</p>"},{"location":"06_transfer_learning_in_tensorflow_part_3_scaling_up/#06-transfer-learning-with-tensorflow-part-3-scaling-up-food-vision-mini","title":"06. Transfer Learning with TensorFlow Part 3: Scaling up (\ud83c\udf54\ud83d\udc41 Food Vision mini)\u00b6","text":"<p>In the previous two notebooks (transfer learning part 1: feature extraction and part 2: fine-tuning) we've seen the power of transfer learning.</p> <p>Now we know our smaller modelling experiments are working, it's time to step things up a notch with more data.</p> <p>This is a common practice in machine learning and deep learning: get a model working on a small amount of data before scaling it up to a larger amount of data.</p> <p>\ud83d\udd11 Note: You haven't forgotten the machine learning practitioners motto have you? \"Experiment, experiment, experiment.\"</p> <p>It's time to get closer to our Food Vision project coming to life. In this notebook we're going to scale up from using 10 classes of the Food101 data to using all of the classes in the Food101 dataset.</p> <p>Our goal is to beat the original Food101 paper's results with 10% of data.</p> <p> Machine learning practitioners are serial experimenters. Start small, get a model working, see if your experiments work then gradually scale them up to where you want to go (we're going to be looking at scaling up throughout this notebook).</p>"},{"location":"06_transfer_learning_in_tensorflow_part_3_scaling_up/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>We're going to go through the follow with TensorFlow:</p> <ul> <li>Downloading and preparing 10% of the Food101 data (10% of training data)</li> <li>Training a feature extraction transfer learning model on 10% of the Food101 training data</li> <li>Fine-tuning our feature extraction model</li> <li>Saving and loaded our trained model</li> <li>Evaluating the performance of our Food Vision model trained on 10% of the training data<ul> <li>Finding our model's most wrong predictions</li> </ul> </li> <li>Making predictions with our Food Vision model on custom images of food</li> </ul>"},{"location":"06_transfer_learning_in_tensorflow_part_3_scaling_up/#how-you-can-use-this-notebook","title":"How you can use this notebook\u00b6","text":"<p>You can read through the descriptions and the code (it should all run, except for the cells which error on purpose), but there's a better option.</p> <p>Write all of the code yourself.</p> <p>Yes. I'm serious. Create a new notebook, and rewrite each line by yourself. Investigate it, see if you can break it, why does it break?</p> <p>You don't have to write the text descriptions but writing the code yourself is a great way to get hands-on experience.</p> <p>Don't worry if you make mistakes, we all do. The way to get better and make less mistakes is to write more code.</p> <p>\ud83d\udcd6 Resources:</p> <ul> <li>See the full set of course materials on GitHub: https://github.com/mrdbourke/tensorflow-deep-learning</li> <li>See updates for this notebook here: https://github.com/mrdbourke/tensorflow-deep-learning/discussions/549</li> </ul>"},{"location":"06_transfer_learning_in_tensorflow_part_3_scaling_up/#creating-helper-functions","title":"Creating helper functions\u00b6","text":"<p>We've created a series of helper functions throughout the previous notebooks. Instead of rewriting them (tedious), we'll import the <code>helper_functions.py</code> file from the GitHub repo.</p>"},{"location":"06_transfer_learning_in_tensorflow_part_3_scaling_up/#101-food-classes-working-with-less-data","title":"101 Food Classes: Working with less data\u00b6","text":"<p>So far we've confirmed the transfer learning model's we've been using work pretty well with the 10 Food Classes dataset. Now it's time to step it up and see how they go with the full 101 Food Classes.</p> <p>In the original Food101 dataset there's 1000 images per class (750 of each class in the training set and 250 of each class in the test set), totalling 101,000 imags.</p> <p>We could start modelling straight away on this large dataset but in the spirit of continually experimenting, we're going to see how our previously working model's go with 10% of the training data.</p> <p>This means for each of the 101 food classes we'll be building a model on 75 training images and evaluating it on 250 test images.</p>"},{"location":"06_transfer_learning_in_tensorflow_part_3_scaling_up/#downloading-and-preprocessing-the-data","title":"Downloading and preprocessing the data\u00b6","text":"<p>Just as before we'll download a subset of the Food101 dataset which has been extracted from the original dataset (to see the preprocessing of the data check out the Food Vision preprocessing notebook).</p> <p>We download the data as a zip file so we'll use our <code>unzip_data()</code> function to unzip it.</p>"},{"location":"06_transfer_learning_in_tensorflow_part_3_scaling_up/#train-a-big-dog-model-with-transfer-learning-on-10-of-101-food-classes","title":"Train a big dog model with transfer learning on 10% of 101 food classes\u00b6","text":"<p>Our food image data has been imported into TensorFlow, time to model it.</p> <p>To keep our experiments swift, we're going to start by using feature extraction transfer learning with a pre-trained model for a few epochs and then fine-tune for a few more epochs.</p> <p>More specifically, our goal will be to see if we can beat the baseline from original Food101 paper (50.76% accuracy on 101 classes) with 10% of the training data and the following modelling setup:</p> <ul> <li>A <code>ModelCheckpoint</code> callback to save our progress during training, this means we could experiment with further training later without having to train from scratch every time</li> <li>Data augmentation built right into the model</li> <li>A headless (no top layers) <code>EfficientNetB0</code> architecture from <code>tf.keras.applications</code> as our base model</li> <li>A <code>Dense</code> layer with 101 hidden neurons (same as number of food classes) and softmax activation as the output layer</li> <li>Categorical crossentropy as the loss function since we're dealing with more than two classes</li> <li>The Adam optimizer with the default settings</li> <li>Fitting for 5 full passes on the training data while evaluating on 15% of the test data</li> </ul> <p>It seems like a lot but these are all things we've covered before in the Transfer Learning in TensorFlow Part 2: Fine-tuning notebook.</p> <p>Let's start by creating the <code>ModelCheckpoint</code> callback.</p> <p>Since we want our model to perform well on unseen data we'll set it to monitor the validation accuracy metric and save the model weights which score the best on that.</p>"},{"location":"06_transfer_learning_in_tensorflow_part_3_scaling_up/#fine-tuning","title":"Fine-tuning\u00b6","text":"<p>Our feature extraction transfer learning model is performing well. Why don't we try to fine-tune a few layers in the base model and see if we gain any improvements?</p> <p>The good news is, thanks to the <code>ModelCheckpoint</code> callback, we've got the saved weights of our already well-performing model so if fine-tuning doesn't add any benefits, we can revert back.</p> <p>To fine-tune the base model we'll first set its <code>trainable</code> attribute to <code>True</code>, unfreezing all of the frozen.</p> <p>Then since we've got a relatively small training dataset, we'll refreeze every layer except for the last 5, making them trainable.</p>"},{"location":"06_transfer_learning_in_tensorflow_part_3_scaling_up/#saving-our-trained-model","title":"Saving our trained model\u00b6","text":"<p>To prevent having to retrain our model from scratch, let's save it to file using the <code>save()</code> method.</p>"},{"location":"06_transfer_learning_in_tensorflow_part_3_scaling_up/#evaluating-the-performance-of-the-big-dog-model-across-all-different-classes","title":"Evaluating the performance of the big dog model across all different classes\u00b6","text":"<p>We've got a trained and saved model which according to the evaluation metrics we've used is performing fairly well.</p> <p>But metrics schmetrics, let's dive a little deeper into our model's performance and get some visualizations going.</p> <p>To do so, we'll load in the saved model and use it to make some predictions on the test dataset.</p> <p>\ud83d\udd11 Note: Evaluating a machine learning model is as important as training one. Metrics can be deceiving. You should always visualize your model's performance on unseen data to make sure you aren't being fooled good looking training numbers.</p>"},{"location":"06_transfer_learning_in_tensorflow_part_3_scaling_up/#making-predictions-with-our-trained-model","title":"Making predictions with our trained model\u00b6","text":"<p>To evaluate our trained model, we need to make some predictions with it and then compare those predictions to the test dataset.</p> <p>Because the model has never seen the test dataset, this should give us an indication of how the model will perform in the real world on data similar to what it has been trained on.</p> <p>To make predictions with our trained model, we can use the <code>predict()</code> method passing it the test data.</p> <p>Since our data is multi-class, doing this will return a prediction probably tensor for each sample.</p> <p>In other words, every time the trained model see's an image it will compare it to all of the patterns it learned during training and return an output for every class (all 101 of them) of how likely the image is to be that class.</p>"},{"location":"06_transfer_learning_in_tensorflow_part_3_scaling_up/#evaluating-our-models-predictions","title":"Evaluating our models predictions\u00b6","text":"<p>A very simple evaluation is to use Scikit-Learn's <code>accuracy_score()</code> function which compares truth labels to predicted labels and returns an accuracy score.</p> <p>If we've created our <code>y_labels</code> and <code>pred_classes</code> arrays correctly, this should return the same accuracy value (or at least very close) as the <code>evaluate()</code> method we used earlier.</p>"},{"location":"06_transfer_learning_in_tensorflow_part_3_scaling_up/#visualizing-predictions-on-test-images","title":"Visualizing predictions on test images\u00b6","text":"<p>Time for the real test. Visualizing predictions on actual images. You can look at all the metrics you want but until you've visualized some predictions, you won't really know how your model is performing.</p> <p>As it stands, our model can't just predict on any image of our choice. The image first has to be loaded into a tensor.</p> <p>So to begin predicting on any given image, we'll create a function to load an image into a tensor.</p> <p>Specifically, it'll:</p> <ul> <li>Read in a target image filepath using <code>tf.io.read_file()</code>.</li> <li>Turn the image into a <code>Tensor</code> using <code>tf.io.decode_image()</code>.</li> <li>Resize the image to be the same size as the images our model has been trained on (224 x 224) using <code>tf.image.resize()</code>.</li> <li>Scale the image to get all the pixel values between 0 &amp; 1 if necessary.</li> </ul>"},{"location":"06_transfer_learning_in_tensorflow_part_3_scaling_up/#finding-the-most-wrong-predictions","title":"Finding the most wrong predictions\u00b6","text":"<p>It's a good idea to go through at least 100+ random instances of your model's predictions to get a good feel for how it's doing.</p> <p>After a while you might notice the model predicting on some images with a very high prediction probability, meaning it's very confident with its prediction but still getting the label wrong.</p> <p>These most wrong predictions can help to give further insight into your model's performance.</p> <p>So how about we write some code to collect all of the predictions where the model has output a high prediction probability for an image (e.g. 0.95+) but gotten the prediction wrong.</p> <p>We'll go through the following steps:</p> <ol> <li>Get all of the image file paths in the test dataset using the <code>list_files()</code> method.</li> <li>Create a pandas DataFrame of the image filepaths, ground truth labels, prediction classes, max prediction probabilities, ground truth class names and predicted class names.</li> </ol> <ul> <li>Note: We don't necessarily have to create a DataFrame like this but it'll help us visualize things as we go.</li> </ul> <ol> <li>Use our DataFrame to find all the wrong predictions (where the ground truth doesn't match the prediction).</li> <li>Sort the DataFrame based on wrong predictions and highest max prediction probabilities.</li> <li>Visualize the images with the highest prediction probabilities but have the wrong prediction.</li> </ol>"},{"location":"06_transfer_learning_in_tensorflow_part_3_scaling_up/#test-out-the-big-dog-model-on-test-images-as-well-as-custom-images-of-food","title":"Test out the big dog model on test images as well as custom images of food\u00b6","text":"<p>So far we've visualized some our model's predictions from the test dataset but it's time for the real test: using our model to make predictions on our own custom images of food.</p> <p>For this you might want to upload your own images to Google Colab or by putting them in a folder you can load into the notebook.</p> <p>In my case, I've prepared my own small dataset of six or so images of various foods.</p> <p>Let's download them and unzip them.</p>"},{"location":"06_transfer_learning_in_tensorflow_part_3_scaling_up/#exercises","title":"\ud83d\udee0 Exercises\u00b6","text":"<ol> <li>Take 3 of your own photos of food and use the trained model to make predictions on them, share your predictions with the other students in Discord and show off your Food Vision model \ud83c\udf54\ud83d\udc41.</li> <li>Train a feature-extraction transfer learning model for 10 epochs on the same data and compare its performance versus a model which used feature extraction for 5 epochs and fine-tuning for 5 epochs (like we've used in this notebook). Which method is better?</li> <li>Recreate our first model (the feature extraction model) with <code>mixed_precision</code> turned on.</li> </ol> <ul> <li>Does it make the model train faster?</li> <li>Does it effect the accuracy or performance of our model?</li> <li>What's the advatanges of using <code>mixed_precision</code> training?</li> </ul>"},{"location":"06_transfer_learning_in_tensorflow_part_3_scaling_up/#extra-curriculum","title":"\ud83d\udcd6 Extra-curriculum\u00b6","text":"<ul> <li>Spend 15-minutes reading up on the EarlyStopping callback. What does it do? How could we use it in our model training?</li> <li>Spend an hour reading about Streamlit. What does it do? How might you integrate some of the things we've done in this notebook in a Streamlit app?</li> </ul>"},{"location":"07_food_vision_milestone_project_1/","title":"07 Milestone Project 1: \ud83c\udf54\ud83d\udc41 Food Vision Big\u2122","text":"In\u00a0[60]: Copied! <pre># Get GPU name\n!nvidia-smi -L\n</pre> # Get GPU name !nvidia-smi -L <pre>GPU 0: NVIDIA A100-SXM4-40GB (UUID: GPU-269f6413-0643-12da-9e68-ef2cb8b4aad3)\n</pre> <p>Since mixed precision training was introduced in TensorFlow 2.4.0, make sure you've got at least TensorFlow 2.4.0+.</p> In\u00a0[61]: Copied! <pre># Note: As of May 2023, there have been some issues with TensorFlow versions 2.9-2.12\n# with the following code. \n# However, these seemed to have been fixed in version 2.13+.\n# TensorFlow version 2.13 is available in tf-nightly as of May 2023 (will be default in Google Colab soon).\n# Therefore, to prevent errors we'll install tf-nightly first.\n# See more here: https://github.com/mrdbourke/tensorflow-deep-learning/discussions/550 \n\n# Install tf-nightly (required until 2.13.0+ is the default in Google Colab)\n!pip install -U -q tf-nightly\n\n# Check TensorFlow version (should be minimum 2.4.0+ but 2.13.0+ is better)\nimport tensorflow as tf\nprint(f\"TensorFlow version: {tf.__version__}\")\n\n# Add timestamp\nimport datetime\nprint(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\")\n</pre> # Note: As of May 2023, there have been some issues with TensorFlow versions 2.9-2.12 # with the following code.  # However, these seemed to have been fixed in version 2.13+. # TensorFlow version 2.13 is available in tf-nightly as of May 2023 (will be default in Google Colab soon). # Therefore, to prevent errors we'll install tf-nightly first. # See more here: https://github.com/mrdbourke/tensorflow-deep-learning/discussions/550   # Install tf-nightly (required until 2.13.0+ is the default in Google Colab) !pip install -U -q tf-nightly  # Check TensorFlow version (should be minimum 2.4.0+ but 2.13.0+ is better) import tensorflow as tf print(f\"TensorFlow version: {tf.__version__}\")  # Add timestamp import datetime print(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\") <pre>TensorFlow version: 2.14.0-dev20230518\nNotebook last run (end-to-end): 2023-05-19 02:54:07.955201\n</pre> In\u00a0[3]: Copied! <pre># Get helper functions file\nimport os \n\nif not os.path.exists(\"helper_functions.py\"):\n    !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\nelse:\n    print(\"[INFO] 'helper_functions.py' already exists, skipping download.\")\n</pre> # Get helper functions file import os   if not os.path.exists(\"helper_functions.py\"):     !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py else:     print(\"[INFO] 'helper_functions.py' already exists, skipping download.\") <pre>--2023-05-19 02:13:56--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 10246 (10K) [text/plain]\nSaving to: \u2018helper_functions.py\u2019\n\nhelper_functions.py 100%[===================&gt;]  10.01K  --.-KB/s    in 0s      \n\n2023-05-19 02:13:56 (100 MB/s) - \u2018helper_functions.py\u2019 saved [10246/10246]\n\n</pre> In\u00a0[4]: Copied! <pre># Import series of helper functions for the notebook (we've created/used these in previous notebooks)\nfrom helper_functions import create_tensorboard_callback, plot_loss_curves, compare_historys\n</pre> # Import series of helper functions for the notebook (we've created/used these in previous notebooks) from helper_functions import create_tensorboard_callback, plot_loss_curves, compare_historys In\u00a0[5]: Copied! <pre># Get TensorFlow Datasets\nimport tensorflow_datasets as tfds\n</pre> # Get TensorFlow Datasets import tensorflow_datasets as tfds <p>To find all of the available datasets in TensorFlow Datasets, you can use the <code>list_builders()</code> method.</p> <p>After doing so, we can check to see if the one we're after (<code>\"food101\"</code>) is present.</p> In\u00a0[6]: Copied! <pre># Get all available datasets in TFDS\ndatasets_list = tfds.list_builders()\n\n# Set our target dataset and see if it exists\ntarget_dataset = \"food101\"\nprint(f\"'{target_dataset}' in TensorFlow Datasets: {target_dataset in datasets_list}\")\n</pre> # Get all available datasets in TFDS datasets_list = tfds.list_builders()  # Set our target dataset and see if it exists target_dataset = \"food101\" print(f\"'{target_dataset}' in TensorFlow Datasets: {target_dataset in datasets_list}\") <pre>'food101' in TensorFlow Datasets: True\n</pre> <p>Beautiful! It looks like the dataset we're after is available (note there are plenty more available but we're on Food101).</p> <p>To get access to the Food101 dataset from the TFDS, we can use the <code>tfds.load()</code> method.</p> <p>In particular, we'll have to pass it a few parameters to let it know what we're after:</p> <ul> <li><code>name</code> (str) : the target dataset (e.g. <code>\"food101\"</code>)</li> <li><code>split</code> (list, optional) : what splits of the dataset we're after (e.g. <code>[\"train\", \"validation\"]</code>)<ul> <li>the <code>split</code> parameter is quite tricky. See the documentation for more.</li> </ul> </li> <li><code>shuffle_files</code> (bool) : whether or not to shuffle the files on download, defaults to <code>False</code></li> <li><code>as_supervised</code> (bool) : <code>True</code> to download data samples in tuple format (<code>(data, label)</code>) or <code>False</code> for dictionary format</li> <li><code>with_info</code> (bool) : <code>True</code> to download dataset metadata (labels, number of samples, etc)</li> </ul> <p>\ud83d\udd11 Note: Calling the <code>tfds.load()</code> method will start to download a target dataset to disk if the <code>download=True</code> parameter is set (default). This dataset could be 100GB+, so make sure you have space.</p> In\u00a0[\u00a0]: Copied! <pre># Load in the data (takes about 5-6 minutes in Google Colab)\n(train_data, test_data), ds_info = tfds.load(name=\"food101\", # target dataset to get from TFDS\n                                             split=[\"train\", \"validation\"], # what splits of data should we get? note: not all datasets have train, valid, test\n                                             shuffle_files=True, # shuffle files on download?\n                                             as_supervised=True, # download data in tuple format (sample, label), e.g. (image, label)\n                                             with_info=True) # include dataset metadata? if so, tfds.load() returns tuple (data, ds_info)\n</pre> # Load in the data (takes about 5-6 minutes in Google Colab) (train_data, test_data), ds_info = tfds.load(name=\"food101\", # target dataset to get from TFDS                                              split=[\"train\", \"validation\"], # what splits of data should we get? note: not all datasets have train, valid, test                                              shuffle_files=True, # shuffle files on download?                                              as_supervised=True, # download data in tuple format (sample, label), e.g. (image, label)                                              with_info=True) # include dataset metadata? if so, tfds.load() returns tuple (data, ds_info) <p>Wonderful! After a few minutes of downloading, we've now got access to entire Food101 dataset (in tensor format) ready for modelling.</p> <p>Now let's get a little information from our dataset, starting with the class names.</p> <p>Getting class names from a TensorFlow Datasets dataset requires downloading the \"<code>dataset_info</code>\" variable (by using the <code>as_supervised=True</code> parameter in the <code>tfds.load()</code> method, note: this will only work for supervised datasets in TFDS).</p> <p>We can access the class names of a particular dataset using the <code>dataset_info.features</code> attribute and accessing <code>names</code> attribute of the the <code>\"label\"</code> key.</p> In\u00a0[8]: Copied! <pre># Features of Food101 TFDS\nds_info.features\n</pre> # Features of Food101 TFDS ds_info.features Out[8]: <pre>FeaturesDict({\n    'image': Image(shape=(None, None, 3), dtype=uint8),\n    'label': ClassLabel(shape=(), dtype=int64, num_classes=101),\n})</pre> In\u00a0[9]: Copied! <pre># Get class names\nclass_names = ds_info.features[\"label\"].names\nclass_names[:10]\n</pre> # Get class names class_names = ds_info.features[\"label\"].names class_names[:10] Out[9]: <pre>['apple_pie',\n 'baby_back_ribs',\n 'baklava',\n 'beef_carpaccio',\n 'beef_tartare',\n 'beet_salad',\n 'beignets',\n 'bibimbap',\n 'bread_pudding',\n 'breakfast_burrito']</pre> In\u00a0[10]: Copied! <pre># Take one sample off the training data\ntrain_one_sample = train_data.take(1) # samples are in format (image_tensor, label)\n</pre> # Take one sample off the training data train_one_sample = train_data.take(1) # samples are in format (image_tensor, label) <p>Because we used the <code>as_supervised=True</code> parameter in our <code>tfds.load()</code> method above, data samples come in the tuple format structure <code>(data, label)</code> or in our case <code>(image_tensor, label)</code>.</p> In\u00a0[11]: Copied! <pre># What does one sample of our training data look like?\ntrain_one_sample\n</pre> # What does one sample of our training data look like? train_one_sample Out[11]: <pre>&lt;_TakeDataset element_spec=(TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))&gt;</pre> <p>Let's loop through our single training sample and get some info from the <code>image_tensor</code> and <code>label</code>.</p> In\u00a0[12]: Copied! <pre># Output info about our training sample\nfor image, label in train_one_sample:\n  print(f\"\"\"\n  Image shape: {image.shape}\n  Image dtype: {image.dtype}\n  Target class from Food101 (tensor form): {label}\n  Class name (str form): {class_names[label.numpy()]}\n        \"\"\")\n</pre> # Output info about our training sample for image, label in train_one_sample:   print(f\"\"\"   Image shape: {image.shape}   Image dtype: {image.dtype}   Target class from Food101 (tensor form): {label}   Class name (str form): {class_names[label.numpy()]}         \"\"\") <pre>\n  Image shape: (512, 512, 3)\n  Image dtype: &lt;dtype: 'uint8'&gt;\n  Target class from Food101 (tensor form): 90\n  Class name (str form): spaghetti_bolognese\n        \n</pre> <p>Because we set the <code>shuffle_files=True</code> parameter in our <code>tfds.load()</code> method above, running the cell above a few times will give a different result each time.</p> <p>Checking these you might notice some of the images have different shapes, for example <code>(512, 342, 3)</code> and <code>(512, 512, 3)</code> (height, width, color_channels).</p> <p>Let's see what one of the image tensors from TFDS's Food101 dataset looks like.</p> In\u00a0[13]: Copied! <pre># What does an image tensor from TFDS's Food101 look like?\nimage\n</pre> # What does an image tensor from TFDS's Food101 look like? image Out[13]: <pre>&lt;tf.Tensor: shape=(512, 512, 3), dtype=uint8, numpy=\narray([[[ 12,  13,   7],\n        [ 12,  13,   7],\n        [ 13,  14,   8],\n        ...,\n        [ 21,  11,   0],\n        [ 21,  11,   0],\n        [ 21,  11,   0]],\n\n       [[ 12,  13,   7],\n        [ 11,  12,   6],\n        [ 11,  12,   6],\n        ...,\n        [ 21,  11,   0],\n        [ 21,  11,   0],\n        [ 21,  11,   0]],\n\n       [[  7,   8,   2],\n        [  7,   8,   2],\n        [  7,   8,   2],\n        ...,\n        [ 22,  12,   2],\n        [ 21,  11,   1],\n        [ 20,  10,   0]],\n\n       ...,\n\n       [[188, 191, 184],\n        [188, 191, 184],\n        [188, 191, 184],\n        ...,\n        [243, 248, 244],\n        [243, 248, 244],\n        [242, 247, 243]],\n\n       [[187, 190, 183],\n        [189, 192, 185],\n        [190, 193, 186],\n        ...,\n        [241, 245, 244],\n        [241, 245, 244],\n        [241, 245, 244]],\n\n       [[186, 189, 182],\n        [189, 192, 185],\n        [191, 194, 187],\n        ...,\n        [238, 242, 241],\n        [239, 243, 242],\n        [239, 243, 242]]], dtype=uint8)&gt;</pre> In\u00a0[14]: Copied! <pre># What are the min and max values?\ntf.reduce_min(image), tf.reduce_max(image)\n</pre> # What are the min and max values? tf.reduce_min(image), tf.reduce_max(image) Out[14]: <pre>(&lt;tf.Tensor: shape=(), dtype=uint8, numpy=0&gt;,\n &lt;tf.Tensor: shape=(), dtype=uint8, numpy=255&gt;)</pre> <p>Alright looks like our image tensors have values of between 0 &amp; 255 (standard red, green, blue colour values) and the values are of data type <code>unit8</code>.</p> <p>We might have to preprocess these before passing them to a neural network. But we'll handle this later.</p> <p>In the meantime, let's see if we can plot an image sample.</p> In\u00a0[15]: Copied! <pre># Plot an image tensor\nimport matplotlib.pyplot as plt\nplt.imshow(image)\nplt.title(class_names[label.numpy()]) # add title to image by indexing on class_names list\nplt.axis(False);\n</pre> # Plot an image tensor import matplotlib.pyplot as plt plt.imshow(image) plt.title(class_names[label.numpy()]) # add title to image by indexing on class_names list plt.axis(False); <p>Delicious!</p> <p>Okay, looks like the Food101 data we've got from TFDS is similar to the datasets we've been using in previous notebooks.</p> <p>Now let's preprocess it and get it ready for use with a neural network.</p> In\u00a0[16]: Copied! <pre># Make a function for preprocessing images\ndef preprocess_img(image, label, img_shape=224):\n    \"\"\"\n    Converts image datatype from 'uint8' -&gt; 'float32' and reshapes image to\n    [img_shape, img_shape, color_channels]\n    \"\"\"\n    image = tf.image.resize(image, [img_shape, img_shape]) # reshape to img_shape\n    return tf.cast(image, tf.float32), label # return (float32_image, label) tuple\n</pre> # Make a function for preprocessing images def preprocess_img(image, label, img_shape=224):     \"\"\"     Converts image datatype from 'uint8' -&gt; 'float32' and reshapes image to     [img_shape, img_shape, color_channels]     \"\"\"     image = tf.image.resize(image, [img_shape, img_shape]) # reshape to img_shape     return tf.cast(image, tf.float32), label # return (float32_image, label) tuple <p>Our <code>preprocess_img()</code> function above takes image and label as input (even though it does nothing to the label) because our dataset is currently in the tuple structure <code>(image, label)</code>.</p> <p>Let's try our function out on a target image.</p> In\u00a0[17]: Copied! <pre># Preprocess a single sample image and check the outputs\npreprocessed_img = preprocess_img(image, label)[0]\nprint(f\"Image before preprocessing:\\n {image[:2]}...,\\nShape: {image.shape},\\nDatatype: {image.dtype}\\n\")\nprint(f\"Image after preprocessing:\\n {preprocessed_img[:2]}...,\\nShape: {preprocessed_img.shape},\\nDatatype: {preprocessed_img.dtype}\")\n</pre> # Preprocess a single sample image and check the outputs preprocessed_img = preprocess_img(image, label)[0] print(f\"Image before preprocessing:\\n {image[:2]}...,\\nShape: {image.shape},\\nDatatype: {image.dtype}\\n\") print(f\"Image after preprocessing:\\n {preprocessed_img[:2]}...,\\nShape: {preprocessed_img.shape},\\nDatatype: {preprocessed_img.dtype}\") <pre>Image before preprocessing:\n [[[12 13  7]\n  [12 13  7]\n  [13 14  8]\n  ...\n  [21 11  0]\n  [21 11  0]\n  [21 11  0]]\n\n [[12 13  7]\n  [11 12  6]\n  [11 12  6]\n  ...\n  [21 11  0]\n  [21 11  0]\n  [21 11  0]]]...,\nShape: (512, 512, 3),\nDatatype: &lt;dtype: 'uint8'&gt;\n\nImage after preprocessing:\n [[[11.586735   12.586735    6.586735  ]\n  [11.714286   12.714286    6.714286  ]\n  [ 8.857142    9.857142    4.8571424 ]\n  ...\n  [20.714308   11.142836    1.2857144 ]\n  [20.668371   10.668372    0.        ]\n  [21.         11.          0.        ]]\n\n [[ 2.3571415   3.3571415   0.1428566 ]\n  [ 3.1530607   4.153061    0.07653028]\n  [ 3.0561223   4.0561223   0.        ]\n  ...\n  [26.071407   18.071407    7.0714073 ]\n  [24.785702   14.785702    4.7857018 ]\n  [22.499966   12.499966    2.4999657 ]]]...,\nShape: (224, 224, 3),\nDatatype: &lt;dtype: 'float32'&gt;\n</pre> <p>Excellent! Looks like our <code>preprocess_img()</code> function is working as expected.</p> <p>The input image gets converted from <code>uint8</code> to <code>float32</code> and gets reshaped from its current shape to <code>(224, 224, 3)</code>.</p> <p>How does it look?</p> In\u00a0[18]: Copied! <pre># We can still plot our preprocessed image as long as we \n# divide by 255 (for matplotlib capatibility)\nplt.imshow(preprocessed_img/255.)\nplt.title(class_names[label])\nplt.axis(False);\n</pre> # We can still plot our preprocessed image as long as we  # divide by 255 (for matplotlib capatibility) plt.imshow(preprocessed_img/255.) plt.title(class_names[label]) plt.axis(False); <p>All this food visualization is making me hungry. How about we start preparing to model it?</p> In\u00a0[19]: Copied! <pre># Map preprocessing function to training data (and paralellize)\ntrain_data = train_data.map(map_func=preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n# Shuffle train_data and turn it into batches and prefetch it (load it faster)\ntrain_data = train_data.shuffle(buffer_size=1000).batch(batch_size=32).prefetch(buffer_size=tf.data.AUTOTUNE)\n\n# Map prepreprocessing function to test data\ntest_data = test_data.map(preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n# Turn test data into batches (don't need to shuffle)\ntest_data = test_data.batch(32).prefetch(tf.data.AUTOTUNE)\n</pre> # Map preprocessing function to training data (and paralellize) train_data = train_data.map(map_func=preprocess_img, num_parallel_calls=tf.data.AUTOTUNE) # Shuffle train_data and turn it into batches and prefetch it (load it faster) train_data = train_data.shuffle(buffer_size=1000).batch(batch_size=32).prefetch(buffer_size=tf.data.AUTOTUNE)  # Map prepreprocessing function to test data test_data = test_data.map(preprocess_img, num_parallel_calls=tf.data.AUTOTUNE) # Turn test data into batches (don't need to shuffle) test_data = test_data.batch(32).prefetch(tf.data.AUTOTUNE) <p>And now let's check out what our prepared datasets look like.</p> In\u00a0[20]: Copied! <pre>train_data, test_data\n</pre> train_data, test_data Out[20]: <pre>(&lt;_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))&gt;,\n &lt;_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))&gt;)</pre> <p>Excellent! Looks like our data is now in tutples of <code>(image, label)</code> with datatypes of <code>(tf.float32, tf.int64)</code>, just what our model is after.</p> <p>\ud83d\udd11 Note: You can get away without calling the <code>prefetch()</code> method on the end of your datasets, however, you'd probably see significantly slower data loading speeds when building a model. So most of your dataset input pipelines should end with a call to <code>prefecth()</code>.</p> <p>Onward.</p> In\u00a0[21]: Copied! <pre># Create TensorBoard callback (already have \"create_tensorboard_callback()\" from a previous notebook)\nfrom helper_functions import create_tensorboard_callback\n\n# Create ModelCheckpoint callback to save model's progress\ncheckpoint_path = \"model_checkpoints/cp.ckpt\" # saving weights requires \".ckpt\" extension\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n                                                      monitor=\"val_accuracy\", # save the model weights with best validation accuracy\n                                                      save_best_only=True, # only save the best weights\n                                                      save_weights_only=True, # only save model weights (not whole model)\n                                                      verbose=0) # don't print out whether or not model is being saved\n</pre> # Create TensorBoard callback (already have \"create_tensorboard_callback()\" from a previous notebook) from helper_functions import create_tensorboard_callback  # Create ModelCheckpoint callback to save model's progress checkpoint_path = \"model_checkpoints/cp.ckpt\" # saving weights requires \".ckpt\" extension model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,                                                       monitor=\"val_accuracy\", # save the model weights with best validation accuracy                                                       save_best_only=True, # only save the best weights                                                       save_weights_only=True, # only save model weights (not whole model)                                                       verbose=0) # don't print out whether or not model is being saved  In\u00a0[22]: Copied! <pre># Turn on mixed precision training\nfrom tensorflow.keras import mixed_precision\nmixed_precision.set_global_policy(policy=\"mixed_float16\") # set global policy to mixed precision\n</pre> # Turn on mixed precision training from tensorflow.keras import mixed_precision mixed_precision.set_global_policy(policy=\"mixed_float16\") # set global policy to mixed precision  <p>Nice! As long as the GPU you're using has a compute capability of 7.0+ the cell above should run without error.</p> <p>Now we can check the global dtype policy (the policy which will be used by layers in our model) using the <code>mixed_precision.global_policy()</code> method.</p> In\u00a0[23]: Copied! <pre>mixed_precision.global_policy() # should output \"mixed_float16\" (if your GPU is compatible with mixed precision)\n</pre> mixed_precision.global_policy() # should output \"mixed_float16\" (if your GPU is compatible with mixed precision) Out[23]: <pre>&lt;Policy \"mixed_float16\"&gt;</pre> <p>Great, since the global dtype policy is now <code>\"mixed_float16\"</code> our model will automatically take advantage of float16 variables where possible and in turn speed up training.</p> In\u00a0[24]: Copied! <pre>from tensorflow.keras import layers\n\n# Create base model\ninput_shape = (224, 224, 3)\nbase_model = tf.keras.applications.EfficientNetB0(include_top=False)\nbase_model.trainable = False # freeze base model layers\n\n# Create Functional model \ninputs = layers.Input(shape=input_shape, name=\"input_layer\")\n# Note: EfficientNetBX models have rescaling built-in but if your model didn't you could have a layer like below\n# x = layers.Rescaling(1./255)(x)\nx = base_model(inputs, training=False) # set base_model to inference mode only\nx = layers.GlobalAveragePooling2D(name=\"pooling_layer\")(x)\nx = layers.Dense(len(class_names))(x) # want one output neuron per class \n# Separate activation of output layer so we can output float32 activations\noutputs = layers.Activation(\"softmax\", dtype=tf.float32, name=\"softmax_float32\")(x) \nmodel = tf.keras.Model(inputs, outputs)\n\n# Compile the model\nmodel.compile(loss=\"sparse_categorical_crossentropy\", # Use sparse_categorical_crossentropy when labels are *not* one-hot\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=[\"accuracy\"])\n</pre> from tensorflow.keras import layers  # Create base model input_shape = (224, 224, 3) base_model = tf.keras.applications.EfficientNetB0(include_top=False) base_model.trainable = False # freeze base model layers  # Create Functional model  inputs = layers.Input(shape=input_shape, name=\"input_layer\") # Note: EfficientNetBX models have rescaling built-in but if your model didn't you could have a layer like below # x = layers.Rescaling(1./255)(x) x = base_model(inputs, training=False) # set base_model to inference mode only x = layers.GlobalAveragePooling2D(name=\"pooling_layer\")(x) x = layers.Dense(len(class_names))(x) # want one output neuron per class  # Separate activation of output layer so we can output float32 activations outputs = layers.Activation(\"softmax\", dtype=tf.float32, name=\"softmax_float32\")(x)  model = tf.keras.Model(inputs, outputs)  # Compile the model model.compile(loss=\"sparse_categorical_crossentropy\", # Use sparse_categorical_crossentropy when labels are *not* one-hot               optimizer=tf.keras.optimizers.Adam(),               metrics=[\"accuracy\"]) <pre>Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n16705208/16705208 [==============================] - 2s 0us/step\n</pre> In\u00a0[25]: Copied! <pre># Check out our model\nmodel.summary()\n</pre> # Check out our model model.summary() <pre>Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n                                                                 \n efficientnetb0 (Functional  (None, None, None, 1280   4049571   \n )                           )                                   \n                                                                 \n pooling_layer (GlobalAvera  (None, 1280)              0         \n gePooling2D)                                                    \n                                                                 \n dense (Dense)               (None, 101)               129381    \n                                                                 \n softmax_float32 (Activatio  (None, 101)               0         \n n)                                                              \n                                                                 \n=================================================================\nTotal params: 4178952 (15.94 MB)\nTrainable params: 129381 (505.39 KB)\nNon-trainable params: 4049571 (15.45 MB)\n_________________________________________________________________\n</pre> In\u00a0[26]: Copied! <pre># Check the dtype_policy attributes of layers in our model\nfor layer in model.layers:\n    print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy) # Check the dtype policy of layers\n</pre> # Check the dtype_policy attributes of layers in our model for layer in model.layers:     print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy) # Check the dtype policy of layers <pre>input_layer True float32 &lt;Policy \"float32\"&gt;\nefficientnetb0 False float32 &lt;Policy \"mixed_float16\"&gt;\npooling_layer True float32 &lt;Policy \"mixed_float16\"&gt;\ndense True float32 &lt;Policy \"mixed_float16\"&gt;\nsoftmax_float32 True float32 &lt;Policy \"float32\"&gt;\n</pre> <p>Going through the above we see:</p> <ul> <li><code>layer.name</code> (str) : a layer's human-readable name, can be defined by the <code>name</code> parameter on construction</li> <li><code>layer.trainable</code> (bool) : whether or not a layer is trainable (all of our layers are trainable except the efficientnetb0 layer since we set it's <code>trainable</code> attribute to <code>False</code></li> <li><code>layer.dtype</code> : the data type a layer stores its variables in</li> <li><code>layer.dtype_policy</code> : the data type a layer computes in</li> </ul> <p>\ud83d\udd11 Note: A layer can have a dtype of <code>float32</code> and a dtype policy of <code>\"mixed_float16\"</code> because it stores its variables (weights &amp; biases) in <code>float32</code> (more numerically stable), however it computes in <code>float16</code> (faster).</p> <p>We can also check the same details for our model's base model.</p> In\u00a0[27]: Copied! <pre># Check the layers in the base model and see what dtype policy they're using\nfor layer in model.layers[1].layers[:20]: # only check the first 20 layers to save output space\n    print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)\n</pre> # Check the layers in the base model and see what dtype policy they're using for layer in model.layers[1].layers[:20]: # only check the first 20 layers to save output space     print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy) <pre>input_1 False float32 &lt;Policy \"float32\"&gt;\nrescaling False float32 &lt;Policy \"mixed_float16\"&gt;\nnormalization False float32 &lt;Policy \"mixed_float16\"&gt;\nrescaling_1 False float32 &lt;Policy \"mixed_float16\"&gt;\nstem_conv_pad False float32 &lt;Policy \"mixed_float16\"&gt;\nstem_conv False float32 &lt;Policy \"mixed_float16\"&gt;\nstem_bn False float32 &lt;Policy \"mixed_float16\"&gt;\nstem_activation False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_dwconv False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_bn False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_activation False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_se_squeeze False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_se_reshape False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_se_reduce False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_se_expand False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_se_excite False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_project_conv False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_project_bn False float32 &lt;Policy \"mixed_float16\"&gt;\nblock2a_expand_conv False float32 &lt;Policy \"mixed_float16\"&gt;\nblock2a_expand_bn False float32 &lt;Policy \"mixed_float16\"&gt;\n</pre> <p>\ud83d\udd11 Note: The mixed precision API automatically causes layers which can benefit from using the <code>\"mixed_float16\"</code> dtype policy to use it. It also prevents layers which shouldn't use it from using it (e.g. the normalization layer at the start of the base model).</p> In\u00a0[28]: Copied! <pre># Turn off all warnings except for errors\ntf.get_logger().setLevel('ERROR')\n\n# Fit the model with callbacks\nhistory_101_food_classes_feature_extract = model.fit(train_data, \n                                                     epochs=3,\n                                                     steps_per_epoch=len(train_data),\n                                                     validation_data=test_data,\n                                                     validation_steps=int(0.15 * len(test_data)),\n                                                     callbacks=[create_tensorboard_callback(\"training_logs\", \n                                                                                            \"efficientnetb0_101_classes_all_data_feature_extract\"),\n                                                                model_checkpoint])\n</pre> # Turn off all warnings except for errors tf.get_logger().setLevel('ERROR')  # Fit the model with callbacks history_101_food_classes_feature_extract = model.fit(train_data,                                                       epochs=3,                                                      steps_per_epoch=len(train_data),                                                      validation_data=test_data,                                                      validation_steps=int(0.15 * len(test_data)),                                                      callbacks=[create_tensorboard_callback(\"training_logs\",                                                                                              \"efficientnetb0_101_classes_all_data_feature_extract\"),                                                                 model_checkpoint]) <pre>Saving TensorBoard log files to: training_logs/efficientnetb0_101_classes_all_data_feature_extract/20230519-022415\nEpoch 1/3\n2368/2368 [==============================] - 67s 22ms/step - loss: 1.7186 - accuracy: 0.5808 - val_loss: 1.1152 - val_accuracy: 0.7018\nEpoch 2/3\n2368/2368 [==============================] - 51s 21ms/step - loss: 1.1989 - accuracy: 0.6896 - val_loss: 1.0340 - val_accuracy: 0.7135\nEpoch 3/3\n2368/2368 [==============================] - 51s 21ms/step - loss: 1.0530 - accuracy: 0.7241 - val_loss: 0.9952 - val_accuracy: 0.7240\n</pre> <p>Nice, looks like our feature extraction model is performing pretty well. How about we evaluate it on the whole test dataset?</p> In\u00a0[29]: Copied! <pre># Evaluate model (unsaved version) on whole test dataset\nresults_feature_extract_model = model.evaluate(test_data)\nresults_feature_extract_model\n</pre> # Evaluate model (unsaved version) on whole test dataset results_feature_extract_model = model.evaluate(test_data) results_feature_extract_model <pre>790/790 [==============================] - 11s 14ms/step - loss: 0.9993 - accuracy: 0.7279\n</pre> Out[29]: <pre>[0.9992507100105286, 0.7279207706451416]</pre> <p>And since we used the <code>ModelCheckpoint</code> callback, we've got a saved version of our model in the <code>model_checkpoints</code> directory.</p> <p>Let's load it in and make sure it performs just as well.</p> In\u00a0[30]: Copied! <pre># 1. Create a function to recreate the original model\ndef create_model():\n  # Create base model\n  input_shape = (224, 224, 3)\n  base_model = tf.keras.applications.efficientnet.EfficientNetB0(include_top=False)\n  base_model.trainable = False # freeze base model layers\n\n  # Create Functional model \n  inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n  # Note: EfficientNetBX models have rescaling built-in but if your model didn't you could have a layer like below\n  # x = layers.Rescaling(1./255)(x)\n  x = base_model(inputs, training=False) # set base_model to inference mode only\n  x = layers.GlobalAveragePooling2D(name=\"pooling_layer\")(x)\n  x = layers.Dense(len(class_names))(x) # want one output neuron per class \n  # Separate activation of output layer so we can output float32 activations\n  outputs = layers.Activation(\"softmax\", dtype=tf.float32, name=\"softmax_float32\")(x) \n  model = tf.keras.Model(inputs, outputs)\n  \n  return model\n\n# 2. Create and compile a new version of the original model (new weights)\ncreated_model = create_model()\ncreated_model.compile(loss=\"sparse_categorical_crossentropy\",\n                      optimizer=tf.keras.optimizers.Adam(),\n                      metrics=[\"accuracy\"])\n\n# 3. Load the saved weights\ncreated_model.load_weights(checkpoint_path)\n\n# 4. Evaluate the model with loaded weights\nresults_created_model_with_loaded_weights = created_model.evaluate(test_data)\n</pre> # 1. Create a function to recreate the original model def create_model():   # Create base model   input_shape = (224, 224, 3)   base_model = tf.keras.applications.efficientnet.EfficientNetB0(include_top=False)   base_model.trainable = False # freeze base model layers    # Create Functional model    inputs = layers.Input(shape=input_shape, name=\"input_layer\")   # Note: EfficientNetBX models have rescaling built-in but if your model didn't you could have a layer like below   # x = layers.Rescaling(1./255)(x)   x = base_model(inputs, training=False) # set base_model to inference mode only   x = layers.GlobalAveragePooling2D(name=\"pooling_layer\")(x)   x = layers.Dense(len(class_names))(x) # want one output neuron per class    # Separate activation of output layer so we can output float32 activations   outputs = layers.Activation(\"softmax\", dtype=tf.float32, name=\"softmax_float32\")(x)    model = tf.keras.Model(inputs, outputs)      return model  # 2. Create and compile a new version of the original model (new weights) created_model = create_model() created_model.compile(loss=\"sparse_categorical_crossentropy\",                       optimizer=tf.keras.optimizers.Adam(),                       metrics=[\"accuracy\"])  # 3. Load the saved weights created_model.load_weights(checkpoint_path)  # 4. Evaluate the model with loaded weights results_created_model_with_loaded_weights = created_model.evaluate(test_data) <pre>790/790 [==============================] - 15s 15ms/step - loss: 0.9993 - accuracy: 0.7279\n</pre> <p>Our <code>created_model</code> with loaded weight's results should be very close to the feature extraction model's results (if the cell below errors, something went wrong).</p> In\u00a0[31]: Copied! <pre># 5. Loaded checkpoint weights should return very similar results to checkpoint weights prior to saving\nimport numpy as np\nassert np.isclose(results_feature_extract_model, results_created_model_with_loaded_weights).all(), \"Loaded weights results are not close to original model.\"  # check if all elements in array are close\n</pre> # 5. Loaded checkpoint weights should return very similar results to checkpoint weights prior to saving import numpy as np assert np.isclose(results_feature_extract_model, results_created_model_with_loaded_weights).all(), \"Loaded weights results are not close to original model.\"  # check if all elements in array are close <p>Cloning the model preserves <code>dtype_policy</code>'s of layers (but doesn't preserve weights) so if we wanted to continue fine-tuning our <code>created_model</code>, we could and it would still use the mixed precision dtype policy.</p> In\u00a0[32]: Copied! <pre># Check the layers in the base model and see what dtype policy they're using\nfor layer in created_model.layers[1].layers[:20]: # check only the first 20 layers to save printing space\n    print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)\n</pre> # Check the layers in the base model and see what dtype policy they're using for layer in created_model.layers[1].layers[:20]: # check only the first 20 layers to save printing space     print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy) <pre>input_2 False float32 &lt;Policy \"float32\"&gt;\nrescaling_2 False float32 &lt;Policy \"mixed_float16\"&gt;\nnormalization_1 False float32 &lt;Policy \"mixed_float16\"&gt;\nrescaling_3 False float32 &lt;Policy \"mixed_float16\"&gt;\nstem_conv_pad False float32 &lt;Policy \"mixed_float16\"&gt;\nstem_conv False float32 &lt;Policy \"mixed_float16\"&gt;\nstem_bn False float32 &lt;Policy \"mixed_float16\"&gt;\nstem_activation False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_dwconv False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_bn False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_activation False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_se_squeeze False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_se_reshape False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_se_reduce False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_se_expand False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_se_excite False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_project_conv False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_project_bn False float32 &lt;Policy \"mixed_float16\"&gt;\nblock2a_expand_conv False float32 &lt;Policy \"mixed_float16\"&gt;\nblock2a_expand_bn False float32 &lt;Policy \"mixed_float16\"&gt;\n</pre> In\u00a0[33]: Copied! <pre># ## Saving model to Google Drive (optional)\n\n# # Create save path to drive \n# save_dir = \"drive/MyDrive/tensorflow_course/food_vision/07_efficientnetb0_feature_extract_model_mixed_precision/\"\n# # os.makedirs(save_dir) # Make directory if it doesn't exist\n\n# # Save model\n# model.save(save_dir)\n</pre> # ## Saving model to Google Drive (optional)  # # Create save path to drive  # save_dir = \"drive/MyDrive/tensorflow_course/food_vision/07_efficientnetb0_feature_extract_model_mixed_precision/\" # # os.makedirs(save_dir) # Make directory if it doesn't exist  # # Save model # model.save(save_dir) <p>We can also save it directly to our Google Colab instance.</p> <p>\ud83d\udd11 Note: Google Colab storage is ephemeral and your model will delete itself (along with any other saved files) when the Colab session expires.</p> In\u00a0[34]: Copied! <pre># Save model locally (if you're using Google Colab, your saved model will Colab instance terminates)\nsave_dir = \"07_efficientnetb0_feature_extract_model_mixed_precision\"\nmodel.save(save_dir)\n</pre> # Save model locally (if you're using Google Colab, your saved model will Colab instance terminates) save_dir = \"07_efficientnetb0_feature_extract_model_mixed_precision\" model.save(save_dir) <p>And again, we can check whether or not our model saved correctly by loading it in and evaluating it.</p> In\u00a0[35]: Copied! <pre># Load model previously saved above\nloaded_saved_model = tf.keras.models.load_model(save_dir)\n</pre> # Load model previously saved above loaded_saved_model = tf.keras.models.load_model(save_dir) <p>Loading a <code>SavedModel</code> also retains all of the underlying layers <code>dtype_policy</code> (we want them to be <code>\"mixed_float16\"</code>).</p> In\u00a0[36]: Copied! <pre># Check the layers in the base model and see what dtype policy they're using\nfor layer in loaded_saved_model.layers[1].layers[:20]: # check only the first 20 layers to save output space\n    print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)\n</pre> # Check the layers in the base model and see what dtype policy they're using for layer in loaded_saved_model.layers[1].layers[:20]: # check only the first 20 layers to save output space     print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy) <pre>input_1 True float32 &lt;Policy \"float32\"&gt;\nrescaling False float32 &lt;Policy \"mixed_float16\"&gt;\nnormalization False float32 &lt;Policy \"mixed_float16\"&gt;\nrescaling_1 False float32 &lt;Policy \"mixed_float16\"&gt;\nstem_conv_pad False float32 &lt;Policy \"mixed_float16\"&gt;\nstem_conv False float32 &lt;Policy \"mixed_float16\"&gt;\nstem_bn False float32 &lt;Policy \"mixed_float16\"&gt;\nstem_activation False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_dwconv False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_bn False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_activation False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_se_squeeze False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_se_reshape False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_se_reduce False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_se_expand False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_se_excite False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_project_conv False float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_project_bn False float32 &lt;Policy \"mixed_float16\"&gt;\nblock2a_expand_conv False float32 &lt;Policy \"mixed_float16\"&gt;\nblock2a_expand_bn False float32 &lt;Policy \"mixed_float16\"&gt;\n</pre> In\u00a0[37]: Copied! <pre># Check loaded model performance (this should be the same as results_feature_extract_model)\nresults_loaded_saved_model = loaded_saved_model.evaluate(test_data)\nresults_loaded_saved_model\n</pre> # Check loaded model performance (this should be the same as results_feature_extract_model) results_loaded_saved_model = loaded_saved_model.evaluate(test_data) results_loaded_saved_model <pre>790/790 [==============================] - 15s 16ms/step - loss: 0.9993 - accuracy: 0.7279\n</pre> Out[37]: <pre>[0.9992507696151733, 0.7279207706451416]</pre> In\u00a0[38]: Copied! <pre># The loaded model's results should equal (or at least be very close) to the model's results prior to saving\n# Note: this will only work if you've instatiated results variables \nimport numpy as np\nassert np.isclose(results_feature_extract_model, results_loaded_saved_model).all()\n</pre> # The loaded model's results should equal (or at least be very close) to the model's results prior to saving # Note: this will only work if you've instatiated results variables  import numpy as np assert np.isclose(results_feature_extract_model, results_loaded_saved_model).all() <p>That's what we want! Our loaded model performing as it should.</p> <p>\ud83d\udd11 Note: We spent a fair bit of time making sure our model saved correctly because training on a lot of data can be time-consuming, so we want to make sure we don't have to continaully train from scratch.</p> In\u00a0[39]: Copied! <pre># Download the saved model from Google Storage\n!wget https://storage.googleapis.com/ztm_tf_course/food_vision/07_efficientnetb0_feature_extract_model_mixed_precision.zip\n</pre> # Download the saved model from Google Storage !wget https://storage.googleapis.com/ztm_tf_course/food_vision/07_efficientnetb0_feature_extract_model_mixed_precision.zip  <pre>--2023-05-19 02:28:24--  https://storage.googleapis.com/ztm_tf_course/food_vision/07_efficientnetb0_feature_extract_model_mixed_precision.zip\nResolving storage.googleapis.com (storage.googleapis.com)... 142.250.4.128, 142.251.10.128, 142.251.12.128, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|142.250.4.128|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 16976857 (16M) [application/zip]\nSaving to: \u201807_efficientnetb0_feature_extract_model_mixed_precision.zip\u2019\n\n07_efficientnetb0_f 100%[===================&gt;]  16.19M  8.80MB/s    in 1.8s    \n\n2023-05-19 02:28:27 (8.80 MB/s) - \u201807_efficientnetb0_feature_extract_model_mixed_precision.zip\u2019 saved [16976857/16976857]\n\n</pre> In\u00a0[40]: Copied! <pre># Unzip the SavedModel downloaded from Google Stroage\n!mkdir downloaded_gs_model # create new dir to store downloaded feature extraction model\n!unzip 07_efficientnetb0_feature_extract_model_mixed_precision.zip -d downloaded_gs_model\n</pre> # Unzip the SavedModel downloaded from Google Stroage !mkdir downloaded_gs_model # create new dir to store downloaded feature extraction model !unzip 07_efficientnetb0_feature_extract_model_mixed_precision.zip -d downloaded_gs_model <pre>Archive:  07_efficientnetb0_feature_extract_model_mixed_precision.zip\n   creating: downloaded_gs_model/07_efficientnetb0_feature_extract_model_mixed_precision/\n   creating: downloaded_gs_model/07_efficientnetb0_feature_extract_model_mixed_precision/variables/\n  inflating: downloaded_gs_model/07_efficientnetb0_feature_extract_model_mixed_precision/variables/variables.data-00000-of-00001  \n  inflating: downloaded_gs_model/07_efficientnetb0_feature_extract_model_mixed_precision/variables/variables.index  \n  inflating: downloaded_gs_model/07_efficientnetb0_feature_extract_model_mixed_precision/saved_model.pb  \n   creating: downloaded_gs_model/07_efficientnetb0_feature_extract_model_mixed_precision/assets/\n</pre> In\u00a0[\u00a0]: Copied! <pre># Load and evaluate downloaded GS model\nloaded_gs_model = tf.keras.models.load_model(\"downloaded_gs_model/07_efficientnetb0_feature_extract_model_mixed_precision\")\n</pre> # Load and evaluate downloaded GS model loaded_gs_model = tf.keras.models.load_model(\"downloaded_gs_model/07_efficientnetb0_feature_extract_model_mixed_precision\") In\u00a0[42]: Copied! <pre># Get a summary of our downloaded model\nloaded_gs_model.summary()\n</pre> # Get a summary of our downloaded model loaded_gs_model.summary() <pre>Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n                                                                 \n efficientnetb0 (Functional  (None, None, None, 1280   4049571   \n )                           )                                   \n                                                                 \n pooling_layer (GlobalAvera  (None, 1280)              0         \n gePooling2D)                                                    \n                                                                 \n dense (Dense)               (None, 101)               129381    \n                                                                 \n softmax_float32 (Activatio  (None, 101)               0         \n n)                                                              \n                                                                 \n=================================================================\nTotal params: 4178952 (15.94 MB)\nTrainable params: 129381 (505.39 KB)\nNon-trainable params: 4049571 (15.45 MB)\n_________________________________________________________________\n</pre> <p>And now let's make sure our loaded model is performing as expected.</p> In\u00a0[43]: Copied! <pre># How does the loaded model perform?\nresults_loaded_gs_model = loaded_gs_model.evaluate(test_data)\nresults_loaded_gs_model\n</pre> # How does the loaded model perform? results_loaded_gs_model = loaded_gs_model.evaluate(test_data) results_loaded_gs_model <pre>790/790 [==============================] - 15s 16ms/step - loss: 1.0881 - accuracy: 0.7067\n</pre> Out[43]: <pre>[1.0880972146987915, 0.7066534757614136]</pre> <p>Great, our loaded model is performing as expected.</p> <p>When we first created our model, we froze all of the layers in the base model by setting <code>base_model.trainable=False</code> but since we've loaded in our model from file, let's check whether or not the layers are trainable or not.</p> In\u00a0[44]: Copied! <pre># Are any of the layers in our model frozen?\nfor layer in loaded_gs_model.layers:\n    layer.trainable = True # set all layers to trainable\n    print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy) # make sure loaded model is using mixed precision dtype_policy (\"mixed_float16\")\n</pre> # Are any of the layers in our model frozen? for layer in loaded_gs_model.layers:     layer.trainable = True # set all layers to trainable     print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy) # make sure loaded model is using mixed precision dtype_policy (\"mixed_float16\") <pre>input_layer True float32 &lt;Policy \"float32\"&gt;\nefficientnetb0 True float32 &lt;Policy \"mixed_float16\"&gt;\npooling_layer True float32 &lt;Policy \"mixed_float16\"&gt;\ndense True float32 &lt;Policy \"mixed_float16\"&gt;\nsoftmax_float32 True float32 &lt;Policy \"float32\"&gt;\n</pre> <p>Alright, it seems like each layer in our loaded model is trainable. But what if we got a little deeper and inspected each of the layers in our base model?</p> <p>\ud83e\udd14 Question: Which layer in the loaded model is our base model?</p> <p>Before saving the Functional model to file, we created it with five layers (layers below are 0-indexed): 0. The input layer</p> <ol> <li>The pre-trained base model layer (<code>tf.keras.applications.efficientnet.EfficientNetB0</code>)</li> <li>The pooling layer</li> <li>The fully-connected (dense) layer</li> <li>The output softmax activation (with float32 dtype)</li> </ol> <p>Therefore to inspect our base model layer, we can access the <code>layers</code> attribute of the layer at index 1 in our model.</p> In\u00a0[45]: Copied! <pre># Check the layers in the base model and see what dtype policy they're using\nfor layer in loaded_gs_model.layers[1].layers[:20]:\n    print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)\n</pre> # Check the layers in the base model and see what dtype policy they're using for layer in loaded_gs_model.layers[1].layers[:20]:     print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy) <pre>input_1 True float32 &lt;Policy \"float32\"&gt;\nrescaling True float32 &lt;Policy \"mixed_float16\"&gt;\nnormalization True float32 &lt;Policy \"float32\"&gt;\nstem_conv_pad True float32 &lt;Policy \"mixed_float16\"&gt;\nstem_conv True float32 &lt;Policy \"mixed_float16\"&gt;\nstem_bn True float32 &lt;Policy \"mixed_float16\"&gt;\nstem_activation True float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_dwconv True float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_bn True float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_activation True float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_se_squeeze True float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_se_reshape True float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_se_reduce True float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_se_expand True float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_se_excite True float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_project_conv True float32 &lt;Policy \"mixed_float16\"&gt;\nblock1a_project_bn True float32 &lt;Policy \"mixed_float16\"&gt;\nblock2a_expand_conv True float32 &lt;Policy \"mixed_float16\"&gt;\nblock2a_expand_bn True float32 &lt;Policy \"mixed_float16\"&gt;\nblock2a_expand_activation True float32 &lt;Policy \"mixed_float16\"&gt;\n</pre> <p>Wonderful, it looks like each layer in our base model is trainable (unfrozen) and every layer which should be using the dtype policy <code>\"mixed_policy16\"</code> is using it.</p> <p>Since we've got so much data (750 images x 101 training classes = 75750 training images), let's keep all of our base model's layers unfrozen.</p> <p>\ud83d\udd11 Note: If you've got a small amount of data (less than 100 images per class), you may want to only unfreeze and fine-tune a small number of layers in the base model at a time. Otherwise, you risk overfitting.</p> In\u00a0[46]: Copied! <pre># Setup EarlyStopping callback to stop training if model's val_loss doesn't improve for 3 epochs\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", # watch the val loss metric\n                                                  patience=3) # if val loss decreases for 3 epochs in a row, stop training\n\n# Create ModelCheckpoint callback to save best model during fine-tuning\ncheckpoint_path = \"fine_tune_checkpoints/\"\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n                                                      save_best_only=True,\n                                                      monitor=\"val_loss\")\n</pre> # Setup EarlyStopping callback to stop training if model's val_loss doesn't improve for 3 epochs early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", # watch the val loss metric                                                   patience=3) # if val loss decreases for 3 epochs in a row, stop training  # Create ModelCheckpoint callback to save best model during fine-tuning checkpoint_path = \"fine_tune_checkpoints/\" model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,                                                       save_best_only=True,                                                       monitor=\"val_loss\") <p>Woohoo! Fine-tuning callbacks ready.</p> <p>If you're planning on training large models, the <code>ModelCheckpoint</code> and <code>EarlyStopping</code> are two callbacks you'll want to become very familiar with.</p> <p>We're almost ready to start fine-tuning our model but there's one more callback we're going to implement: <code>ReduceLROnPlateau</code>.</p> <p>Remember how the learning rate is the most important model hyperparameter you can tune? (if not, treat this as a reminder).</p> <p>Well, the <code>ReduceLROnPlateau</code> callback helps to tune the learning rate for you.</p> <p>Like the <code>ModelCheckpoint</code> and <code>EarlyStopping</code> callbacks, the <code>ReduceLROnPlateau</code> callback montiors a specified metric and when that metric stops improving, it reduces the learning rate by a specified factor (e.g. divides the learning rate by 10).</p> <p>\ud83e\udd14 Question: Why lower the learning rate?</p> <p>Imagine having a coin at the back of the couch and you're trying to grab with your fingers.</p> <p>Now think of the learning rate as the size of the movements your hand makes towards the coin.</p> <p>The closer you get, the smaller you want your hand movements to be, otherwise the coin will be lost.</p> <p>Our model's ideal performance is the equivalent of grabbing the coin. So as training goes on and our model gets closer and closer to it's ideal performance (also called convergence), we want the amount it learns to be less and less.</p> <p>To do this we'll create an instance of the <code>ReduceLROnPlateau</code> callback to monitor the validation loss just like the <code>EarlyStopping</code> callback.</p> <p>Once the validation loss stops improving for two or more epochs, we'll reduce the learning rate by a factor of 5 (e.g. <code>0.001</code> to <code>0.0002</code>).</p> <p>And to make sure the learning rate doesn't get too low (and potentially result in our model learning nothing), we'll set the minimum learning rate to <code>1e-7</code>.</p> In\u00a0[47]: Copied! <pre># Creating learning rate reduction callback\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",  \n                                                 factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)\n                                                 patience=2,\n                                                 verbose=1, # print out when learning rate goes down \n                                                 min_lr=1e-7)\n</pre> # Creating learning rate reduction callback reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",                                                    factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)                                                  patience=2,                                                  verbose=1, # print out when learning rate goes down                                                   min_lr=1e-7) <p>Learning rate reduction ready to go!</p> <p>Now before we start training, we've got to recompile our model.</p> <p>We'll use sparse categorical crossentropy as the loss and since we're fine-tuning, we'll use a 10x lower learning rate than the Adam optimizers default (<code>1e-4</code> instead of <code>1e-3</code>).</p> In\u00a0[48]: Copied! <pre># Compile the model\nloaded_gs_model.compile(loss=\"sparse_categorical_crossentropy\", # sparse_categorical_crossentropy for labels that are *not* one-hot\n                        optimizer=tf.keras.optimizers.Adam(0.0001), # 10x lower learning rate than the default\n                        metrics=[\"accuracy\"])\n</pre> # Compile the model loaded_gs_model.compile(loss=\"sparse_categorical_crossentropy\", # sparse_categorical_crossentropy for labels that are *not* one-hot                         optimizer=tf.keras.optimizers.Adam(0.0001), # 10x lower learning rate than the default                         metrics=[\"accuracy\"]) <p>Okay, model compiled.</p> <p>Now let's fit it on all of the data.</p> <p>We'll set it up to run for up to 100 epochs.</p> <p>Since we're going to be using the <code>EarlyStopping</code> callback, it might stop before reaching 100 epochs.</p> <p>\ud83d\udd11 Note: Running the cell below will set the model up to fine-tune all of the pre-trained weights in the base model on all of the Food101 data. Doing so with unoptimized data pipelines and without mixed precision training will take a fairly long time per epoch depending on what type of GPU you're using (about 15-20 minutes on Colab GPUs). But don't worry, the code we've written above will ensure it runs much faster (more like 4-5 minutes per epoch).</p> In\u00a0[49]: Copied! <pre># Start to fine-tune (all layers)\nhistory_101_food_classes_all_data_fine_tune = loaded_gs_model.fit(train_data,\n                                                        epochs=100, # fine-tune for a maximum of 100 epochs\n                                                        steps_per_epoch=len(train_data),\n                                                        validation_data=test_data,\n                                                        validation_steps=int(0.15 * len(test_data)), # validation during training on 15% of test data\n                                                        callbacks=[create_tensorboard_callback(\"training_logs\", \"efficientb0_101_classes_all_data_fine_tuning\"), # track the model training logs\n                                                                   model_checkpoint, # save only the best model during training\n                                                                   early_stopping, # stop model after X epochs of no improvements\n                                                                   reduce_lr]) # reduce the learning rate after X epochs of no improvements\n</pre> # Start to fine-tune (all layers) history_101_food_classes_all_data_fine_tune = loaded_gs_model.fit(train_data,                                                         epochs=100, # fine-tune for a maximum of 100 epochs                                                         steps_per_epoch=len(train_data),                                                         validation_data=test_data,                                                         validation_steps=int(0.15 * len(test_data)), # validation during training on 15% of test data                                                         callbacks=[create_tensorboard_callback(\"training_logs\", \"efficientb0_101_classes_all_data_fine_tuning\"), # track the model training logs                                                                    model_checkpoint, # save only the best model during training                                                                    early_stopping, # stop model after X epochs of no improvements                                                                    reduce_lr]) # reduce the learning rate after X epochs of no improvements <pre>Saving TensorBoard log files to: training_logs/efficientb0_101_classes_all_data_fine_tuning/20230519-022854\nEpoch 1/100\n2368/2368 [==============================] - 246s 81ms/step - loss: 0.9223 - accuracy: 0.7525 - val_loss: 0.7872 - val_accuracy: 0.7749 - lr: 1.0000e-04\nEpoch 2/100\n2368/2368 [==============================] - 191s 81ms/step - loss: 0.5795 - accuracy: 0.8399 - val_loss: 0.7839 - val_accuracy: 0.7831 - lr: 1.0000e-04\nEpoch 3/100\n2368/2368 [==============================] - 162s 68ms/step - loss: 0.3299 - accuracy: 0.9063 - val_loss: 0.8827 - val_accuracy: 0.7765 - lr: 1.0000e-04\nEpoch 4/100\n2368/2368 [==============================] - ETA: 0s - loss: 0.1722 - accuracy: 0.9486\nEpoch 4: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n2368/2368 [==============================] - 162s 68ms/step - loss: 0.1722 - accuracy: 0.9486 - val_loss: 0.9571 - val_accuracy: 0.7850 - lr: 1.0000e-04\nEpoch 5/100\n2368/2368 [==============================] - 162s 68ms/step - loss: 0.0359 - accuracy: 0.9920 - val_loss: 1.0549 - val_accuracy: 0.8032 - lr: 2.0000e-05\n</pre> <p>\ud83d\udd11 Note: If you didn't use mixed precision or use techniques such as <code>prefetch()</code> in the Batch &amp; prepare datasets section, your model fine-tuning probably takes up to 2.5-3x longer per epoch (see the output below for an example).</p> Prefetch and mixed precision No prefetch and no mixed precision Time per epoch ~280-300s ~1127-1397s <p>Results from fine-tuning \ud83c\udf54\ud83d\udc41 Food Vision Big\u2122 on Food101 dataset using an EfficienetNetB0 backbone using a Google Colab Tesla T4 GPU.</p> <pre><code>Saving TensorBoard log files to: training_logs/efficientB0_101_classes_all_data_fine_tuning/20200928-013008\nEpoch 1/100\n2368/2368 [==============================] - 1397s 590ms/step - loss: 1.2068 - accuracy: 0.6820 - val_loss: 1.1623 - val_accuracy: 0.6894\nEpoch 2/100\n2368/2368 [==============================] - 1193s 504ms/step - loss: 0.9459 - accuracy: 0.7444 - val_loss: 1.1549 - val_accuracy: 0.6872\nEpoch 3/100\n2368/2368 [==============================] - 1143s 482ms/step - loss: 0.7848 - accuracy: 0.7838 - val_loss: 1.0402 - val_accuracy: 0.7142\nEpoch 4/100\n2368/2368 [==============================] - 1127s 476ms/step - loss: 0.6599 - accuracy: 0.8149 - val_loss: 0.9599 - val_accuracy: 0.7373\n</code></pre> <p>Example fine-tuning time for non-prefetched data as well as non-mixed precision training (~2.5-3x longer per epoch).</p> <p>Let's make sure we save our model before we start evaluating it.</p> <p>From the above, does it look like our model is overfitting or underfitting?</p> <p>Remember, if the training loss is significantly lower than the validation loss, it's a hint that the model has overfit the training data and not learned generalizable patterns to unseen data.</p> <p>But it does look like our model has gained a few performance points from fine-tuning, let's evaluate on the whole test dataset and see if managed to beat the DeepFood paper's result of 77.4% accuracy.</p> In\u00a0[50]: Copied! <pre># # Save model to Google Drive (optional)\n# loaded_gs_model.save(\"/content/drive/MyDrive/tensorflow_course/food_vision/07_efficientnetb0_fine_tuned_101_classes_mixed_precision/\")\n</pre> # # Save model to Google Drive (optional) # loaded_gs_model.save(\"/content/drive/MyDrive/tensorflow_course/food_vision/07_efficientnetb0_fine_tuned_101_classes_mixed_precision/\") In\u00a0[51]: Copied! <pre># Save model locally (note: if you're using Google Colab and you save your model locally, it will be deleted when your Google Colab session ends)\nloaded_gs_model.save(\"07_efficientnetb0_fine_tuned_101_classes_mixed_precision\")\n</pre> # Save model locally (note: if you're using Google Colab and you save your model locally, it will be deleted when your Google Colab session ends) loaded_gs_model.save(\"07_efficientnetb0_fine_tuned_101_classes_mixed_precision\") In\u00a0[52]: Copied! <pre># Download and evaluate fine-tuned model from Google Storage\n!wget https://storage.googleapis.com/ztm_tf_course/food_vision/07_efficientnetb0_fine_tuned_101_classes_mixed_precision.zip\n</pre> # Download and evaluate fine-tuned model from Google Storage !wget https://storage.googleapis.com/ztm_tf_course/food_vision/07_efficientnetb0_fine_tuned_101_classes_mixed_precision.zip <pre>--2023-05-19 02:44:48--  https://storage.googleapis.com/ztm_tf_course/food_vision/07_efficientnetb0_fine_tuned_101_classes_mixed_precision.zip\nResolving storage.googleapis.com (storage.googleapis.com)... 142.250.4.128, 142.251.10.128, 142.251.12.128, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|142.250.4.128|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 46790356 (45M) [application/zip]\nSaving to: \u201807_efficientnetb0_fine_tuned_101_classes_mixed_precision.zip\u2019\n\n07_efficientnetb0_f 100%[===================&gt;]  44.62M  14.1MB/s    in 3.2s    \n\n2023-05-19 02:44:51 (14.1 MB/s) - \u201807_efficientnetb0_fine_tuned_101_classes_mixed_precision.zip\u2019 saved [46790356/46790356]\n\n</pre> <p>The downloaded model comes in zip format (<code>.zip</code>) so we'll unzip it into the Google Colab instance.</p> In\u00a0[53]: Copied! <pre># Unzip fine-tuned model\n!mkdir downloaded_fine_tuned_gs_model # create separate directory for fine-tuned model downloaded from Google Storage\n!unzip 07_efficientnetb0_fine_tuned_101_classes_mixed_precision -d downloaded_fine_tuned_gs_model\n</pre> # Unzip fine-tuned model !mkdir downloaded_fine_tuned_gs_model # create separate directory for fine-tuned model downloaded from Google Storage !unzip 07_efficientnetb0_fine_tuned_101_classes_mixed_precision -d downloaded_fine_tuned_gs_model <pre>Archive:  07_efficientnetb0_fine_tuned_101_classes_mixed_precision.zip\n   creating: downloaded_fine_tuned_gs_model/07_efficientnetb0_fine_tuned_101_classes_mixed_precision/\n   creating: downloaded_fine_tuned_gs_model/07_efficientnetb0_fine_tuned_101_classes_mixed_precision/variables/\n  inflating: downloaded_fine_tuned_gs_model/07_efficientnetb0_fine_tuned_101_classes_mixed_precision/variables/variables.data-00000-of-00001  \n  inflating: downloaded_fine_tuned_gs_model/07_efficientnetb0_fine_tuned_101_classes_mixed_precision/variables/variables.index  \n  inflating: downloaded_fine_tuned_gs_model/07_efficientnetb0_fine_tuned_101_classes_mixed_precision/saved_model.pb  \n   creating: downloaded_fine_tuned_gs_model/07_efficientnetb0_fine_tuned_101_classes_mixed_precision/assets/\n</pre> <p>Now we can load it using the <code>tf.keras.models.load_model()</code> method and get a summary (it should be the exact same as the model we created above).</p> In\u00a0[\u00a0]: Copied! <pre># Load in fine-tuned model from Google Storage and evaluate\nloaded_fine_tuned_gs_model = tf.keras.models.load_model(\"downloaded_fine_tuned_gs_model/07_efficientnetb0_fine_tuned_101_classes_mixed_precision\")\n</pre> # Load in fine-tuned model from Google Storage and evaluate loaded_fine_tuned_gs_model = tf.keras.models.load_model(\"downloaded_fine_tuned_gs_model/07_efficientnetb0_fine_tuned_101_classes_mixed_precision\") In\u00a0[55]: Copied! <pre># Get a model summary (same model architecture as above)\nloaded_fine_tuned_gs_model.summary()\n</pre> # Get a model summary (same model architecture as above) loaded_fine_tuned_gs_model.summary() <pre>Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n                                                                 \n efficientnetb0 (Functional  (None, None, None, 1280   4049571   \n )                           )                                   \n                                                                 \n pooling_layer (GlobalAvera  (None, 1280)              0         \n gePooling2D)                                                    \n                                                                 \n dense (Dense)               (None, 101)               129381    \n                                                                 \n softmax_float32 (Activatio  (None, 101)               0         \n n)                                                              \n                                                                 \n=================================================================\nTotal params: 4178952 (15.94 MB)\nTrainable params: 4136929 (15.78 MB)\nNon-trainable params: 42023 (164.16 KB)\n_________________________________________________________________\n</pre> <p>Finally, we can evaluate our model on the test data (this requires the <code>test_data</code> variable to be loaded.</p> In\u00a0[56]: Copied! <pre># Note: Even if you're loading in the model from Google Storage, you will still need to load the test_data variable for this cell to work\nresults_downloaded_fine_tuned_gs_model = loaded_fine_tuned_gs_model.evaluate(test_data)\nresults_downloaded_fine_tuned_gs_model\n</pre> # Note: Even if you're loading in the model from Google Storage, you will still need to load the test_data variable for this cell to work results_downloaded_fine_tuned_gs_model = loaded_fine_tuned_gs_model.evaluate(test_data) results_downloaded_fine_tuned_gs_model <pre>790/790 [==============================] - 15s 16ms/step - loss: 0.9072 - accuracy: 0.8017\n</pre> Out[56]: <pre>[0.9072489738464355, 0.801663339138031]</pre> <p>Excellent! Our saved model is performing as expected (better results than the DeepFood paper!).</p> <p>Congrautlations! You should be excited! You just trained a computer vision model with competitive performance to a research paper and in far less time (our model took ~20 minutes to train versus DeepFood's quoted 2-3 days).</p> <p>In other words, you brought Food Vision life!</p> <p>If you really wanted to step things up, you could try using the <code>EfficientNetB4</code> model (a larger version of <code>EfficientNetB0</code>). At at the time of writing, the EfficientNet family has the state of the art classification results on the Food101 dataset.</p> <p>\ud83d\udcd6 Resource: To see which models are currently performing the best on a given dataset or problem type as well as the latest trending machine learning research, be sure to check out paperswithcode.com and sotabench.com.</p> In\u00a0[57]: Copied! <pre># Upload experiment results to TensorBoard (uncomment to run)\n# !tensorboard dev upload --logdir ./training_logs \\\n#   --name \"Fine-tuning EfficientNetB0 on all Food101 Data\" \\\n#   --description \"Training results for fine-tuning EfficientNetB0 on Food101 Data with learning rate 0.0001\" \\\n#   --one_shot\n</pre> # Upload experiment results to TensorBoard (uncomment to run) # !tensorboard dev upload --logdir ./training_logs \\ #   --name \"Fine-tuning EfficientNetB0 on all Food101 Data\" \\ #   --description \"Training results for fine-tuning EfficientNetB0 on Food101 Data with learning rate 0.0001\" \\ #   --one_shot <p>Viewing at our model's training curves on TensorBoard.dev, it looks like our fine-tuning model gains boost in performance but starts to overfit as training goes on.</p> <p>See the training curves on TensorBoard.dev here: https://tensorboard.dev/experiment/2KINdYxgSgW2bUg7dIvevw/</p> <p>To fix this, in future experiments, we might try things like:</p> <ul> <li>A different iteration of <code>EfficientNet</code> (e.g. <code>EfficientNetB4</code> instead of <code>EfficientNetB0</code>).</li> <li>Unfreezing less layers of the base model and training them rather than unfreezing the whole base model in one go.</li> </ul> In\u00a0[58]: Copied! <pre># View past TensorBoard experiments\n# !tensorboard dev list\n</pre> # View past TensorBoard experiments # !tensorboard dev list In\u00a0[59]: Copied! <pre># Delete past TensorBoard experiments\n# !tensorboard dev delete --experiment_id YOUR_EXPERIMENT_ID\n\n# Example\n# !tensorboard dev delete --experiment_id OAE6KXizQZKQxDiqI3cnUQ\n</pre> # Delete past TensorBoard experiments # !tensorboard dev delete --experiment_id YOUR_EXPERIMENT_ID  # Example # !tensorboard dev delete --experiment_id OAE6KXizQZKQxDiqI3cnUQ"},{"location":"07_food_vision_milestone_project_1/#07-milestone-project-1-food-vision-bigtm","title":"07 Milestone Project 1: \ud83c\udf54\ud83d\udc41 Food Vision Big\u2122\u00b6","text":"<p>In the previous notebook (transfer learning part 3: scaling up) we built Food Vision mini: a transfer learning model which beat the original results of the Food101 paper with only 10% of the data.</p> <p>But you might be wondering, what would happen if we used all the data?</p> <p>Well, that's what we're going to find out in this notebook!</p> <p>We're going to be building Food Vision Big\u2122, using all of the data from the Food101 dataset.</p> <p>Yep. All 75,750 training images and 25,250 testing images.</p> <p>And guess what...</p> <p>This time we've got the goal of beating DeepFood, a 2016 paper which used a Convolutional Neural Network trained for 2-3 days to achieve 77.4% top-1 accuracy.</p> <p>\ud83d\udd11 Note: Top-1 accuracy means \"accuracy for the top softmax activation value output by the model\" (because softmax ouputs a value for every class, but top-1 means only the highest one is evaluated). Top-5 accuracy means \"accuracy for the top 5 softmax activation values output by the model\", in other words, did the true label appear in the top 5 activation values? Top-5 accuracy scores are usually noticeably higher than top-1.</p> \ud83c\udf54\ud83d\udc41 Food Vision Big\u2122 \ud83c\udf54\ud83d\udc41 Food Vision mini Dataset source TensorFlow Datasets Preprocessed download from Kaggle Train data 75,750 images 7,575 images Test data 25,250 images 25,250 images Mixed precision Yes No Data loading Performanant tf.data API TensorFlow pre-built function Target results 77.4% top-1 accuracy (beat DeepFood paper) 50.76% top-1 accuracy (beat Food101 paper) <p>Table comparing difference between Food Vision Big (this notebook) versus Food Vision mini (previous notebook).</p> <p>Alongside attempting to beat the DeepFood paper, we're going to learn about two methods to significantly improve the speed of our model training:</p> <ol> <li>Prefetching</li> <li>Mixed precision training</li> </ol> <p>But more on these later.</p>"},{"location":"07_food_vision_milestone_project_1/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<ul> <li>Using TensorFlow Datasets to download and explore data</li> <li>Creating preprocessing function for our data</li> <li>Batching &amp; preparing datasets for modelling (making our datasets run fast)</li> <li>Creating modelling callbacks</li> <li>Setting up mixed precision training</li> <li>Building a feature extraction model (see transfer learning part 1: feature extraction)</li> <li>Fine-tuning the feature extraction model (see transfer learning part 2: fine-tuning)</li> <li>Viewing training results on TensorBoard</li> </ul>"},{"location":"07_food_vision_milestone_project_1/#how-you-should-approach-this-notebook","title":"How you should approach this notebook\u00b6","text":"<p>You can read through the descriptions and the code (it should all run, except for the cells which error on purpose), but there's a better option.</p> <p>Write all of the code yourself.</p> <p>Yes. I'm serious. Create a new notebook, and rewrite each line by yourself. Investigate it, see if you can break it, why does it break?</p> <p>You don't have to write the text descriptions but writing the code yourself is a great way to get hands-on experience.</p> <p>Don't worry if you make mistakes, we all do. The way to get better and make less mistakes is to write more code.</p> <p>\ud83d\udcd6 Resources:</p> <ul> <li>See the full set of course materials on GitHub: https://github.com/mrdbourke/tensorflow-deep-learning</li> <li>See updates to this notebook on GitHub: https://github.com/mrdbourke/tensorflow-deep-learning/discussions/550</li> </ul>"},{"location":"07_food_vision_milestone_project_1/#check-gpu","title":"Check GPU\u00b6","text":"<p>For this notebook, we're going to be doing something different.</p> <p>We're going to be using mixed precision training.</p> <p>Mixed precision training was introduced in TensorFlow 2.4.0 (a very new feature at the time of writing).</p> <p>What does mixed precision training do?</p> <p>Mixed precision training uses a combination of single precision (float32) and half-preicison (float16) data types to speed up model training (up 3x on modern GPUs).</p> <p>We'll talk about this more later on but in the meantime you can read the TensorFlow documentation on mixed precision for more details.</p> <p>For now, before we can move forward if we want to use mixed precision training, we need to make sure the GPU powering our Google Colab instance (if you're using Google Colab) is compataible.</p> <p>For mixed precision training to work, you need access to a GPU with a compute compability score of 7.0+.</p> <p>Google Colab offers several kinds of GPU.</p> <p>However, some of them aren't compatiable with mixed precision training.</p> <p>Therefore to make sure you have access to mixed precision training in Google Colab, you can check your GPU compute capability score on Nvidia's developer website.</p> <p>As of May 2023, the GPUs available on Google Colab which allow mixed precision training are:</p> <ul> <li>NVIDIA A100 (available with Google Colab Pro)</li> <li>NVIDIA Tesla T4</li> </ul> <p>\ud83d\udd11 Note: You can run the cell below to check your GPU name and then compare it to list of GPUs on NVIDIA's developer page to see if it's capable of using mixed precision training.</p>"},{"location":"07_food_vision_milestone_project_1/#get-helper-functions","title":"Get helper functions\u00b6","text":"<p>We've created a series of helper functions throughout the previous notebooks in the course. Instead of rewriting them (tedious), we'll import the <code>helper_functions.py</code> file from the GitHub repo.</p>"},{"location":"07_food_vision_milestone_project_1/#use-tensorflow-datasets-to-download-data","title":"Use TensorFlow Datasets to Download Data\u00b6","text":"<p>In previous notebooks, we've downloaded our food images (from the Food101 dataset) from Google Storage.</p> <p>And this is a typical workflow you'd use if you're working on your own datasets.</p> <p>However, there's another way to get datasets ready to use with TensorFlow.</p> <p>For many of the most popular datasets in the machine learning world (often referred to and used as benchmarks), you can access them through TensorFlow Datasets (TFDS).</p> <p>What is TensorFlow Datasets?</p> <p>A place for prepared and ready-to-use machine learning datasets.</p> <p>Why use TensorFlow Datasets?</p> <ul> <li>Load data already in Tensors</li> <li>Practice on well established datasets</li> <li>Experiment with differet data loading techniques (like we're going to use in this notebook)</li> <li>Experiment with new TensorFlow features quickly (such as mixed precision training)</li> </ul> <p>Why not use TensorFlow Datasets?</p> <ul> <li>The datasets are static (they don't change, like your real-world datasets would)</li> <li>Might not be suited for your particular problem (but great for experimenting)</li> </ul> <p>To begin using TensorFlow Datasets we can import it under the alias <code>tfds</code>.</p>"},{"location":"07_food_vision_milestone_project_1/#exploring-the-food101-data-from-tensorflow-datasets","title":"Exploring the Food101 data from TensorFlow Datasets\u00b6","text":"<p>Now we've downloaded the Food101 dataset from TensorFlow Datasets, how about we do what any good data explorer should?</p> <p>In other words, \"visualize, visualize, visualize\".</p> <p>Let's find out a few details about our dataset:</p> <ul> <li>The shape of our input data (image tensors)</li> <li>The datatype of our input data</li> <li>What the labels of our input data look like (e.g. one-hot encoded versus label-encoded)</li> <li>Do the labels match up with the class names?</li> </ul> <p>To do, let's take one sample off the training data (using the <code>.take()</code> method) and explore it.</p>"},{"location":"07_food_vision_milestone_project_1/#plot-an-image-from-tensorflow-datasets","title":"Plot an image from TensorFlow Datasets\u00b6","text":"<p>We've seen our image tensors in tensor format, now let's really adhere to our motto.</p> <p>\"Visualize, visualize, visualize!\"</p> <p>Let's plot one of the image samples using <code>matplotlib.pyplot.imshow()</code> and set the title to target class name.</p>"},{"location":"07_food_vision_milestone_project_1/#create-preprocessing-functions-for-our-data","title":"Create preprocessing functions for our data\u00b6","text":"<p>In previous notebooks, when our images were in folder format we used the method <code>tf.keras.utils.image_dataset_from_directory()</code> to load them in.</p> <p>Doing this meant our data was loaded into a format ready to be used with our models.</p> <p>However, since we've downloaded the data from TensorFlow Datasets, there are a couple of preprocessing steps we have to take before it's ready to model.</p> <p>More specifically, our data is currently:</p> <ul> <li>In <code>uint8</code> data type</li> <li>Comprised of all differnet sized tensors (different sized images)</li> <li>Not scaled (the pixel values are between 0 &amp; 255)</li> </ul> <p>Whereas, models like data to be:</p> <ul> <li>In <code>float32</code> data type</li> <li>Have all of the same size tensors (batches require all tensors have the same shape, e.g. <code>(224, 224, 3)</code>)</li> <li>Scaled (values between 0 &amp; 1), also called normalized</li> </ul> <p>To take care of these, we'll create a <code>preprocess_img()</code> function which:</p> <ul> <li>Resizes an input image tensor to a specified size using <code>tf.image.resize()</code></li> <li>Converts an input image tensor's current datatype to <code>tf.float32</code> using <code>tf.cast()</code></li> </ul> <p>\ud83d\udd11 Note: Pretrained EfficientNetBX models in <code>tf.keras.applications.efficientnet</code> (what we're going to be using) have rescaling built-in. But for many other model architectures you'll want to rescale your data (e.g. get its values between 0 &amp; 1). This could be incorporated inside your \"<code>preprocess_img()</code>\" function (like the one below) or within your model as a <code>tf.keras.layers.Rescaling</code> layer.</p>"},{"location":"07_food_vision_milestone_project_1/#batch-prepare-datasets","title":"Batch &amp; prepare datasets\u00b6","text":"<p>Before we can model our data, we have to turn it into batches.</p> <p>Why?</p> <p>Because computing on batches is memory efficient.</p> <p>We turn our data from 101,000 image tensors and labels (train and test combined) into batches of 32 image and label pairs, thus enabling it to fit into the memory of our GPU.</p> <p>To do this in effective way, we're going to be leveraging a number of methods from the <code>tf.data</code> API.</p> <p>\ud83d\udcd6 Resource: For loading data in the most performant way possible, see the TensorFlow docuemntation on Better performance with the tf.data API.</p> <p>Specifically, we're going to be using:</p> <ul> <li><code>map()</code> - maps a predefined function to a target dataset (e.g. <code>preprocess_img()</code> to our image tensors)</li> <li><code>shuffle()</code> - randomly shuffles the elements of a target dataset up <code>buffer_size</code> (ideally, the <code>buffer_size</code> is equal to the size of the dataset, however, this may have implications on memory)</li> <li><code>batch()</code> - turns elements of a target dataset into batches (size defined by parameter <code>batch_size</code>)</li> <li><code>prefetch()</code> - prepares subsequent batches of data whilst other batches of data are being computed on (improves data loading speed but costs memory)</li> <li>Extra: <code>cache()</code> - caches (saves them for later) elements in a target dataset, saving loading time (will only work if your dataset is small enough to fit in memory, standard Colab instances only have 12GB of memory)</li> </ul> <p>Things to note:</p> <ul> <li>Can't batch tensors of different shapes (e.g. different image sizes, need to reshape images first, hence our <code>preprocess_img()</code> function)</li> <li><code>shuffle()</code> keeps a buffer of the number you pass it images shuffled, ideally this number would be all of the samples in your training set, however, if your training set is large, this buffer might not fit in memory (a fairly large number like 1000 or 10000 is usually suffice for shuffling)</li> <li>For methods with the <code>num_parallel_calls</code> parameter available (such as <code>map()</code>), setting it to<code>num_parallel_calls=tf.data.AUTOTUNE</code> will parallelize preprocessing and significantly improve speed</li> <li>Can't use <code>cache()</code> unless your dataset can fit in memory</li> </ul> <p>Woah, the above is alot. But once we've coded below, it'll start to make sense.</p> <p>We're going to through things in the following order:</p> <pre><code>Original dataset (e.g. train_data) -&gt; map() -&gt; shuffle() -&gt; batch() -&gt; prefetch() -&gt; PrefetchDataset\n</code></pre> <p>This is like saying,</p> <p>\"Hey, map this preprocessing function across our training dataset, then shuffle a number of elements before batching them together and make sure you prepare new batches (prefetch) whilst the model is looking through the current batch\".</p> <p></p> <p>What happens when you use prefetching (faster) versus what happens when you don't use prefetching (slower). Source: Page 422 of Hands-On Machine Learning with Scikit-Learn, Keras &amp; TensorFlow Book by Aur\u00e9lien G\u00e9ron.</p>"},{"location":"07_food_vision_milestone_project_1/#create-modelling-callbacks","title":"Create modelling callbacks\u00b6","text":"<p>Since we're going to be training on a large amount of data and training could take a long time, it's a good idea to set up some modelling callbacks so we be sure of things like our model's training logs being tracked and our model being checkpointed (saved) after various training milestones.</p> <p>To do each of these we'll use the following callbacks:</p> <ul> <li><code>tf.keras.callbacks.TensorBoard()</code> - allows us to keep track of our model's training history so we can inspect it later (note: we've created this callback before have imported it from <code>helper_functions.py</code> as <code>create_tensorboard_callback()</code>)</li> <li><code>tf.keras.callbacks.ModelCheckpoint()</code> - saves our model's progress at various intervals so we can load it and resuse it later without having to retrain it<ul> <li>Checkpointing is also helpful so we can start fine-tuning our model at a particular epoch and revert back to a previous state if fine-tuning offers no benefits</li> </ul> </li> </ul>"},{"location":"07_food_vision_milestone_project_1/#setup-mixed-precision-training","title":"Setup mixed precision training\u00b6","text":"<p>We touched on mixed precision training above.</p> <p>However, we didn't quite explain it.</p> <p>Normally, tensors in TensorFlow default to the float32 datatype (unless otherwise specified).</p> <p>In computer science, float32 is also known as single-precision floating-point format. The 32 means it usually occupies 32 bits in computer memory.</p> <p>Your GPU has a limited memory, therefore it can only handle a number of float32 tensors at the same time.</p> <p>This is where mixed precision training comes in.</p> <p>Mixed precision training involves using a mix of float16 and float32 tensors to make better use of your GPU's memory.</p> <p>Can you guess what float16 means?</p> <p>Well, if you thought since float32 meant single-precision floating-point, you might've guessed float16 means half-precision floating-point format. And if you did, you're right! And if not, no trouble, now you know.</p> <p>For tensors in float16 format, each element occupies 16 bits in computer memory.</p> <p>So, where does this leave us?</p> <p>As mentioned before, when using mixed precision training, your model will make use of float32 and float16 data types to use less memory where possible and in turn run faster (using less memory per tensor means more tensors can be computed on simultaneously).</p> <p>As a result, using mixed precision training can improve your performance on modern GPUs (those with a compute capability score of 7.0+) by up to 3x.</p> <p>For a more detailed explanation, I encourage you to read through the TensorFlow mixed precision guide (I'd highly recommend at least checking out the summary).</p> <p> Because mixed precision training uses a combination of float32 and float16 data types, you may see up to a 3x speedup on modern GPUs.</p> <p>\ud83d\udd11 Note: If your GPU doesn't have a score of over 7.0+ (e.g. P100 in Google Colab), mixed precision won't work (see: \"Supported Hardware\" in the mixed precision guide for more).</p> <p>\ud83d\udcd6 Resource: If you'd like to learn more about precision in computer science (the detail to which a numerical quantity is expressed by a computer), see the Wikipedia page) (and accompanying resources).</p> <p>Okay, enough talk, let's see how we can turn on mixed precision training in TensorFlow.</p> <p>The beautiful thing is, the <code>tensorflow.keras.mixed_precision</code> API has made it very easy for us to get started.</p> <p>First, we'll import the API and then use the <code>set_global_policy()</code> method to set the dtype policy to <code>\"mixed_float16\"</code>.</p>"},{"location":"07_food_vision_milestone_project_1/#build-feature-extraction-model","title":"Build feature extraction model\u00b6","text":"<p>Callbacks: ready to roll.</p> <p>Mixed precision: turned on.</p> <p>Let's build a model.</p> <p>Because our dataset is quite large, we're going to move towards fine-tuning an existing pretrained model (EfficienetNetB0).</p> <p>But before we get into fine-tuning, let's set up a feature-extraction model.</p> <p>Recall, the typical order for using transfer learning is:</p> <ol> <li>Build a feature extraction model (replace the top few layers of a pretrained model)</li> <li>Train for a few epochs with lower layers frozen</li> <li>Fine-tune if necessary with multiple layers unfrozen</li> </ol> <p> Before fine-tuning, it's best practice to train a feature extraction model with custom top layers.</p> <p>To build the feature extraction model (covered in Transfer Learning in TensorFlow Part 1: Feature extraction), we'll:</p> <ul> <li>Use <code>EfficientNetB0</code> from <code>tf.keras.applications</code> pre-trained on ImageNet as our base model<ul> <li>We'll download this without the top layers using <code>include_top=False</code> parameter so we can create our own output layers</li> </ul> </li> <li>Freeze the base model layers so we can use the pre-learned patterns the base model has found on ImageNet</li> <li>Put together the input, base model, pooling and output layers in a Functional model</li> <li>Compile the Functional model using the Adam optimizer and sparse categorical crossentropy as the loss function (since our labels aren't one-hot encoded)</li> <li>Fit the model for 3 epochs using the TensorBoard and ModelCheckpoint callbacks</li> </ul> <p>\ud83d\udd11 Note: Since we're using mixed precision training, our model needs a separate output layer with a hard-coded <code>dtype=float32</code>, for example, <code>layers.Activation(\"softmax\", dtype=tf.float32)</code>. This ensures the outputs of our model are returned back to the float32 data type which is more numerically stable than the float16 datatype (important for loss calculations). See the \"Building the model\" section in the TensorFlow mixed precision guide for more.</p> <p> Turning mixed precision on in TensorFlow with 3 lines of code.</p>"},{"location":"07_food_vision_milestone_project_1/#checking-layer-dtype-policies-are-we-using-mixed-precision","title":"Checking layer dtype policies (are we using mixed precision?)\u00b6","text":"<p>Model ready to go!</p> <p>Before we said the mixed precision API will automatically change our layers' dtype policy's to whatever the global dtype policy is (in our case it's <code>\"mixed_float16\"</code>).</p> <p>We can check this by iterating through our model's layers and printing layer attributes such as <code>dtype</code> and <code>dtype_policy</code>.</p>"},{"location":"07_food_vision_milestone_project_1/#fit-the-feature-extraction-model","title":"Fit the feature extraction model\u00b6","text":"<p>Now that's one good looking model. Let's fit it to our data shall we?</p> <p>Three epochs should be enough for our top layers to adjust their weights enough to our food image data.</p> <p>To save time per epoch, we'll also only validate on 15% of the test data.</p>"},{"location":"07_food_vision_milestone_project_1/#load-and-evaluate-checkpoint-weights","title":"Load and evaluate checkpoint weights\u00b6","text":"<p>We can load in and evaluate our model's checkpoints by:</p> <ol> <li>Recreating a new instance of our model called <code>created_model</code> by turning our original model creation code into a function called <code>create_model()</code>.</li> <li>Compiling our <code>created_model</code> with the same loss, optimizer and metrics as the original model (every time you create a new model, you must compile it).</li> <li>Calling the <code>load_weights()</code> method on our <code>created_model</code> and passing it the path to where our checkpointed weights are stored.</li> <li>Calling <code>evaluate()</code> on <code>created_model</code> with loaded weights and saving the results.</li> <li>Comparing the <code>created_model</code> results to our previous <code>model</code> results (these should be the exact same, if not very close).</li> </ol> <p>A reminder, checkpoints are helpful for when you perform an experiment such as fine-tuning your model. In the case you fine-tune your feature extraction model and find it doesn't offer any improvements, you can always revert back to the checkpointed version of your model.</p> <p>Note: This section originally used the <code>tf.keras.clone_model</code> method, however, due to several potential errors with that method, it changed to create a new model (rather than cloning) via a <code>create_model()</code> function. See the discussion on the course GitHub for more.</p>"},{"location":"07_food_vision_milestone_project_1/#save-the-whole-model-to-file","title":"Save the whole model to file\u00b6","text":"<p>We can also save the whole model using the <code>save()</code> method.</p> <p>Since our model is quite large, you might want to save it to Google Drive (if you're using Google Colab) so you can load it in for use later.</p> <p>\ud83d\udd11 Note: Saving to Google Drive requires mounting Google Drive (go to Files -&gt; Mount Drive).</p>"},{"location":"07_food_vision_milestone_project_1/#preparing-our-models-layers-for-fine-tuning","title":"Preparing our model's layers for fine-tuning\u00b6","text":"<p>Our feature-extraction model is showing some great promise after three epochs. But since we've got so much data, it's probably worthwhile that we see what results we can get with fine-tuning (fine-tuning usually works best when you've got quite a large amount of data).</p> <p>Remember our goal of beating the DeepFood paper?</p> <p>They were able to achieve 77.4% top-1 accuracy on Food101 over 2-3 days of training.</p> <p>Do you think fine-tuning will get us there?</p> <p>Let's find out.</p> <p>To start, let's load in our saved model.</p> <p>\ud83d\udd11 Note: It's worth remembering a traditional workflow for fine-tuning is to freeze a pre-trained base model and then train only the output layers for a few iterations so their weights can be updated inline with your custom data (feature extraction). And then unfreeze a number or all of the layers in the base model and continue training until the model stops improving.</p> <p>Like all good cooking shows, I've saved a model I prepared earlier (the feature extraction model from above) to Google Storage.</p> <p>We can download it to make sure we're using the same model going forward.</p>"},{"location":"07_food_vision_milestone_project_1/#a-couple-more-callbacks","title":"A couple more callbacks\u00b6","text":"<p>We're about to start fine-tuning a deep learning model with over 200 layers using over 100,000 (75k+ training, 25K+ testing) images, which means our model's training time is probably going to be much longer than before.</p> <p>\ud83e\udd14 Question: How long does training take?</p> <p>It could be a couple of hours or in the case of the DeepFood paper (the baseline we're trying to beat), their best performing model took 2-3 days of training time.</p> <p>You will really only know how long it'll take once you start training.</p> <p>\ud83e\udd14 Question: When do you stop training?</p> <p>Ideally, when your model stops improving. But again, due to the nature of deep learning, it can be hard to know when exactly a model will stop improving.</p> <p>Luckily, there's a solution: the <code>EarlyStopping</code> callback.</p> <p>The <code>EarlyStopping</code> callback monitors a specified model performance metric (e.g. <code>val_loss</code>) and when it stops improving for a specified number of epochs, automatically stops training.</p> <p>Using the <code>EarlyStopping</code> callback combined with the <code>ModelCheckpoint</code> callback saving the best performing model automatically, we could keep our model training for an unlimited number of epochs until it stops improving.</p> <p>Let's set both of these up to monitor our model's <code>val_loss</code>.</p>"},{"location":"07_food_vision_milestone_project_1/#download-fine-tuned-model-from-google-storage","title":"Download fine-tuned model from Google Storage\u00b6","text":"<p>As mentioned before, training models can take a significant amount of time.</p> <p>And again, like any good cooking show, here's something we prepared earlier...</p> <p>It's a fine-tuned model exactly like the one we trained above but it's saved to Google Storage so it can be accessed, imported and evaluated.</p>"},{"location":"07_food_vision_milestone_project_1/#view-training-results-on-tensorboard","title":"View training results on TensorBoard\u00b6","text":"<p>Since we tracked our model's fine-tuning training logs using the <code>TensorBoard</code> callback, let's upload them and inspect them on TensorBoard.dev.</p>"},{"location":"07_food_vision_milestone_project_1/#exercises","title":"\ud83d\udee0 Exercises\u00b6","text":"<ol> <li>Use the same evaluation techniques on the large-scale Food Vision model as you did in the previous notebook (Transfer Learning Part 3: Scaling up). More specifically, it would be good to see:</li> </ol> <ul> <li>A confusion matrix between all of the model's predictions and true labels.</li> <li>A graph showing the f1-scores of each class.</li> <li>A visualization of the model making predictions on various images and comparing the predictions to the ground truth.<ul> <li>For example, plot a sample image from the test dataset and have the title of the plot show the prediction, the prediction probability and the ground truth label.</li> </ul> </li> </ul> <ol> <li>Take 3 of your own photos of food and use the Food Vision model to make predictions on them. How does it go? Share your images/predictions with the other students.</li> <li>Retrain the model (feature extraction and fine-tuning) we trained in this notebook, except this time use <code>EfficientNetB4</code> as the base model instead of <code>EfficientNetB0</code>. Do you notice an improvement in performance? Does it take longer to train? Are there any tradeoffs to consider?</li> <li>Name one important benefit of mixed precision training, how does this benefit take place?</li> </ol>"},{"location":"07_food_vision_milestone_project_1/#extra-curriculum","title":"\ud83d\udcd6 Extra-curriculum\u00b6","text":"<ul> <li>Read up on learning rate scheduling and the learning rate scheduler callback. What is it? And how might it be helpful to this project?</li> <li>Read up on TensorFlow data loaders (improving TensorFlow data loading performance). Is there anything we've missed? What methods you keep in mind whenever loading data in TensorFlow? Hint: check the summary at the bottom of the page for a gret round up of ideas.</li> <li>Read up on the documentation for TensorFlow mixed precision training. What are the important things to keep in mind when using mixed precision training?</li> </ul>"},{"location":"08_introduction_to_nlp_in_tensorflow/","title":"08. Natural Language Processing with TensorFlow","text":"In\u00a0[1]: Copied! <pre>import datetime\nprint(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\")\n</pre> import datetime print(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\") <pre>Notebook last run (end-to-end): 2023-05-26 00:14:41.453244\n</pre> In\u00a0[2]: Copied! <pre># Check for GPU\n!nvidia-smi -L\n</pre> # Check for GPU !nvidia-smi -L <pre>GPU 0: NVIDIA A100-SXM4-40GB (UUID: GPU-a07b6e3e-3ef6-217b-d41f-dc5c4d6babfd)\n</pre> In\u00a0[3]: Copied! <pre># Download helper functions script\n!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n</pre> # Download helper functions script !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py <pre>--2023-05-26 00:14:41--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 10246 (10K) [text/plain]\nSaving to: \u2018helper_functions.py\u2019\n\nhelper_functions.py 100%[===================&gt;]  10.01K  --.-KB/s    in 0s      \n\n2023-05-26 00:14:41 (94.5 MB/s) - \u2018helper_functions.py\u2019 saved [10246/10246]\n\n</pre> In\u00a0[4]: Copied! <pre># Import series of helper functions for the notebook\nfrom helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys\n</pre> # Import series of helper functions for the notebook from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys In\u00a0[5]: Copied! <pre># Download data (same as from Kaggle)\n!wget \"https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\"\n\n# Unzip data\nunzip_data(\"nlp_getting_started.zip\")\n</pre> # Download data (same as from Kaggle) !wget \"https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\"  # Unzip data unzip_data(\"nlp_getting_started.zip\") <pre>--2023-05-26 00:14:45--  https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\nResolving storage.googleapis.com (storage.googleapis.com)... 172.217.194.128, 74.125.200.128, 74.125.68.128, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|172.217.194.128|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 607343 (593K) [application/zip]\nSaving to: \u2018nlp_getting_started.zip\u2019\n\nnlp_getting_started 100%[===================&gt;] 593.11K   731KB/s    in 0.8s    \n\n2023-05-26 00:14:46 (731 KB/s) - \u2018nlp_getting_started.zip\u2019 saved [607343/607343]\n\n</pre> <p>Unzipping <code>nlp_getting_started.zip</code> gives the following 3 <code>.csv</code> files:</p> <ul> <li><code>sample_submission.csv</code> - an example of the file you'd submit to the Kaggle competition of your model's predictions.</li> <li><code>train.csv</code> - training samples of real and not real diaster Tweets.</li> <li><code>test.csv</code> - testing samples of real and not real diaster Tweets.</li> </ul> In\u00a0[6]: Copied! <pre># Turn .csv files into pandas DataFrame's\nimport pandas as pd\ntrain_df = pd.read_csv(\"train.csv\")\ntest_df = pd.read_csv(\"test.csv\")\ntrain_df.head()\n</pre> # Turn .csv files into pandas DataFrame's import pandas as pd train_df = pd.read_csv(\"train.csv\") test_df = pd.read_csv(\"test.csv\") train_df.head() Out[6]: id keyword location text target 0 1 NaN NaN Our Deeds are the Reason of this #earthquake M... 1 1 4 NaN NaN Forest fire near La Ronge Sask. Canada 1 2 5 NaN NaN All residents asked to 'shelter in place' are ... 1 3 6 NaN NaN 13,000 people receive #wildfires evacuation or... 1 4 7 NaN NaN Just got sent this photo from Ruby #Alaska as ... 1 <p>The training data we downloaded is probably shuffled already. But just to be sure, let's shuffle it again.</p> In\u00a0[7]: Copied! <pre># Shuffle training dataframe\ntrain_df_shuffled = train_df.sample(frac=1, random_state=42) # shuffle with random_state=42 for reproducibility\ntrain_df_shuffled.head()\n</pre> # Shuffle training dataframe train_df_shuffled = train_df.sample(frac=1, random_state=42) # shuffle with random_state=42 for reproducibility train_df_shuffled.head() Out[7]: id keyword location text target 2644 3796 destruction NaN So you have a new weapon that can cause un-ima... 1 2227 3185 deluge NaN The f$&amp;amp;@ing things I do for #GISHWHES Just... 0 5448 7769 police UK DT @georgegalloway: RT @Galloway4Mayor: \u0089\u00db\u00cfThe... 1 132 191 aftershock NaN Aftershock back to school kick off was great. ... 0 6845 9810 trauma Montgomery County, MD in response to trauma Children of Addicts deve... 0 <p>Notice how the training data has a <code>\"target\"</code> column.</p> <p>We're going to be writing code to find patterns (e.g. different combinations of words) in the <code>\"text\"</code> column of the training dataset to predict the value of the <code>\"target\"</code> column.</p> <p>The test dataset doesn't have a <code>\"target\"</code> column.</p> <pre><code>Inputs (text column) -&gt; Machine Learning Algorithm -&gt; Outputs (target column)\n</code></pre> <p> Example text classification inputs and outputs for the problem of classifying whether a Tweet is about a disaster or not.</p> In\u00a0[8]: Copied! <pre># The test data doesn't have a target (that's what we'd try to predict)\ntest_df.head()\n</pre> # The test data doesn't have a target (that's what we'd try to predict) test_df.head() Out[8]: id keyword location text 0 0 NaN NaN Just happened a terrible car crash 1 2 NaN NaN Heard about #earthquake is different cities, s... 2 3 NaN NaN there is a forest fire at spot pond, geese are... 3 9 NaN NaN Apocalypse lighting. #Spokane #wildfires 4 11 NaN NaN Typhoon Soudelor kills 28 in China and Taiwan <p>Let's check how many examples of each target we have.</p> In\u00a0[9]: Copied! <pre># How many examples of each class?\ntrain_df.target.value_counts()\n</pre> # How many examples of each class? train_df.target.value_counts() Out[9]: <pre>0    4342\n1    3271\nName: target, dtype: int64</pre> <p>Since we have two target values, we're dealing with a binary classification problem.</p> <p>It's fairly balanced too, about 60% negative class (<code>target = 0</code>) and 40% positive class (<code>target = 1</code>).</p> <p>Where,</p> <ul> <li><code>1</code> = a real disaster Tweet</li> <li><code>0</code> = not a real disaster Tweet</li> </ul> <p>And what about the total number of samples we have?</p> In\u00a0[10]: Copied! <pre># How many samples total?\nprint(f\"Total training samples: {len(train_df)}\")\nprint(f\"Total test samples: {len(test_df)}\")\nprint(f\"Total samples: {len(train_df) + len(test_df)}\")\n</pre> # How many samples total? print(f\"Total training samples: {len(train_df)}\") print(f\"Total test samples: {len(test_df)}\") print(f\"Total samples: {len(train_df) + len(test_df)}\") <pre>Total training samples: 7613\nTotal test samples: 3263\nTotal samples: 10876\n</pre> <p>Alright, seems like we've got a decent amount of training and test data. If anything, we've got an abundance of testing examples, usually a split of 90/10 (90% training, 10% testing) or 80/20 is suffice.</p> <p>Okay, time to visualize, let's write some code to visualize random text samples.</p> <p>\ud83e\udd14 Question: Why visualize random samples? You could visualize samples in order but this could lead to only seeing a certain subset of data. Better to visualize a substantial quantity (100+) of random samples to get an idea of the different kinds of data you're working with. In machine learning, never underestimate the power of randomness.</p> In\u00a0[11]: Copied! <pre># Let's visualize some random training examples\nimport random\nrandom_index = random.randint(0, len(train_df)-5) # create random indexes not higher than the total number of samples\nfor row in train_df_shuffled[[\"text\", \"target\"]][random_index:random_index+5].itertuples():\n  _, text, target = row\n  print(f\"Target: {target}\", \"(real disaster)\" if target &gt; 0 else \"(not real disaster)\")\n  print(f\"Text:\\n{text}\\n\")\n  print(\"---\\n\")\n</pre> # Let's visualize some random training examples import random random_index = random.randint(0, len(train_df)-5) # create random indexes not higher than the total number of samples for row in train_df_shuffled[[\"text\", \"target\"]][random_index:random_index+5].itertuples():   _, text, target = row   print(f\"Target: {target}\", \"(real disaster)\" if target &gt; 0 else \"(not real disaster)\")   print(f\"Text:\\n{text}\\n\")   print(\"---\\n\") <pre>Target: 0 (not real disaster)\nText:\n@JamesMelville Some old testimony of weapons used to promote conflicts\nTactics - corruption &amp;amp; infiltration of groups\nhttps://t.co/cyU8zxw1oH\n\n---\n\nTarget: 1 (real disaster)\nText:\nNow Trending in Nigeria: Police charge traditional ruler others with informant\u0089\u00db\u00aas  murder http://t.co/93inFxzhX0\n\n---\n\nTarget: 1 (real disaster)\nText:\nREPORTED: HIT &amp;amp; RUN-IN ROADWAY-PROPERTY DAMAGE at 15901 STATESVILLE RD\n\n---\n\nTarget: 1 (real disaster)\nText:\nohH NO FUKURODANI DIDN'T SURVIVE THE APOCALYPSE BOKUTO FEELS HORRIBLE  my poor boy my ppor child\n\n---\n\nTarget: 1 (real disaster)\nText:\nMaryland mansion fire that killed 6 caused by damaged plug under Christmas tree report says - Into the flames: Firefighter's bravery...\n\n---\n\n</pre> In\u00a0[12]: Copied! <pre>from sklearn.model_selection import train_test_split\n\n# Use train_test_split to split training data into training and validation sets\ntrain_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n                                                                            train_df_shuffled[\"target\"].to_numpy(),\n                                                                            test_size=0.1, # dedicate 10% of samples to validation set\n                                                                            random_state=42) # random state for reproducibility\n</pre> from sklearn.model_selection import train_test_split  # Use train_test_split to split training data into training and validation sets train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),                                                                             train_df_shuffled[\"target\"].to_numpy(),                                                                             test_size=0.1, # dedicate 10% of samples to validation set                                                                             random_state=42) # random state for reproducibility In\u00a0[13]: Copied! <pre># Check the lengths\nlen(train_sentences), len(train_labels), len(val_sentences), len(val_labels)\n</pre> # Check the lengths len(train_sentences), len(train_labels), len(val_sentences), len(val_labels) Out[13]: <pre>(6851, 6851, 762, 762)</pre> In\u00a0[14]: Copied! <pre># View the first 10 training sentences and their labels\ntrain_sentences[:10], train_labels[:10]\n</pre> # View the first 10 training sentences and their labels train_sentences[:10], train_labels[:10] Out[14]: <pre>(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n        'Imagine getting flattened by Kurt Zouma',\n        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n        'destroy the free fandom honestly',\n        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n       dtype=object),\n array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1]))</pre> In\u00a0[15]: Copied! <pre>import tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization # after TensorFlow 2.6\n\n# Before TensorFlow 2.6\n# from tensorflow.keras.layers.experimental.preprocessing import TextVectorization \n# Note: in TensorFlow 2.6+, you no longer need \"layers.experimental.preprocessing\"\n# you can use: \"tf.keras.layers.TextVectorization\", see https://github.com/tensorflow/tensorflow/releases/tag/v2.6.0 for more\n\n# Use the default TextVectorization variables\ntext_vectorizer = TextVectorization(max_tokens=None, # how many words in the vocabulary (all of the different words in your text)\n                                    standardize=\"lower_and_strip_punctuation\", # how to process text\n                                    split=\"whitespace\", # how to split tokens\n                                    ngrams=None, # create groups of n-words?\n                                    output_mode=\"int\", # how to map tokens to numbers\n                                    output_sequence_length=None) # how long should the output sequence of tokens be?\n                                    # pad_to_max_tokens=True) # Not valid if using max_tokens=None\n</pre> import tensorflow as tf from tensorflow.keras.layers import TextVectorization # after TensorFlow 2.6  # Before TensorFlow 2.6 # from tensorflow.keras.layers.experimental.preprocessing import TextVectorization  # Note: in TensorFlow 2.6+, you no longer need \"layers.experimental.preprocessing\" # you can use: \"tf.keras.layers.TextVectorization\", see https://github.com/tensorflow/tensorflow/releases/tag/v2.6.0 for more  # Use the default TextVectorization variables text_vectorizer = TextVectorization(max_tokens=None, # how many words in the vocabulary (all of the different words in your text)                                     standardize=\"lower_and_strip_punctuation\", # how to process text                                     split=\"whitespace\", # how to split tokens                                     ngrams=None, # create groups of n-words?                                     output_mode=\"int\", # how to map tokens to numbers                                     output_sequence_length=None) # how long should the output sequence of tokens be?                                     # pad_to_max_tokens=True) # Not valid if using max_tokens=None <p>We've initialized a <code>TextVectorization</code> object with the default settings but let's customize it a little bit for our own use case.</p> <p>In particular, let's set values for <code>max_tokens</code> and <code>output_sequence_length</code>.</p> <p>For <code>max_tokens</code> (the number of words in the vocabulary), multiples of 10,000 (<code>10,000</code>, <code>20,000</code>, <code>30,000</code>) or the exact number of unique words in your text (e.g. <code>32,179</code>) are common values.</p> <p>For our use case, we'll use <code>10,000</code>.</p> <p>And for the <code>output_sequence_length</code> we'll use the average number of tokens per Tweet in the training set. But first, we'll need to find it.</p> In\u00a0[16]: Copied! <pre># Find average number of tokens (words) in training Tweets\nround(sum([len(i.split()) for i in train_sentences])/len(train_sentences))\n</pre> # Find average number of tokens (words) in training Tweets round(sum([len(i.split()) for i in train_sentences])/len(train_sentences)) Out[16]: <pre>15</pre> <p>Now let's create another <code>TextVectorization</code> object using our custom parameters.</p> In\u00a0[17]: Copied! <pre># Setup text vectorization with custom variables\nmax_vocab_length = 10000 # max number of words to have in our vocabulary\nmax_length = 15 # max length our sequences will be (e.g. how many words from a Tweet does our model see?)\n\ntext_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n                                    output_mode=\"int\",\n                                    output_sequence_length=max_length)\n</pre> # Setup text vectorization with custom variables max_vocab_length = 10000 # max number of words to have in our vocabulary max_length = 15 # max length our sequences will be (e.g. how many words from a Tweet does our model see?)  text_vectorizer = TextVectorization(max_tokens=max_vocab_length,                                     output_mode=\"int\",                                     output_sequence_length=max_length) <p>Beautiful!</p> <p>To map our <code>TextVectorization</code> instance <code>text_vectorizer</code> to our data, we can call the <code>adapt()</code> method on it whilst passing it our training text.</p> In\u00a0[18]: Copied! <pre># Fit the text vectorizer to the training text\ntext_vectorizer.adapt(train_sentences)\n</pre> # Fit the text vectorizer to the training text text_vectorizer.adapt(train_sentences) <p>Training data mapped! Let's try our <code>text_vectorizer</code> on a custom sentence (one similar to what you might see in the training data).</p> In\u00a0[19]: Copied! <pre># Create sample sentence and tokenize it\nsample_sentence = \"There's a flood in my street!\"\ntext_vectorizer([sample_sentence])\n</pre> # Create sample sentence and tokenize it sample_sentence = \"There's a flood in my street!\" text_vectorizer([sample_sentence]) Out[19]: <pre>&lt;tf.Tensor: shape=(1, 15), dtype=int64, numpy=\narray([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n          0,   0]])&gt;</pre> <p>Wonderful, it seems we've got a way to turn our text into numbers (in this case, word-level tokenization). Notice the 0's at the end of the returned tensor, this is because we set <code>output_sequence_length=15</code>, meaning no matter the size of the sequence we pass to <code>text_vectorizer</code>, it always returns a sequence with a length of 15.</p> <p>How about we try our <code>text_vectorizer</code> on a few random sentences?</p> In\u00a0[20]: Copied! <pre># Choose a random sentence from the training dataset and tokenize it\nrandom_sentence = random.choice(train_sentences)\nprint(f\"Original text:\\n{random_sentence}\\\n      \\n\\nVectorized version:\")\ntext_vectorizer([random_sentence])\n</pre> # Choose a random sentence from the training dataset and tokenize it random_sentence = random.choice(train_sentences) print(f\"Original text:\\n{random_sentence}\\       \\n\\nVectorized version:\") text_vectorizer([random_sentence]) <pre>Original text:\nnew summer long thin body bag hip A word skirt Blue http://t.co/lvKoEMsq8m http://t.co/CjiRhHh4vj      \n\nVectorized version:\n</pre> Out[20]: <pre>&lt;tf.Tensor: shape=(1, 15), dtype=int64, numpy=\narray([[  50,  270,  480, 3335,   83,  322, 2436,    3, 1448, 3407,  824,\n           1,    1,    0,    0]])&gt;</pre> <p>Looking good!</p> <p>Finally, we can check the unique tokens in our vocabulary using the <code>get_vocabulary()</code> method.</p> In\u00a0[21]: Copied! <pre># Get the unique words in the vocabulary\nwords_in_vocab = text_vectorizer.get_vocabulary()\ntop_5_words = words_in_vocab[:5] # most common tokens (notice the [UNK] token for \"unknown\" words)\nbottom_5_words = words_in_vocab[-5:] # least common tokens\nprint(f\"Number of words in vocab: {len(words_in_vocab)}\")\nprint(f\"Top 5 most common words: {top_5_words}\") \nprint(f\"Bottom 5 least common words: {bottom_5_words}\")\n</pre> # Get the unique words in the vocabulary words_in_vocab = text_vectorizer.get_vocabulary() top_5_words = words_in_vocab[:5] # most common tokens (notice the [UNK] token for \"unknown\" words) bottom_5_words = words_in_vocab[-5:] # least common tokens print(f\"Number of words in vocab: {len(words_in_vocab)}\") print(f\"Top 5 most common words: {top_5_words}\")  print(f\"Bottom 5 least common words: {bottom_5_words}\") <pre>Number of words in vocab: 10000\nTop 5 most common words: ['', '[UNK]', 'the', 'a', 'in']\nBottom 5 least common words: ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n</pre> In\u00a0[22]: Copied! <pre>tf.random.set_seed(42)\nfrom tensorflow.keras import layers\n\nembedding = layers.Embedding(input_dim=max_vocab_length, # set input shape\n                             output_dim=128, # set size of embedding vector\n                             embeddings_initializer=\"uniform\", # default, intialize randomly\n                             input_length=max_length, # how long is each input\n                             name=\"embedding_1\") \n\nembedding\n</pre> tf.random.set_seed(42) from tensorflow.keras import layers  embedding = layers.Embedding(input_dim=max_vocab_length, # set input shape                              output_dim=128, # set size of embedding vector                              embeddings_initializer=\"uniform\", # default, intialize randomly                              input_length=max_length, # how long is each input                              name=\"embedding_1\")   embedding Out[22]: <pre>&lt;keras.layers.core.embedding.Embedding at 0x7f0c118dcc40&gt;</pre> <p>Excellent, notice how <code>embedding</code> is a TensoFlow layer? This is important because we can use it as part of a model, meaning its parameters (word representations) can be updated and improved as the model learns.</p> <p>How about we try it out on a sample sentence?</p> In\u00a0[23]: Copied! <pre># Get a random sentence from training set\nrandom_sentence = random.choice(train_sentences)\nprint(f\"Original text:\\n{random_sentence}\\\n      \\n\\nEmbedded version:\")\n\n# Embed the random sentence (turn it into numerical representation)\nsample_embed = embedding(text_vectorizer([random_sentence]))\nsample_embed\n</pre> # Get a random sentence from training set random_sentence = random.choice(train_sentences) print(f\"Original text:\\n{random_sentence}\\       \\n\\nEmbedded version:\")  # Embed the random sentence (turn it into numerical representation) sample_embed = embedding(text_vectorizer([random_sentence])) sample_embed <pre>Original text:\nNow on #ComDev #Asia: Radio stations in #Bangladesh broadcasting #programs ?to address the upcoming cyclone #komen http://t.co/iOVr4yMLKp      \n\nEmbedded version:\n</pre> Out[23]: <pre>&lt;tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\narray([[[-0.04868475, -0.03902867, -0.01375594, ...,  0.01682534,\n         -0.0439401 , -0.04604518],\n        [-0.04827927, -0.00328457,  0.02171678, ..., -0.03261749,\n         -0.01061803, -0.0481179 ],\n        [-0.02431345,  0.01104342,  0.00933889, ..., -0.04607272,\n         -0.00651377,  0.03853123],\n        ...,\n        [-0.03270339,  0.03608486,  0.03573406, ...,  0.03622421,\n          0.03427652, -0.03483479],\n        [-0.0489977 ,  0.01962234,  0.02186165, ...,  0.03139002,\n         -0.00744159,  0.0428594 ],\n        [ 0.01265842,  0.02462569, -0.04731182, ...,  0.00403734,\n          0.0431679 ,  0.03959754]]], dtype=float32)&gt;</pre> <p>Each token in the sentence gets turned into a length 128 feature vector.</p> In\u00a0[24]: Copied! <pre># Check out a single token's embedding\nsample_embed[0][0]\n</pre> # Check out a single token's embedding sample_embed[0][0] Out[24]: <pre>&lt;tf.Tensor: shape=(128,), dtype=float32, numpy=\narray([-0.04868475, -0.03902867, -0.01375594,  0.01587117, -0.02964617,\n        0.00738639, -0.03109504,  0.03008839, -0.01458266, -0.03069887,\n       -0.04926676, -0.03454053, -0.04019499, -0.04406711,  0.01975099,\n        0.02852687, -0.04052209, -0.03800124,  0.03438697, -0.0118026 ,\n       -0.03470664, -0.01146972,  0.0449667 , -0.00269016,  0.02131964,\n       -0.04141569, -0.03724197,  0.01624352,  0.03269556,  0.03813741,\n        0.03606123,  0.00698509, -0.03569689,  0.02056131, -0.03467314,\n        0.01110398,  0.02095172,  0.02219674, -0.04576088, -0.04229112,\n       -0.02345047,  0.02578488,  0.02985479, -0.00203061,  0.03920727,\n        0.04065951,  0.03973453,  0.03947322,  0.01699554,  0.0021927 ,\n        0.03676197, -0.04327145,  0.02495482,  0.02447238, -0.04413594,\n       -0.01388069, -0.00375951, -0.0328602 , -0.00067427,  0.01808068,\n        0.04227355,  0.02817165,  0.01965401, -0.01514393,  0.01905935,\n       -0.03820103, -0.04916845,  0.02303007,  0.00830983,  0.01011454,\n       -0.04043181,  0.02080727, -0.03319015,  0.04188809, -0.01183917,\n       -0.01822531, -0.02172413,  0.03059311,  0.02727925, -0.00328885,\n       -0.00808424, -0.02095444, -0.00894216,  0.00770078, -0.00439024,\n        0.03637768,  0.02007255, -0.02650907, -0.01374531,  0.01806785,\n       -0.03309877, -0.01076321, -0.04107616,  0.01709371,  0.04567242,\n       -0.01824218,  0.02805582,  0.02974418, -0.04001283, -0.04077357,\n        0.00323737,  0.04038842, -0.00992844, -0.03974843,  0.04533138,\n        0.04738795,  0.02837384,  0.03874009, -0.01673441, -0.00258055,\n       -0.01975214, -0.04166807, -0.02483889, -0.02804886,  0.04608755,\n        0.03544754,  0.02697959,  0.00242041,  0.00101637, -0.01162767,\n       -0.00497937,  0.00540714, -0.01258825,  0.00779672,  0.02742722,\n        0.01682534, -0.0439401 , -0.04604518], dtype=float32)&gt;</pre> <p>These values might not mean much to us but they're what our computer sees each word as. When our model looks for patterns in different samples, these values will be updated as necessary.</p> <p>\ud83d\udd11 Note: The previous two concepts (tokenization and embeddings) are the foundation for many NLP tasks. So if you're not sure about anything, be sure to research and conduct your own experiments to further help your understanding.</p> In\u00a0[25]: Copied! <pre>from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\n\n# Create tokenization and modelling pipeline\nmodel_0 = Pipeline([\n                    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n                    (\"clf\", MultinomialNB()) # model the text\n])\n\n# Fit the pipeline to the training data\nmodel_0.fit(train_sentences, train_labels)\n</pre> from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.naive_bayes import MultinomialNB from sklearn.pipeline import Pipeline  # Create tokenization and modelling pipeline model_0 = Pipeline([                     (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf                     (\"clf\", MultinomialNB()) # model the text ])  # Fit the pipeline to the training data model_0.fit(train_sentences, train_labels) Out[25]: <pre>Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])</pre>TfidfVectorizer<pre>TfidfVectorizer()</pre>MultinomialNB<pre>MultinomialNB()</pre> <p>The benefit of using a shallow model like Multinomial Naive Bayes is that training is very fast.</p> <p>Let's evaluate our model and find our baseline metric.</p> In\u00a0[26]: Copied! <pre>baseline_score = model_0.score(val_sentences, val_labels)\nprint(f\"Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%\")\n</pre> baseline_score = model_0.score(val_sentences, val_labels) print(f\"Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%\") <pre>Our baseline model achieves an accuracy of: 79.27%\n</pre> <p>How about we make some predictions with our baseline model?</p> In\u00a0[27]: Copied! <pre># Make predictions\nbaseline_preds = model_0.predict(val_sentences)\nbaseline_preds[:20]\n</pre> # Make predictions baseline_preds = model_0.predict(val_sentences) baseline_preds[:20] Out[27]: <pre>array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1])</pre> In\u00a0[28]: Copied! <pre># Function to evaluate: accuracy, precision, recall, f1-score\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ndef calculate_results(y_true, y_pred):\n  \"\"\"\n  Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n\n  Args:\n  -----\n  y_true = true labels in the form of a 1D array\n  y_pred = predicted labels in the form of a 1D array\n\n  Returns a dictionary of accuracy, precision, recall, f1-score.\n  \"\"\"\n  # Calculate model accuracy\n  model_accuracy = accuracy_score(y_true, y_pred) * 100\n  # Calculate model precision, recall and f1 score using \"weighted\" average\n  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n  model_results = {\"accuracy\": model_accuracy,\n                  \"precision\": model_precision,\n                  \"recall\": model_recall,\n                  \"f1\": model_f1}\n  return model_results\n</pre> # Function to evaluate: accuracy, precision, recall, f1-score from sklearn.metrics import accuracy_score, precision_recall_fscore_support  def calculate_results(y_true, y_pred):   \"\"\"   Calculates model accuracy, precision, recall and f1 score of a binary classification model.    Args:   -----   y_true = true labels in the form of a 1D array   y_pred = predicted labels in the form of a 1D array    Returns a dictionary of accuracy, precision, recall, f1-score.   \"\"\"   # Calculate model accuracy   model_accuracy = accuracy_score(y_true, y_pred) * 100   # Calculate model precision, recall and f1 score using \"weighted\" average   model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")   model_results = {\"accuracy\": model_accuracy,                   \"precision\": model_precision,                   \"recall\": model_recall,                   \"f1\": model_f1}   return model_results In\u00a0[29]: Copied! <pre># Get baseline results\nbaseline_results = calculate_results(y_true=val_labels,\n                                     y_pred=baseline_preds)\nbaseline_results\n</pre> # Get baseline results baseline_results = calculate_results(y_true=val_labels,                                      y_pred=baseline_preds) baseline_results Out[29]: <pre>{'accuracy': 79.26509186351706,\n 'precision': 0.8111390004213173,\n 'recall': 0.7926509186351706,\n 'f1': 0.7862189758049549}</pre> In\u00a0[30]: Copied! <pre># Create tensorboard callback (need to create a new one for each model)\nfrom helper_functions import create_tensorboard_callback\n\n# Create directory to save TensorBoard logs\nSAVE_DIR = \"model_logs\"\n</pre> # Create tensorboard callback (need to create a new one for each model) from helper_functions import create_tensorboard_callback  # Create directory to save TensorBoard logs SAVE_DIR = \"model_logs\" <p>Now we've got a TensorBoard callback function ready to go, let's build our first deep model.</p> In\u00a0[31]: Copied! <pre># Build model with the Functional API\nfrom tensorflow.keras import layers\ninputs = layers.Input(shape=(1,), dtype=\"string\") # inputs are 1-dimensional strings\nx = text_vectorizer(inputs) # turn the input text into numbers\nx = embedding(x) # create an embedding of the numerized numbers\nx = layers.GlobalAveragePooling1D()(x) # lower the dimensionality of the embedding (try running the model without this layer and see what happens)\noutputs = layers.Dense(1, activation=\"sigmoid\")(x) # create the output layer, want binary outputs so use sigmoid activation\nmodel_1 = tf.keras.Model(inputs, outputs, name=\"model_1_dense\") # construct the model\n</pre> # Build model with the Functional API from tensorflow.keras import layers inputs = layers.Input(shape=(1,), dtype=\"string\") # inputs are 1-dimensional strings x = text_vectorizer(inputs) # turn the input text into numbers x = embedding(x) # create an embedding of the numerized numbers x = layers.GlobalAveragePooling1D()(x) # lower the dimensionality of the embedding (try running the model without this layer and see what happens) outputs = layers.Dense(1, activation=\"sigmoid\")(x) # create the output layer, want binary outputs so use sigmoid activation model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_dense\") # construct the model <p>Looking good. Our model takes a 1-dimensional string as input (in our case, a Tweet), it then tokenizes the string using <code>text_vectorizer</code> and creates an embedding using <code>embedding</code>.</p> <p>We then (optionally) pool the outputs of the embedding layer to reduce the dimensionality of the tensor we pass to the output layer.</p> <p>\ud83d\udee0 Exercise: Try building <code>model_1</code> with and without a <code>GlobalAveragePooling1D()</code> layer after the <code>embedding</code> layer. What happens? Why do you think this is?</p> <p>Finally, we pass the output of the pooling layer to a dense layer with sigmoid activation (we use sigmoid since our problem is binary classification).</p> <p>Before we can fit our model to the data, we've got to compile it. Since we're working with binary classification, we'll use <code>\"binary_crossentropy\"</code> as our loss function and the Adam optimizer.</p> In\u00a0[32]: Copied! <pre># Compile model\nmodel_1.compile(loss=\"binary_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])\n</pre> # Compile model model_1.compile(loss=\"binary_crossentropy\",                 optimizer=tf.keras.optimizers.Adam(),                 metrics=[\"accuracy\"]) <p>Model compiled. Let's get a summary.</p> In\u00a0[33]: Copied! <pre># Get a summary of the model\nmodel_1.summary()\n</pre> # Get a summary of the model model_1.summary() <pre>Model: \"model_1_dense\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 1)]               0         \n                                                                 \n text_vectorization_1 (TextV  (None, 15)               0         \n ectorization)                                                   \n                                                                 \n embedding_1 (Embedding)     (None, 15, 128)           1280000   \n                                                                 \n global_average_pooling1d (G  (None, 128)              0         \n lobalAveragePooling1D)                                          \n                                                                 \n dense (Dense)               (None, 1)                 129       \n                                                                 \n=================================================================\nTotal params: 1,280,129\nTrainable params: 1,280,129\nNon-trainable params: 0\n_________________________________________________________________\n</pre> <p>Most of the trainable parameters are contained within the embedding layer. Recall we created an embedding of size 128 (<code>output_dim=128</code>) for a vocabulary of size 10,000 (<code>input_dim=10000</code>), hence the 1,280,000 trainable parameters.</p> <p>Alright, our model is compiled, let's fit it to our training data for 5 epochs. We'll also pass our TensorBoard callback function to make sure our model's training metrics are logged.</p> In\u00a0[34]: Copied! <pre># Fit the model\nmodel_1_history = model_1.fit(train_sentences, # input sentences can be a list of strings due to text preprocessing layer built-in model\n                              train_labels,\n                              epochs=5,\n                              validation_data=(val_sentences, val_labels),\n                              callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR, \n                                                                     experiment_name=\"simple_dense_model\")])\n</pre> # Fit the model model_1_history = model_1.fit(train_sentences, # input sentences can be a list of strings due to text preprocessing layer built-in model                               train_labels,                               epochs=5,                               validation_data=(val_sentences, val_labels),                               callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,                                                                       experiment_name=\"simple_dense_model\")]) <pre>Saving TensorBoard log files to: model_logs/simple_dense_model/20230526-001451\nEpoch 1/5\n215/215 [==============================] - 18s 55ms/step - loss: 0.6098 - accuracy: 0.6936 - val_loss: 0.5360 - val_accuracy: 0.7559\nEpoch 2/5\n215/215 [==============================] - 2s 11ms/step - loss: 0.4417 - accuracy: 0.8194 - val_loss: 0.4691 - val_accuracy: 0.7887\nEpoch 3/5\n215/215 [==============================] - 2s 10ms/step - loss: 0.3471 - accuracy: 0.8616 - val_loss: 0.4588 - val_accuracy: 0.7887\nEpoch 4/5\n215/215 [==============================] - 2s 7ms/step - loss: 0.2856 - accuracy: 0.8921 - val_loss: 0.4637 - val_accuracy: 0.7913\nEpoch 5/5\n215/215 [==============================] - 2s 8ms/step - loss: 0.2388 - accuracy: 0.9115 - val_loss: 0.4760 - val_accuracy: 0.7861\n</pre> <p>Nice! Since we're using such a simple model, each epoch processes very quickly.</p> <p>Let's check our model's performance on the validation set.</p> In\u00a0[35]: Copied! <pre># Check the results\nmodel_1.evaluate(val_sentences, val_labels)\n</pre> # Check the results model_1.evaluate(val_sentences, val_labels) <pre>24/24 [==============================] - 0s 2ms/step - loss: 0.4760 - accuracy: 0.7861\n</pre> Out[35]: <pre>[0.4760194718837738, 0.7860892415046692]</pre> In\u00a0[36]: Copied! <pre>embedding.weights\n</pre> embedding.weights Out[36]: <pre>[&lt;tf.Variable 'embedding_1/embeddings:0' shape=(10000, 128) dtype=float32, numpy=\n array([[-0.01078545,  0.05590528,  0.03125916, ..., -0.0312557 ,\n         -0.05340781, -0.03800201],\n        [-0.02370532,  0.01161508,  0.0097667 , ..., -0.04962142,\n         -0.00636482,  0.03781125],\n        [-0.05472897,  0.05356752,  0.02146765, ...,  0.05501205,\n          0.01705659, -0.05321405],\n        ...,\n        [ 0.01756669, -0.03676652, -0.00949616, ..., -0.00987446,\n         -0.04183743,  0.03016822],\n        [-0.07823883,  0.06081628, -0.07657789, ...,  0.07998865,\n         -0.05281445, -0.02332675],\n        [-0.03393482,  0.08871375, -0.06819566, ...,  0.06992952,\n         -0.09992232, -0.02705033]], dtype=float32)&gt;]</pre> In\u00a0[37]: Copied! <pre>embed_weights = model_1.get_layer(\"embedding_1\").get_weights()[0]\nprint(embed_weights.shape)\n</pre> embed_weights = model_1.get_layer(\"embedding_1\").get_weights()[0] print(embed_weights.shape) <pre>(10000, 128)\n</pre> <p>And since we tracked our model's training logs with TensorBoard, how about we visualize them?</p> <p>We can do so by uploading our TensorBoard log files (contained in the <code>model_logs</code> directory) to TensorBoard.dev.</p> <p>\ud83d\udd11 Note: Remember, whatever you upload to TensorBoard.dev becomes public. If there are training logs you don't want to share, don't upload them.</p> In\u00a0[38]: Copied! <pre># # View tensorboard logs of transfer learning modelling experiments (should be 4 models)\n# # Upload TensorBoard dev records\n# !tensorboard dev upload --logdir ./model_logs \\\n#   --name \"First deep model on text data\" \\\n#   --description \"Trying a dense model with an embedding layer\" \\\n#   --one_shot # exits the uploader when upload has finished\n</pre> # # View tensorboard logs of transfer learning modelling experiments (should be 4 models) # # Upload TensorBoard dev records # !tensorboard dev upload --logdir ./model_logs \\ #   --name \"First deep model on text data\" \\ #   --description \"Trying a dense model with an embedding layer\" \\ #   --one_shot # exits the uploader when upload has finished In\u00a0[39]: Copied! <pre># If you need to remove previous experiments, you can do so using the following command\n# !tensorboard dev delete --experiment_id EXPERIMENT_ID_TO_DELETE\n</pre> # If you need to remove previous experiments, you can do so using the following command # !tensorboard dev delete --experiment_id EXPERIMENT_ID_TO_DELETE <p>The TensorBoard.dev experiment for our first deep model can be viewed here: https://tensorboard.dev/experiment/5d1Xm10aT6m6MgyW3HAGfw/</p> <p></p> <p>What the training curves of our model look like on TensorBoard. From looking at the curves can you tell if the model is overfitting or underfitting?</p> <p>Beautiful! Those are some colorful training curves. Would you say the model is overfitting or underfitting?</p> <p>We've built and trained our first deep model, the next step is to make some predictions with it.</p> In\u00a0[40]: Copied! <pre># Make predictions (these come back in the form of probabilities)\nmodel_1_pred_probs = model_1.predict(val_sentences)\nmodel_1_pred_probs[:10] # only print out the first 10 prediction probabilities\n</pre> # Make predictions (these come back in the form of probabilities) model_1_pred_probs = model_1.predict(val_sentences) model_1_pred_probs[:10] # only print out the first 10 prediction probabilities <pre>24/24 [==============================] - 0s 2ms/step\n</pre> Out[40]: <pre>array([[0.4068562 ],\n       [0.74714756],\n       [0.9978309 ],\n       [0.10913013],\n       [0.10925023],\n       [0.93645686],\n       [0.91428435],\n       [0.99250424],\n       [0.96829313],\n       [0.26842445]], dtype=float32)</pre> <p>Since our final layer uses a sigmoid activation function, we get our predictions back in the form of probabilities.</p> <p>To convert them to prediction classes, we'll use <code>tf.round()</code>, meaning prediction probabilities below 0.5 will be rounded to 0 and those above 0.5 will be rounded to 1.</p> <p>\ud83d\udd11 Note: In practice, the output threshold of a sigmoid prediction probability doesn't necessarily have to 0.5. For example, through testing, you may find that a cut off of 0.25 is better for your chosen evaluation metrics. A common example of this threshold cutoff is the precision-recall tradeoff (search for the keyword \"tradeoff\" to learn about the phenomenon).</p> In\u00a0[41]: Copied! <pre># Turn prediction probabilities into single-dimension tensor of floats\nmodel_1_preds = tf.squeeze(tf.round(model_1_pred_probs)) # squeeze removes single dimensions\nmodel_1_preds[:20]\n</pre> # Turn prediction probabilities into single-dimension tensor of floats model_1_preds = tf.squeeze(tf.round(model_1_pred_probs)) # squeeze removes single dimensions model_1_preds[:20] Out[41]: <pre>&lt;tf.Tensor: shape=(20,), dtype=float32, numpy=\narray([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n       0., 0., 1.], dtype=float32)&gt;</pre> <p>Now we've got our model's predictions in the form of classes, we can use our <code>calculate_results()</code> function to compare them to the ground truth validation labels.</p> In\u00a0[42]: Copied! <pre># Calculate model_1 metrics\nmodel_1_results = calculate_results(y_true=val_labels, \n                                    y_pred=model_1_preds)\nmodel_1_results\n</pre> # Calculate model_1 metrics model_1_results = calculate_results(y_true=val_labels,                                      y_pred=model_1_preds) model_1_results Out[42]: <pre>{'accuracy': 78.60892388451444,\n 'precision': 0.7903277546022673,\n 'recall': 0.7860892388451444,\n 'f1': 0.7832971347503846}</pre> <p>How about we compare our first deep model to our baseline model?</p> In\u00a0[43]: Copied! <pre># Is our simple Keras model better than our baseline model?\nimport numpy as np\nnp.array(list(model_1_results.values())) &gt; np.array(list(baseline_results.values()))\n</pre> # Is our simple Keras model better than our baseline model? import numpy as np np.array(list(model_1_results.values())) &gt; np.array(list(baseline_results.values())) Out[43]: <pre>array([False, False, False, False])</pre> <p>Since we'll be doing this kind of comparison (baseline compared to new model) quite a few times, let's create a function to help us out.</p> In\u00a0[44]: Copied! <pre># Create a helper function to compare our baseline results to new model results\ndef compare_baseline_to_new_results(baseline_results, new_model_results):\n  for key, value in baseline_results.items():\n    print(f\"Baseline {key}: {value:.2f}, New {key}: {new_model_results[key]:.2f}, Difference: {new_model_results[key]-value:.2f}\")\n\ncompare_baseline_to_new_results(baseline_results=baseline_results, \n                                new_model_results=model_1_results)\n</pre> # Create a helper function to compare our baseline results to new model results def compare_baseline_to_new_results(baseline_results, new_model_results):   for key, value in baseline_results.items():     print(f\"Baseline {key}: {value:.2f}, New {key}: {new_model_results[key]:.2f}, Difference: {new_model_results[key]-value:.2f}\")  compare_baseline_to_new_results(baseline_results=baseline_results,                                  new_model_results=model_1_results) <pre>Baseline accuracy: 79.27, New accuracy: 78.61, Difference: -0.66\nBaseline precision: 0.81, New precision: 0.79, Difference: -0.02\nBaseline recall: 0.79, New recall: 0.79, Difference: -0.01\nBaseline f1: 0.79, New f1: 0.78, Difference: -0.00\n</pre> In\u00a0[45]: Copied! <pre># Get the vocabulary from the text vectorization layer\nwords_in_vocab = text_vectorizer.get_vocabulary()\nlen(words_in_vocab), words_in_vocab[:10]\n</pre> # Get the vocabulary from the text vectorization layer words_in_vocab = text_vectorizer.get_vocabulary() len(words_in_vocab), words_in_vocab[:10] Out[45]: <pre>(10000, ['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is'])</pre> <p>And now let's get our embedding layer's weights (these are the numerical representations of each word).</p> In\u00a0[46]: Copied! <pre>model_1.summary()\n</pre> model_1.summary() <pre>Model: \"model_1_dense\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 1)]               0         \n                                                                 \n text_vectorization_1 (TextV  (None, 15)               0         \n ectorization)                                                   \n                                                                 \n embedding_1 (Embedding)     (None, 15, 128)           1280000   \n                                                                 \n global_average_pooling1d (G  (None, 128)              0         \n lobalAveragePooling1D)                                          \n                                                                 \n dense (Dense)               (None, 1)                 129       \n                                                                 \n=================================================================\nTotal params: 1,280,129\nTrainable params: 1,280,129\nNon-trainable params: 0\n_________________________________________________________________\n</pre> In\u00a0[47]: Copied! <pre># Get the weight matrix of embedding layer \n# (these are the numerical patterns between the text in the training dataset the model has learned)\nembed_weights = model_1.get_layer(\"embedding_1\").get_weights()[0]\nprint(embed_weights.shape) # same size as vocab size and embedding_dim (each word is a embedding_dim size vector)\n</pre> # Get the weight matrix of embedding layer  # (these are the numerical patterns between the text in the training dataset the model has learned) embed_weights = model_1.get_layer(\"embedding_1\").get_weights()[0] print(embed_weights.shape) # same size as vocab size and embedding_dim (each word is a embedding_dim size vector) <pre>(10000, 128)\n</pre> <p>Now we've got these two objects, we can use the Embedding Projector tool to visualize our embedding.</p> <p>To use the Embedding Projector tool, we need two files:</p> <ul> <li>The embedding vectors (same as embedding weights).</li> <li>The meta data of the embedding vectors (the words they represent - our vocabulary).</li> </ul> <p>Right now, we've got of these files as Python objects. To download them to file, we're going to use the code example available on the TensorFlow word embeddings tutorial page.</p> In\u00a0[48]: Copied! <pre># # Code below is adapted from: https://www.tensorflow.org/tutorials/text/word_embeddings#retrieve_the_trained_word_embeddings_and_save_them_to_disk\n# import io\n\n# # Create output writers\n# out_v = io.open(\"embedding_vectors.tsv\", \"w\", encoding=\"utf-8\")\n# out_m = io.open(\"embedding_metadata.tsv\", \"w\", encoding=\"utf-8\")\n\n# # Write embedding vectors and words to file\n# for num, word in enumerate(words_in_vocab):\n#   if num == 0: \n#      continue # skip padding token\n#   vec = embed_weights[num]\n#   out_m.write(word + \"\\n\") # write words to file\n#   out_v.write(\"\\t\".join([str(x) for x in vec]) + \"\\n\") # write corresponding word vector to file\n# out_v.close()\n# out_m.close()\n\n# # Download files locally to upload to Embedding Projector\n# try:\n#   from google.colab import files\n# except ImportError:\n#   pass\n# else:\n#   files.download(\"embedding_vectors.tsv\")\n#   files.download(\"embedding_metadata.tsv\")\n</pre> # # Code below is adapted from: https://www.tensorflow.org/tutorials/text/word_embeddings#retrieve_the_trained_word_embeddings_and_save_them_to_disk # import io  # # Create output writers # out_v = io.open(\"embedding_vectors.tsv\", \"w\", encoding=\"utf-8\") # out_m = io.open(\"embedding_metadata.tsv\", \"w\", encoding=\"utf-8\")  # # Write embedding vectors and words to file # for num, word in enumerate(words_in_vocab): #   if num == 0:  #      continue # skip padding token #   vec = embed_weights[num] #   out_m.write(word + \"\\n\") # write words to file #   out_v.write(\"\\t\".join([str(x) for x in vec]) + \"\\n\") # write corresponding word vector to file # out_v.close() # out_m.close()  # # Download files locally to upload to Embedding Projector # try: #   from google.colab import files # except ImportError: #   pass # else: #   files.download(\"embedding_vectors.tsv\") #   files.download(\"embedding_metadata.tsv\") <p>Once you've downloaded the embedding vectors and metadata, you can visualize them using Embedding Vector tool:</p> <ol> <li>Go to  http://projector.tensorflow.org/</li> <li>Click on \"Load data\"</li> <li>Upload the two files you downloaded (<code>embedding_vectors.tsv</code> and <code>embedding_metadata.tsv</code>)</li> <li>Explore</li> <li>Optional: You can share the data you've created by clicking \"Publish\"</li> </ol> <p>What do you find?</p> <p>Are words with similar meanings close together?</p> <p>Remember, they might not be. The embeddings we downloaded are how our model interprets words, not necessarily how we interpret them.</p> <p>Also, since the embedding has been learned purely from Tweets, it may contain some strange values as Tweets are a very unique style of natural language.</p> <p>\ud83e\udd14 Question: Do you have to visualize embeddings every time?</p> <p>No. Although helpful for gaining an intuition of what natural language embeddings are, it's not completely necessary. Especially as the dimensions of your vocabulary and embeddings grow, trying to comprehend them would become an increasingly difficult task.</p> In\u00a0[49]: Copied! <pre># Set random seed and create embedding layer (new embedding layer for each model)\ntf.random.set_seed(42)\nfrom tensorflow.keras import layers\nmodel_2_embedding = layers.Embedding(input_dim=max_vocab_length,\n                                     output_dim=128,\n                                     embeddings_initializer=\"uniform\",\n                                     input_length=max_length,\n                                     name=\"embedding_2\")\n\n\n# Create LSTM model\ninputs = layers.Input(shape=(1,), dtype=\"string\")\nx = text_vectorizer(inputs)\nx = model_2_embedding(x)\nprint(x.shape)\n# x = layers.LSTM(64, return_sequences=True)(x) # return vector for each word in the Tweet (you can stack RNN cells as long as return_sequences=True)\nx = layers.LSTM(64)(x) # return vector for whole sequence\nprint(x.shape)\n# x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer on top of output of LSTM cell\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel_2 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")\n</pre> # Set random seed and create embedding layer (new embedding layer for each model) tf.random.set_seed(42) from tensorflow.keras import layers model_2_embedding = layers.Embedding(input_dim=max_vocab_length,                                      output_dim=128,                                      embeddings_initializer=\"uniform\",                                      input_length=max_length,                                      name=\"embedding_2\")   # Create LSTM model inputs = layers.Input(shape=(1,), dtype=\"string\") x = text_vectorizer(inputs) x = model_2_embedding(x) print(x.shape) # x = layers.LSTM(64, return_sequences=True)(x) # return vector for each word in the Tweet (you can stack RNN cells as long as return_sequences=True) x = layers.LSTM(64)(x) # return vector for whole sequence print(x.shape) # x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer on top of output of LSTM cell outputs = layers.Dense(1, activation=\"sigmoid\")(x) model_2 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\") <pre>(None, 15, 128)\n(None, 64)\n</pre> <p>\ud83d\udd11 Note: Reading the documentation for the TensorFlow LSTM layer, you'll find a plethora of parameters. Many of these have been tuned to make sure they compute as fast as possible. The main ones you'll be looking to adjust are <code>units</code> (number of hidden units) and <code>return_sequences</code> (set this to <code>True</code> when stacking LSTM or other recurrent layers).</p> <p>Now we've got our LSTM model built, let's compile it using <code>\"binary_crossentropy\"</code> loss and the Adam optimizer.</p> In\u00a0[50]: Copied! <pre># Compile model\nmodel_2.compile(loss=\"binary_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])\n</pre> # Compile model model_2.compile(loss=\"binary_crossentropy\",                 optimizer=tf.keras.optimizers.Adam(),                 metrics=[\"accuracy\"]) <p>And before we fit our model to the data, let's get a summary.</p> In\u00a0[51]: Copied! <pre>model_2.summary()\n</pre> model_2.summary() <pre>Model: \"model_2_LSTM\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 1)]               0         \n                                                                 \n text_vectorization_1 (TextV  (None, 15)               0         \n ectorization)                                                   \n                                                                 \n embedding_2 (Embedding)     (None, 15, 128)           1280000   \n                                                                 \n lstm (LSTM)                 (None, 64)                49408     \n                                                                 \n dense_1 (Dense)             (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 1,329,473\nTrainable params: 1,329,473\nNon-trainable params: 0\n_________________________________________________________________\n</pre> <p>Looking good! You'll notice a fair few more trainable parameters within our LSTM layer than <code>model_1</code>.</p> <p>If you'd like to know where this number comes from, I recommend going through the above resources as well the following on calculating the number of parameters in an LSTM cell:</p> <ul> <li>Stack Overflow answer to calculate the number of parameters in an LSTM cell by Marcin Mo\u017cejko</li> <li>Calculating number of parameters in a LSTM unit and layer by Shridhar Priyadarshi</li> </ul> <p>Now our first RNN model's compiled let's fit it to our training data, validating it on the validation data and tracking its training parameters using our TensorBoard callback.</p> In\u00a0[52]: Copied! <pre># Fit model\nmodel_2_history = model_2.fit(train_sentences,\n                              train_labels,\n                              epochs=5,\n                              validation_data=(val_sentences, val_labels),\n                              callbacks=[create_tensorboard_callback(SAVE_DIR, \n                                                                     \"LSTM\")])\n</pre> # Fit model model_2_history = model_2.fit(train_sentences,                               train_labels,                               epochs=5,                               validation_data=(val_sentences, val_labels),                               callbacks=[create_tensorboard_callback(SAVE_DIR,                                                                       \"LSTM\")]) <pre>Saving TensorBoard log files to: model_logs/LSTM/20230526-001518\nEpoch 1/5\n215/215 [==============================] - 13s 44ms/step - loss: 0.5074 - accuracy: 0.7460 - val_loss: 0.4590 - val_accuracy: 0.7743\nEpoch 2/5\n215/215 [==============================] - 2s 10ms/step - loss: 0.3168 - accuracy: 0.8716 - val_loss: 0.5119 - val_accuracy: 0.7756\nEpoch 3/5\n215/215 [==============================] - 2s 10ms/step - loss: 0.2198 - accuracy: 0.9155 - val_loss: 0.5876 - val_accuracy: 0.7677\nEpoch 4/5\n215/215 [==============================] - 2s 8ms/step - loss: 0.1577 - accuracy: 0.9442 - val_loss: 0.5923 - val_accuracy: 0.7795\nEpoch 5/5\n215/215 [==============================] - 2s 8ms/step - loss: 0.1108 - accuracy: 0.9577 - val_loss: 0.8550 - val_accuracy: 0.7559\n</pre> <p>Nice! We've got our first trained RNN model using LSTM cells. Let's make some predictions with it.</p> <p>The same thing will happen as before, due to the sigmoid activiation function in the final layer, when we call the <code>predict()</code> method on our model, it'll return prediction probabilities rather than classes.</p> In\u00a0[53]: Copied! <pre># Make predictions on the validation dataset\nmodel_2_pred_probs = model_2.predict(val_sentences)\nmodel_2_pred_probs.shape, model_2_pred_probs[:10] # view the first 10\n</pre> # Make predictions on the validation dataset model_2_pred_probs = model_2.predict(val_sentences) model_2_pred_probs.shape, model_2_pred_probs[:10] # view the first 10 <pre>24/24 [==============================] - 0s 2ms/step\n</pre> Out[53]: <pre>((762, 1),\n array([[0.00630066],\n        [0.7862389 ],\n        [0.9991792 ],\n        [0.06841089],\n        [0.00448257],\n        [0.99932086],\n        [0.8617405 ],\n        [0.99968505],\n        [0.9993248 ],\n        [0.57989997]], dtype=float32))</pre> <p>We can turn these prediction probabilities into prediction classes by rounding to the nearest integer (by default, prediction probabilities under 0.5 will go to 0 and those over 0.5 will go to 1).</p> In\u00a0[54]: Copied! <pre># Round out predictions and reduce to 1-dimensional array\nmodel_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\nmodel_2_preds[:10]\n</pre> # Round out predictions and reduce to 1-dimensional array model_2_preds = tf.squeeze(tf.round(model_2_pred_probs)) model_2_preds[:10] Out[54]: <pre>&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)&gt;</pre> <p>Beautiful, now let's use our <code>caculate_results()</code> function to evaluate our LSTM model and our <code>compare_baseline_to_new_results()</code> function to compare it to our baseline model.</p> In\u00a0[55]: Copied! <pre># Calculate LSTM model results\nmodel_2_results = calculate_results(y_true=val_labels,\n                                    y_pred=model_2_preds)\nmodel_2_results\n</pre> # Calculate LSTM model results model_2_results = calculate_results(y_true=val_labels,                                     y_pred=model_2_preds) model_2_results Out[55]: <pre>{'accuracy': 75.59055118110236,\n 'precision': 0.7567160722556739,\n 'recall': 0.7559055118110236,\n 'f1': 0.7539595513230887}</pre> In\u00a0[56]: Copied! <pre># Compare model 2 to baseline\ncompare_baseline_to_new_results(baseline_results, model_2_results)\n</pre> # Compare model 2 to baseline compare_baseline_to_new_results(baseline_results, model_2_results) <pre>Baseline accuracy: 79.27, New accuracy: 75.59, Difference: -3.67\nBaseline precision: 0.81, New precision: 0.76, Difference: -0.05\nBaseline recall: 0.79, New recall: 0.76, Difference: -0.04\nBaseline f1: 0.79, New f1: 0.75, Difference: -0.03\n</pre> In\u00a0[57]: Copied! <pre># Set random seed and create embedding layer (new embedding layer for each model)\ntf.random.set_seed(42)\nfrom tensorflow.keras import layers\nmodel_3_embedding = layers.Embedding(input_dim=max_vocab_length,\n                                     output_dim=128,\n                                     embeddings_initializer=\"uniform\",\n                                     input_length=max_length,\n                                     name=\"embedding_3\")\n\n# Build an RNN using the GRU cell\ninputs = layers.Input(shape=(1,), dtype=\"string\")\nx = text_vectorizer(inputs)\nx = model_3_embedding(x)\n# x = layers.GRU(64, return_sequences=True) # stacking recurrent cells requires return_sequences=True\nx = layers.GRU(64)(x) \n# x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer after GRU cell\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel_3 = tf.keras.Model(inputs, outputs, name=\"model_3_GRU\")\n</pre> # Set random seed and create embedding layer (new embedding layer for each model) tf.random.set_seed(42) from tensorflow.keras import layers model_3_embedding = layers.Embedding(input_dim=max_vocab_length,                                      output_dim=128,                                      embeddings_initializer=\"uniform\",                                      input_length=max_length,                                      name=\"embedding_3\")  # Build an RNN using the GRU cell inputs = layers.Input(shape=(1,), dtype=\"string\") x = text_vectorizer(inputs) x = model_3_embedding(x) # x = layers.GRU(64, return_sequences=True) # stacking recurrent cells requires return_sequences=True x = layers.GRU(64)(x)  # x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer after GRU cell outputs = layers.Dense(1, activation=\"sigmoid\")(x) model_3 = tf.keras.Model(inputs, outputs, name=\"model_3_GRU\") <p>TensorFlow makes it easy to use powerful components such as the GRU cell in our models. And now our third model is built, let's compile it, just as before.</p> In\u00a0[58]: Copied! <pre># Compile GRU model\nmodel_3.compile(loss=\"binary_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])\n</pre> # Compile GRU model model_3.compile(loss=\"binary_crossentropy\",                 optimizer=tf.keras.optimizers.Adam(),                 metrics=[\"accuracy\"]) <p>What does a summary of our model look like?</p> In\u00a0[59]: Copied! <pre># Get a summary of the GRU model\nmodel_3.summary()\n</pre> # Get a summary of the GRU model model_3.summary() <pre>Model: \"model_3_GRU\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_3 (InputLayer)        [(None, 1)]               0         \n                                                                 \n text_vectorization_1 (TextV  (None, 15)               0         \n ectorization)                                                   \n                                                                 \n embedding_3 (Embedding)     (None, 15, 128)           1280000   \n                                                                 \n gru (GRU)                   (None, 64)                37248     \n                                                                 \n dense_2 (Dense)             (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 1,317,313\nTrainable params: 1,317,313\nNon-trainable params: 0\n_________________________________________________________________\n</pre> <p>Notice the difference in number of trainable parameters between <code>model_2</code> (LSTM) and <code>model_3</code> (GRU). The difference comes from the LSTM cell having more trainable parameters than the GRU cell.</p> <p>We'll fit our model just as we've been doing previously. We'll also track our models results using our <code>create_tensorboard_callback()</code> function.</p> In\u00a0[60]: Copied! <pre># Fit model\nmodel_3_history = model_3.fit(train_sentences,\n                              train_labels,\n                              epochs=5,\n                              validation_data=(val_sentences, val_labels),\n                              callbacks=[create_tensorboard_callback(SAVE_DIR, \"GRU\")])\n</pre> # Fit model model_3_history = model_3.fit(train_sentences,                               train_labels,                               epochs=5,                               validation_data=(val_sentences, val_labels),                               callbacks=[create_tensorboard_callback(SAVE_DIR, \"GRU\")]) <pre>Saving TensorBoard log files to: model_logs/GRU/20230526-001539\nEpoch 1/5\n215/215 [==============================] - 11s 43ms/step - loss: 0.5274 - accuracy: 0.7231 - val_loss: 0.4539 - val_accuracy: 0.7795\nEpoch 2/5\n215/215 [==============================] - 2s 10ms/step - loss: 0.3179 - accuracy: 0.8686 - val_loss: 0.4850 - val_accuracy: 0.7848\nEpoch 3/5\n215/215 [==============================] - 2s 9ms/step - loss: 0.2149 - accuracy: 0.9187 - val_loss: 0.5544 - val_accuracy: 0.7717\nEpoch 4/5\n215/215 [==============================] - 2s 8ms/step - loss: 0.1517 - accuracy: 0.9488 - val_loss: 0.6279 - val_accuracy: 0.7835\nEpoch 5/5\n215/215 [==============================] - 2s 8ms/step - loss: 0.1145 - accuracy: 0.9609 - val_loss: 0.6063 - val_accuracy: 0.7756\n</pre> <p>Due to the optimized default settings of the GRU cell in TensorFlow, training doesn't take long at all.</p> <p>Time to make some predictions on the validation samples.</p> In\u00a0[61]: Copied! <pre># Make predictions on the validation data\nmodel_3_pred_probs = model_3.predict(val_sentences)\nmodel_3_pred_probs.shape, model_3_pred_probs[:10]\n</pre> # Make predictions on the validation data model_3_pred_probs = model_3.predict(val_sentences) model_3_pred_probs.shape, model_3_pred_probs[:10] <pre>24/24 [==============================] - 0s 2ms/step\n</pre> Out[61]: <pre>((762, 1),\n array([[0.31703022],\n        [0.9160779 ],\n        [0.9977792 ],\n        [0.14830083],\n        [0.01086212],\n        [0.9908326 ],\n        [0.6938264 ],\n        [0.9978917 ],\n        [0.99662066],\n        [0.4299642 ]], dtype=float32))</pre> <p>Again we get an array of prediction probabilities back which we can convert to prediction classes by rounding them.</p> In\u00a0[62]: Copied! <pre># Convert prediction probabilities to prediction classes\nmodel_3_preds = tf.squeeze(tf.round(model_3_pred_probs))\nmodel_3_preds[:10]\n</pre> # Convert prediction probabilities to prediction classes model_3_preds = tf.squeeze(tf.round(model_3_pred_probs)) model_3_preds[:10] Out[62]: <pre>&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)&gt;</pre> <p>Now we've got predicted classes, let's evaluate them against the ground truth labels.</p> In\u00a0[63]: Copied! <pre># Calcuate model_3 results\nmodel_3_results = calculate_results(y_true=val_labels, \n                                    y_pred=model_3_preds)\nmodel_3_results\n</pre> # Calcuate model_3 results model_3_results = calculate_results(y_true=val_labels,                                      y_pred=model_3_preds) model_3_results Out[63]: <pre>{'accuracy': 77.55905511811024,\n 'precision': 0.776326889347514,\n 'recall': 0.7755905511811023,\n 'f1': 0.7740902496040959}</pre> <p>Finally we can compare our GRU model's results to our baseline.</p> In\u00a0[64]: Copied! <pre># Compare to baseline\ncompare_baseline_to_new_results(baseline_results, model_3_results)\n</pre> # Compare to baseline compare_baseline_to_new_results(baseline_results, model_3_results) <pre>Baseline accuracy: 79.27, New accuracy: 77.56, Difference: -1.71\nBaseline precision: 0.81, New precision: 0.78, Difference: -0.03\nBaseline recall: 0.79, New recall: 0.78, Difference: -0.02\nBaseline f1: 0.79, New f1: 0.77, Difference: -0.01\n</pre> In\u00a0[65]: Copied! <pre># Set random seed and create embedding layer (new embedding layer for each model)\ntf.random.set_seed(42)\nfrom tensorflow.keras import layers\nmodel_4_embedding = layers.Embedding(input_dim=max_vocab_length,\n                                     output_dim=128,\n                                     embeddings_initializer=\"uniform\",\n                                     input_length=max_length,\n                                     name=\"embedding_4\")\n\n# Build a Bidirectional RNN in TensorFlow\ninputs = layers.Input(shape=(1,), dtype=\"string\")\nx = text_vectorizer(inputs)\nx = model_4_embedding(x)\n# x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x) # stacking RNN layers requires return_sequences=True\nx = layers.Bidirectional(layers.LSTM(64))(x) # bidirectional goes both ways so has double the parameters of a regular LSTM layer\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel_4 = tf.keras.Model(inputs, outputs, name=\"model_4_Bidirectional\")\n</pre> # Set random seed and create embedding layer (new embedding layer for each model) tf.random.set_seed(42) from tensorflow.keras import layers model_4_embedding = layers.Embedding(input_dim=max_vocab_length,                                      output_dim=128,                                      embeddings_initializer=\"uniform\",                                      input_length=max_length,                                      name=\"embedding_4\")  # Build a Bidirectional RNN in TensorFlow inputs = layers.Input(shape=(1,), dtype=\"string\") x = text_vectorizer(inputs) x = model_4_embedding(x) # x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x) # stacking RNN layers requires return_sequences=True x = layers.Bidirectional(layers.LSTM(64))(x) # bidirectional goes both ways so has double the parameters of a regular LSTM layer outputs = layers.Dense(1, activation=\"sigmoid\")(x) model_4 = tf.keras.Model(inputs, outputs, name=\"model_4_Bidirectional\") <p>\ud83d\udd11 Note: You can use the <code>Bidirectional</code> wrapper on any RNN cell in TensorFlow. For example, <code>layers.Bidirectional(layers.GRU(64))</code> creates a bidirectional GRU cell.</p> <p>Our bidirectional model is built, let's compile it.</p> In\u00a0[66]: Copied! <pre># Compile\nmodel_4.compile(loss=\"binary_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])\n</pre> # Compile model_4.compile(loss=\"binary_crossentropy\",                 optimizer=tf.keras.optimizers.Adam(),                 metrics=[\"accuracy\"]) <p>And of course, we'll check out a summary.</p> In\u00a0[67]: Copied! <pre># Get a summary of our bidirectional model\nmodel_4.summary()\n</pre> # Get a summary of our bidirectional model model_4.summary() <pre>Model: \"model_4_Bidirectional\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_4 (InputLayer)        [(None, 1)]               0         \n                                                                 \n text_vectorization_1 (TextV  (None, 15)               0         \n ectorization)                                                   \n                                                                 \n embedding_4 (Embedding)     (None, 15, 128)           1280000   \n                                                                 \n bidirectional (Bidirectiona  (None, 128)              98816     \n l)                                                              \n                                                                 \n dense_3 (Dense)             (None, 1)                 129       \n                                                                 \n=================================================================\nTotal params: 1,378,945\nTrainable params: 1,378,945\nNon-trainable params: 0\n_________________________________________________________________\n</pre> <p>Notice the increased number of trainable parameters in <code>model_4</code> (bidirectional LSTM) compared to <code>model_2</code> (regular LSTM). This is due to the bidirectionality we added to our RNN.</p> <p>Time to fit our bidirectional model and track its performance.</p> In\u00a0[68]: Copied! <pre># Fit the model (takes longer because of the bidirectional layers)\nmodel_4_history = model_4.fit(train_sentences,\n                              train_labels,\n                              epochs=5,\n                              validation_data=(val_sentences, val_labels),\n                              callbacks=[create_tensorboard_callback(SAVE_DIR, \"bidirectional_RNN\")])\n</pre> # Fit the model (takes longer because of the bidirectional layers) model_4_history = model_4.fit(train_sentences,                               train_labels,                               epochs=5,                               validation_data=(val_sentences, val_labels),                               callbacks=[create_tensorboard_callback(SAVE_DIR, \"bidirectional_RNN\")]) <pre>Saving TensorBoard log files to: model_logs/bidirectional_RNN/20230526-001559\nEpoch 1/5\n215/215 [==============================] - 14s 47ms/step - loss: 0.5096 - accuracy: 0.7447 - val_loss: 0.4585 - val_accuracy: 0.7861\nEpoch 2/5\n215/215 [==============================] - 2s 12ms/step - loss: 0.3140 - accuracy: 0.8726 - val_loss: 0.5086 - val_accuracy: 0.7743\nEpoch 3/5\n215/215 [==============================] - 2s 11ms/step - loss: 0.2139 - accuracy: 0.9183 - val_loss: 0.5716 - val_accuracy: 0.7730\nEpoch 4/5\n215/215 [==============================] - 2s 9ms/step - loss: 0.1486 - accuracy: 0.9504 - val_loss: 0.6707 - val_accuracy: 0.7703\nEpoch 5/5\n215/215 [==============================] - 2s 10ms/step - loss: 0.1058 - accuracy: 0.9648 - val_loss: 0.6658 - val_accuracy: 0.7677\n</pre> <p>Due to the bidirectionality of our model we see a slight increase in training time.</p> <p>Not to worry, it's not too dramatic of an increase.</p> <p>Let's make some predictions with it.</p> In\u00a0[69]: Copied! <pre># Make predictions with bidirectional RNN on the validation data\nmodel_4_pred_probs = model_4.predict(val_sentences)\nmodel_4_pred_probs[:10]\n</pre> # Make predictions with bidirectional RNN on the validation data model_4_pred_probs = model_4.predict(val_sentences) model_4_pred_probs[:10] <pre>24/24 [==============================] - 1s 3ms/step\n</pre> Out[69]: <pre>array([[0.05258294],\n       [0.8495521 ],\n       [0.99898857],\n       [0.15441437],\n       [0.00566462],\n       [0.99576193],\n       [0.952807  ],\n       [0.9993511 ],\n       [0.99936384],\n       [0.19425693]], dtype=float32)</pre> <p>And we'll convert them to prediction classes and evaluate them against the ground truth labels and baseline model.</p> In\u00a0[70]: Copied! <pre># Convert prediction probabilities to labels\nmodel_4_preds = tf.squeeze(tf.round(model_4_pred_probs))\nmodel_4_preds[:10]\n</pre> # Convert prediction probabilities to labels model_4_preds = tf.squeeze(tf.round(model_4_pred_probs)) model_4_preds[:10] Out[70]: <pre>&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)&gt;</pre> In\u00a0[71]: Copied! <pre># Calculate bidirectional RNN model results\nmodel_4_results = calculate_results(val_labels, model_4_preds)\nmodel_4_results\n</pre> # Calculate bidirectional RNN model results model_4_results = calculate_results(val_labels, model_4_preds) model_4_results Out[71]: <pre>{'accuracy': 76.77165354330708,\n 'precision': 0.7675450859410361,\n 'recall': 0.7677165354330708,\n 'f1': 0.7667932666650168}</pre> In\u00a0[72]: Copied! <pre># Check to see how the bidirectional model performs against the baseline\ncompare_baseline_to_new_results(baseline_results, model_4_results)\n</pre> # Check to see how the bidirectional model performs against the baseline compare_baseline_to_new_results(baseline_results, model_4_results) <pre>Baseline accuracy: 79.27, New accuracy: 76.77, Difference: -2.49\nBaseline precision: 0.81, New precision: 0.77, Difference: -0.04\nBaseline recall: 0.79, New recall: 0.77, Difference: -0.02\nBaseline f1: 0.79, New f1: 0.77, Difference: -0.02\n</pre> In\u00a0[73]: Copied! <pre># Test out the embedding, 1D convolutional and max pooling\nembedding_test = embedding(text_vectorizer([\"this is a test sentence\"])) # turn target sentence into embedding\nconv_1d = layers.Conv1D(filters=32, kernel_size=5, activation=\"relu\") # convolve over target sequence 5 words at a time\nconv_1d_output = conv_1d(embedding_test) # pass embedding through 1D convolutional layer\nmax_pool = layers.GlobalMaxPool1D() \nmax_pool_output = max_pool(conv_1d_output) # get the most important features\nembedding_test.shape, conv_1d_output.shape, max_pool_output.shape\n</pre> # Test out the embedding, 1D convolutional and max pooling embedding_test = embedding(text_vectorizer([\"this is a test sentence\"])) # turn target sentence into embedding conv_1d = layers.Conv1D(filters=32, kernel_size=5, activation=\"relu\") # convolve over target sequence 5 words at a time conv_1d_output = conv_1d(embedding_test) # pass embedding through 1D convolutional layer max_pool = layers.GlobalMaxPool1D()  max_pool_output = max_pool(conv_1d_output) # get the most important features embedding_test.shape, conv_1d_output.shape, max_pool_output.shape Out[73]: <pre>(TensorShape([1, 15, 128]), TensorShape([1, 11, 32]), TensorShape([1, 32]))</pre> <p>Notice the output shapes of each layer.</p> <p>The embedding has an output shape dimension of the parameters we set it to (<code>input_length=15</code> and <code>output_dim=128</code>).</p> <p>The 1-dimensional convolutional layer has an output which has been compressed inline with its parameters. And the same goes for the max pooling layer output.</p> <p>Our text starts out as a string but gets converted to a feature vector of length 64 through various transformation steps (from tokenization to embedding to 1-dimensional convolution to max pool).</p> <p>Let's take a peak at what each of these transformations looks like.</p> In\u00a0[74]: Copied! <pre># See the outputs of each layer\nembedding_test[:1], conv_1d_output[:1], max_pool_output[:1]\n</pre> # See the outputs of each layer embedding_test[:1], conv_1d_output[:1], max_pool_output[:1] Out[74]: <pre>(&lt;tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n array([[[ 0.01675646, -0.03352517,  0.04817378, ..., -0.02946043,\n          -0.03770737,  0.01220698],\n         [-0.00607298,  0.06020833, -0.05641982, ...,  0.08325578,\n          -0.01878556, -0.08398241],\n         [-0.0362346 ,  0.00904451, -0.03833614, ...,  0.0051756 ,\n          -0.00220015, -0.0017492 ],\n         ...,\n         [-0.01078545,  0.05590528,  0.03125916, ..., -0.0312557 ,\n          -0.05340781, -0.03800201],\n         [-0.01078545,  0.05590528,  0.03125916, ..., -0.0312557 ,\n          -0.05340781, -0.03800201],\n         [-0.01078545,  0.05590528,  0.03125916, ..., -0.0312557 ,\n          -0.05340781, -0.03800201]]], dtype=float32)&gt;,\n &lt;tf.Tensor: shape=(1, 11, 32), dtype=float32, numpy=\n array([[[0.        , 0.10975833, 0.        , 0.        , 0.        ,\n          0.06834612, 0.        , 0.02298634, 0.        , 0.        ,\n          0.        , 0.        , 0.        , 0.        , 0.06889185,\n          0.08162662, 0.        , 0.        , 0.03804683, 0.        ,\n          0.        , 0.        , 0.        , 0.00810859, 0.02383356,\n          0.        , 0.00385817, 0.        , 0.01310921, 0.        ,\n          0.        , 0.16110645],\n         [0.05000008, 0.        , 0.03852113, 0.0149918 , 0.03014192,\n          0.04613257, 0.        , 0.        , 0.        , 0.05233994,\n          0.        , 0.        , 0.07095916, 0.03590994, 0.        ,\n          0.        , 0.        , 0.        , 0.        , 0.05599808,\n          0.04344876, 0.04021783, 0.        , 0.06110618, 0.        ,\n          0.        , 0.        , 0.00198402, 0.        , 0.03175152,\n          0.        , 0.04452901],\n         [0.        , 0.05068349, 0.06747732, 0.        , 0.        ,\n          0.04893802, 0.        , 0.        , 0.        , 0.        ,\n          0.        , 0.        , 0.0853087 , 0.01114925, 0.00223987,\n          0.        , 0.        , 0.        , 0.        , 0.        ,\n          0.        , 0.        , 0.        , 0.10797694, 0.02317763,\n          0.        , 0.01130794, 0.        , 0.01777459, 0.        ,\n          0.        , 0.02142338],\n         [0.        , 0.01030538, 0.        , 0.        , 0.02127263,\n          0.06377578, 0.        , 0.        , 0.        , 0.        ,\n          0.03660904, 0.        , 0.13293687, 0.06086106, 0.        ,\n          0.        , 0.        , 0.03161986, 0.00114628, 0.02163697,\n          0.        , 0.        , 0.        , 0.04408561, 0.        ,\n          0.01193662, 0.        , 0.        , 0.01174912, 0.03890226,\n          0.        , 0.06139129],\n         [0.        , 0.        , 0.00959204, 0.        , 0.03472092,\n          0.03202822, 0.        , 0.        , 0.        , 0.        ,\n          0.00390257, 0.        , 0.07451484, 0.00349154, 0.        ,\n          0.        , 0.02155435, 0.        , 0.        , 0.        ,\n          0.        , 0.        , 0.        , 0.09407972, 0.        ,\n          0.        , 0.00077316, 0.        , 0.        , 0.        ,\n          0.        , 0.06074456],\n         [0.        , 0.03225943, 0.01736662, 0.        , 0.01197381,\n          0.02301392, 0.        , 0.        , 0.        , 0.00205472,\n          0.02762672, 0.        , 0.06565619, 0.00253076, 0.        ,\n          0.        , 0.00745697, 0.        , 0.        , 0.        ,\n          0.        , 0.        , 0.        , 0.09595221, 0.        ,\n          0.        , 0.        , 0.        , 0.01910873, 0.        ,\n          0.        , 0.06507206],\n         [0.        , 0.03225943, 0.01736662, 0.        , 0.01197381,\n          0.02301392, 0.        , 0.        , 0.        , 0.00205472,\n          0.02762672, 0.        , 0.06565619, 0.00253076, 0.        ,\n          0.        , 0.00745697, 0.        , 0.        , 0.        ,\n          0.        , 0.        , 0.        , 0.09595221, 0.        ,\n          0.        , 0.        , 0.        , 0.01910873, 0.        ,\n          0.        , 0.06507206],\n         [0.        , 0.03225943, 0.01736662, 0.        , 0.01197381,\n          0.02301392, 0.        , 0.        , 0.        , 0.00205472,\n          0.02762672, 0.        , 0.06565619, 0.00253076, 0.        ,\n          0.        , 0.00745697, 0.        , 0.        , 0.        ,\n          0.        , 0.        , 0.        , 0.09595221, 0.        ,\n          0.        , 0.        , 0.        , 0.01910873, 0.        ,\n          0.        , 0.06507206],\n         [0.        , 0.03225943, 0.01736662, 0.        , 0.01197381,\n          0.02301392, 0.        , 0.        , 0.        , 0.00205472,\n          0.02762672, 0.        , 0.06565619, 0.00253076, 0.        ,\n          0.        , 0.00745697, 0.        , 0.        , 0.        ,\n          0.        , 0.        , 0.        , 0.09595221, 0.        ,\n          0.        , 0.        , 0.        , 0.01910873, 0.        ,\n          0.        , 0.06507206],\n         [0.        , 0.03225943, 0.01736662, 0.        , 0.01197381,\n          0.02301392, 0.        , 0.        , 0.        , 0.00205472,\n          0.02762672, 0.        , 0.06565619, 0.00253076, 0.        ,\n          0.        , 0.00745697, 0.        , 0.        , 0.        ,\n          0.        , 0.        , 0.        , 0.09595221, 0.        ,\n          0.        , 0.        , 0.        , 0.01910873, 0.        ,\n          0.        , 0.06507206],\n         [0.        , 0.03225943, 0.01736662, 0.        , 0.01197381,\n          0.02301392, 0.        , 0.        , 0.        , 0.00205472,\n          0.02762672, 0.        , 0.06565619, 0.00253076, 0.        ,\n          0.        , 0.00745697, 0.        , 0.        , 0.        ,\n          0.        , 0.        , 0.        , 0.09595221, 0.        ,\n          0.        , 0.        , 0.        , 0.01910873, 0.        ,\n          0.        , 0.06507206]]], dtype=float32)&gt;,\n &lt;tf.Tensor: shape=(1, 32), dtype=float32, numpy=\n array([[0.05000008, 0.10975833, 0.06747732, 0.0149918 , 0.03472092,\n         0.06834612, 0.        , 0.02298634, 0.        , 0.05233994,\n         0.03660904, 0.        , 0.13293687, 0.06086106, 0.06889185,\n         0.08162662, 0.02155435, 0.03161986, 0.03804683, 0.05599808,\n         0.04344876, 0.04021783, 0.        , 0.10797694, 0.02383356,\n         0.01193662, 0.01130794, 0.00198402, 0.01910873, 0.03890226,\n         0.        , 0.16110645]], dtype=float32)&gt;)</pre> <p>Alright, we've seen the outputs of several components of a CNN for sequences, let's put them together and construct a full model, compile it (just as we've done with our other models) and get a summary.</p> In\u00a0[75]: Copied! <pre># Set random seed and create embedding layer (new embedding layer for each model)\ntf.random.set_seed(42)\nfrom tensorflow.keras import layers\nmodel_5_embedding = layers.Embedding(input_dim=max_vocab_length,\n                                     output_dim=128,\n                                     embeddings_initializer=\"uniform\",\n                                     input_length=max_length,\n                                     name=\"embedding_5\")\n\n# Create 1-dimensional convolutional layer to model sequences\nfrom tensorflow.keras import layers\ninputs = layers.Input(shape=(1,), dtype=\"string\")\nx = text_vectorizer(inputs)\nx = model_5_embedding(x)\nx = layers.Conv1D(filters=32, kernel_size=5, activation=\"relu\")(x)\nx = layers.GlobalMaxPool1D()(x)\n# x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel_5 = tf.keras.Model(inputs, outputs, name=\"model_5_Conv1D\")\n\n# Compile Conv1D model\nmodel_5.compile(loss=\"binary_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])\n\n# Get a summary of our 1D convolution model\nmodel_5.summary()\n</pre> # Set random seed and create embedding layer (new embedding layer for each model) tf.random.set_seed(42) from tensorflow.keras import layers model_5_embedding = layers.Embedding(input_dim=max_vocab_length,                                      output_dim=128,                                      embeddings_initializer=\"uniform\",                                      input_length=max_length,                                      name=\"embedding_5\")  # Create 1-dimensional convolutional layer to model sequences from tensorflow.keras import layers inputs = layers.Input(shape=(1,), dtype=\"string\") x = text_vectorizer(inputs) x = model_5_embedding(x) x = layers.Conv1D(filters=32, kernel_size=5, activation=\"relu\")(x) x = layers.GlobalMaxPool1D()(x) # x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer outputs = layers.Dense(1, activation=\"sigmoid\")(x) model_5 = tf.keras.Model(inputs, outputs, name=\"model_5_Conv1D\")  # Compile Conv1D model model_5.compile(loss=\"binary_crossentropy\",                 optimizer=tf.keras.optimizers.Adam(),                 metrics=[\"accuracy\"])  # Get a summary of our 1D convolution model model_5.summary() <pre>Model: \"model_5_Conv1D\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_5 (InputLayer)        [(None, 1)]               0         \n                                                                 \n text_vectorization_1 (TextV  (None, 15)               0         \n ectorization)                                                   \n                                                                 \n embedding_5 (Embedding)     (None, 15, 128)           1280000   \n                                                                 \n conv1d_1 (Conv1D)           (None, 11, 32)            20512     \n                                                                 \n global_max_pooling1d_1 (Glo  (None, 32)               0         \n balMaxPooling1D)                                                \n                                                                 \n dense_4 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1,300,545\nTrainable params: 1,300,545\nNon-trainable params: 0\n_________________________________________________________________\n</pre> <p>Woohoo! Looking great! Notice how the number of trainable parameters for the 1-dimensional convolutional layer is similar to that of the LSTM layer in <code>model_2</code>.</p> <p>Let's fit our 1D CNN model to our text data. In line with previous experiments, we'll save its results using our <code>create_tensorboard_callback()</code> function.</p> In\u00a0[76]: Copied! <pre># Fit the model\nmodel_5_history = model_5.fit(train_sentences,\n                              train_labels,\n                              epochs=5,\n                              validation_data=(val_sentences, val_labels),\n                              callbacks=[create_tensorboard_callback(SAVE_DIR, \n                                                                     \"Conv1D\")])\n</pre> # Fit the model model_5_history = model_5.fit(train_sentences,                               train_labels,                               epochs=5,                               validation_data=(val_sentences, val_labels),                               callbacks=[create_tensorboard_callback(SAVE_DIR,                                                                       \"Conv1D\")]) <pre>Saving TensorBoard log files to: model_logs/Conv1D/20230526-001626\nEpoch 1/5\n215/215 [==============================] - 11s 42ms/step - loss: 0.5693 - accuracy: 0.7108 - val_loss: 0.4736 - val_accuracy: 0.7769\nEpoch 2/5\n215/215 [==============================] - 2s 9ms/step - loss: 0.3426 - accuracy: 0.8600 - val_loss: 0.4677 - val_accuracy: 0.7874\nEpoch 3/5\n215/215 [==============================] - 2s 9ms/step - loss: 0.2130 - accuracy: 0.9202 - val_loss: 0.5374 - val_accuracy: 0.7677\nEpoch 4/5\n215/215 [==============================] - 1s 7ms/step - loss: 0.1366 - accuracy: 0.9564 - val_loss: 0.6076 - val_accuracy: 0.7756\nEpoch 5/5\n215/215 [==============================] - 2s 7ms/step - loss: 0.0958 - accuracy: 0.9667 - val_loss: 0.6706 - val_accuracy: 0.7874\n</pre> <p>Nice! Thanks to GPU acceleration, our 1D convolutional model trains nice and fast. Let's make some predictions with it and evaluate them just as before.</p> In\u00a0[77]: Copied! <pre># Make predictions with model_5\nmodel_5_pred_probs = model_5.predict(val_sentences)\nmodel_5_pred_probs[:10]\n</pre> # Make predictions with model_5 model_5_pred_probs = model_5.predict(val_sentences) model_5_pred_probs[:10] <pre>24/24 [==============================] - 0s 2ms/step\n</pre> Out[77]: <pre>array([[0.7295443 ],\n       [0.63939744],\n       [0.9997949 ],\n       [0.05865377],\n       [0.0070557 ],\n       [0.99556965],\n       [0.90180606],\n       [0.9973731 ],\n       [0.99953437],\n       [0.6327795 ]], dtype=float32)</pre> In\u00a0[78]: Copied! <pre># Convert model_5 prediction probabilities to labels\nmodel_5_preds = tf.squeeze(tf.round(model_5_pred_probs))\nmodel_5_preds[:10]\n</pre> # Convert model_5 prediction probabilities to labels model_5_preds = tf.squeeze(tf.round(model_5_pred_probs)) model_5_preds[:10] Out[78]: <pre>&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)&gt;</pre> In\u00a0[79]: Copied! <pre># Calculate model_5 evaluation metrics \nmodel_5_results = calculate_results(y_true=val_labels, \n                                    y_pred=model_5_preds)\nmodel_5_results\n</pre> # Calculate model_5 evaluation metrics  model_5_results = calculate_results(y_true=val_labels,                                      y_pred=model_5_preds) model_5_results Out[79]: <pre>{'accuracy': 78.74015748031496,\n 'precision': 0.7900609457201325,\n 'recall': 0.7874015748031497,\n 'f1': 0.7852275674790494}</pre> In\u00a0[80]: Copied! <pre># Compare model_5 results to baseline \ncompare_baseline_to_new_results(baseline_results, model_5_results)\n</pre> # Compare model_5 results to baseline  compare_baseline_to_new_results(baseline_results, model_5_results) <pre>Baseline accuracy: 79.27, New accuracy: 78.74, Difference: -0.52\nBaseline precision: 0.81, New precision: 0.79, Difference: -0.02\nBaseline recall: 0.79, New recall: 0.79, Difference: -0.01\nBaseline f1: 0.79, New f1: 0.79, Difference: -0.00\n</pre> In\u00a0[81]: Copied! <pre># Example of pretrained embedding with universal sentence encoder - https://tfhub.dev/google/universal-sentence-encoder/4\nimport tensorflow_hub as hub\nembed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\") # load Universal Sentence Encoder\nembed_samples = embed([sample_sentence,\n                      \"When you call the universal sentence encoder on a sentence, it turns it into numbers.\"])\n\nprint(embed_samples[0][:50])\n</pre> # Example of pretrained embedding with universal sentence encoder - https://tfhub.dev/google/universal-sentence-encoder/4 import tensorflow_hub as hub embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\") # load Universal Sentence Encoder embed_samples = embed([sample_sentence,                       \"When you call the universal sentence encoder on a sentence, it turns it into numbers.\"])  print(embed_samples[0][:50]) <pre>tf.Tensor(\n[-0.01154496  0.02487099  0.0287963  -0.01272263  0.03969951  0.08829075\n  0.02682647  0.05582222 -0.01078761 -0.00596655  0.00640638 -0.01816132\n  0.0002885   0.09106605  0.05874373 -0.03175148  0.01510153 -0.05164852\n  0.0099434  -0.06867751 -0.04210396  0.0267539   0.03008907  0.00320448\n -0.00336865 -0.04790529  0.02267517 -0.00984557 -0.04066692 -0.01285528\n -0.04665243  0.05630673 -0.03952145  0.00521895  0.02495948 -0.07011835\n  0.02873133  0.04945794 -0.00634555 -0.08959357  0.02807156 -0.00809173\n -0.01363956  0.05998395 -0.1036155  -0.05192674  0.00232459 -0.02326531\n -0.03752431  0.0333298 ], shape=(50,), dtype=float32)\n</pre> In\u00a0[82]: Copied! <pre># Each sentence has been encoded into a 512 dimension vector\nembed_samples[0].shape\n</pre> # Each sentence has been encoded into a 512 dimension vector embed_samples[0].shape Out[82]: <pre>TensorShape([512])</pre> <p>Passing our sentences to the Universal Sentence Encoder (USE) encodes them from strings to 512 dimensional vectors, which make no sense to us but hopefully make sense to our machine learning models.</p> <p>Speaking of models, let's build one with the USE as our embedding layer.</p> <p>We can convert the TensorFlow Hub USE module into a Keras layer using the <code>hub.KerasLayer</code> class.</p> <p>\ud83d\udd11 Note: Due to the size of the USE TensorFlow Hub module, it may take a little while to download. Once it's downloaded though, it'll be cached and ready to use. And as with many TensorFlow Hub modules, there is a \"lite\" version of the USE which takes up less space but sacrifices some performance and requires more preprocessing steps. However, depending on your available compute power, the lite version may be better for your application use case.</p> In\u00a0[83]: Copied! <pre># We can use this encoding layer in place of our text_vectorizer and embedding layer\nsentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n                                        input_shape=[], # shape of inputs coming to our model \n                                        dtype=tf.string, # data type of inputs coming to the USE layer\n                                        trainable=False, # keep the pretrained weights (we'll create a feature extractor)\n                                        name=\"USE\")\n</pre> # We can use this encoding layer in place of our text_vectorizer and embedding layer sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",                                         input_shape=[], # shape of inputs coming to our model                                          dtype=tf.string, # data type of inputs coming to the USE layer                                         trainable=False, # keep the pretrained weights (we'll create a feature extractor)                                         name=\"USE\")  <p>Beautiful! Now we've got the USE as a Keras layer, we can use it in a Keras Sequential model.</p> In\u00a0[84]: Copied! <pre># Create model using the Sequential API\nmodel_6 = tf.keras.Sequential([\n  sentence_encoder_layer, # take in sentences and then encode them into an embedding\n  layers.Dense(64, activation=\"relu\"),\n  layers.Dense(1, activation=\"sigmoid\")\n], name=\"model_6_USE\")\n\n# Compile model\nmodel_6.compile(loss=\"binary_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])\n\nmodel_6.summary()\n</pre> # Create model using the Sequential API model_6 = tf.keras.Sequential([   sentence_encoder_layer, # take in sentences and then encode them into an embedding   layers.Dense(64, activation=\"relu\"),   layers.Dense(1, activation=\"sigmoid\") ], name=\"model_6_USE\")  # Compile model model_6.compile(loss=\"binary_crossentropy\",                 optimizer=tf.keras.optimizers.Adam(),                 metrics=[\"accuracy\"])  model_6.summary() <pre>Model: \"model_6_USE\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n USE (KerasLayer)            (None, 512)               256797824 \n                                                                 \n dense_5 (Dense)             (None, 64)                32832     \n                                                                 \n dense_6 (Dense)             (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 256,830,721\nTrainable params: 32,897\nNon-trainable params: 256,797,824\n_________________________________________________________________\n</pre> <p>Notice the number of paramters in the USE layer, these are the pretrained weights its learned on various text sources (Wikipedia, web news, web question-answer forums, etc, see the Universal Sentence Encoder paper for more).</p> <p>The trainable parameters are only in our output layers, in other words, we're keeping the USE weights frozen and using it as a feature-extractor. We could fine-tune these weights by setting <code>trainable=True</code> when creating the <code>hub.KerasLayer</code> instance.</p> <p>Now we've got a feature extractor model ready, let's train it and track its results to TensorBoard using our <code>create_tensorboard_callback()</code> function.</p> In\u00a0[85]: Copied! <pre># Train a classifier on top of pretrained embeddings\nmodel_6_history = model_6.fit(train_sentences,\n                              train_labels,\n                              epochs=5,\n                              validation_data=(val_sentences, val_labels),\n                              callbacks=[create_tensorboard_callback(SAVE_DIR, \n                                                                     \"tf_hub_sentence_encoder\")])\n</pre> # Train a classifier on top of pretrained embeddings model_6_history = model_6.fit(train_sentences,                               train_labels,                               epochs=5,                               validation_data=(val_sentences, val_labels),                               callbacks=[create_tensorboard_callback(SAVE_DIR,                                                                       \"tf_hub_sentence_encoder\")]) <pre>Saving TensorBoard log files to: model_logs/tf_hub_sentence_encoder/20230526-001739\nEpoch 1/5\n215/215 [==============================] - 6s 11ms/step - loss: 0.5014 - accuracy: 0.7870 - val_loss: 0.4469 - val_accuracy: 0.7992\nEpoch 2/5\n215/215 [==============================] - 2s 9ms/step - loss: 0.4145 - accuracy: 0.8140 - val_loss: 0.4359 - val_accuracy: 0.8097\nEpoch 3/5\n215/215 [==============================] - 2s 9ms/step - loss: 0.4000 - accuracy: 0.8216 - val_loss: 0.4319 - val_accuracy: 0.8163\nEpoch 4/5\n215/215 [==============================] - 2s 9ms/step - loss: 0.3927 - accuracy: 0.8262 - val_loss: 0.4280 - val_accuracy: 0.8176\nEpoch 5/5\n215/215 [==============================] - 2s 9ms/step - loss: 0.3862 - accuracy: 0.8288 - val_loss: 0.4299 - val_accuracy: 0.8176\n</pre> <p>USE model trained! Let's make some predictions with it an evaluate them as we've done with our other models.</p> In\u00a0[86]: Copied! <pre># Make predictions with USE TF Hub model\nmodel_6_pred_probs = model_6.predict(val_sentences)\nmodel_6_pred_probs[:10]\n</pre> # Make predictions with USE TF Hub model model_6_pred_probs = model_6.predict(val_sentences) model_6_pred_probs[:10] <pre>24/24 [==============================] - 1s 7ms/step\n</pre> Out[86]: <pre>array([[0.14814094],\n       [0.74057853],\n       [0.9886474 ],\n       [0.22455953],\n       [0.7404941 ],\n       [0.6678845 ],\n       [0.98305696],\n       [0.9746391 ],\n       [0.923527  ],\n       [0.08624077]], dtype=float32)</pre> In\u00a0[87]: Copied! <pre># Convert prediction probabilities to labels\nmodel_6_preds = tf.squeeze(tf.round(model_6_pred_probs))\nmodel_6_preds[:10]\n</pre> # Convert prediction probabilities to labels model_6_preds = tf.squeeze(tf.round(model_6_pred_probs)) model_6_preds[:10] Out[87]: <pre>&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 1., 1., 1., 1., 1., 0.], dtype=float32)&gt;</pre> In\u00a0[88]: Copied! <pre># Calculate model 6 performance metrics\nmodel_6_results = calculate_results(val_labels, model_6_preds)\nmodel_6_results\n</pre> # Calculate model 6 performance metrics model_6_results = calculate_results(val_labels, model_6_preds) model_6_results Out[88]: <pre>{'accuracy': 81.75853018372703,\n 'precision': 0.8206021490415145,\n 'recall': 0.8175853018372703,\n 'f1': 0.8158792847350168}</pre> In\u00a0[89]: Copied! <pre># Compare TF Hub model to baseline\ncompare_baseline_to_new_results(baseline_results, model_6_results)\n</pre> # Compare TF Hub model to baseline compare_baseline_to_new_results(baseline_results, model_6_results) <pre>Baseline accuracy: 79.27, New accuracy: 81.76, Difference: 2.49\nBaseline precision: 0.81, New precision: 0.82, Difference: 0.01\nBaseline recall: 0.79, New recall: 0.82, Difference: 0.02\nBaseline f1: 0.79, New f1: 0.82, Difference: 0.03\n</pre> In\u00a0[90]: Copied! <pre>### NOTE: Making splits like this will lead to data leakage ###\n### (some of the training examples in the validation set) ###\n\n### WRONG WAY TO MAKE SPLITS (train_df_shuffled has already been split) ### \n\n# # Create subsets of 10% of the training data\n# train_10_percent = train_df_shuffled[[\"text\", \"target\"]].sample(frac=0.1, random_state=42)\n# train_sentences_10_percent = train_10_percent[\"text\"].to_list()\n# train_labels_10_percent = train_10_percent[\"target\"].to_list()\n# len(train_sentences_10_percent), len(train_labels_10_percent)\n</pre> ### NOTE: Making splits like this will lead to data leakage ### ### (some of the training examples in the validation set) ###  ### WRONG WAY TO MAKE SPLITS (train_df_shuffled has already been split) ###   # # Create subsets of 10% of the training data # train_10_percent = train_df_shuffled[[\"text\", \"target\"]].sample(frac=0.1, random_state=42) # train_sentences_10_percent = train_10_percent[\"text\"].to_list() # train_labels_10_percent = train_10_percent[\"target\"].to_list() # len(train_sentences_10_percent), len(train_labels_10_percent) In\u00a0[91]: Copied! <pre># One kind of correct way (there are more) to make data subset\n# (split the already split train_sentences/train_labels)\ntrain_sentences_90_percent, train_sentences_10_percent, train_labels_90_percent, train_labels_10_percent = train_test_split(np.array(train_sentences),\n                                                                                                                            train_labels,\n                                                                                                                            test_size=0.1,\n                                                                                                                            random_state=42)\n</pre> # One kind of correct way (there are more) to make data subset # (split the already split train_sentences/train_labels) train_sentences_90_percent, train_sentences_10_percent, train_labels_90_percent, train_labels_10_percent = train_test_split(np.array(train_sentences),                                                                                                                             train_labels,                                                                                                                             test_size=0.1,                                                                                                                             random_state=42)  In\u00a0[92]: Copied! <pre># Check length of 10 percent datasets\nprint(f\"Total training examples: {len(train_sentences)}\")\nprint(f\"Length of 10% training examples: {len(train_sentences_10_percent)}\")\n</pre> # Check length of 10 percent datasets print(f\"Total training examples: {len(train_sentences)}\") print(f\"Length of 10% training examples: {len(train_sentences_10_percent)}\") <pre>Total training examples: 6851\nLength of 10% training examples: 686\n</pre> <p>Because we've selected a random subset of the training samples, the classes should be roughly balanced (as they are in the full training dataset).</p> In\u00a0[93]: Copied! <pre># Check the number of targets in our subset of data \n# (this should be close to the distribution of labels in the original train_labels)\npd.Series(train_labels_10_percent).value_counts()\n</pre> # Check the number of targets in our subset of data  # (this should be close to the distribution of labels in the original train_labels) pd.Series(train_labels_10_percent).value_counts() Out[93]: <pre>0    415\n1    271\ndtype: int64</pre> <p>To make sure we're making an appropriate comparison between our model's ability to learn from the full training set and 10% subset, we'll clone our USE model (<code>model_6</code>) using the <code>tf.keras.models.clone_model()</code> method.</p> <p>Doing this will create the same architecture but reset the learned weights of the clone target (pretrained weights from the USE will remain but all others will be reset).</p> In\u00a0[94]: Copied! <pre># Clone model_6 but reset weights\nmodel_7 = tf.keras.models.clone_model(model_6)\n\n# Compile model\nmodel_7.compile(loss=\"binary_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])\n\n# Get a summary (will be same as model_6)\nmodel_7.summary()\n</pre> # Clone model_6 but reset weights model_7 = tf.keras.models.clone_model(model_6)  # Compile model model_7.compile(loss=\"binary_crossentropy\",                 optimizer=tf.keras.optimizers.Adam(),                 metrics=[\"accuracy\"])  # Get a summary (will be same as model_6) model_7.summary() <pre>Model: \"model_6_USE\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n USE (KerasLayer)            (None, 512)               256797824 \n                                                                 \n dense_5 (Dense)             (None, 64)                32832     \n                                                                 \n dense_6 (Dense)             (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 256,830,721\nTrainable params: 32,897\nNon-trainable params: 256,797,824\n_________________________________________________________________\n</pre> <p>Notice the layout of <code>model_7</code> is the same as <code>model_6</code>. Now let's train the newly created model on our 10% training data subset.</p> In\u00a0[95]: Copied! <pre># Fit the model to 10% of the training data\nmodel_7_history = model_7.fit(x=train_sentences_10_percent,\n                              y=train_labels_10_percent,\n                              epochs=5,\n                              validation_data=(val_sentences, val_labels),\n                              callbacks=[create_tensorboard_callback(SAVE_DIR, \"10_percent_tf_hub_sentence_encoder\")])\n</pre> # Fit the model to 10% of the training data model_7_history = model_7.fit(x=train_sentences_10_percent,                               y=train_labels_10_percent,                               epochs=5,                               validation_data=(val_sentences, val_labels),                               callbacks=[create_tensorboard_callback(SAVE_DIR, \"10_percent_tf_hub_sentence_encoder\")]) <pre>Saving TensorBoard log files to: model_logs/10_percent_tf_hub_sentence_encoder/20230526-001758\nEpoch 1/5\n22/22 [==============================] - 4s 41ms/step - loss: 0.6671 - accuracy: 0.6997 - val_loss: 0.6443 - val_accuracy: 0.7415\nEpoch 2/5\n22/22 [==============================] - 0s 18ms/step - loss: 0.5895 - accuracy: 0.8309 - val_loss: 0.5846 - val_accuracy: 0.7467\nEpoch 3/5\n22/22 [==============================] - 0s 18ms/step - loss: 0.5116 - accuracy: 0.8382 - val_loss: 0.5336 - val_accuracy: 0.7677\nEpoch 4/5\n22/22 [==============================] - 0s 18ms/step - loss: 0.4492 - accuracy: 0.8411 - val_loss: 0.5040 - val_accuracy: 0.7703\nEpoch 5/5\n22/22 [==============================] - 0s 18ms/step - loss: 0.4080 - accuracy: 0.8469 - val_loss: 0.4880 - val_accuracy: 0.7703\n</pre> <p>Due to the smaller amount of training data, training happens even quicker than before.</p> <p>Let's evaluate our model's performance after learning on 10% of the training data.</p> In\u00a0[96]: Copied! <pre># Make predictions with the model trained on 10% of the data\nmodel_7_pred_probs = model_7.predict(val_sentences)\nmodel_7_pred_probs[:10]\n</pre> # Make predictions with the model trained on 10% of the data model_7_pred_probs = model_7.predict(val_sentences) model_7_pred_probs[:10] <pre>24/24 [==============================] - 1s 7ms/step\n</pre> Out[96]: <pre>array([[0.24178001],\n       [0.8116845 ],\n       [0.91511923],\n       [0.32094172],\n       [0.587357  ],\n       [0.82938445],\n       [0.8401675 ],\n       [0.8496708 ],\n       [0.8371127 ],\n       [0.14010696]], dtype=float32)</pre> In\u00a0[97]: Copied! <pre># Convert prediction probabilities to labels\nmodel_7_preds = tf.squeeze(tf.round(model_7_pred_probs))\nmodel_7_preds[:10]\n</pre> # Convert prediction probabilities to labels model_7_preds = tf.squeeze(tf.round(model_7_pred_probs)) model_7_preds[:10] Out[97]: <pre>&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 1., 1., 1., 1., 1., 0.], dtype=float32)&gt;</pre> In\u00a0[98]: Copied! <pre># Calculate model results\nmodel_7_results = calculate_results(val_labels, model_7_preds)\nmodel_7_results\n</pre> # Calculate model results model_7_results = calculate_results(val_labels, model_7_preds) model_7_results Out[98]: <pre>{'accuracy': 77.03412073490814,\n 'precision': 0.7760118694840564,\n 'recall': 0.7703412073490814,\n 'f1': 0.7665375100103654}</pre> In\u00a0[99]: Copied! <pre># Compare to baseline\ncompare_baseline_to_new_results(baseline_results, model_7_results)\n</pre> # Compare to baseline compare_baseline_to_new_results(baseline_results, model_7_results) <pre>Baseline accuracy: 79.27, New accuracy: 77.03, Difference: -2.23\nBaseline precision: 0.81, New precision: 0.78, Difference: -0.04\nBaseline recall: 0.79, New recall: 0.77, Difference: -0.02\nBaseline f1: 0.79, New f1: 0.77, Difference: -0.02\n</pre> In\u00a0[100]: Copied! <pre># Combine model results into a DataFrame\nall_model_results = pd.DataFrame({\"baseline\": baseline_results,\n                                  \"simple_dense\": model_1_results,\n                                  \"lstm\": model_2_results,\n                                  \"gru\": model_3_results,\n                                  \"bidirectional\": model_4_results,\n                                  \"conv1d\": model_5_results,\n                                  \"tf_hub_sentence_encoder\": model_6_results,\n                                  \"tf_hub_10_percent_data\": model_7_results})\nall_model_results = all_model_results.transpose()\nall_model_results\n</pre> # Combine model results into a DataFrame all_model_results = pd.DataFrame({\"baseline\": baseline_results,                                   \"simple_dense\": model_1_results,                                   \"lstm\": model_2_results,                                   \"gru\": model_3_results,                                   \"bidirectional\": model_4_results,                                   \"conv1d\": model_5_results,                                   \"tf_hub_sentence_encoder\": model_6_results,                                   \"tf_hub_10_percent_data\": model_7_results}) all_model_results = all_model_results.transpose() all_model_results Out[100]: accuracy precision recall f1 baseline 79.265092 0.811139 0.792651 0.786219 simple_dense 78.608924 0.790328 0.786089 0.783297 lstm 75.590551 0.756716 0.755906 0.753960 gru 77.559055 0.776327 0.775591 0.774090 bidirectional 76.771654 0.767545 0.767717 0.766793 conv1d 78.740157 0.790061 0.787402 0.785228 tf_hub_sentence_encoder 81.758530 0.820602 0.817585 0.815879 tf_hub_10_percent_data 77.034121 0.776012 0.770341 0.766538 In\u00a0[101]: Copied! <pre># Reduce the accuracy to same scale as other metrics\nall_model_results[\"accuracy\"] = all_model_results[\"accuracy\"]/100\n</pre> # Reduce the accuracy to same scale as other metrics all_model_results[\"accuracy\"] = all_model_results[\"accuracy\"]/100 In\u00a0[102]: Copied! <pre># Plot and compare all of the model results\nall_model_results.plot(kind=\"bar\", figsize=(10, 7)).legend(bbox_to_anchor=(1.0, 1.0));\n</pre> # Plot and compare all of the model results all_model_results.plot(kind=\"bar\", figsize=(10, 7)).legend(bbox_to_anchor=(1.0, 1.0)); <p>Looks like our pretrained USE TensorFlow Hub models have the best performance, even the one with only 10% of the training data seems to outperform the other models. This goes to show the power of transfer learning.</p> <p>How about we drill down and get the F1-score's of each model?</p> In\u00a0[103]: Copied! <pre># Sort model results by f1-score\nall_model_results.sort_values(\"f1\", ascending=False)[\"f1\"].plot(kind=\"bar\", figsize=(10, 7));\n</pre> # Sort model results by f1-score all_model_results.sort_values(\"f1\", ascending=False)[\"f1\"].plot(kind=\"bar\", figsize=(10, 7)); <p>Drilling down into a single metric we see our USE TensorFlow Hub models performing  better than all of the other models. Interestingly, the baseline's F1-score isn't too far off the rest of the deeper models.</p> <p>We can also visualize all of our model's training logs using TensorBoard.dev.</p> In\u00a0[104]: Copied! <pre># # View tensorboard logs of transfer learning modelling experiments (should be 4 models)\n# # Upload TensorBoard dev records\n# !tensorboard dev upload --logdir ./model_logs \\\n#   --name \"NLP modelling experiments\" \\\n#   --description \"A series of different NLP modellings experiments with various models\" \\\n#   --one_shot # exits the uploader when upload has finished\n</pre> # # View tensorboard logs of transfer learning modelling experiments (should be 4 models) # # Upload TensorBoard dev records # !tensorboard dev upload --logdir ./model_logs \\ #   --name \"NLP modelling experiments\" \\ #   --description \"A series of different NLP modellings experiments with various models\" \\ #   --one_shot # exits the uploader when upload has finished <p>The TensorBoard logs of the different modelling experiments we ran can be viewed here: https://tensorboard.dev/experiment/LkoAakb7QIKBZ0RL97cXbw/</p> In\u00a0[105]: Copied! <pre># If you need to remove previous experiments, you can do so using the following command\n# !tensorboard dev delete --experiment_id EXPERIMENT_ID_TO_DELETE\n</pre> # If you need to remove previous experiments, you can do so using the following command # !tensorboard dev delete --experiment_id EXPERIMENT_ID_TO_DELETE In\u00a0[106]: Copied! <pre># Get mean pred probs for 3 models\nbaseline_pred_probs = np.max(model_0.predict_proba(val_sentences), axis=1) # get the prediction probabilities from baseline model\ncombined_pred_probs = baseline_pred_probs + tf.squeeze(model_2_pred_probs, axis=1) + tf.squeeze(model_6_pred_probs)\ncombined_preds = tf.round(combined_pred_probs/3) # average and round the prediction probabilities to get prediction classes\ncombined_preds[:20]\n</pre> # Get mean pred probs for 3 models baseline_pred_probs = np.max(model_0.predict_proba(val_sentences), axis=1) # get the prediction probabilities from baseline model combined_pred_probs = baseline_pred_probs + tf.squeeze(model_2_pred_probs, axis=1) + tf.squeeze(model_6_pred_probs) combined_preds = tf.round(combined_pred_probs/3) # average and round the prediction probabilities to get prediction classes combined_preds[:20] Out[106]: <pre>&lt;tf.Tensor: shape=(20,), dtype=float32, numpy=\narray([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.,\n       0., 0., 1.], dtype=float32)&gt;</pre> <p>Wonderful! We've got a combined predictions array of different classes, let's evaluate them against the true labels and add our stacked model's results to our <code>all_model_results</code> DataFrame.</p> In\u00a0[107]: Copied! <pre># Calculate results from averaging the prediction probabilities\nensemble_results = calculate_results(val_labels, combined_preds)\nensemble_results\n</pre> # Calculate results from averaging the prediction probabilities ensemble_results = calculate_results(val_labels, combined_preds) ensemble_results Out[107]: <pre>{'accuracy': 77.95275590551181,\n 'precision': 0.7792442137914578,\n 'recall': 0.7795275590551181,\n 'f1': 0.7789463852322546}</pre> In\u00a0[108]: Copied! <pre># Add our combined model's results to the results DataFrame\nall_model_results.loc[\"ensemble_results\"] = ensemble_results\n</pre> # Add our combined model's results to the results DataFrame all_model_results.loc[\"ensemble_results\"] = ensemble_results In\u00a0[109]: Copied! <pre># Convert the accuracy to the same scale as the rest of the results\nall_model_results.loc[\"ensemble_results\"][\"accuracy\"] = all_model_results.loc[\"ensemble_results\"][\"accuracy\"]/100\n</pre> # Convert the accuracy to the same scale as the rest of the results all_model_results.loc[\"ensemble_results\"][\"accuracy\"] = all_model_results.loc[\"ensemble_results\"][\"accuracy\"]/100 In\u00a0[110]: Copied! <pre>all_model_results\n</pre> all_model_results Out[110]: accuracy precision recall f1 baseline 0.792651 0.811139 0.792651 0.786219 simple_dense 0.786089 0.790328 0.786089 0.783297 lstm 0.755906 0.756716 0.755906 0.753960 gru 0.775591 0.776327 0.775591 0.774090 bidirectional 0.767717 0.767545 0.767717 0.766793 conv1d 0.787402 0.790061 0.787402 0.785228 tf_hub_sentence_encoder 0.817585 0.820602 0.817585 0.815879 tf_hub_10_percent_data 0.770341 0.776012 0.770341 0.766538 ensemble_results 0.779528 0.779244 0.779528 0.778946 <p>How did the stacked model go against the other models?</p> <p>\ud83d\udd11 Note: It seems many of our model's results are similar. This may mean there are some limitations to what can be learned from our data. When many of your modelling experiments return similar results, it's a good idea to revisit your data, we'll do this shortly.</p> In\u00a0[111]: Copied! <pre># Save TF Hub Sentence Encoder model to HDF5 format\nmodel_6.save(\"model_6.h5\")\n</pre> # Save TF Hub Sentence Encoder model to HDF5 format model_6.save(\"model_6.h5\") <p>If you save a model as a <code>HDF5</code>, when loading it back in, you need to let TensorFlow know about any custom objects you've used (e.g. components which aren't built from pure TensorFlow, such as TensorFlow Hub components).</p> In\u00a0[112]: Copied! <pre># Load model with custom Hub Layer (required with HDF5 format)\nloaded_model_6 = tf.keras.models.load_model(\"model_6.h5\", \n                                            custom_objects={\"KerasLayer\": hub.KerasLayer})\n</pre> # Load model with custom Hub Layer (required with HDF5 format) loaded_model_6 = tf.keras.models.load_model(\"model_6.h5\",                                              custom_objects={\"KerasLayer\": hub.KerasLayer}) In\u00a0[113]: Copied! <pre># How does our loaded model perform?\nloaded_model_6.evaluate(val_sentences, val_labels)\n</pre> # How does our loaded model perform? loaded_model_6.evaluate(val_sentences, val_labels) <pre>24/24 [==============================] - 1s 7ms/step - loss: 0.4299 - accuracy: 0.8176\n</pre> Out[113]: <pre>[0.4298648238182068, 0.817585289478302]</pre> <p>Calling the <code>save()</code> method on our target model and passing it a filepath allows us to save our model in the <code>SavedModel</code> format.</p> In\u00a0[114]: Copied! <pre># Save TF Hub Sentence Encoder model to SavedModel format (default)\nmodel_6.save(\"model_6_SavedModel_format\")\n</pre> # Save TF Hub Sentence Encoder model to SavedModel format (default) model_6.save(\"model_6_SavedModel_format\") <pre>WARNING:absl:Function `_wrapped_model` contains input name(s) USE_input with unsupported characters which will be renamed to use_input in the SavedModel.\n</pre> <p>If you use SavedModel format (default), you can reload your model without specifying custom objects using the <code>tensorflow.keras.models.load_model()</code> function.</p> In\u00a0[115]: Copied! <pre># Load TF Hub Sentence Encoder SavedModel\nloaded_model_6_SavedModel = tf.keras.models.load_model(\"model_6_SavedModel_format\")\n</pre> # Load TF Hub Sentence Encoder SavedModel loaded_model_6_SavedModel = tf.keras.models.load_model(\"model_6_SavedModel_format\") In\u00a0[116]: Copied! <pre># Evaluate loaded SavedModel format\nloaded_model_6_SavedModel.evaluate(val_sentences, val_labels)\n</pre> # Evaluate loaded SavedModel format loaded_model_6_SavedModel.evaluate(val_sentences, val_labels) <pre>24/24 [==============================] - 1s 8ms/step - loss: 0.4299 - accuracy: 0.8176\n</pre> Out[116]: <pre>[0.4298648238182068, 0.817585289478302]</pre> <p>As you can see saving and loading our model with either format results in the same performance.</p> <p>\ud83e\udd14 Question: Should you used the <code>SavedModel</code> format or <code>HDF5</code> format?</p> <p>For most use cases, the <code>SavedModel</code> format will suffice. However, this is a TensorFlow specific standard. If you need a more general-purpose data standard, <code>HDF5</code> might be better. For more, check out the TensorFlow documentation on saving and loading models.</p> In\u00a0[117]: Copied! <pre># Create dataframe with validation sentences and best performing model predictions\nval_df = pd.DataFrame({\"text\": val_sentences,\n                       \"target\": val_labels,\n                       \"pred\": model_6_preds,\n                       \"pred_prob\": tf.squeeze(model_6_pred_probs)})\nval_df.head()\n</pre> # Create dataframe with validation sentences and best performing model predictions val_df = pd.DataFrame({\"text\": val_sentences,                        \"target\": val_labels,                        \"pred\": model_6_preds,                        \"pred_prob\": tf.squeeze(model_6_pred_probs)}) val_df.head() Out[117]: text target pred pred_prob 0 DFR EP016 Monthly Meltdown - On Dnbheaven 2015... 0 0.0 0.148141 1 FedEx no longer to transport bioterror germs i... 0 1.0 0.740579 2 Gunmen kill four in El Salvador bus attack: Su... 1 1.0 0.988647 3 @camilacabello97 Internally and externally scr... 1 0.0 0.224560 4 Radiation emergency #preparedness starts with ... 1 1.0 0.740494 <p>Oh yeah! Now let's find our model's wrong predictions (where <code>target != pred</code>) and sort them by their prediction probability (the <code>pred_prob</code> column).</p> In\u00a0[118]: Copied! <pre># Find the wrong predictions and sort by prediction probabilities\nmost_wrong = val_df[val_df[\"target\"] != val_df[\"pred\"]].sort_values(\"pred_prob\", ascending=False)\nmost_wrong[:10]\n</pre> # Find the wrong predictions and sort by prediction probabilities most_wrong = val_df[val_df[\"target\"] != val_df[\"pred\"]].sort_values(\"pred_prob\", ascending=False) most_wrong[:10] Out[118]: text target pred pred_prob 31 ? High Skies - Burning Buildings ? http://t.co... 0 1.0 0.906832 628 @noah_anyname That's where the concentration c... 0 1.0 0.866348 759 FedEx will no longer transport bioterror patho... 0 1.0 0.859502 393 @SonofLiberty357 all illuminated by the bright... 0 1.0 0.855963 49 @madonnamking RSPCA site multiple 7 story high... 0 1.0 0.839930 209 Ashes 2015: Australia\u0089\u00db\u00aas collapse at Trent Br... 0 1.0 0.815515 251 @AshGhebranious civil rights continued in the ... 0 1.0 0.807973 109 [55436] 1950 LIONEL TRAINS SMOKE LOCOMOTIVES W... 0 1.0 0.806746 698 \u00e5\u00c8MGN-AFRICA\u00e5\u00a8 pin:263789F4 \u00e5\u00c8 Correction: Ten... 0 1.0 0.782425 695 A look at state actions a year after Ferguson'... 0 1.0 0.759534 <p>Finally, we can write some code to visualize the sample text, truth label, prediction class and prediction probability. Because we've sorted our samples by prediction probability, viewing samples from the head of our <code>most_wrong</code> DataFrame will show us false positives.</p> <p>A reminder:</p> <ul> <li><code>0</code> = Not a real diaster Tweet</li> <li><code>1</code> = Real diaster Tweet</li> </ul> In\u00a0[119]: Copied! <pre># Check the false positives (model predicted 1 when should've been 0)\nfor row in most_wrong[:10].itertuples(): # loop through the top 10 rows (change the index to view different rows)\n  _, text, target, pred, prob = row\n  print(f\"Target: {target}, Pred: {int(pred)}, Prob: {prob}\")\n  print(f\"Text:\\n{text}\\n\")\n  print(\"----\\n\")\n</pre> # Check the false positives (model predicted 1 when should've been 0) for row in most_wrong[:10].itertuples(): # loop through the top 10 rows (change the index to view different rows)   _, text, target, pred, prob = row   print(f\"Target: {target}, Pred: {int(pred)}, Prob: {prob}\")   print(f\"Text:\\n{text}\\n\")   print(\"----\\n\") <pre>Target: 0, Pred: 1, Prob: 0.9068315625190735\nText:\n? High Skies - Burning Buildings ? http://t.co/uVq41i3Kx2 #nowplaying\n\n----\n\nTarget: 0, Pred: 1, Prob: 0.8663479685783386\nText:\n@noah_anyname That's where the concentration camps and mass murder come in. \n \nEVERY. FUCKING. TIME.\n\n----\n\nTarget: 0, Pred: 1, Prob: 0.859502375125885\nText:\nFedEx will no longer transport bioterror pathogens in wake of anthrax lab mishaps http://t.co/lHpgxc4b8J\n\n----\n\nTarget: 0, Pred: 1, Prob: 0.8559632897377014\nText:\n@SonofLiberty357 all illuminated by the brightly burning buildings all around the town!\n\n----\n\nTarget: 0, Pred: 1, Prob: 0.8399295806884766\nText:\n@madonnamking RSPCA site multiple 7 story high rise buildings next to low density character residential in an area that floods\n\n----\n\nTarget: 0, Pred: 1, Prob: 0.8155148029327393\nText:\nAshes 2015: Australia\u0089\u00db\u00aas collapse at Trent Bridge among worst in history: England bundled out Australia for 60 ... http://t.co/t5TrhjUAU0\n\n----\n\nTarget: 0, Pred: 1, Prob: 0.8079732060432434\nText:\n@AshGhebranious civil rights continued in the 60s. And what about trans-generational trauma? if anything we should listen to the Americans.\n\n----\n\nTarget: 0, Pred: 1, Prob: 0.8067457675933838\nText:\n[55436] 1950 LIONEL TRAINS SMOKE LOCOMOTIVES WITH MAGNE-TRACTION INSTRUCTIONS http://t.co/xEZBs3sq0y http://t.co/C2x0QoKGlY\n\n----\n\nTarget: 0, Pred: 1, Prob: 0.7824245095252991\nText:\n\u00e5\u00c8MGN-AFRICA\u00e5\u00a8 pin:263789F4 \u00e5\u00c8 Correction: Tent Collapse Story: Correction: Tent Collapse story \u00e5\u00c8 http://t.co/fDJUYvZMrv @wizkidayo\n\n----\n\nTarget: 0, Pred: 1, Prob: 0.7595335841178894\nText:\nA look at state actions a year after Ferguson's upheaval http://t.co/GZEkQWzijq\n\n----\n\n</pre> <p>We can view the bottom end of our <code>most_wrong</code> DataFrame to inspect false negatives (model predicts 0, not a real diaster Tweet, when it should've predicted 1, real diaster Tweet).</p> In\u00a0[120]: Copied! <pre># Check the most wrong false negatives (model predicted 0 when should've predict 1)\nfor row in most_wrong[-10:].itertuples():\n  _, text, target, pred, prob = row\n  print(f\"Target: {target}, Pred: {int(pred)}, Prob: {prob}\")\n  print(f\"Text:\\n{text}\\n\")\n  print(\"----\\n\")\n</pre> # Check the most wrong false negatives (model predicted 0 when should've predict 1) for row in most_wrong[-10:].itertuples():   _, text, target, pred, prob = row   print(f\"Target: {target}, Pred: {int(pred)}, Prob: {prob}\")   print(f\"Text:\\n{text}\\n\")   print(\"----\\n\") <pre>Target: 1, Pred: 0, Prob: 0.06247330829501152\nText:\ngoing to redo my nails and watch behind the scenes of desolation of smaug ayyy\n\n----\n\nTarget: 1, Pred: 0, Prob: 0.05949299782514572\nText:\n@BoyInAHorsemask its a panda trapped in a dogs body\n\n----\n\nTarget: 1, Pred: 0, Prob: 0.056083984673023224\nText:\n@willienelson We need help! Horses will die!Please RT &amp;amp; sign petition!Take a stand &amp;amp; be a voice for them! #gilbert23 https://t.co/e8dl1lNCVu\n\n----\n\nTarget: 1, Pred: 0, Prob: 0.055036477744579315\nText:\nLucas Duda is Ghost Rider. Not the Nic Cage version but an actual 'engulfed in flames' badass. #Mets\n\n----\n\nTarget: 1, Pred: 0, Prob: 0.054454777389764786\nText:\nYou can never escape me. Bullets don't harm me. Nothing harms me. But I know pain. I know pain. Sometimes I share it. With someone like you.\n\n----\n\nTarget: 1, Pred: 0, Prob: 0.046157706528902054\nText:\nI get to smoke my shit in peace\n\n----\n\nTarget: 1, Pred: 0, Prob: 0.03960023820400238\nText:\nWhy are you deluged with low self-image? Take the quiz: http://t.co/XsPqdOrIqj http://t.co/CQYvFR4UCy\n\n----\n\nTarget: 1, Pred: 0, Prob: 0.03830057382583618\nText:\nRon &amp;amp; Fez - Dave's High School Crush https://t.co/aN3W16c8F6 via @YouTube\n\n----\n\nTarget: 1, Pred: 0, Prob: 0.03802212327718735\nText:\n@SoonerMagic_ I mean I'm a fan but I don't need a girl sounding off like a damn siren\n\n----\n\nTarget: 1, Pred: 0, Prob: 0.03466600552201271\nText:\nReddit Will Now Quarantine\u0089\u00db_ http://t.co/pkUAMXw6pm #onlinecommunities #reddit #amageddon #freespeech #Business http://t.co/PAWvNJ4sAP\n\n----\n\n</pre> <p>Do you notice anything interesting about the most wrong samples?</p> <p>Are the ground truth labels correct? What do you think would happen if we went back and corrected the labels which aren't?</p> In\u00a0[121]: Copied! <pre># Making predictions on the test dataset\ntest_sentences = test_df[\"text\"].to_list()\ntest_samples = random.sample(test_sentences, 10)\nfor test_sample in test_samples:\n  pred_prob = tf.squeeze(model_6.predict([test_sample])) # has to be list\n  pred = tf.round(pred_prob)\n  print(f\"Pred: {int(pred)}, Prob: {pred_prob}\")\n  print(f\"Text:\\n{test_sample}\\n\")\n  print(\"----\\n\")\n</pre> # Making predictions on the test dataset test_sentences = test_df[\"text\"].to_list() test_samples = random.sample(test_sentences, 10) for test_sample in test_samples:   pred_prob = tf.squeeze(model_6.predict([test_sample])) # has to be list   pred = tf.round(pred_prob)   print(f\"Pred: {int(pred)}, Prob: {pred_prob}\")   print(f\"Text:\\n{test_sample}\\n\")   print(\"----\\n\") <pre>1/1 [==============================] - 0s 79ms/step\nPred: 0, Prob: 0.05416637659072876\nText:\nWHAT a day's cricket that was. Has destroyed any plans I had for exercise today.\n\n----\n\n1/1 [==============================] - 0s 39ms/step\nPred: 1, Prob: 0.5330829620361328\nText:\nAny other generation this would've been fatality  http://t.co/zcCtZM9f0o\n\n----\n\n1/1 [==============================] - 0s 40ms/step\nPred: 1, Prob: 0.9940084218978882\nText:\nArson suspect linked to 30 fires caught in Northern California http://t.co/HkFPyNb4PS\n\n----\n\n1/1 [==============================] - 0s 40ms/step\nPred: 1, Prob: 0.9726524353027344\nText:\nHelp support the victims of the Japanese Earthquake and Pacific Tsunami http://t.co/O5GbPBQH http://t.co/MN5wnxf0 #hope4japan #pray4japan\n\n----\n\n1/1 [==============================] - 0s 40ms/step\nPred: 0, Prob: 0.36651405692100525\nText:\nthis is from my show last night and im still panicking over the fact i saw sweaty ashton with my own two eyes http://t.co/yyJ76WBC9y\n\n----\n\n1/1 [==============================] - 0s 41ms/step\nPred: 0, Prob: 0.42949625849723816\nText:\nHe came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\n\n----\n\n1/1 [==============================] - 0s 41ms/step\nPred: 1, Prob: 0.7450974583625793\nText:\nJane Kelsey on the FIRE Economy\n5th Aug 5:30\u0089\u00db\u00d27:30pm\nOld Govt Buildings Wgton\nThe context &amp;amp; the driver for #TPP and #TRADEinSERVICESAgreement\n\n----\n\n1/1 [==============================] - 0s 41ms/step\nPred: 0, Prob: 0.13024944067001343\nText:\nDetonation fashionable mountaineering electronic watch water-resistant couples leisure tab\u0089\u00db_ http://t.co/GH48B54riS http://t.co/2PqTm06Lid\n\n----\n\n1/1 [==============================] - 0s 41ms/step\nPred: 1, Prob: 0.8552481532096863\nText:\n@AlbertBrooks Don't like the Ayatollah Khomeini Memorial Nuclear Reactor for the Annihilation of Israel? Racist!\n\n----\n\n1/1 [==============================] - 0s 40ms/step\nPred: 0, Prob: 0.06269928067922592\nText:\nCan you imagine how traumatised Makoto would be if he could see himself in the dub (aka Jersey Shore AU) rn? Well done America\n\n----\n\n</pre> <p>How do our model's predictions look on the test dataset?</p> <p>It's important to do these kind of visualization checks as often as possible to get a glance of how your model performs on unseen data and subsequently how it might perform on the real test: Tweets from the wild.</p> In\u00a0[122]: Copied! <pre># Turn Tweet into string\ndaniels_tweet = \"Life like an ensemble: take the best choices from others and make your own\"\n</pre> # Turn Tweet into string daniels_tweet = \"Life like an ensemble: take the best choices from others and make your own\" <p>Now we'll write a small function to take a model and an example sentence and return a prediction.</p> In\u00a0[123]: Copied! <pre>def predict_on_sentence(model, sentence):\n  \"\"\"\n  Uses model to make a prediction on sentence.\n\n  Returns the sentence, the predicted label and the prediction probability.\n  \"\"\"\n  pred_prob = model.predict([sentence])\n  pred_label = tf.squeeze(tf.round(pred_prob)).numpy()\n  print(f\"Pred: {pred_label}\", \"(real disaster)\" if pred_label &gt; 0 else \"(not real disaster)\", f\"Prob: {pred_prob[0][0]}\")\n  print(f\"Text:\\n{sentence}\")\n</pre> def predict_on_sentence(model, sentence):   \"\"\"   Uses model to make a prediction on sentence.    Returns the sentence, the predicted label and the prediction probability.   \"\"\"   pred_prob = model.predict([sentence])   pred_label = tf.squeeze(tf.round(pred_prob)).numpy()   print(f\"Pred: {pred_label}\", \"(real disaster)\" if pred_label &gt; 0 else \"(not real disaster)\", f\"Prob: {pred_prob[0][0]}\")   print(f\"Text:\\n{sentence}\") <p>Great! Time to test our model out.</p> In\u00a0[124]: Copied! <pre># Make a prediction on Tweet from the wild\npredict_on_sentence(model=model_6, # use the USE model\n                    sentence=daniels_tweet)\n</pre> # Make a prediction on Tweet from the wild predict_on_sentence(model=model_6, # use the USE model                     sentence=daniels_tweet) <pre>1/1 [==============================] - 0s 39ms/step\nPred: 0.0 (not real disaster) Prob: 0.044768452644348145\nText:\nLife like an ensemble: take the best choices from others and make your own\n</pre> <p>Woohoo! Our model predicted correctly. My Tweet wasn't about a diaster.</p> <p>How about we find a few Tweets about actual diasters?</p> <p>Such as the following two Tweets about the 2020 Beirut explosions.</p> In\u00a0[125]: Copied! <pre># Source - https://twitter.com/BeirutCityGuide/status/1290696551376007168\nbeirut_tweet_1 = \"Reports that the smoke in Beirut sky contains nitric acid, which is toxic. Please share and refrain from stepping outside unless urgent. #Lebanon\"\n\n# Source - https://twitter.com/BeirutCityGuide/status/1290773498743476224\nbeirut_tweet_2 = \"#Beirut declared a \u201cdevastated city\u201d, two-week state of emergency officially declared. #Lebanon\"\n</pre> # Source - https://twitter.com/BeirutCityGuide/status/1290696551376007168 beirut_tweet_1 = \"Reports that the smoke in Beirut sky contains nitric acid, which is toxic. Please share and refrain from stepping outside unless urgent. #Lebanon\"  # Source - https://twitter.com/BeirutCityGuide/status/1290773498743476224 beirut_tweet_2 = \"#Beirut declared a \u201cdevastated city\u201d, two-week state of emergency officially declared. #Lebanon\" In\u00a0[126]: Copied! <pre># Predict on diaster Tweet 1\npredict_on_sentence(model=model_6, \n                    sentence=beirut_tweet_1)\n</pre> # Predict on diaster Tweet 1 predict_on_sentence(model=model_6,                      sentence=beirut_tweet_1) <pre>1/1 [==============================] - 0s 42ms/step\nPred: 1.0 (real disaster) Prob: 0.9650391936302185\nText:\nReports that the smoke in Beirut sky contains nitric acid, which is toxic. Please share and refrain from stepping outside unless urgent. #Lebanon\n</pre> In\u00a0[127]: Copied! <pre># Predict on diaster Tweet 2\npredict_on_sentence(model=model_6, \n                    sentence=beirut_tweet_2)\n</pre> # Predict on diaster Tweet 2 predict_on_sentence(model=model_6,                      sentence=beirut_tweet_2) <pre>1/1 [==============================] - 0s 40ms/step\nPred: 1.0 (real disaster) Prob: 0.9686568379402161\nText:\n#Beirut declared a \u201cdevastated city\u201d, two-week state of emergency officially declared. #Lebanon\n</pre> <p>Looks like our model is performing as expected, predicting both of the diaster Tweets as actual diasters.</p> <p>\ud83d\udd11 Note: The above examples are cherry-picked and are cases where you'd expect a model to function at high performance. For actual production systems, you'll want to continaully perform tests to see how your model is performing.</p> In\u00a0[128]: Copied! <pre># Calculate the time of predictions\nimport time\ndef pred_timer(model, samples):\n  \"\"\"\n  Times how long a model takes to make predictions on samples.\n  \n  Args:\n  ----\n  model = a trained model\n  sample = a list of samples\n\n  Returns:\n  ----\n  total_time = total elapsed time for model to make predictions on samples\n  time_per_pred = time in seconds per single sample\n  \"\"\"\n  start_time = time.perf_counter() # get start time\n  model.predict(samples) # make predictions\n  end_time = time.perf_counter() # get finish time\n  total_time = end_time-start_time # calculate how long predictions took to make\n  time_per_pred = total_time/len(val_sentences) # find prediction time per sample\n  return total_time, time_per_pred\n</pre> # Calculate the time of predictions import time def pred_timer(model, samples):   \"\"\"   Times how long a model takes to make predictions on samples.      Args:   ----   model = a trained model   sample = a list of samples    Returns:   ----   total_time = total elapsed time for model to make predictions on samples   time_per_pred = time in seconds per single sample   \"\"\"   start_time = time.perf_counter() # get start time   model.predict(samples) # make predictions   end_time = time.perf_counter() # get finish time   total_time = end_time-start_time # calculate how long predictions took to make   time_per_pred = total_time/len(val_sentences) # find prediction time per sample   return total_time, time_per_pred <p>Looking good!</p> <p>Now let's use our <code>pred_timer()</code> function to evaluate the prediction times of our best performing model (<code>model_6</code>) and our baseline model (<code>model_0</code>).</p> In\u00a0[129]: Copied! <pre># Calculate TF Hub Sentence Encoder prediction times\nmodel_6_total_pred_time, model_6_time_per_pred = pred_timer(model_6, val_sentences)\nmodel_6_total_pred_time, model_6_time_per_pred\n</pre> # Calculate TF Hub Sentence Encoder prediction times model_6_total_pred_time, model_6_time_per_pred = pred_timer(model_6, val_sentences) model_6_total_pred_time, model_6_time_per_pred <pre>24/24 [==============================] - 0s 7ms/step\n</pre> Out[129]: <pre>(0.2243557769999711, 0.0002944301535432692)</pre> In\u00a0[130]: Copied! <pre># Calculate Naive Bayes prediction times\nbaseline_total_pred_time, baseline_time_per_pred = pred_timer(model_0, val_sentences)\nbaseline_total_pred_time, baseline_time_per_pred\n</pre> # Calculate Naive Bayes prediction times baseline_total_pred_time, baseline_time_per_pred = pred_timer(model_0, val_sentences) baseline_total_pred_time, baseline_time_per_pred Out[130]: <pre>(0.013254724999967493, 1.739465223092847e-05)</pre> <p>It seems with our current hardware (in my case, I'm using a Google Colab notebook) our best performing model takes over 10x the time to make predictions as our baseline model.</p> <p>Is that extra prediction time worth it?</p> <p>Let's compare time per prediction versus our model's F1-scores.</p> In\u00a0[131]: Copied! <pre>import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 7))\nplt.scatter(baseline_time_per_pred, baseline_results[\"f1\"], label=\"baseline\")\nplt.scatter(model_6_time_per_pred, model_6_results[\"f1\"], label=\"tf_hub_sentence_encoder\")\nplt.legend()\nplt.title(\"F1-score versus time per prediction\")\nplt.xlabel(\"Time per prediction\")\nplt.ylabel(\"F1-Score\");\n</pre> import matplotlib.pyplot as plt  plt.figure(figsize=(10, 7)) plt.scatter(baseline_time_per_pred, baseline_results[\"f1\"], label=\"baseline\") plt.scatter(model_6_time_per_pred, model_6_results[\"f1\"], label=\"tf_hub_sentence_encoder\") plt.legend() plt.title(\"F1-score versus time per prediction\") plt.xlabel(\"Time per prediction\") plt.ylabel(\"F1-Score\"); <p> Ideal position for speed and performance tradeoff model (fast predictions with great results).</p> <p>Of course, the ideal position for each of these dots is to be in the top left of the plot (low time per prediction, high F1-score).</p> <p>In our case, there's a clear tradeoff for time per prediction and performance. Our best performing model takes an order of magnitude longer per prediction but only results in a few F1-score point increase.</p> <p>This kind of tradeoff is something you'll need to keep in mind when incorporating machine learning models into your own applications.</p> <p>\ud83d\udcd6 Resource: See the full set of course materials on GitHub: https://github.com/mrdbourke/tensorflow-deep-learning</p>"},{"location":"08_introduction_to_nlp_in_tensorflow/#08-natural-language-processing-with-tensorflow","title":"08. Natural Language Processing with TensorFlow\u00b6","text":"<p> A handful of example natural language processing (NLP) and natural language understanding (NLU) problems. These are also often referred to as sequence problems (going from one sequence to another).</p> <p>The main goal of natural language processing (NLP) is to derive information from natural language.</p> <p>Natural language is a broad term but you can consider it to cover any of the following:</p> <ul> <li>Text (such as that contained in an email, blog post, book, Tweet)</li> <li>Speech (a conversation you have with a doctor, voice commands you give to a smart speaker)</li> </ul> <p>Under the umbrellas of text and speech there are many different things you might want to do.</p> <p>If you're building an email application, you might want to scan incoming emails to see if they're spam or not spam (classification).</p> <p>If you're trying to analyse customer feedback complaints, you might want to discover which section of your business they're for.</p> <p>\ud83d\udd11 Note: Both of these types of data are often referred to as sequences (a sentence is a sequence of words). So a common term you'll come across in NLP problems is called seq2seq, in other words, finding information in one sequence to produce another sequence (e.g. converting a speech command to a sequence of text-based steps).</p> <p>To get hands-on with NLP in TensorFlow, we're going to practice the steps we've used previously but this time with text data:</p> <pre><code>Text -&gt; turn into numbers -&gt; build a model -&gt; train the model to find patterns -&gt; use patterns (make predictions)\n</code></pre> <p>\ud83d\udcd6 Resource: For a great overview of NLP and the different problems within it, read the article A Simple Introduction to Natural Language Processing.</p>"},{"location":"08_introduction_to_nlp_in_tensorflow/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>Let's get specific hey?</p> <ul> <li>Downloading a text dataset</li> <li>Visualizing text data</li> <li>Converting text into numbers using tokenization</li> <li>Turning our tokenized text into an embedding</li> <li>Modelling a text dataset<ul> <li>Starting with a baseline (TF-IDF)</li> <li>Building several deep learning text models<ul> <li>Dense, LSTM, GRU, Conv1D, Transfer learning</li> </ul> </li> </ul> </li> <li>Comparing the performance of each our models</li> <li>Combining our models into an ensemble</li> <li>Saving and loading a trained model</li> <li>Find the most wrong predictions</li> </ul>"},{"location":"08_introduction_to_nlp_in_tensorflow/#how-you-should-approach-this-notebook","title":"How you should approach this notebook\u00b6","text":"<p>You can read through the descriptions and the code (it should all run, except for the cells which error on purpose), but there's a better option.</p> <p>Write all of the code yourself.</p> <p>Yes. I'm serious. Create a new notebook, and rewrite each line by yourself. Investigate it, see if you can break it, why does it break?</p> <p>You don't have to write the text descriptions but writing the code yourself is a great way to get hands-on experience.</p> <p>Don't worry if you make mistakes, we all do. The way to get better and make less mistakes is to write more code.</p> <p>\ud83d\udcd6 Resource: See the full set of course materials on GitHub: https://github.com/mrdbourke/tensorflow-deep-learning</p>"},{"location":"08_introduction_to_nlp_in_tensorflow/#check-for-gpu","title":"Check for GPU\u00b6","text":"<p>In order for our deep learning models to run as fast as possible, we'll need access to a GPU.</p> <p>In Google Colab, you can set this up by going to Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU.</p> <p>After selecting GPU, you may have to restart the runtime.</p>"},{"location":"08_introduction_to_nlp_in_tensorflow/#get-helper-functions","title":"Get helper functions\u00b6","text":"<p>In past modules, we've created a bunch of helper functions to do small tasks required for our notebooks.</p> <p>Rather than rewrite all of these, we can import a script and load them in from there.</p> <p>The script containing our helper functions can be found on GitHub.</p>"},{"location":"08_introduction_to_nlp_in_tensorflow/#download-a-text-dataset","title":"Download a text dataset\u00b6","text":"<p>Let's start by download a text dataset. We'll be using the Real or Not? dataset from Kaggle which contains text-based Tweets about natural disasters.</p> <p>The Real Tweets are actually about disasters, for example:</p> <pre><code>Jetstar and Virgin forced to cancel Bali flights again because of ash from Mount Raung volcano\n</code></pre> <p>The Not Real Tweets are Tweets not about disasters (they can be on anything), for example:</p> <pre><code>'Education is the most powerful weapon which you can use to change the world.' Nelson #Mandela #quote\n</code></pre> <p>For convenience, the dataset has been downloaded from Kaggle (doing this requires a Kaggle account) and uploaded as a downloadable zip file.</p> <p>\ud83d\udd11 Note: The original downloaded data has not been altered to how you would download it from Kaggle.</p>"},{"location":"08_introduction_to_nlp_in_tensorflow/#visualizing-a-text-dataset","title":"Visualizing a text dataset\u00b6","text":"<p>Once you've acquired a new dataset to work with, what should you do first?</p> <p>Explore it? Inspect it? Verify it? Become one with it?</p> <p>All correct.</p> <p>Remember the motto: visualize, visualize, visualize.</p> <p>Right now, our text data samples are in the form of <code>.csv</code> files. For an easy way to make them visual, let's turn them into pandas DataFrame's.</p> <p>\ud83d\udcd6 Reading: You might come across text datasets in many different formats. Aside from CSV files (what we're working with), you'll probably encounter <code>.txt</code> files and <code>.json</code> files too. For working with these type of files, I'd recommend reading the two following articles by RealPython:</p> <ul> <li>How to Read and Write Files in Python</li> <li>Working with JSON Data in Python</li> </ul>"},{"location":"08_introduction_to_nlp_in_tensorflow/#split-data-into-training-and-validation-sets","title":"Split data into training and validation sets\u00b6","text":"<p>Since the test set has no labels and we need a way to evalaute our trained models, we'll split off some of the training data and create a validation set.</p> <p>When our model trains (tries patterns in the Tweet samples), it'll only see data from the training set and we can see how it performs on unseen data using the validation set.</p> <p>We'll convert our splits from pandas Series datatypes to lists of strings (for the text) and lists of ints (for the labels) for ease of use later.</p> <p>To split our training dataset and create a validation dataset, we'll use Scikit-Learn's <code>train_test_split()</code> method and dedicate 10% of the training samples to the validation set.</p>"},{"location":"08_introduction_to_nlp_in_tensorflow/#converting-text-into-numbers","title":"Converting text into numbers\u00b6","text":"<p>Wonderful! We've got a training set and a validation set containing Tweets and labels.</p> <p>Our labels are in numerical form (<code>0</code> and <code>1</code>) but our Tweets are in string form.</p> <p>\ud83e\udd14 Question: What do you think we have to do before we can use a machine learning algorithm with our text data?</p> <p>If you answered something along the lines of \"turn it into numbers\", you're correct. A machine learning algorithm requires its inputs to be in numerical form.</p> <p>In NLP, there are two main concepts for turning text into numbers:</p> <ul> <li>Tokenization - A straight mapping from word or character or sub-word to a numerical value. There are three main levels of tokenization:<ol> <li>Using word-level tokenization with the sentence \"I love TensorFlow\" might result in \"I\" being <code>0</code>, \"love\" being <code>1</code> and \"TensorFlow\" being <code>2</code>. In this case, every word in a sequence considered a single token.</li> <li>Character-level tokenization, such as converting the letters A-Z to values <code>1-26</code>. In this case, every character in a sequence considered a single token.</li> <li>Sub-word tokenization is in between word-level and character-level tokenization. It involves breaking invidual words into smaller parts and then converting those smaller parts into numbers. For example, \"my favourite food is pineapple pizza\" might become \"my, fav, avour, rite, fo, oo, od, is, pin, ine, app, le, piz, za\". After doing this, these sub-words would then be mapped to a numerical value. In this case, every word could be considered multiple tokens.</li> </ol> </li> <li>Embeddings - An embedding is a representation of natural language which can be learned. Representation comes in the form of a feature vector. For example, the word \"dance\" could be represented by the 5-dimensional vector <code>[-0.8547, 0.4559, -0.3332, 0.9877, 0.1112]</code>. It's important to note here, the size of the feature vector is tuneable. There are two ways to use embeddings:<ol> <li>Create your own embedding - Once your text has been turned into numbers (required for an embedding), you can put them through an embedding layer (such as <code>tf.keras.layers.Embedding</code>) and an embedding representation will be learned during model training.</li> <li>Reuse a pre-learned embedding - Many pre-trained embeddings exist online. These pre-trained embeddings have often been learned on large corpuses of text (such as all of Wikipedia) and thus have a good underlying representation of natural language. You can use a pre-trained embedding to initialize your model and fine-tune it to your own specific task.</li> </ol> </li> </ul> <p> Example of tokenization (straight mapping from word to number) and embedding (richer representation of relationships between tokens).</p> <p>\ud83e\udd14 Question: What level of tokenzation should I use? What embedding should should I choose?</p> <p>It depends on your problem. You could try character-level tokenization/embeddings and word-level tokenization/embeddings and see which perform best. You might even want to try stacking them (e.g. combining the outputs of your embedding layers using <code>tf.keras.layers.concatenate</code>).</p> <p>If you're looking for pre-trained word embeddings, Word2vec embeddings, GloVe embeddings and many of the options available on TensorFlow Hub are great places to start.</p> <p>\ud83d\udd11 Note: Much like searching for a pre-trained computer vision model, you can search for pre-trained word embeddings to use for your problem. Try searching for something like \"use pre-trained word embeddings in TensorFlow\".</p>"},{"location":"08_introduction_to_nlp_in_tensorflow/#text-vectorization-tokenization","title":"Text vectorization (tokenization)\u00b6","text":"<p>Enough talking about tokenization and embeddings, let's create some.</p> <p>We'll practice tokenzation (mapping our words to numbers) first.</p> <p>To tokenize our words, we'll use the helpful preprocessing layer <code>tf.keras.layers.experimental.preprocessing.TextVectorization</code>.</p> <p>The <code>TextVectorization</code> layer takes the following parameters:</p> <ul> <li><code>max_tokens</code> - The maximum number of words in your vocabulary (e.g. 20000 or the number of unique words in your text), includes a value for OOV (out of vocabulary) tokens.</li> <li><code>standardize</code> - Method for standardizing text. Default is <code>\"lower_and_strip_punctuation\"</code> which lowers text and removes all punctuation marks.</li> <li><code>split</code> - How to split text, default is <code>\"whitespace\"</code> which splits on spaces.</li> <li><code>ngrams</code> - How many words to contain per token split, for example, <code>ngrams=2</code> splits tokens into continuous sequences of 2.</li> <li><code>output_mode</code> -  How to output tokens, can be <code>\"int\"</code> (integer mapping), <code>\"binary\"</code> (one-hot encoding), <code>\"count\"</code> or <code>\"tf-idf\"</code>. See documentation for more.</li> <li><code>output_sequence_length</code> - Length of tokenized sequence to output. For example, if <code>output_sequence_length=150</code>, all tokenized sequences will be 150 tokens long.</li> <li><code>pad_to_max_tokens</code> - Defaults to <code>False</code>, if <code>True</code>, the output feature axis will be padded to <code>max_tokens</code> even if the number of unique tokens in the vocabulary is less than <code>max_tokens</code>. Only valid in certain modes, see docs for more.</li> </ul> <p>Let's see it in action.</p>"},{"location":"08_introduction_to_nlp_in_tensorflow/#creating-an-embedding-using-an-embedding-layer","title":"Creating an Embedding using an Embedding Layer\u00b6","text":"<p>We've got a way to map our text to numbers. How about we go a step further and turn those numbers into an embedding?</p> <p>The powerful thing about an embedding is it can be learned during training. This means rather than just being static (e.g. <code>1</code> = I, <code>2</code> = love, <code>3</code> = TensorFlow), a word's numeric representation can be improved as a model goes through data samples.</p> <p>We can see what an embedding of a word looks like by using the <code>tf.keras.layers.Embedding</code> layer.</p> <p>The main parameters we're concerned about here are:</p> <ul> <li><code>input_dim</code> - The size of the vocabulary (e.g. <code>len(text_vectorizer.get_vocabulary()</code>).</li> <li><code>output_dim</code> - The size of the output embedding vector, for example, a value of <code>100</code> outputs a  feature vector of size 100 for each word.</li> <li><code>embeddings_initializer</code> - How to initialize the embeddings matrix, default is <code>\"uniform\"</code> which randomly initalizes embedding matrix with uniform distribution. This can be changed for using pre-learned embeddings.</li> <li><code>input_length</code> - Length of sequences being passed to embedding layer.</li> </ul> <p>Knowing these, let's make an embedding layer.</p>"},{"location":"08_introduction_to_nlp_in_tensorflow/#modelling-a-text-dataset","title":"Modelling a text dataset\u00b6","text":"<p> Once you've got your inputs and outputs prepared, it's a matter of figuring out which machine learning model to build in between them to bridge the gap.</p> <p>Now that we've got a way to turn our text data into numbers, we can start to build machine learning models to model it.</p> <p>To get plenty of practice, we're going to build a series of different models, each as its own experiment. We'll then compare the results of each model and see which one performed best.</p> <p>More specifically, we'll be building the following:</p> <ul> <li>Model 0: Naive Bayes (baseline)</li> <li>Model 1: Feed-forward neural network (dense model)</li> <li>Model 2: LSTM model</li> <li>Model 3: GRU model</li> <li>Model 4: Bidirectional-LSTM model</li> <li>Model 5: 1D Convolutional Neural Network</li> <li>Model 6: TensorFlow Hub Pretrained Feature Extractor</li> <li>Model 7: Same as model 6 with 10% of training data</li> </ul> <p>Model 0 is the simplest to acquire a baseline which we'll expect each other of the other deeper models to beat.</p> <p>Each experiment will go through the following steps:</p> <ul> <li>Construct the model</li> <li>Train the model</li> <li>Make predictions with the model</li> <li>Track prediction evaluation metrics for later comparison</li> </ul> <p>Let's get started.</p>"},{"location":"08_introduction_to_nlp_in_tensorflow/#model-0-getting-a-baseline","title":"Model 0: Getting a baseline\u00b6","text":"<p>As with all machine learning modelling experiments, it's important to create a baseline model so you've got a benchmark for future experiments to build upon.</p> <p>To create our baseline, we'll create a Scikit-Learn Pipeline using the TF-IDF (term frequency-inverse document frequency) formula to convert our words to numbers and then model them with the Multinomial Naive Bayes algorithm. This was chosen via referring to the Scikit-Learn machine learning map.</p> <p>\ud83d\udcd6 Reading: The ins and outs of TF-IDF algorithm is beyond the scope of this notebook, however, the curious reader is encouraged to check out the Scikit-Learn documentation for more.</p>"},{"location":"08_introduction_to_nlp_in_tensorflow/#creating-an-evaluation-function-for-our-model-experiments","title":"Creating an evaluation function for our model experiments\u00b6","text":"<p>We could evaluate these as they are but since we're going to be evaluating several models in the same way going forward, let's create a helper function which takes an array of predictions and ground truth labels and computes the following:</p> <ul> <li>Accuracy</li> <li>Precision</li> <li>Recall</li> <li>F1-score</li> </ul> <p>\ud83d\udd11 Note: Since we're dealing with a classification problem, the above metrics are the most appropriate. If we were working with a regression problem, other metrics such as MAE (mean absolute error) would be a better choice.</p>"},{"location":"08_introduction_to_nlp_in_tensorflow/#model-1-a-simple-dense-model","title":"Model 1: A simple dense model\u00b6","text":"<p>The first \"deep\" model we're going to build is a single layer dense model. In fact, it's barely going to have a single layer.</p> <p>It'll take our text and labels as input, tokenize the text, create an embedding, find the average of the embedding (using Global Average Pooling) and then pass the average through a fully connected layer with one output unit and a sigmoid activation function.</p> <p>If the previous sentence sounds like a mouthful, it'll make sense when we code it out (remember, if in doubt, code it out).</p> <p>And since we're going to be building a number of TensorFlow deep learning models, we'll import our <code>create_tensorboard_callback()</code> function from <code>helper_functions.py</code> to keep track of the results of each.</p>"},{"location":"08_introduction_to_nlp_in_tensorflow/#visualizing-learned-embeddings","title":"Visualizing learned embeddings\u00b6","text":"<p>Our first model (<code>model_1</code>) contained an embedding layer (<code>embedding</code>) which learned a way of representing words as feature vectors by passing over the training data.</p> <p>Hearing this for the first few times may sound confusing.</p> <p>So to further help understand what a text embedding is, let's visualize the embedding our model learned.</p> <p>To do so, let's remind ourselves of the words in our vocabulary.</p>"},{"location":"08_introduction_to_nlp_in_tensorflow/#recurrent-neural-networks-rnns","title":"Recurrent Neural Networks (RNN's)\u00b6","text":"<p>For our next series of modelling experiments we're going to be using a special kind of neural network called a Recurrent Neural Network (RNN).</p> <p>The premise of an RNN is simple: use information from the past to help you with the future (this is where the term recurrent comes from). In other words, take an input (<code>X</code>) and compute an output (<code>y</code>) based on all previous inputs.</p> <p>This concept is especially helpful when dealing with sequences such as passages of natural language text (such as our Tweets).</p> <p>For example, when you read this sentence, you take into context the previous words when deciphering the meaning of the current word dog.</p> <p>See what happened there?</p> <p>I put the word \"dog\" at the end which is a valid word but it doesn't make sense in the context of the rest of the sentence.</p> <p>When an RNN looks at a sequence of text (already in numerical form), the patterns it learns are continually updated based on the order of the sequence.</p> <p>For a simple example, take two sentences:</p> <ol> <li>Massive earthquake last week, no?</li> <li>No massive earthquake last week.</li> </ol> <p>Both contain exactly the same words but have different meaning. The order of the words determines the meaning (one could argue punctuation marks also dictate the meaning but for simplicity sake, let's stay focused on the words).</p> <p>Recurrent neural networks can be used for a number of sequence-based problems:</p> <ul> <li>One to one: one input, one output, such as image classification.</li> <li>One to many: one input, many outputs, such as image captioning (image input, a sequence of text as caption output).</li> <li>Many to one: many inputs, one outputs, such as text classification (classifying a Tweet as real diaster or not real diaster).</li> <li>Many to many: many inputs, many outputs, such as machine translation (translating English to Spanish) or speech to text (audio wave as input, text as output).</li> </ul> <p>When you come across RNN's in the wild, you'll most likely come across variants of the following:</p> <ul> <li>Long short-term memory cells (LSTMs).</li> <li>Gated recurrent units (GRUs).</li> <li>Bidirectional RNN's (passes forward and backward along a sequence, left to right and right to left).</li> </ul> <p>Going into the details of each these is beyond the scope of this notebook (we're going to focus on using them instead), the main thing you should know for now is that they've proven very effective at modelling sequences.</p> <p>For a deeper understanding of what's happening behind the scenes of the code we're about to write, I'd recommend the following resources:</p> <p>\ud83d\udcd6 Resources:</p> <ul> <li>MIT Deep Learning Lecture on Recurrent Neural Networks - explains the background of recurrent neural networks and introduces LSTMs.</li> <li>The Unreasonable Effectiveness of Recurrent Neural Networks by Andrej Karpathy - demonstrates the power of RNN's with examples generating various sequences.</li> <li>Understanding LSTMs by Chris Olah - an in-depth (and technical) look at the mechanics of the LSTM cell, possibly the most popular RNN building block.</li> </ul>"},{"location":"08_introduction_to_nlp_in_tensorflow/#model-2-lstm","title":"Model 2: LSTM\u00b6","text":"<p>With all this talk of what RNN's are and what they're good for, I'm sure you're eager to build one.</p> <p>We're going to start with an LSTM-powered RNN.</p> <p>To harness the power of the LSTM cell (LSTM cell and LSTM layer are often used interchangably) in TensorFlow, we'll use <code>tensorflow.keras.layers.LSTM()</code>.</p> <p> Coloured block example of the structure of an recurrent neural network.</p> <p>Our model is going to take on a very similar structure to <code>model_1</code>:</p> <pre><code>Input (text) -&gt; Tokenize -&gt; Embedding -&gt; Layers -&gt; Output (label probability)\n</code></pre> <p>The main difference will be that we're going to add an LSTM layer between our embedding and output.</p> <p>And to make sure we're not getting reusing trained embeddings (this would involve data leakage between models, leading to an uneven comparison later on), we'll create another embedding layer (<code>model_2_embedding</code>) for our model. The <code>text_vectorizer</code> layer can be reused since it doesn't get updated during training.</p> <p>\ud83d\udd11 Note: The reason we use a new embedding layer for each model is since the embedding layer is a learned representation of words (as numbers), if we were to use the same embedding layer (<code>embedding_1</code>) for each model, we'd be mixing what one model learned with the next. And because we want to compare our models later on, starting them with their own embedding layer each time is a better idea.</p>"},{"location":"08_introduction_to_nlp_in_tensorflow/#model-3-gru","title":"Model 3: GRU\u00b6","text":"<p>Another popular and effective RNN component is the GRU or gated recurrent unit.</p> <p>The GRU cell has similar features to an LSTM cell but has less parameters.</p> <p>\ud83d\udcd6 Resource: A full explanation of the GRU cell is beyond the scope of this noteook but I'd suggest the following resources to learn more:</p> <ul> <li>Gated Recurrent Unit Wikipedia page</li> <li>Understanding GRU networks by Simeon Kostadinov</li> </ul> <p>To use the GRU cell in TensorFlow, we can call the <code>tensorflow.keras.layers.GRU()</code> class.</p> <p>The architecture of the GRU-powered model will follow the same structure we've been using:</p> <pre><code>Input (text) -&gt; Tokenize -&gt; Embedding -&gt; Layers -&gt; Output (label probability)\n</code></pre> <p>Again, the only difference will be the layer(s) we use between the embedding and the output.</p>"},{"location":"08_introduction_to_nlp_in_tensorflow/#model-4-bidirectonal-rnn-model","title":"Model 4: Bidirectonal RNN model\u00b6","text":"<p>Look at us go! We've already built two RNN's with GRU and LSTM cells. Now we're going to look into another kind of RNN, the bidirectional RNN.</p> <p>A standard RNN will process a sequence from left to right, where as a bidirectional RNN will process the sequence from left to right and then again from right to left.</p> <p>Intuitively, this can be thought of as if you were reading a sentence for the first time in the normal fashion (left to right) but for some reason it didn't make sense so you traverse back through the words and go back over them again (right to left).</p> <p>In practice, many sequence models often see and improvement in performance when using bidirectional RNN's.</p> <p>However, this improvement in performance often comes at the cost of longer training times and increased model parameters (since the model goes left to right and right to left, the number of trainable parameters doubles).</p> <p>Okay enough talk, let's build a bidirectional RNN.</p> <p>Once again, TensorFlow helps us out by providing the <code>tensorflow.keras.layers.Bidirectional</code> class. We can use the <code>Bidirectional</code> class to wrap our existing RNNs, instantly making them bidirectional.</p>"},{"location":"08_introduction_to_nlp_in_tensorflow/#convolutional-neural-networks-for-text","title":"Convolutional Neural Networks for Text\u00b6","text":"<p>You might've used convolutional neural networks (CNNs) for images before but they can also be used for sequences.</p> <p>The main difference between using CNNs for images and sequences is the shape of the data. Images come in 2-dimensions (height x width) where as sequences are often 1-dimensional (a string of text).</p> <p>So to use CNNs with sequences, we use a 1-dimensional convolution instead of a 2-dimensional convolution.</p> <p>A typical CNN architecture for sequences will look like the following:</p> <pre><code>Inputs (text) -&gt; Tokenization -&gt; Embedding -&gt; Layers -&gt; Outputs (class probabilities)\n</code></pre> <p>You might be thinking \"that just looks like the architecture layout we've been using for the other models...\"</p> <p>And you'd be right.</p> <p>The difference again is in the layers component. Instead of using an LSTM or GRU cell, we're going to use a <code>tensorflow.keras.layers.Conv1D()</code> layer followed by a <code>tensorflow.keras.layers.GlobablMaxPool1D()</code> layer.</p> <ol> <li>1-dimensional convolving filters are used as ngram detectors, each filter specializing in a closely-related family of ngrams (an ngram is a collection of n-words, for example, an ngram of 5 might result in \"hello, my name is Daniel\").</li> <li>Max-pooling over time extracts the relevant ngrams for making a decision.</li> <li>The rest of the network classifies the text based on this information.</li> </ol> <p>\ud83d\udcd6 Resource: The intuition here is explained succinctly in the paper Understanding Convolutional Neural Networks for Text Classification, where they state that CNNs classify text through the following steps:</p>"},{"location":"08_introduction_to_nlp_in_tensorflow/#model-5-conv1d","title":"Model 5: Conv1D\u00b6","text":"<p>Before we build a full 1-dimensional CNN model, let's see a 1-dimensional convolutional layer (also called a temporal convolution) in action.</p> <p>We'll first create an embedding of a sample of text and experiment passing it through a <code>Conv1D()</code> layer and <code>GlobalMaxPool1D()</code> layer.</p>"},{"location":"08_introduction_to_nlp_in_tensorflow/#using-pretrained-embeddings-transfer-learning-for-nlp","title":"Using Pretrained Embeddings (transfer learning for NLP)\u00b6","text":"<p>For all of the previous deep learning models we've built and trained, we've created and used our own embeddings from scratch each time.</p> <p>However, a common practice is to leverage pretrained embeddings through transfer learning. This is one of the main benefits of using deep models: being able to take what one (often larger) model has learned (often on a large amount of data) and adjust it for our own use case.</p> <p>For our next model, instead of using our own embedding layer, we're going to replace it with a pretrained embedding layer.</p> <p>More specifically, we're going to be using the Universal Sentence Encoder from TensorFlow Hub (a great resource containing a plethora of pretrained model resources for a variety of tasks).</p> <p>\ud83d\udd11 Note: There are many different pretrained text embedding options on TensorFlow Hub, however, some require different levels of text preprocessing than others. Best to experiment with a few and see which best suits your use case.</p>"},{"location":"08_introduction_to_nlp_in_tensorflow/#model-6-tensorflow-hub-pretrained-sentence-encoder","title":"Model 6: TensorFlow Hub Pretrained Sentence Encoder\u00b6","text":"<p>The main difference between the embedding layer we created and the Universal Sentence Encoder is that rather than create a word-level embedding, the Universal Sentence Encoder, as you might've guessed, creates a whole sentence-level embedding.</p> <p>Our embedding layer also outputs an a 128 dimensional vector for each word, where as, the Universal Sentence Encoder outputs a 512 dimensional vector for each sentence.</p> <p> The feature extractor model we're building through the eyes of an encoder/decoder model.</p> <p>\ud83d\udd11 Note: An encoder is the name for a model which converts raw data such as text into a numerical representation (feature vector), a decoder converts the numerical representation to a desired output.</p> <p>As usual, this is best demonstrated with an example.</p> <p>We can load in a TensorFlow Hub module using the <code>hub.load()</code> method and passing it the target URL of the module we'd like to use, in our case, it's \"https://tfhub.dev/google/universal-sentence-encoder/4\".</p> <p>Let's load the Universal Sentence Encoder model and test it on a couple of sentences.</p>"},{"location":"08_introduction_to_nlp_in_tensorflow/#model-7-tensorflow-hub-pretrained-sentence-encoder-10-of-the-training-data","title":"Model 7: TensorFlow Hub Pretrained Sentence Encoder 10% of the training data\u00b6","text":"<p>One of the benefits of using transfer learning methods, such as, the pretrained embeddings within the USE is the ability to get great results on a small amount of data (the USE paper even mentions this in the abstract).</p> <p>To put this to the test, we're going to make a small subset of the training data (10%), train a model and evaluate it.</p>"},{"location":"08_introduction_to_nlp_in_tensorflow/#comparing-the-performance-of-each-of-our-models","title":"Comparing the performance of each of our models\u00b6","text":"<p>Woah. We've come a long way! From training a baseline to several deep models.</p> <p>Now it's time to compare our model's results.</p> <p>But just before we do, it's worthwhile mentioning, this type of practice is a standard deep learning workflow. Training various different models, then comparing them to see which one performed best and continuing to train it if necessary.</p> <p>The important thing to note is that for all of our modelling experiments we used the same training data (except for <code>model_7</code> where we used 10% of the training data).</p> <p>To visualize our model's performances, let's create a pandas DataFrame we our results dictionaries and then plot it.</p>"},{"location":"08_introduction_to_nlp_in_tensorflow/#combining-our-models-model-ensemblingstacking","title":"Combining our models (model ensembling/stacking)\u00b6","text":"<p>Many production systems use an ensemble (multiple different models combined) of models to make a prediction.</p> <p>The idea behind model stacking is that if several uncorrelated models agree on a prediction, then the prediction must be more robust than a prediction made by a singular model.</p> <p>The keyword in the sentence above is uncorrelated, which is another way of saying, different types of models. For example, in our case, we might combine our baseline, our bidirectional model and our TensorFlow Hub USE model.</p> <p>Although these models are all trained on the same data, they all have a different way of finding patterns.</p> <p>If we were to use three similarly trained models, such as three LSTM models, the predictions they output will likely be very similar.</p> <p>Think of it as trying to decide where to eat with your friends. If you all have similar tastes, you'll probably all pick the same restaurant. But if you've all got different tastes and still end up picking the same restaurant, the restaurant must be good.</p> <p>Since we're working with a classification problem, there are a few of ways we can combine our models:</p> <ol> <li>Averaging - Take the output prediction probabilities of each model for each sample, combine them and then average them.</li> <li>Majority vote (mode) - Make class predictions with each of your models on all samples, the predicted class is the one in majority. For example, if three different models predict <code>[1, 0, 1]</code> respectively, the majority class is <code>1</code>, therefore, that would be the predicted label.</li> <li>Model stacking - Take the outputs of each of your chosen models and use them as inputs to another model.</li> </ol> <p>\ud83d\udcd6 Resource: The above methods for model stacking/ensembling were adapted from Chapter 6 of the Machine Learning Engineering Book by Andriy Burkov. If you're looking to enter the field of machine learning engineering, not only building models but production-scale machine learning systems, I'd highly recommend reading it in its entirety.</p> <p>Again, the concept of model stacking is best seen in action.</p> <p>We're going to combine our baseline model (<code>model_0</code>), LSTM model (<code>model_2</code>) and our USE model trained on the full training data (<code>model_6</code>) by averaging the combined prediction probabilities of each.</p>"},{"location":"08_introduction_to_nlp_in_tensorflow/#saving-and-loading-a-trained-model","title":"Saving and loading a trained model\u00b6","text":"<p>Although training time didn't take very long, it's good practice to save your trained models to avoid having to retrain them.</p> <p>Saving your models also enables you to export them for use elsewhere outside of your notebooks, such as in a web application.</p> <p>There are two main ways of saving a model in TensorFlow:</p> <ol> <li>The <code>HDF5</code> format.</li> <li>The <code>SavedModel</code> format (default).</li> </ol> <p>Let's take a look at both.</p>"},{"location":"08_introduction_to_nlp_in_tensorflow/#finding-the-most-wrong-examples","title":"Finding the most wrong examples\u00b6","text":"<p>We mentioned before that if many of our modelling experiments are returning similar results, despite using different kinds of models, it's a good idea to return to the data and inspect why this might be.</p> <p>One of the best ways to inspect your data is to sort your model's predictions and find the samples it got most wrong, meaning, what predictions had a high prediction probability but turned out to be wrong.</p> <p>Once again, visualization is your friend. Visualize, visualize, visualize.</p> <p>To make things visual, let's take our best performing model's prediction probabilities and classes along with the validation samples (text and ground truth labels) and combine them in a pandas DataFrame.</p> <ul> <li>If our best model still isn't perfect, what examples is it getting wrong?</li> <li>Which ones are the most wrong?</li> <li>Are there some labels which are wrong? E.g. the model gets it right but the ground truth label doesn't reflect this</li> </ul>"},{"location":"08_introduction_to_nlp_in_tensorflow/#making-predictions-on-the-test-dataset","title":"Making predictions on the test dataset\u00b6","text":"<p>Alright we've seen how our model's perform on the validation set.</p> <p>But how about the test dataset?</p> <p>We don't have labels for the test dataset so we're going to have to make some predictions and inspect them for ourselves.</p> <p>Let's write some code to make predictions on random samples from the test dataset and visualize them.</p>"},{"location":"08_introduction_to_nlp_in_tensorflow/#predicting-on-tweets-from-the-wild","title":"Predicting on Tweets from the wild\u00b6","text":"<p>How about we find some Tweets and use our model to predict whether or not they're about a diaster or not?</p> <p>To start, let's take one of my own Tweets on living life like an ensemble model.</p>"},{"location":"08_introduction_to_nlp_in_tensorflow/#the-speedscore-tradeoff","title":"The speed/score tradeoff\u00b6","text":"<p>One of the final tests we're going to do is to find the speed/score tradeoffs between our best model and baseline model.</p> <p>Why is this important?</p> <p>Although it can be tempting to just choose the best performing model you find through experimentation, this model might not actually work in a production setting.</p> <p>Put it this way, imagine you're Twitter and receive 1 million Tweets per hour (this is a made up number, the actual number is much higher). And you're trying to build a diaster detection system to read Tweets and alert authorities with details about a diaster in close to real-time.</p> <p>Compute power isn't free so you're limited to a single compute machine for the project. On that machine, one of your models makes 10,000 predictions per second at 80% accuracy where as another one of your models (a larger model) makes 100 predictions per second at 85% accuracy.</p> <p>Which model do you choose?</p> <p>Is the second model's performance boost worth missing out on the extra capacity?</p> <p>Of course, there are many options you could try here, such as sending as many Tweets as possible to the first model and then sending the ones which the model is least certain of to the second model.</p> <p>The point here is to illustrate the best model you find through experimentation, might not be the model you end up using in production.</p> <p>To make this more concrete, let's write a function to take a model and a number of samples and time how long the given model takes to make predictions on those samples.</p>"},{"location":"08_introduction_to_nlp_in_tensorflow/#exercises","title":"\ud83d\udee0 Exercises\u00b6","text":"<ol> <li>Rebuild, compile and train <code>model_1</code>, <code>model_2</code> and <code>model_5</code> using the Keras Sequential API instead of the Functional API.</li> <li>Retrain the baseline model with 10% of the training data. How does perform compared to the Universal Sentence Encoder model with 10% of the training data?</li> <li>Try fine-tuning the TF Hub Universal Sentence Encoder model by setting <code>training=True</code> when instantiating it as a Keras layer.</li> </ol> <pre><code>We can use this encoding layer in place of our text_vectorizer and embedding layer\n\nsentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n                                        input_shape=[],\n                                        dtype=tf.string,\n                                        trainable=True) # turn training on to fine-tune the TensorFlow Hub model\n</code></pre> <ol> <li>Retrain the best model you've got so far on the whole training set (no validation split). Then use this trained model to make predictions on the test dataset and format the predictions into the same format as the <code>sample_submission.csv</code> file from Kaggle (see the Files tab in Colab for what the <code>sample_submission.csv</code> file looks like). Once you've done this, make a submission to the Kaggle competition, how did your model perform?</li> <li>Combine the ensemble predictions using the majority vote (mode), how does this perform compare to averaging the prediction probabilities of each model?</li> <li>Make a confusion matrix with the best performing model's predictions on the validation set and the validation ground truth labels.</li> </ol>"},{"location":"08_introduction_to_nlp_in_tensorflow/#extra-curriculum","title":"\ud83d\udcd6 Extra-curriculum\u00b6","text":"<p>To practice what you've learned, a good idea would be to spend an hour on 3 of the following (3-hours total, you could through them all if you want) and then write a blog post about what you've learned.</p> <ul> <li>For an overview of the different problems within NLP and how to solve them read through:</li> <li>A Simple Introduction to Natural Language Processing</li> <li>How to solve 90% of NLP problems: a step-by-step guide</li> <li>Go through MIT's Recurrent Neural Networks lecture. This will be one of the greatest additions to what's happening behind the RNN model's you've been building.</li> <li>Read through the word embeddings page on the TensorFlow website. Embeddings are such a large part of NLP. We've covered them throughout this notebook but extra practice would be well worth it. A good exercise would be to write out all the code in the guide in a new notebook.</li> <li>For more on RNN's in TensorFlow, read and reproduce the TensorFlow RNN guide. We've covered many of the concepts in this guide, but it's worth writing the code again for yourself.</li> <li>Text data doesn't always come in a nice package like the data we've downloaded. So if you're after more on preparing different text sources for being with your TensorFlow deep learning models, it's worth checking out the following:</li> <li>TensorFlow text loading tutorial.</li> <li>Reading text files with Python by Real Python.</li> <li>This notebook has focused on writing NLP code. For a mathematically rich overview of how NLP with Deep Learning happens, read Standford's Natural Language Processing with Deep Learning lecture notes Part 1.<ul> <li>For an even deeper dive, you could even do the whole CS224n (Natural Language Processing with Deep Learning) course.</li> </ul> </li> <li>Great blog posts to read:<ul> <li>Andrei Karpathy's The Unreasonable Effectiveness of RNNs dives into generating Shakespeare text with RNNs.</li> <li>Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT by Mauro Di Pietro. An overview of different techniques for turning text into numbers and then classifying it.</li> <li>What are word embeddings? by Machine Learning Mastery.</li> </ul> </li> <li>Other topics worth looking into:<ul> <li>Attention mechanisms. These are a foundational component of the transformer architecture and also often add improvments to deep NLP models.</li> <li>Transformer architectures. This model architecture has recently taken the NLP world by storm, achieving state of the art on many benchmarks. However, it does take a little more processing to get off the ground, the HuggingFace Models (formerly HuggingFace Transformers) library is probably your best quick start.<ul> <li>And now HuggingFace even have their own course on how their library works! I haven't done it but anything HuggingFace makes is world-class.</li> </ul> </li> </ul> </li> </ul>"},{"location":"09_SkimLit_nlp_milestone_project_2/","title":"09. Milestone Project 2: SkimLit \ud83d\udcc4\ud83d\udd25","text":"In\u00a0[1]: Copied! <pre>import datetime\nprint(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\")\n</pre> import datetime print(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\") <pre>Notebook last run (end-to-end): 2023-05-26 03:47:27.469424\n</pre> In\u00a0[2]: Copied! <pre># Check for GPU\n!nvidia-smi -L\n</pre> # Check for GPU !nvidia-smi -L <pre>GPU 0: NVIDIA A100-SXM4-40GB (UUID: GPU-061bcc09-64a4-755a-357c-181dfae1f4a9)\n</pre> In\u00a0[3]: Copied! <pre>!git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git\n!ls pubmed-rct\n</pre> !git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git !ls pubmed-rct <pre>Cloning into 'pubmed-rct'...\nremote: Enumerating objects: 33, done.\nremote: Counting objects: 100% (8/8), done.\nremote: Compressing objects: 100% (3/3), done.\nremote: Total 33 (delta 5), reused 5 (delta 5), pack-reused 25\nUnpacking objects: 100% (33/33), 177.08 MiB | 9.05 MiB/s, done.\nPubMed_200k_RCT\nPubMed_200k_RCT_numbers_replaced_with_at_sign\nPubMed_20k_RCT\nPubMed_20k_RCT_numbers_replaced_with_at_sign\nREADME.md\n</pre> <p>Checking the contents of the downloaded repository, you can see there are four folders.</p> <p>Each contains a different version of the PubMed 200k RCT dataset.</p> <p>Looking at the README file from the GitHub page, we get the following information:</p> <ul> <li>PubMed 20k is a subset of PubMed 200k. I.e., any abstract present in PubMed 20k is also present in PubMed 200k.</li> <li><code>PubMed_200k_RCT</code> is the same as <code>PubMed_200k_RCT_numbers_replaced_with_at_sign</code>, except that in the latter all numbers had been replaced by <code>@</code>. (same for <code>PubMed_20k_RCT</code> vs. <code>PubMed_20k_RCT_numbers_replaced_with_at_sign</code>).</li> <li>Since Github file size limit is 100 MiB, we had to compress <code>PubMed_200k_RCT\\train.7z</code> and <code>PubMed_200k_RCT_numbers_replaced_with_at_sign\\train.zip</code>. To uncompress <code>train.7z</code>, you may use 7-Zip on Windows, Keka on Mac OS X, or p7zip on Linux.</li> </ul> <p>To begin with, the dataset we're going to be focused on is <code>PubMed_20k_RCT_numbers_replaced_with_at_sign</code>.</p> <p>Why this one?</p> <p>Rather than working with the whole 200k dataset, we'll keep our experiments quick by starting with a smaller subset. We could've chosen the dataset with numbers instead of having them replaced with <code>@</code> but we didn't.</p> <p>Let's check the file contents.</p> In\u00a0[4]: Copied! <pre># Check what files are in the PubMed_20K dataset \n!ls pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign\n</pre> # Check what files are in the PubMed_20K dataset  !ls pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign <pre>dev.txt  test.txt  train.txt\n</pre> <p>Beautiful, looks like we've got three separate text files:</p> <ul> <li><code>train.txt</code> - training samples.</li> <li><code>dev.txt</code> - dev is short for development set, which is another name for validation set (in our case, we'll be using and referring to this file as our validation set).</li> <li><code>test.txt</code> - test samples.</li> </ul> <p>To save ourselves typing out the filepath to our target directory each time, let's turn it into a variable.</p> In\u00a0[5]: Copied! <pre># Start by using the 20k dataset\ndata_dir = \"pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\"\n</pre> # Start by using the 20k dataset data_dir = \"pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\" In\u00a0[6]: Copied! <pre># Check all of the filenames in the target directory\nimport os\nfilenames = [data_dir + filename for filename in os.listdir(data_dir)]\nfilenames\n</pre> # Check all of the filenames in the target directory import os filenames = [data_dir + filename for filename in os.listdir(data_dir)] filenames Out[6]: <pre>['pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/test.txt',\n 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt',\n 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/dev.txt']</pre> In\u00a0[7]: Copied! <pre># Create function to read the lines of a document\ndef get_lines(filename):\n  \"\"\"\n  Reads filename (a text file) and returns the lines of text as a list.\n  \n  Args:\n      filename: a string containing the target filepath to read.\n  \n  Returns:\n      A list of strings with one string per line from the target filename.\n      For example:\n      [\"this is the first line of filename\",\n       \"this is the second line of filename\",\n       \"...\"]\n  \"\"\"\n  with open(filename, \"r\") as f:\n    return f.readlines()\n</pre> # Create function to read the lines of a document def get_lines(filename):   \"\"\"   Reads filename (a text file) and returns the lines of text as a list.      Args:       filename: a string containing the target filepath to read.      Returns:       A list of strings with one string per line from the target filename.       For example:       [\"this is the first line of filename\",        \"this is the second line of filename\",        \"...\"]   \"\"\"   with open(filename, \"r\") as f:     return f.readlines() <p>Alright, we've got a little function, <code>get_lines()</code> which takes the filepath of a text file, opens it, reads each of the lines and returns them.</p> <p>Let's try it out on the training data (<code>train.txt</code>).</p> In\u00a0[8]: Copied! <pre>train_lines = get_lines(data_dir+\"train.txt\")\ntrain_lines[:20] # the whole first example of an abstract + a little more of the next one\n</pre> train_lines = get_lines(data_dir+\"train.txt\") train_lines[:20] # the whole first example of an abstract + a little more of the next one Out[8]: <pre>['###24293578\\n',\n 'OBJECTIVE\\tTo investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .\\n',\n 'METHODS\\tA total of @ patients with primary knee OA were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .\\n',\n 'METHODS\\tOutcome measures included pain reduction and improvement in function scores and systemic inflammation markers .\\n',\n 'METHODS\\tPain was assessed using the visual analog pain scale ( @-@ mm ) .\\n',\n 'METHODS\\tSecondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and @-min walk distance ( @MWD ) .\\n',\n 'METHODS\\tSerum levels of interleukin @ ( IL-@ ) , IL-@ , tumor necrosis factor ( TNF ) - , and high-sensitivity C-reactive protein ( hsCRP ) were measured .\\n',\n 'RESULTS\\tThere was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , PGA , and @MWD at @ weeks .\\n',\n 'RESULTS\\tThe mean difference between treatment arms ( @ % CI ) was @ ( @-@ @ ) , p &lt; @ ; @ ( @-@ @ ) , p &lt; @ ; @ ( @-@ @ ) , p &lt; @ ; and @ ( @-@ @ ) , p &lt; @ , respectively .\\n',\n 'RESULTS\\tFurther , there was a clinically relevant reduction in the serum levels of IL-@ , IL-@ , TNF - , and hsCRP at @ weeks in the intervention group when compared to the placebo group .\\n',\n 'RESULTS\\tThese differences remained significant at @ weeks .\\n',\n 'RESULTS\\tThe Outcome Measures in Rheumatology Clinical Trials-Osteoarthritis Research Society International responder rate was @ % in the intervention group and @ % in the placebo group ( p &lt; @ ) .\\n',\n 'CONCLUSIONS\\tLow-dose oral prednisolone had both a short-term and a longer sustained effect resulting in less knee pain , better physical function , and attenuation of systemic inflammation in older patients with knee OA ( ClinicalTrials.gov identifier NCT@ ) .\\n',\n '\\n',\n '###24854809\\n',\n 'BACKGROUND\\tEmotional eating is associated with overeating and the development of obesity .\\n',\n 'BACKGROUND\\tYet , empirical evidence for individual ( trait ) differences in emotional eating and cognitive mechanisms that contribute to eating during sad mood remain equivocal .\\n',\n 'OBJECTIVE\\tThe aim of this study was to test if attention bias for food moderates the effect of self-reported emotional eating during sad mood ( vs neutral mood ) on actual food intake .\\n',\n 'OBJECTIVE\\tIt was expected that emotional eating is predictive of elevated attention for food and higher food intake after an experimentally induced sad mood and that attentional maintenance on food predicts food intake during a sad versus a neutral mood .\\n',\n 'METHODS\\tParticipants ( N = @ ) were randomly assigned to one of the two experimental mood induction conditions ( sad/neutral ) .\\n']</pre> <p>Reading the lines from the training text file results in a list of strings containing different abstract samples, the sentences in a sample along with the role the sentence plays in the abstract.</p> <p>The role of each sentence is prefixed at the start of each line separated by a tab (<code>\\t</code>) and each sentence finishes with a new line (<code>\\n</code>).</p> <p>Different abstracts are separated by abstract ID's (lines beginning with <code>###</code>) and newlines (<code>\\n</code>).</p> <p>Knowing this, it looks like we've got a couple of steps to do to get our samples ready to pass as training data to our future machine learning model.</p> <p>Let's write a function to perform the following steps:</p> <ul> <li>Take a target file of abstract samples.</li> <li>Read the lines in the target file.</li> <li>For each line in the target file:<ul> <li>If the line begins with <code>###</code> mark it as an abstract ID and the beginning of a new abstract.<ul> <li>Keep count of the number of lines in a sample.</li> </ul> </li> <li>If the line begins with <code>\\n</code> mark it as the end of an abstract sample.<ul> <li>Keep count of the total lines in a sample.</li> </ul> </li> <li>Record the text before the <code>\\t</code> as the label of the line.</li> <li>Record the text after the <code>\\t</code> as the text of the line.</li> </ul> </li> <li>Return all of the lines in the target text file as a list of dictionaries containing the key/value pairs:<ul> <li><code>\"line_number\"</code> - the position of the line in the abstract (e.g. <code>3</code>).</li> <li><code>\"target\"</code> - the role of the line in the abstract (e.g. <code>OBJECTIVE</code>).</li> <li><code>\"text\"</code> - the text of the line in the abstract.</li> <li><code>\"total_lines\"</code> - the total lines in an abstract sample (e.g. <code>14</code>).</li> </ul> </li> <li>Abstract ID's and newlines should be omitted from the returned preprocessed data.</li> </ul> <p>Example returned preprocessed sample (a single line from an abstract):</p> <pre><code>[{'line_number': 0,\n  'target': 'OBJECTIVE',\n  'text': 'to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n  'total_lines': 11},\n  ...]\n</code></pre> In\u00a0[9]: Copied! <pre>def preprocess_text_with_line_numbers(filename):\n  \"\"\"Returns a list of dictionaries of abstract line data.\n\n  Takes in filename, reads its contents and sorts through each line,\n  extracting things like the target label, the text of the sentence,\n  how many sentences are in the current abstract and what sentence number\n  the target line is.\n\n  Args:\n      filename: a string of the target text file to read and extract line data\n      from.\n\n  Returns:\n      A list of dictionaries each containing a line from an abstract,\n      the lines label, the lines position in the abstract and the total number\n      of lines in the abstract where the line is from. For example:\n\n      [{\"target\": 'CONCLUSION',\n        \"text\": The study couldn't have gone better, turns out people are kinder than you think\",\n        \"line_number\": 8,\n        \"total_lines\": 8}]\n  \"\"\"\n  input_lines = get_lines(filename) # get all lines from filename\n  abstract_lines = \"\" # create an empty abstract\n  abstract_samples = [] # create an empty list of abstracts\n  \n  # Loop through each line in target file\n  for line in input_lines:\n    if line.startswith(\"###\"): # check to see if line is an ID line\n      abstract_id = line\n      abstract_lines = \"\" # reset abstract string\n    elif line.isspace(): # check to see if line is a new line\n      abstract_line_split = abstract_lines.splitlines() # split abstract into separate lines\n\n      # Iterate through each line in abstract and count them at the same time\n      for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n        line_data = {} # create empty dict to store data from line\n        target_text_split = abstract_line.split(\"\\t\") # split target label from text\n        line_data[\"target\"] = target_text_split[0] # get target label\n        line_data[\"text\"] = target_text_split[1].lower() # get target text and lower it\n        line_data[\"line_number\"] = abstract_line_number # what number line does the line appear in the abstract?\n        line_data[\"total_lines\"] = len(abstract_line_split) - 1 # how many total lines are in the abstract? (start from 0)\n        abstract_samples.append(line_data) # add line data to abstract samples list\n    \n    else: # if the above conditions aren't fulfilled, the line contains a labelled sentence\n      abstract_lines += line\n  \n  return abstract_samples\n</pre> def preprocess_text_with_line_numbers(filename):   \"\"\"Returns a list of dictionaries of abstract line data.    Takes in filename, reads its contents and sorts through each line,   extracting things like the target label, the text of the sentence,   how many sentences are in the current abstract and what sentence number   the target line is.    Args:       filename: a string of the target text file to read and extract line data       from.    Returns:       A list of dictionaries each containing a line from an abstract,       the lines label, the lines position in the abstract and the total number       of lines in the abstract where the line is from. For example:        [{\"target\": 'CONCLUSION',         \"text\": The study couldn't have gone better, turns out people are kinder than you think\",         \"line_number\": 8,         \"total_lines\": 8}]   \"\"\"   input_lines = get_lines(filename) # get all lines from filename   abstract_lines = \"\" # create an empty abstract   abstract_samples = [] # create an empty list of abstracts      # Loop through each line in target file   for line in input_lines:     if line.startswith(\"###\"): # check to see if line is an ID line       abstract_id = line       abstract_lines = \"\" # reset abstract string     elif line.isspace(): # check to see if line is a new line       abstract_line_split = abstract_lines.splitlines() # split abstract into separate lines        # Iterate through each line in abstract and count them at the same time       for abstract_line_number, abstract_line in enumerate(abstract_line_split):         line_data = {} # create empty dict to store data from line         target_text_split = abstract_line.split(\"\\t\") # split target label from text         line_data[\"target\"] = target_text_split[0] # get target label         line_data[\"text\"] = target_text_split[1].lower() # get target text and lower it         line_data[\"line_number\"] = abstract_line_number # what number line does the line appear in the abstract?         line_data[\"total_lines\"] = len(abstract_line_split) - 1 # how many total lines are in the abstract? (start from 0)         abstract_samples.append(line_data) # add line data to abstract samples list          else: # if the above conditions aren't fulfilled, the line contains a labelled sentence       abstract_lines += line      return abstract_samples <p>Beautiful! That's one good looking function. Let's use it to preprocess each of our RCT 20k datasets.</p> In\u00a0[10]: Copied! <pre># Get data from file and preprocess it\n%%time\ntrain_samples = preprocess_text_with_line_numbers(data_dir + \"train.txt\")\nval_samples = preprocess_text_with_line_numbers(data_dir + \"dev.txt\") # dev is another name for validation set\ntest_samples = preprocess_text_with_line_numbers(data_dir + \"test.txt\")\nlen(train_samples), len(val_samples), len(test_samples)\n</pre> # Get data from file and preprocess it %%time train_samples = preprocess_text_with_line_numbers(data_dir + \"train.txt\") val_samples = preprocess_text_with_line_numbers(data_dir + \"dev.txt\") # dev is another name for validation set test_samples = preprocess_text_with_line_numbers(data_dir + \"test.txt\") len(train_samples), len(val_samples), len(test_samples) <pre>CPU times: user 344 ms, sys: 77.2 ms, total: 421 ms\nWall time: 420 ms\n</pre> Out[10]: <pre>(180040, 30212, 30135)</pre> <p>How do our training samples look?</p> In\u00a0[11]: Copied! <pre># Check the first abstract of our training data\ntrain_samples[:14]\n</pre> # Check the first abstract of our training data train_samples[:14] Out[11]: <pre>[{'target': 'OBJECTIVE',\n  'text': 'to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n  'line_number': 0,\n  'total_lines': 11},\n {'target': 'METHODS',\n  'text': 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n  'line_number': 1,\n  'total_lines': 11},\n {'target': 'METHODS',\n  'text': 'outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .',\n  'line_number': 2,\n  'total_lines': 11},\n {'target': 'METHODS',\n  'text': 'pain was assessed using the visual analog pain scale ( @-@ mm ) .',\n  'line_number': 3,\n  'total_lines': 11},\n {'target': 'METHODS',\n  'text': 'secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .',\n  'line_number': 4,\n  'total_lines': 11},\n {'target': 'METHODS',\n  'text': 'serum levels of interleukin @ ( il-@ ) , il-@ , tumor necrosis factor ( tnf ) - , and high-sensitivity c-reactive protein ( hscrp ) were measured .',\n  'line_number': 5,\n  'total_lines': 11},\n {'target': 'RESULTS',\n  'text': 'there was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , pga , and @mwd at @ weeks .',\n  'line_number': 6,\n  'total_lines': 11},\n {'target': 'RESULTS',\n  'text': 'the mean difference between treatment arms ( @ % ci ) was @ ( @-@ @ ) , p &lt; @ ; @ ( @-@ @ ) , p &lt; @ ; @ ( @-@ @ ) , p &lt; @ ; and @ ( @-@ @ ) , p &lt; @ , respectively .',\n  'line_number': 7,\n  'total_lines': 11},\n {'target': 'RESULTS',\n  'text': 'further , there was a clinically relevant reduction in the serum levels of il-@ , il-@ , tnf - , and hscrp at @ weeks in the intervention group when compared to the placebo group .',\n  'line_number': 8,\n  'total_lines': 11},\n {'target': 'RESULTS',\n  'text': 'these differences remained significant at @ weeks .',\n  'line_number': 9,\n  'total_lines': 11},\n {'target': 'RESULTS',\n  'text': 'the outcome measures in rheumatology clinical trials-osteoarthritis research society international responder rate was @ % in the intervention group and @ % in the placebo group ( p &lt; @ ) .',\n  'line_number': 10,\n  'total_lines': 11},\n {'target': 'CONCLUSIONS',\n  'text': 'low-dose oral prednisolone had both a short-term and a longer sustained effect resulting in less knee pain , better physical function , and attenuation of systemic inflammation in older patients with knee oa ( clinicaltrials.gov identifier nct@ ) .',\n  'line_number': 11,\n  'total_lines': 11},\n {'target': 'BACKGROUND',\n  'text': 'emotional eating is associated with overeating and the development of obesity .',\n  'line_number': 0,\n  'total_lines': 10},\n {'target': 'BACKGROUND',\n  'text': 'yet , empirical evidence for individual ( trait ) differences in emotional eating and cognitive mechanisms that contribute to eating during sad mood remain equivocal .',\n  'line_number': 1,\n  'total_lines': 10}]</pre> <p>Fantastic! Looks like our <code>preprocess_text_with_line_numbers()</code> function worked great.</p> <p>How about we turn our list of dictionaries into pandas DataFrame's so we visualize them better?</p> In\u00a0[12]: Copied! <pre>import pandas as pd\ntrain_df = pd.DataFrame(train_samples)\nval_df = pd.DataFrame(val_samples)\ntest_df = pd.DataFrame(test_samples)\ntrain_df.head(14)\n</pre> import pandas as pd train_df = pd.DataFrame(train_samples) val_df = pd.DataFrame(val_samples) test_df = pd.DataFrame(test_samples) train_df.head(14) Out[12]: target text line_number total_lines 0 OBJECTIVE to investigate the efficacy of @ weeks of dail... 0 11 1 METHODS a total of @ patients with primary knee oa wer... 1 11 2 METHODS outcome measures included pain reduction and i... 2 11 3 METHODS pain was assessed using the visual analog pain... 3 11 4 METHODS secondary outcome measures included the wester... 4 11 5 METHODS serum levels of interleukin @ ( il-@ ) , il-@ ... 5 11 6 RESULTS there was a clinically relevant reduction in t... 6 11 7 RESULTS the mean difference between treatment arms ( @... 7 11 8 RESULTS further , there was a clinically relevant redu... 8 11 9 RESULTS these differences remained significant at @ we... 9 11 10 RESULTS the outcome measures in rheumatology clinical ... 10 11 11 CONCLUSIONS low-dose oral prednisolone had both a short-te... 11 11 12 BACKGROUND emotional eating is associated with overeating... 0 10 13 BACKGROUND yet , empirical evidence for individual ( trai... 1 10 <p>Now our data is in DataFrame form, we can perform some data analysis on it.</p> In\u00a0[13]: Copied! <pre># Distribution of labels in training data\ntrain_df.target.value_counts()\n</pre> # Distribution of labels in training data train_df.target.value_counts() Out[13]: <pre>METHODS        59353\nRESULTS        57953\nCONCLUSIONS    27168\nBACKGROUND     21727\nOBJECTIVE      13839\nName: target, dtype: int64</pre> <p>Looks like sentences with the <code>OBJECTIVE</code> label are the least common.</p> <p>How about we check the distribution of our abstract lengths?</p> In\u00a0[14]: Copied! <pre>train_df.total_lines.plot.hist();\n</pre> train_df.total_lines.plot.hist(); <p>Okay, looks like most of the abstracts are around 7 to 15 sentences in length.</p> <p>It's good to check these things out to make sure when we do train a model or test it on unseen samples, our results aren't outlandish.</p> In\u00a0[15]: Copied! <pre># Convert abstract text lines into lists \ntrain_sentences = train_df[\"text\"].tolist()\nval_sentences = val_df[\"text\"].tolist()\ntest_sentences = test_df[\"text\"].tolist()\nlen(train_sentences), len(val_sentences), len(test_sentences)\n</pre> # Convert abstract text lines into lists  train_sentences = train_df[\"text\"].tolist() val_sentences = val_df[\"text\"].tolist() test_sentences = test_df[\"text\"].tolist() len(train_sentences), len(val_sentences), len(test_sentences) Out[15]: <pre>(180040, 30212, 30135)</pre> In\u00a0[16]: Copied! <pre># View first 10 lines of training sentences\ntrain_sentences[:10]\n</pre> # View first 10 lines of training sentences train_sentences[:10] Out[16]: <pre>['to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n 'outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .',\n 'pain was assessed using the visual analog pain scale ( @-@ mm ) .',\n 'secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .',\n 'serum levels of interleukin @ ( il-@ ) , il-@ , tumor necrosis factor ( tnf ) - , and high-sensitivity c-reactive protein ( hscrp ) were measured .',\n 'there was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , pga , and @mwd at @ weeks .',\n 'the mean difference between treatment arms ( @ % ci ) was @ ( @-@ @ ) , p &lt; @ ; @ ( @-@ @ ) , p &lt; @ ; @ ( @-@ @ ) , p &lt; @ ; and @ ( @-@ @ ) , p &lt; @ , respectively .',\n 'further , there was a clinically relevant reduction in the serum levels of il-@ , il-@ , tnf - , and hscrp at @ weeks in the intervention group when compared to the placebo group .',\n 'these differences remained significant at @ weeks .']</pre> <p>Alright, we've separated our text samples. As you might've guessed, we'll have to write code to convert the text to numbers before we can use it with our machine learning models, we'll get to this soon.</p> In\u00a0[17]: Copied! <pre># One hot encode labels\nfrom sklearn.preprocessing import OneHotEncoder\none_hot_encoder = OneHotEncoder(sparse=False)\ntrain_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1, 1))\nval_labels_one_hot = one_hot_encoder.transform(val_df[\"target\"].to_numpy().reshape(-1, 1))\ntest_labels_one_hot = one_hot_encoder.transform(test_df[\"target\"].to_numpy().reshape(-1, 1))\n\n# Check what training labels look like\ntrain_labels_one_hot\n</pre> # One hot encode labels from sklearn.preprocessing import OneHotEncoder one_hot_encoder = OneHotEncoder(sparse=False) train_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1, 1)) val_labels_one_hot = one_hot_encoder.transform(val_df[\"target\"].to_numpy().reshape(-1, 1)) test_labels_one_hot = one_hot_encoder.transform(test_df[\"target\"].to_numpy().reshape(-1, 1))  # Check what training labels look like train_labels_one_hot <pre>/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n</pre> Out[17]: <pre>array([[0., 0., 0., 1., 0.],\n       [0., 0., 1., 0., 0.],\n       [0., 0., 1., 0., 0.],\n       ...,\n       [0., 0., 0., 0., 1.],\n       [0., 1., 0., 0., 0.],\n       [0., 1., 0., 0., 0.]])</pre> In\u00a0[18]: Copied! <pre># Extract labels (\"target\" columns) and encode them into integers \nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ntrain_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy())\nval_labels_encoded = label_encoder.transform(val_df[\"target\"].to_numpy())\ntest_labels_encoded = label_encoder.transform(test_df[\"target\"].to_numpy())\n\n# Check what training labels look like\ntrain_labels_encoded\n</pre> # Extract labels (\"target\" columns) and encode them into integers  from sklearn.preprocessing import LabelEncoder label_encoder = LabelEncoder() train_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy()) val_labels_encoded = label_encoder.transform(val_df[\"target\"].to_numpy()) test_labels_encoded = label_encoder.transform(test_df[\"target\"].to_numpy())  # Check what training labels look like train_labels_encoded Out[18]: <pre>array([3, 2, 2, ..., 4, 1, 1])</pre> <p>Now we've trained an instance of <code>LabelEncoder</code>, we can get the class names and number of classes using the <code>classes_</code> attribute.</p> In\u00a0[19]: Copied! <pre># Get class names and number of classes from LabelEncoder instance \nnum_classes = len(label_encoder.classes_)\nclass_names = label_encoder.classes_\nnum_classes, class_names\n</pre> # Get class names and number of classes from LabelEncoder instance  num_classes = len(label_encoder.classes_) class_names = label_encoder.classes_ num_classes, class_names Out[19]: <pre>(5,\n array(['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'],\n       dtype=object))</pre> In\u00a0[20]: Copied! <pre>from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\n\n# Create a pipeline\nmodel_0 = Pipeline([\n  (\"tf-idf\", TfidfVectorizer()),\n  (\"clf\", MultinomialNB())\n])\n\n# Fit the pipeline to the training data\nmodel_0.fit(X=train_sentences, \n            y=train_labels_encoded);\n</pre> from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.naive_bayes import MultinomialNB from sklearn.pipeline import Pipeline  # Create a pipeline model_0 = Pipeline([   (\"tf-idf\", TfidfVectorizer()),   (\"clf\", MultinomialNB()) ])  # Fit the pipeline to the training data model_0.fit(X=train_sentences,              y=train_labels_encoded); <p>Due to the speed of the Multinomial Naive Bayes algorithm, it trains very quickly.</p> <p>We can evaluate our model's accuracy on the validation dataset using the <code>score()</code> method.</p> In\u00a0[21]: Copied! <pre># Evaluate baseline on validation dataset\nmodel_0.score(X=val_sentences,\n              y=val_labels_encoded)\n</pre> # Evaluate baseline on validation dataset model_0.score(X=val_sentences,               y=val_labels_encoded) Out[21]: <pre>0.7218323844829869</pre> <p>Nice! Looks like 72.1% accuracy will be the number to beat with our deeper models.</p> <p>Now let's make some predictions with our baseline model to further evaluate it.</p> In\u00a0[22]: Copied! <pre># Make predictions\nbaseline_preds = model_0.predict(val_sentences)\nbaseline_preds\n</pre> # Make predictions baseline_preds = model_0.predict(val_sentences) baseline_preds Out[22]: <pre>array([4, 1, 3, ..., 4, 4, 1])</pre> <p>To evaluate our baseline's predictions, we'll import the <code>calculate_results()</code> function we created in the previous notebook and added it to our <code>helper_functions.py</code> script to compare them to the ground truth labels.</p> <p>More specificially the <code>calculate_results()</code> function will help us obtain the following:</p> <ul> <li>Accuracy</li> <li>Precision</li> <li>Recall</li> <li>F1-score</li> </ul> In\u00a0[23]: Copied! <pre># Download helper functions script\n!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n</pre> # Download helper functions script !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py <pre>--2023-05-26 03:47:55--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 10246 (10K) [text/plain]\nSaving to: \u2018helper_functions.py\u2019\n\n\rhelper_functions.py   0%[                    ]       0  --.-KB/s               \rhelper_functions.py 100%[===================&gt;]  10.01K  --.-KB/s    in 0s      \n\n2023-05-26 03:47:55 (118 MB/s) - \u2018helper_functions.py\u2019 saved [10246/10246]\n\n</pre> <p>Now we've got the helper functions script we can import the <code>caculate_results()</code> function and see how our baseline model went.</p> In\u00a0[24]: Copied! <pre># Import calculate_results helper function\nfrom helper_functions import calculate_results\n</pre> # Import calculate_results helper function from helper_functions import calculate_results In\u00a0[25]: Copied! <pre># Calculate baseline results\nbaseline_results = calculate_results(y_true=val_labels_encoded,\n                                     y_pred=baseline_preds)\nbaseline_results\n</pre> # Calculate baseline results baseline_results = calculate_results(y_true=val_labels_encoded,                                      y_pred=baseline_preds) baseline_results Out[25]: <pre>{'accuracy': 72.1832384482987,\n 'precision': 0.7186466952323352,\n 'recall': 0.7218323844829869,\n 'f1': 0.6989250353450294}</pre> In\u00a0[26]: Copied! <pre>import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n</pre> import numpy as np import tensorflow as tf from tensorflow.keras import layers <p>Since we'll be turning our sentences into numbers, it's a good idea to figure out how many words are in each sentence.</p> <p>When our model goes through our sentences, it works best when they're all the same length (this is important for creating batches of the same size tensors).</p> <p>For example, if one sentence is eight words long and another is 29 words long, we want to pad the eight word sentence with zeros so it ends up being the same length as the 29 word sentence.</p> <p>Let's write some code to find the average length of sentences in the training set.</p> In\u00a0[27]: Copied! <pre># How long is each sentence on average?\nsent_lens = [len(sentence.split()) for sentence in train_sentences]\navg_sent_len = np.mean(sent_lens)\navg_sent_len # return average sentence length (in tokens)\n</pre> # How long is each sentence on average? sent_lens = [len(sentence.split()) for sentence in train_sentences] avg_sent_len = np.mean(sent_lens) avg_sent_len # return average sentence length (in tokens) Out[27]: <pre>26.338269273494777</pre> <p>How about the distribution of sentence lengths?</p> In\u00a0[28]: Copied! <pre># What's the distribution look like?\nimport matplotlib.pyplot as plt\nplt.hist(sent_lens, bins=7);\n</pre> # What's the distribution look like? import matplotlib.pyplot as plt plt.hist(sent_lens, bins=7); <p>Looks like the vast majority of sentences are between 0 and 50 tokens in length.</p> <p>We can use NumPy's <code>percentile</code> to find the value which covers 95% of the sentence lengths.</p> In\u00a0[29]: Copied! <pre># How long of a sentence covers 95% of the lengths?\noutput_seq_len = int(np.percentile(sent_lens, 95))\noutput_seq_len\n</pre> # How long of a sentence covers 95% of the lengths? output_seq_len = int(np.percentile(sent_lens, 95)) output_seq_len Out[29]: <pre>55</pre> <p>Wonderful! It looks like 95% of the sentences in our training set have a length of 55 tokens or less.</p> <p>When we create our tokenization layer, we'll use this value to turn all of our sentences into the same length. Meaning sentences with a length below 55 get padded with zeros and sentences with a length above 55 get truncated (words after 55 get cut off).</p> <p>\ud83e\udd14 Question: Why 95%?</p> <p>We could use the max sentence length of the sentences in the training set.</p> In\u00a0[30]: Copied! <pre># Maximum sentence length in the training set\nmax(sent_lens)\n</pre> # Maximum sentence length in the training set max(sent_lens) Out[30]: <pre>296</pre> <p>However, since hardly any sentences even come close to the max length, it would mean the majority of the data we pass to our model would be zeros (sinces all sentences below the max length would get padded with zeros).</p> <p>\ud83d\udd11 Note: The steps we've gone through are good practice when working with a text corpus for a NLP problem. You want to know how long your samples are and what the distribution of them is. See section 4 Data Analysis of the PubMed 200k RCT paper for further examples.</p> In\u00a0[31]: Copied! <pre># How many words are in our vocabulary? (taken from 3.2 in https://arxiv.org/pdf/1710.06071.pdf)\nmax_tokens = 68000\n</pre> # How many words are in our vocabulary? (taken from 3.2 in https://arxiv.org/pdf/1710.06071.pdf) max_tokens = 68000 <p>And since discovered a sentence length of 55 covers 95% of the training sentences, we'll use that as our <code>output_sequence_length</code> parameter.</p> In\u00a0[32]: Copied! <pre># Create text vectorizer\n\n# After TensorFlow 2.6\nfrom tensorflow.keras.layers import TextVectorization\n\n# Before TensorFlow 2.6\n# from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n\ntext_vectorizer = TextVectorization(max_tokens=max_tokens, # number of words in vocabulary\n                                    output_sequence_length=55) # desired output length of vectorized sequences\n</pre> # Create text vectorizer  # After TensorFlow 2.6 from tensorflow.keras.layers import TextVectorization  # Before TensorFlow 2.6 # from tensorflow.keras.layers.experimental.preprocessing import TextVectorization  text_vectorizer = TextVectorization(max_tokens=max_tokens, # number of words in vocabulary                                     output_sequence_length=55) # desired output length of vectorized sequences <p>Great! Looks like our <code>text_vectorizer</code> is ready, let's adapt it to the training data (let it read the training data and figure out what number should represent what word) and then test it out.</p> In\u00a0[33]: Copied! <pre># Adapt text vectorizer to training sentences\ntext_vectorizer.adapt(train_sentences)\n</pre> # Adapt text vectorizer to training sentences text_vectorizer.adapt(train_sentences) In\u00a0[34]: Copied! <pre># Test out text vectorizer\nimport random\ntarget_sentence = random.choice(train_sentences)\nprint(f\"Text:\\n{target_sentence}\")\nprint(f\"\\nLength of text: {len(target_sentence.split())}\")\nprint(f\"\\nVectorized text:\\n{text_vectorizer([target_sentence])}\")\n</pre> # Test out text vectorizer import random target_sentence = random.choice(train_sentences) print(f\"Text:\\n{target_sentence}\") print(f\"\\nLength of text: {len(target_sentence.split())}\") print(f\"\\nVectorized text:\\n{text_vectorizer([target_sentence])}\") <pre>Text:\npatients later completed a ten-question true-or-false knowledge questionnaire and a six-item satisfaction survey .\n\nLength of text: 14\n\nVectorized text:\n[[   12   869   253     8 44042 43177   494   325     3     8 34340   428\n    885     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0]]\n</pre> <p>Cool, we've now got a way to turn our sequences into numbers.</p> <p>\ud83d\udee0 Exercise: Try running the cell above a dozen or so times. What do you notice about sequences with a length less than 55?</p> <p>Using the <code>get_vocabulary()</code> method of our <code>text_vectorizer</code> we can find out a few different tidbits about our text.</p> In\u00a0[35]: Copied! <pre># How many words in our training vocabulary?\nrct_20k_text_vocab = text_vectorizer.get_vocabulary()\nprint(f\"Number of words in vocabulary: {len(rct_20k_text_vocab)}\"), \nprint(f\"Most common words in the vocabulary: {rct_20k_text_vocab[:5]}\")\nprint(f\"Least common words in the vocabulary: {rct_20k_text_vocab[-5:]}\")\n</pre> # How many words in our training vocabulary? rct_20k_text_vocab = text_vectorizer.get_vocabulary() print(f\"Number of words in vocabulary: {len(rct_20k_text_vocab)}\"),  print(f\"Most common words in the vocabulary: {rct_20k_text_vocab[:5]}\") print(f\"Least common words in the vocabulary: {rct_20k_text_vocab[-5:]}\") <pre>Number of words in vocabulary: 64841\nMost common words in the vocabulary: ['', '[UNK]', 'the', 'and', 'of']\nLeast common words in the vocabulary: ['aainduced', 'aaigroup', 'aachener', 'aachen', 'aaacp']\n</pre> <p>And if we wanted to figure out the configuration of our <code>text_vectorizer</code> we can use the <code>get_config()</code> method.</p> In\u00a0[36]: Copied! <pre># Get the config of our text vectorizer\ntext_vectorizer.get_config()\n</pre> # Get the config of our text vectorizer text_vectorizer.get_config() Out[36]: <pre>{'name': 'text_vectorization',\n 'trainable': True,\n 'dtype': 'string',\n 'batch_input_shape': (None,),\n 'max_tokens': 68000,\n 'standardize': 'lower_and_strip_punctuation',\n 'split': 'whitespace',\n 'ngrams': None,\n 'output_mode': 'int',\n 'output_sequence_length': 55,\n 'pad_to_max_tokens': False,\n 'sparse': False,\n 'ragged': False,\n 'vocabulary': None,\n 'idf_weights': None,\n 'encoding': 'utf-8',\n 'vocabulary_size': 64841}</pre> In\u00a0[37]: Copied! <pre># Create token embedding layer\ntoken_embed = layers.Embedding(input_dim=len(rct_20k_text_vocab), # length of vocabulary\n                               output_dim=128, # Note: different embedding sizes result in drastically different numbers of parameters to train\n                               # Use masking to handle variable sequence lengths (save space)\n                               mask_zero=True,\n                               name=\"token_embedding\") \n\n# Show example embedding\nprint(f\"Sentence before vectorization:\\n{target_sentence}\\n\")\nvectorized_sentence = text_vectorizer([target_sentence])\nprint(f\"Sentence after vectorization (before embedding):\\n{vectorized_sentence}\\n\")\nembedded_sentence = token_embed(vectorized_sentence)\nprint(f\"Sentence after embedding:\\n{embedded_sentence}\\n\")\nprint(f\"Embedded sentence shape: {embedded_sentence.shape}\")\n</pre> # Create token embedding layer token_embed = layers.Embedding(input_dim=len(rct_20k_text_vocab), # length of vocabulary                                output_dim=128, # Note: different embedding sizes result in drastically different numbers of parameters to train                                # Use masking to handle variable sequence lengths (save space)                                mask_zero=True,                                name=\"token_embedding\")   # Show example embedding print(f\"Sentence before vectorization:\\n{target_sentence}\\n\") vectorized_sentence = text_vectorizer([target_sentence]) print(f\"Sentence after vectorization (before embedding):\\n{vectorized_sentence}\\n\") embedded_sentence = token_embed(vectorized_sentence) print(f\"Sentence after embedding:\\n{embedded_sentence}\\n\") print(f\"Embedded sentence shape: {embedded_sentence.shape}\") <pre>Sentence before vectorization:\npatients later completed a ten-question true-or-false knowledge questionnaire and a six-item satisfaction survey .\n\nSentence after vectorization (before embedding):\n[[   12   869   253     8 44042 43177   494   325     3     8 34340   428\n    885     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0]]\n\nSentence after embedding:\n[[[ 0.03009653 -0.00744227  0.03255219 ...  0.04350514  0.02615802\n    0.01004974]\n  [ 0.00894579 -0.04471136  0.04325384 ...  0.00689     0.03921053\n   -0.01831497]\n  [ 0.0371519   0.01963044 -0.0322178  ...  0.01794907 -0.00109234\n    0.02466574]\n  ...\n  [ 0.03691586 -0.00786442 -0.04396362 ...  0.04501832  0.00408162\n   -0.02123723]\n  [ 0.03691586 -0.00786442 -0.04396362 ...  0.04501832  0.00408162\n   -0.02123723]\n  [ 0.03691586 -0.00786442 -0.04396362 ...  0.04501832  0.00408162\n   -0.02123723]]]\n\nEmbedded sentence shape: (1, 55, 128)\n</pre> In\u00a0[38]: Copied! <pre># Turn our data into TensorFlow Datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot))\nvalid_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot))\ntest_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_one_hot))\n\ntrain_dataset\n</pre> # Turn our data into TensorFlow Datasets train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot)) valid_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot)) test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_one_hot))  train_dataset Out[38]: <pre>&lt;_TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(5,), dtype=tf.float64, name=None))&gt;</pre> In\u00a0[39]: Copied! <pre># Take the TensorSliceDataset's and turn them into prefetched batches\ntrain_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\nvalid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\ntest_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n\ntrain_dataset\n</pre> # Take the TensorSliceDataset's and turn them into prefetched batches train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE) valid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE) test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)  train_dataset Out[39]: <pre>&lt;_PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))&gt;</pre> In\u00a0[40]: Copied! <pre># Create 1D convolutional model to process sequences\ninputs = layers.Input(shape=(1,), dtype=tf.string)\ntext_vectors = text_vectorizer(inputs) # vectorize text inputs\ntoken_embeddings = token_embed(text_vectors) # create embedding\nx = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(token_embeddings)\nx = layers.GlobalAveragePooling1D()(x) # condense the output of our feature vector\noutputs = layers.Dense(num_classes, activation=\"softmax\")(x)\nmodel_1 = tf.keras.Model(inputs, outputs)\n\n# Compile\nmodel_1.compile(loss=\"categorical_crossentropy\", # if your labels are integer form (not one hot) use sparse_categorical_crossentropy\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])\n</pre> # Create 1D convolutional model to process sequences inputs = layers.Input(shape=(1,), dtype=tf.string) text_vectors = text_vectorizer(inputs) # vectorize text inputs token_embeddings = token_embed(text_vectors) # create embedding x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(token_embeddings) x = layers.GlobalAveragePooling1D()(x) # condense the output of our feature vector outputs = layers.Dense(num_classes, activation=\"softmax\")(x) model_1 = tf.keras.Model(inputs, outputs)  # Compile model_1.compile(loss=\"categorical_crossentropy\", # if your labels are integer form (not one hot) use sparse_categorical_crossentropy                 optimizer=tf.keras.optimizers.Adam(),                 metrics=[\"accuracy\"]) In\u00a0[41]: Copied! <pre># Get summary of Conv1D model\nmodel_1.summary()\n</pre> # Get summary of Conv1D model model_1.summary() <pre>Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 1)]               0         \n                                                                 \n text_vectorization (TextVec  (None, 55)               0         \n torization)                                                     \n                                                                 \n token_embedding (Embedding)  (None, 55, 128)          8299648   \n                                                                 \n conv1d (Conv1D)             (None, 55, 64)            41024     \n                                                                 \n global_average_pooling1d (G  (None, 64)               0         \n lobalAveragePooling1D)                                          \n                                                                 \n dense (Dense)               (None, 5)                 325       \n                                                                 \n=================================================================\nTotal params: 8,340,997\nTrainable params: 8,340,997\nNon-trainable params: 0\n_________________________________________________________________\n</pre> <p>Wonderful! We've got our first deep sequence model built and ready to go.</p> <p>Checking out the model summary, you'll notice the majority of the trainable parameters are within the embedding layer. If we were to increase the size of the embedding (by increasing the <code>output_dim</code> parameter of the <code>Embedding</code> layer), the number of trainable parameters would increase dramatically.</p> <p>It's time to fit our model to the training data but we're going to make a mindful change.</p> <p>Since our training data contains nearly 200,000 sentences, fitting a deep model may take a while even with a GPU. So to keep our experiments swift, we're going to run them on a subset of the training dataset.</p> <p>More specifically, we'll only use the first 10% of batches (about 18,000 samples) of the training set to train on and the first 10% of batches from the validation set to validate on.</p> <p>\ud83d\udd11 Note: It's a standard practice in machine learning to test your models on smaller subsets of data first to make sure they work before scaling them to larger amounts of data. You should aim to run many smaller experiments rather than only a handful of large experiments. And since your time is limited, one of the best ways to run smaller experiments is to reduce the amount of data you're working with (10% of the full dataset is usually a good amount, as long as it covers a similar distribution).</p> In\u00a0[42]: Copied! <pre># Fit the model\nmodel_1_history = model_1.fit(train_dataset,\n                              steps_per_epoch=int(0.1 * len(train_dataset)), # only fit on 10% of batches for faster training time\n                              epochs=3,\n                              validation_data=valid_dataset,\n                              validation_steps=int(0.1 * len(valid_dataset))) # only validate on 10% of batches\n</pre> # Fit the model model_1_history = model_1.fit(train_dataset,                               steps_per_epoch=int(0.1 * len(train_dataset)), # only fit on 10% of batches for faster training time                               epochs=3,                               validation_data=valid_dataset,                               validation_steps=int(0.1 * len(valid_dataset))) # only validate on 10% of batches <pre>Epoch 1/3\n562/562 [==============================] - 43s 58ms/step - loss: 0.9185 - accuracy: 0.6350 - val_loss: 0.6871 - val_accuracy: 0.7380\nEpoch 2/3\n562/562 [==============================] - 7s 12ms/step - loss: 0.6603 - accuracy: 0.7560 - val_loss: 0.6322 - val_accuracy: 0.7696\nEpoch 3/3\n562/562 [==============================] - 4s 7ms/step - loss: 0.6194 - accuracy: 0.7724 - val_loss: 0.5967 - val_accuracy: 0.7812\n</pre> <p>Brilliant! We've got our first trained deep sequence model, and it didn't take too long (and if we didn't prefetch our batched data, it would've taken longer).</p> <p>Time to make some predictions with our model and then evaluate them.</p> In\u00a0[43]: Copied! <pre># Evaluate on whole validation dataset (we only validated on 10% of batches during training)\nmodel_1.evaluate(valid_dataset)\n</pre> # Evaluate on whole validation dataset (we only validated on 10% of batches during training) model_1.evaluate(valid_dataset) <pre>945/945 [==============================] - 2s 3ms/step - loss: 0.5993 - accuracy: 0.7856\n</pre> Out[43]: <pre>[0.5992922186851501, 0.7856481075286865]</pre> In\u00a0[44]: Copied! <pre># Make predictions (our model outputs prediction probabilities for each class)\nmodel_1_pred_probs = model_1.predict(valid_dataset)\nmodel_1_pred_probs\n</pre> # Make predictions (our model outputs prediction probabilities for each class) model_1_pred_probs = model_1.predict(valid_dataset) model_1_pred_probs <pre>945/945 [==============================] - 2s 2ms/step\n</pre> Out[44]: <pre>array([[4.2276028e-01, 1.9543630e-01, 7.8329526e-02, 2.7373073e-01,\n        2.9743196e-02],\n       [4.6485636e-01, 2.7058390e-01, 1.1763435e-02, 2.4558942e-01,\n        7.2069145e-03],\n       [1.4821406e-01, 4.6900036e-03, 1.9654778e-03, 8.4511346e-01,\n        1.7043541e-05],\n       ...,\n       [4.6908235e-06, 6.0919154e-04, 7.0814672e-04, 2.9213641e-06,\n        9.9867505e-01],\n       [4.5269396e-02, 4.8612341e-01, 8.2452036e-02, 5.5686332e-02,\n        3.3046886e-01],\n       [1.8509193e-01, 6.2405616e-01, 5.6467425e-02, 5.4003950e-02,\n        8.0380574e-02]], dtype=float32)</pre> In\u00a0[45]: Copied! <pre># Convert pred probs to classes\nmodel_1_preds = tf.argmax(model_1_pred_probs, axis=1)\nmodel_1_preds\n</pre> # Convert pred probs to classes model_1_preds = tf.argmax(model_1_pred_probs, axis=1) model_1_preds Out[45]: <pre>&lt;tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 0, 3, ..., 4, 1, 1])&gt;</pre> In\u00a0[46]: Copied! <pre># Calculate model_1 results\nmodel_1_results = calculate_results(y_true=val_labels_encoded,\n                                    y_pred=model_1_preds)\nmodel_1_results\n</pre> # Calculate model_1 results model_1_results = calculate_results(y_true=val_labels_encoded,                                     y_pred=model_1_preds) model_1_results Out[46]: <pre>{'accuracy': 78.56480868529061,\n 'precision': 0.7824092300813684,\n 'recall': 0.7856480868529061,\n 'f1': 0.7829939778477344}</pre> In\u00a0[47]: Copied! <pre># Download pretrained TensorFlow Hub USE\nimport tensorflow_hub as hub\ntf_hub_embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n                                        trainable=False,\n                                        name=\"universal_sentence_encoder\")\n</pre> # Download pretrained TensorFlow Hub USE import tensorflow_hub as hub tf_hub_embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",                                         trainable=False,                                         name=\"universal_sentence_encoder\") <p>Beautiful, now our pretrained USE is downloaded and instantiated as a <code>hub.KerasLayer</code> instance, let's test it out on a random sentence.</p> In\u00a0[48]: Copied! <pre># Test out the embedding on a random sentence\nrandom_training_sentence = random.choice(train_sentences)\nprint(f\"Random training sentence:\\n{random_training_sentence}\\n\")\nuse_embedded_sentence = tf_hub_embedding_layer([random_training_sentence])\nprint(f\"Sentence after embedding:\\n{use_embedded_sentence[0][:30]} (truncated output)...\\n\")\nprint(f\"Length of sentence embedding:\\n{len(use_embedded_sentence[0])}\")\n</pre> # Test out the embedding on a random sentence random_training_sentence = random.choice(train_sentences) print(f\"Random training sentence:\\n{random_training_sentence}\\n\") use_embedded_sentence = tf_hub_embedding_layer([random_training_sentence]) print(f\"Sentence after embedding:\\n{use_embedded_sentence[0][:30]} (truncated output)...\\n\") print(f\"Length of sentence embedding:\\n{len(use_embedded_sentence[0])}\") <pre>Random training sentence:\nour data justify the need for personalized integrated antioxidant and energy correction therapy .\n\nSentence after embedding:\n[-0.03416213 -0.05093268 -0.07358423  0.04783727 -0.05375864  0.07531483\n -0.0618328  -0.00023192  0.05290754  0.00731164  0.02909756 -0.03293613\n -0.05592315  0.02519475 -0.08728898 -0.02306924 -0.06509812  0.01185254\n -0.05143833 -0.01459878  0.03741732  0.05964429  0.03533797  0.05038223\n  0.00830391 -0.05242855  0.05673083 -0.06287023  0.08674113  0.06334175] (truncated output)...\n\nLength of sentence embedding:\n512\n</pre> <p>Nice! As we mentioned before the pretrained USE module from TensorFlow Hub takes care of tokenizing our text for us and outputs a 512 dimensional embedding vector.</p> <p>Let's put together and compile a model using our <code>tf_hub_embedding_layer</code>.</p> In\u00a0[49]: Copied! <pre># Define feature extractor model using TF Hub layer\ninputs = layers.Input(shape=[], dtype=tf.string)\npretrained_embedding = tf_hub_embedding_layer(inputs) # tokenize text and create embedding\nx = layers.Dense(128, activation=\"relu\")(pretrained_embedding) # add a fully connected layer on top of the embedding\n# Note: you could add more layers here if you wanted to\noutputs = layers.Dense(5, activation=\"softmax\")(x) # create the output layer\nmodel_2 = tf.keras.Model(inputs=inputs,\n                        outputs=outputs)\n\n# Compile the model\nmodel_2.compile(loss=\"categorical_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])\n</pre> # Define feature extractor model using TF Hub layer inputs = layers.Input(shape=[], dtype=tf.string) pretrained_embedding = tf_hub_embedding_layer(inputs) # tokenize text and create embedding x = layers.Dense(128, activation=\"relu\")(pretrained_embedding) # add a fully connected layer on top of the embedding # Note: you could add more layers here if you wanted to outputs = layers.Dense(5, activation=\"softmax\")(x) # create the output layer model_2 = tf.keras.Model(inputs=inputs,                         outputs=outputs)  # Compile the model model_2.compile(loss=\"categorical_crossentropy\",                 optimizer=tf.keras.optimizers.Adam(),                 metrics=[\"accuracy\"]) In\u00a0[50]: Copied! <pre># Get a summary of the model\nmodel_2.summary()\n</pre> # Get a summary of the model model_2.summary() <pre>Model: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None,)]                 0         \n                                                                 \n universal_sentence_encoder   (None, 512)              256797824 \n (KerasLayer)                                                    \n                                                                 \n dense_1 (Dense)             (None, 128)               65664     \n                                                                 \n dense_2 (Dense)             (None, 5)                 645       \n                                                                 \n=================================================================\nTotal params: 256,864,133\nTrainable params: 66,309\nNon-trainable params: 256,797,824\n_________________________________________________________________\n</pre> <p>Checking the summary of our model we can see there's a large number of total parameters, however, the majority of these are non-trainable. This is because we set <code>training=False</code> when we instatiated our USE feature extractor layer.</p> <p>So when we train our model, only the top two output layers will be trained.</p> In\u00a0[51]: Copied! <pre># Fit feature extractor model for 3 epochs\nmodel_2.fit(train_dataset,\n            steps_per_epoch=int(0.1 * len(train_dataset)),\n            epochs=3,\n            validation_data=valid_dataset,\n            validation_steps=int(0.1 * len(valid_dataset)))\n</pre> # Fit feature extractor model for 3 epochs model_2.fit(train_dataset,             steps_per_epoch=int(0.1 * len(train_dataset)),             epochs=3,             validation_data=valid_dataset,             validation_steps=int(0.1 * len(valid_dataset))) <pre>Epoch 1/3\n562/562 [==============================] - 9s 11ms/step - loss: 0.9207 - accuracy: 0.6496 - val_loss: 0.7936 - val_accuracy: 0.6908\nEpoch 2/3\n562/562 [==============================] - 6s 10ms/step - loss: 0.7659 - accuracy: 0.7047 - val_loss: 0.7499 - val_accuracy: 0.7074\nEpoch 3/3\n562/562 [==============================] - 6s 11ms/step - loss: 0.7471 - accuracy: 0.7145 - val_loss: 0.7328 - val_accuracy: 0.7178\n</pre> Out[51]: <pre>&lt;keras.callbacks.History at 0x7efb005bdff0&gt;</pre> In\u00a0[52]: Copied! <pre># Evaluate on whole validation dataset\nmodel_2.evaluate(valid_dataset)\n</pre> # Evaluate on whole validation dataset model_2.evaluate(valid_dataset) <pre>945/945 [==============================] - 8s 8ms/step - loss: 0.7349 - accuracy: 0.7161\n</pre> Out[52]: <pre>[0.7349329590797424, 0.7161061763763428]</pre> <p>Since we aren't training our own custom embedding layer, training is much quicker.</p> <p>Let's make some predictions and evaluate our feature extraction model.</p> In\u00a0[53]: Copied! <pre># Make predictions with feature extraction model\nmodel_2_pred_probs = model_2.predict(valid_dataset)\nmodel_2_pred_probs\n</pre> # Make predictions with feature extraction model model_2_pred_probs = model_2.predict(valid_dataset) model_2_pred_probs <pre>945/945 [==============================] - 8s 8ms/step\n</pre> Out[53]: <pre>array([[4.1435534e-01, 3.6443055e-01, 2.5627650e-03, 2.1054386e-01,\n        8.1075458e-03],\n       [3.4644338e-01, 4.7939345e-01, 4.0907143e-03, 1.6641928e-01,\n        3.6531438e-03],\n       [2.3935708e-01, 1.6314813e-01, 1.9130943e-02, 5.4523116e-01,\n        3.3132695e-02],\n       ...,\n       [2.2427505e-03, 5.5871238e-03, 4.6554163e-02, 7.6473231e-04,\n        9.4485116e-01],\n       [3.8972907e-03, 4.4690356e-02, 2.0484674e-01, 1.4869794e-03,\n        7.4507862e-01],\n       [1.5384717e-01, 2.5815260e-01, 5.2709770e-01, 7.3189661e-03,\n        5.3583570e-02]], dtype=float32)</pre> In\u00a0[54]: Copied! <pre># Convert the predictions with feature extraction model to classes\nmodel_2_preds = tf.argmax(model_2_pred_probs, axis=1)\nmodel_2_preds\n</pre> # Convert the predictions with feature extraction model to classes model_2_preds = tf.argmax(model_2_pred_probs, axis=1) model_2_preds Out[54]: <pre>&lt;tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 1, 3, ..., 4, 4, 2])&gt;</pre> In\u00a0[55]: Copied! <pre># Calculate results from TF Hub pretrained embeddings results on validation set\nmodel_2_results = calculate_results(y_true=val_labels_encoded,\n                                    y_pred=model_2_preds)\nmodel_2_results\n</pre> # Calculate results from TF Hub pretrained embeddings results on validation set model_2_results = calculate_results(y_true=val_labels_encoded,                                     y_pred=model_2_preds) model_2_results Out[55]: <pre>{'accuracy': 71.61061829736528,\n 'precision': 0.7161610082576743,\n 'recall': 0.7161061829736528,\n 'f1': 0.7128311588798425}</pre> In\u00a0[56]: Copied! <pre># Make function to split sentences into characters\ndef split_chars(text):\n  return \" \".join(list(text))\n\n# Test splitting non-character-level sequence into characters\nsplit_chars(random_training_sentence)\n</pre> # Make function to split sentences into characters def split_chars(text):   return \" \".join(list(text))  # Test splitting non-character-level sequence into characters split_chars(random_training_sentence) Out[56]: <pre>'o u r   d a t a   j u s t i f y   t h e   n e e d   f o r   p e r s o n a l i z e d   i n t e g r a t e d   a n t i o x i d a n t   a n d   e n e r g y   c o r r e c t i o n   t h e r a p y   .'</pre> <p>Great! Looks like our character-splitting function works. Let's create character-level datasets by splitting our sequence datasets into characters.</p> In\u00a0[57]: Copied! <pre># Split sequence-level data splits into character-level data splits\ntrain_chars = [split_chars(sentence) for sentence in train_sentences]\nval_chars = [split_chars(sentence) for sentence in val_sentences]\ntest_chars = [split_chars(sentence) for sentence in test_sentences]\nprint(train_chars[0])\n</pre> # Split sequence-level data splits into character-level data splits train_chars = [split_chars(sentence) for sentence in train_sentences] val_chars = [split_chars(sentence) for sentence in val_sentences] test_chars = [split_chars(sentence) for sentence in test_sentences] print(train_chars[0]) <pre>t o   i n v e s t i g a t e   t h e   e f f i c a c y   o f   @   w e e k s   o f   d a i l y   l o w - d o s e   o r a l   p r e d n i s o l o n e   i n   i m p r o v i n g   p a i n   ,   m o b i l i t y   ,   a n d   s y s t e m i c   l o w - g r a d e   i n f l a m m a t i o n   i n   t h e   s h o r t   t e r m   a n d   w h e t h e r   t h e   e f f e c t   w o u l d   b e   s u s t a i n e d   a t   @   w e e k s   i n   o l d e r   a d u l t s   w i t h   m o d e r a t e   t o   s e v e r e   k n e e   o s t e o a r t h r i t i s   (   o a   )   .\n</pre> <p>To figure out how long our vectorized character sequences should be, let's check the distribution of our character sequence lengths.</p> In\u00a0[58]: Copied! <pre># What's the average character length?\nchar_lens = [len(sentence) for sentence in train_sentences]\nmean_char_len = np.mean(char_lens)\nmean_char_len\n</pre> # What's the average character length? char_lens = [len(sentence) for sentence in train_sentences] mean_char_len = np.mean(char_lens) mean_char_len Out[58]: <pre>149.3662574983337</pre> In\u00a0[59]: Copied! <pre># Check the distribution of our sequences at character-level\nimport matplotlib.pyplot as plt\nplt.hist(char_lens, bins=7);\n</pre> # Check the distribution of our sequences at character-level import matplotlib.pyplot as plt plt.hist(char_lens, bins=7); <p>Okay, looks like most of our sequences are between 0 and 200 characters long.</p> <p>Let's use NumPy's percentile to figure out what length covers 95% of our sequences.</p> In\u00a0[60]: Copied! <pre># Find what character length covers 95% of sequences\noutput_seq_char_len = int(np.percentile(char_lens, 95))\noutput_seq_char_len\n</pre> # Find what character length covers 95% of sequences output_seq_char_len = int(np.percentile(char_lens, 95)) output_seq_char_len Out[60]: <pre>290</pre> <p>Wonderful, now we know the sequence length which covers 95% of sequences, we'll use that in our <code>TextVectorization</code> layer as the <code>output_sequence_length</code> parameter.</p> <p>\ud83d\udd11 Note: You can experiment here to figure out what the optimal <code>output_sequence_length</code> should be, perhaps using the mean results in as good results as using the 95% percentile.</p> <p>We'll set <code>max_tokens</code> (the total number of different characters in our sequences) to 28, in other words, 26 letters of the alphabet + space + OOV (out of vocabulary or unknown) tokens.</p> In\u00a0[61]: Copied! <pre># Get all keyboard characters for char-level embedding\nimport string\nalphabet = string.ascii_lowercase + string.digits + string.punctuation\nalphabet\n</pre> # Get all keyboard characters for char-level embedding import string alphabet = string.ascii_lowercase + string.digits + string.punctuation alphabet Out[61]: <pre>'abcdefghijklmnopqrstuvwxyz0123456789!\"#$%&amp;\\'()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~'</pre> In\u00a0[62]: Copied! <pre># Create char-level token vectorizer instance\nNUM_CHAR_TOKENS = len(alphabet) + 2 # num characters in alphabet + space + OOV token\nchar_vectorizer = TextVectorization(max_tokens=NUM_CHAR_TOKENS,  \n                                    output_sequence_length=output_seq_char_len,\n                                    standardize=\"lower_and_strip_punctuation\",\n                                    name=\"char_vectorizer\")\n\n# Adapt character vectorizer to training characters\nchar_vectorizer.adapt(train_chars)\n</pre> # Create char-level token vectorizer instance NUM_CHAR_TOKENS = len(alphabet) + 2 # num characters in alphabet + space + OOV token char_vectorizer = TextVectorization(max_tokens=NUM_CHAR_TOKENS,                                       output_sequence_length=output_seq_char_len,                                     standardize=\"lower_and_strip_punctuation\",                                     name=\"char_vectorizer\")  # Adapt character vectorizer to training characters char_vectorizer.adapt(train_chars) <p>Nice! Now we've adapted our <code>char_vectorizer</code> to our character-level sequences, let's check out some characteristics about it using the <code>get_vocabulary()</code> method.</p> In\u00a0[63]: Copied! <pre># Check character vocabulary characteristics\nchar_vocab = char_vectorizer.get_vocabulary()\nprint(f\"Number of different characters in character vocab: {len(char_vocab)}\")\nprint(f\"5 most common characters: {char_vocab[:5]}\")\nprint(f\"5 least common characters: {char_vocab[-5:]}\")\n</pre> # Check character vocabulary characteristics char_vocab = char_vectorizer.get_vocabulary() print(f\"Number of different characters in character vocab: {len(char_vocab)}\") print(f\"5 most common characters: {char_vocab[:5]}\") print(f\"5 least common characters: {char_vocab[-5:]}\") <pre>Number of different characters in character vocab: 28\n5 most common characters: ['', '[UNK]', 'e', 't', 'i']\n5 least common characters: ['k', 'x', 'z', 'q', 'j']\n</pre> <p>We can also test it on random sequences of characters to make sure it's working.</p> In\u00a0[64]: Copied! <pre># Test out character vectorizer\nrandom_train_chars = random.choice(train_chars)\nprint(f\"Charified text:\\n{random_train_chars}\")\nprint(f\"\\nLength of chars: {len(random_train_chars.split())}\")\nvectorized_chars = char_vectorizer([random_train_chars])\nprint(f\"\\nVectorized chars:\\n{vectorized_chars}\")\nprint(f\"\\nLength of vectorized chars: {len(vectorized_chars[0])}\")\n</pre> # Test out character vectorizer random_train_chars = random.choice(train_chars) print(f\"Charified text:\\n{random_train_chars}\") print(f\"\\nLength of chars: {len(random_train_chars.split())}\") vectorized_chars = char_vectorizer([random_train_chars]) print(f\"\\nVectorized chars:\\n{vectorized_chars}\") print(f\"\\nLength of vectorized chars: {len(vectorized_chars[0])}\") <pre>Charified text:\ne x a m i n e   t h e   r e l a t i o n s h i p   o f   d e m o g r a p h i c s   a n d   h e a l t h   c o n d i t i o n s   ,   a l o n e   a n d   i n   c o m b i n a t i o n   ,   o n   o b j e c t i v e   m e a s u r e s   o f   c o g n i t i v e   f u n c t i o n   i n   a   l a r g e   s a m p l e   o f   c o m m u n i t y - d w e l l i n g   o l d e r   a d u l t s   .\n\nLength of chars: 162\n\nVectorized chars:\n[[ 2 24  5 15  4  6  2  3 13  2  8  2 12  5  3  4  7  6  9 13  4 14  7 17\n  10  2 15  7 18  8  5 14 13  4 11  9  5  6 10 13  2  5 12  3 13 11  7  6\n  10  4  3  4  7  6  9  5 12  7  6  2  5  6 10  4  6 11  7 15 22  4  6  5\n   3  4  7  6  7  6  7 22 27  2 11  3  4 21  2 15  2  5  9 16  8  2  9  7\n  17 11  7 18  6  4  3  4 21  2 17 16  6 11  3  4  7  6  4  6  5 12  5  8\n  18  2  9  5 15 14 12  2  7 17 11  7 15 15 16  6  4  3 19 10 20  2 12 12\n   4  6 18  7 12 10  2  8  5 10 16 12  3  9  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]]\n\nLength of vectorized chars: 290\n</pre> <p>You'll notice sequences with a length shorter than 290 (<code>output_seq_char_length</code>) get padded with zeros on the end, this ensures all sequences passed to our model are the same length.</p> <p>Also, due to the <code>standardize</code> parameter of <code>TextVectorization</code> being <code>\"lower_and_strip_punctuation\"</code> and the <code>split</code> parameter being <code>\"whitespace\"</code> by default, symbols (such as <code>@</code>) and spaces are removed.</p> <p>\ud83d\udd11 Note: If you didn't want punctuation to be removed (keep the <code>@</code>, <code>%</code> etc), you can create a custom standardization callable and pass it as the <code>standardize</code> parameter. See the <code>TextVectorization</code> layer documentation for more.</p> In\u00a0[65]: Copied! <pre># Create char embedding layer\nchar_embed = layers.Embedding(input_dim=NUM_CHAR_TOKENS, # number of different characters\n                              output_dim=25, # embedding dimension of each character (same as Figure 1 in https://arxiv.org/pdf/1612.05251.pdf)\n                              mask_zero=False, # don't use masks (this messes up model_5 if set to True)\n                              name=\"char_embed\")\n\n# Test out character embedding layer\nprint(f\"Charified text (before vectorization and embedding):\\n{random_train_chars}\\n\")\nchar_embed_example = char_embed(char_vectorizer([random_train_chars]))\nprint(f\"Embedded chars (after vectorization and embedding):\\n{char_embed_example}\\n\")\nprint(f\"Character embedding shape: {char_embed_example.shape}\")\n</pre> # Create char embedding layer char_embed = layers.Embedding(input_dim=NUM_CHAR_TOKENS, # number of different characters                               output_dim=25, # embedding dimension of each character (same as Figure 1 in https://arxiv.org/pdf/1612.05251.pdf)                               mask_zero=False, # don't use masks (this messes up model_5 if set to True)                               name=\"char_embed\")  # Test out character embedding layer print(f\"Charified text (before vectorization and embedding):\\n{random_train_chars}\\n\") char_embed_example = char_embed(char_vectorizer([random_train_chars])) print(f\"Embedded chars (after vectorization and embedding):\\n{char_embed_example}\\n\") print(f\"Character embedding shape: {char_embed_example.shape}\") <pre>Charified text (before vectorization and embedding):\ne x a m i n e   t h e   r e l a t i o n s h i p   o f   d e m o g r a p h i c s   a n d   h e a l t h   c o n d i t i o n s   ,   a l o n e   a n d   i n   c o m b i n a t i o n   ,   o n   o b j e c t i v e   m e a s u r e s   o f   c o g n i t i v e   f u n c t i o n   i n   a   l a r g e   s a m p l e   o f   c o m m u n i t y - d w e l l i n g   o l d e r   a d u l t s   .\n\nEmbedded chars (after vectorization and embedding):\n[[[-0.0242999  -0.04380112  0.03424067 ... -0.00923825 -0.03137535\n    0.04647223]\n  [ 0.03587342 -0.03863243 -0.03250255 ... -0.01955168  0.03022561\n   -0.01134545]\n  [ 0.04515156  0.0074586   0.04069341 ... -0.01211466 -0.02348533\n    0.03110148]\n  ...\n  [-0.04492947  0.00582002  0.02089256 ...  0.02795838 -0.00649694\n   -0.03568561]\n  [-0.04492947  0.00582002  0.02089256 ...  0.02795838 -0.00649694\n   -0.03568561]\n  [-0.04492947  0.00582002  0.02089256 ...  0.02795838 -0.00649694\n   -0.03568561]]]\n\nCharacter embedding shape: (1, 290, 25)\n</pre> <p>Wonderful! Each of the characters in our sequences gets turned into a 25 dimension embedding.</p> In\u00a0[66]: Copied! <pre># Make Conv1D on chars only\ninputs = layers.Input(shape=(1,), dtype=\"string\")\nchar_vectors = char_vectorizer(inputs)\nchar_embeddings = char_embed(char_vectors)\nx = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(char_embeddings)\nx = layers.GlobalMaxPool1D()(x)\noutputs = layers.Dense(num_classes, activation=\"softmax\")(x)\nmodel_3 = tf.keras.Model(inputs=inputs,\n                         outputs=outputs,\n                         name=\"model_3_conv1D_char_embedding\")\n\n# Compile model\nmodel_3.compile(loss=\"categorical_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])\n</pre> # Make Conv1D on chars only inputs = layers.Input(shape=(1,), dtype=\"string\") char_vectors = char_vectorizer(inputs) char_embeddings = char_embed(char_vectors) x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(char_embeddings) x = layers.GlobalMaxPool1D()(x) outputs = layers.Dense(num_classes, activation=\"softmax\")(x) model_3 = tf.keras.Model(inputs=inputs,                          outputs=outputs,                          name=\"model_3_conv1D_char_embedding\")  # Compile model model_3.compile(loss=\"categorical_crossentropy\",                 optimizer=tf.keras.optimizers.Adam(),                 metrics=[\"accuracy\"]) In\u00a0[67]: Copied! <pre># Check the summary of conv1d_char_model\nmodel_3.summary()\n</pre> # Check the summary of conv1d_char_model model_3.summary() <pre>Model: \"model_3_conv1D_char_embedding\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_3 (InputLayer)        [(None, 1)]               0         \n                                                                 \n char_vectorizer (TextVector  (None, 290)              0         \n ization)                                                        \n                                                                 \n char_embed (Embedding)      (None, 290, 25)           1750      \n                                                                 \n conv1d_1 (Conv1D)           (None, 290, 64)           8064      \n                                                                 \n global_max_pooling1d (Globa  (None, 64)               0         \n lMaxPooling1D)                                                  \n                                                                 \n dense_3 (Dense)             (None, 5)                 325       \n                                                                 \n=================================================================\nTotal params: 10,139\nTrainable params: 10,139\nNon-trainable params: 0\n_________________________________________________________________\n</pre> <p>Before fitting our model on the data, we'll create char-level batched <code>PrefetchedDataset</code>'s.</p> In\u00a0[68]: Copied! <pre># Create char datasets\ntrain_char_dataset = tf.data.Dataset.from_tensor_slices((train_chars, train_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\nval_char_dataset = tf.data.Dataset.from_tensor_slices((val_chars, val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n\ntrain_char_dataset\n</pre> # Create char datasets train_char_dataset = tf.data.Dataset.from_tensor_slices((train_chars, train_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE) val_char_dataset = tf.data.Dataset.from_tensor_slices((val_chars, val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)  train_char_dataset Out[68]: <pre>&lt;_PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))&gt;</pre> <p>Just like our token-level sequence model, to save time with our experiments, we'll fit the character-level model on 10% of batches.</p> In\u00a0[69]: Copied! <pre># Fit the model on chars only\nmodel_3_history = model_3.fit(train_char_dataset,\n                              steps_per_epoch=int(0.1 * len(train_char_dataset)),\n                              epochs=3,\n                              validation_data=val_char_dataset,\n                              validation_steps=int(0.1 * len(val_char_dataset)))\n</pre> # Fit the model on chars only model_3_history = model_3.fit(train_char_dataset,                               steps_per_epoch=int(0.1 * len(train_char_dataset)),                               epochs=3,                               validation_data=val_char_dataset,                               validation_steps=int(0.1 * len(val_char_dataset))) <pre>Epoch 1/3\n562/562 [==============================] - 5s 6ms/step - loss: 1.2876 - accuracy: 0.4835 - val_loss: 1.0503 - val_accuracy: 0.5934\nEpoch 2/3\n562/562 [==============================] - 3s 5ms/step - loss: 0.9992 - accuracy: 0.6053 - val_loss: 0.9375 - val_accuracy: 0.6330\nEpoch 3/3\n562/562 [==============================] - 2s 4ms/step - loss: 0.9217 - accuracy: 0.6366 - val_loss: 0.8723 - val_accuracy: 0.6576\n</pre> In\u00a0[70]: Copied! <pre># Evaluate model_3 on whole validation char dataset\nmodel_3.evaluate(val_char_dataset)\n</pre> # Evaluate model_3 on whole validation char dataset model_3.evaluate(val_char_dataset) <pre>945/945 [==============================] - 3s 3ms/step - loss: 0.8830 - accuracy: 0.6543\n</pre> Out[70]: <pre>[0.8829597234725952, 0.6543095707893372]</pre> <p>Nice! Looks like our character-level model is working, let's make some predictions with it and evaluate them.</p> In\u00a0[71]: Copied! <pre># Make predictions with character model only\nmodel_3_pred_probs = model_3.predict(val_char_dataset)\nmodel_3_pred_probs\n</pre> # Make predictions with character model only model_3_pred_probs = model_3.predict(val_char_dataset) model_3_pred_probs <pre>945/945 [==============================] - 2s 2ms/step\n</pre> Out[71]: <pre>array([[0.16555832, 0.40962285, 0.10694715, 0.24029495, 0.07757679],\n       [0.15813953, 0.53640217, 0.01517234, 0.26619416, 0.02409185],\n       [0.12398545, 0.23686218, 0.16067787, 0.41156322, 0.06691124],\n       ...,\n       [0.03653853, 0.05342488, 0.29759905, 0.0883414 , 0.52409613],\n       [0.07498587, 0.1934537 , 0.29657316, 0.10817684, 0.3268104 ],\n       [0.3868036 , 0.46745342, 0.07763865, 0.04975864, 0.01834559]],\n      dtype=float32)</pre> In\u00a0[72]: Copied! <pre># Convert predictions to classes\nmodel_3_preds = tf.argmax(model_3_pred_probs, axis=1)\nmodel_3_preds\n</pre> # Convert predictions to classes model_3_preds = tf.argmax(model_3_pred_probs, axis=1) model_3_preds Out[72]: <pre>&lt;tf.Tensor: shape=(30212,), dtype=int64, numpy=array([1, 1, 3, ..., 4, 4, 1])&gt;</pre> In\u00a0[73]: Copied! <pre># Calculate Conv1D char only model results\nmodel_3_results = calculate_results(y_true=val_labels_encoded,\n                                        y_pred=model_3_preds)\nmodel_3_results\n</pre> # Calculate Conv1D char only model results model_3_results = calculate_results(y_true=val_labels_encoded,                                         y_pred=model_3_preds) model_3_results Out[73]: <pre>{'accuracy': 65.4309545875811,\n 'precision': 0.6475618110732863,\n 'recall': 0.6543095458758109,\n 'f1': 0.6428526324260226}</pre> In\u00a0[74]: Copied! <pre># 1. Setup token inputs/model\ntoken_inputs = layers.Input(shape=[], dtype=tf.string, name=\"token_input\")\ntoken_embeddings = tf_hub_embedding_layer(token_inputs)\ntoken_output = layers.Dense(128, activation=\"relu\")(token_embeddings)\ntoken_model = tf.keras.Model(inputs=token_inputs,\n                             outputs=token_output)\n\n# 2. Setup char inputs/model\nchar_inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"char_input\")\nchar_vectors = char_vectorizer(char_inputs)\nchar_embeddings = char_embed(char_vectors)\nchar_bi_lstm = layers.Bidirectional(layers.LSTM(25))(char_embeddings) # bi-LSTM shown in Figure 1 of https://arxiv.org/pdf/1612.05251.pdf\nchar_model = tf.keras.Model(inputs=char_inputs,\n                            outputs=char_bi_lstm)\n\n# 3. Concatenate token and char inputs (create hybrid token embedding)\ntoken_char_concat = layers.Concatenate(name=\"token_char_hybrid\")([token_model.output, \n                                                                  char_model.output])\n\n# 4. Create output layers - addition of dropout discussed in 4.2 of https://arxiv.org/pdf/1612.05251.pdf\ncombined_dropout = layers.Dropout(0.5)(token_char_concat)\ncombined_dense = layers.Dense(200, activation=\"relu\")(combined_dropout) # slightly different to Figure 1 due to different shapes of token/char embedding layers\nfinal_dropout = layers.Dropout(0.5)(combined_dense)\noutput_layer = layers.Dense(num_classes, activation=\"softmax\")(final_dropout)\n\n# 5. Construct model with char and token inputs\nmodel_4 = tf.keras.Model(inputs=[token_model.input, char_model.input],\n                         outputs=output_layer,\n                         name=\"model_4_token_and_char_embeddings\")\n</pre> # 1. Setup token inputs/model token_inputs = layers.Input(shape=[], dtype=tf.string, name=\"token_input\") token_embeddings = tf_hub_embedding_layer(token_inputs) token_output = layers.Dense(128, activation=\"relu\")(token_embeddings) token_model = tf.keras.Model(inputs=token_inputs,                              outputs=token_output)  # 2. Setup char inputs/model char_inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"char_input\") char_vectors = char_vectorizer(char_inputs) char_embeddings = char_embed(char_vectors) char_bi_lstm = layers.Bidirectional(layers.LSTM(25))(char_embeddings) # bi-LSTM shown in Figure 1 of https://arxiv.org/pdf/1612.05251.pdf char_model = tf.keras.Model(inputs=char_inputs,                             outputs=char_bi_lstm)  # 3. Concatenate token and char inputs (create hybrid token embedding) token_char_concat = layers.Concatenate(name=\"token_char_hybrid\")([token_model.output,                                                                    char_model.output])  # 4. Create output layers - addition of dropout discussed in 4.2 of https://arxiv.org/pdf/1612.05251.pdf combined_dropout = layers.Dropout(0.5)(token_char_concat) combined_dense = layers.Dense(200, activation=\"relu\")(combined_dropout) # slightly different to Figure 1 due to different shapes of token/char embedding layers final_dropout = layers.Dropout(0.5)(combined_dense) output_layer = layers.Dense(num_classes, activation=\"softmax\")(final_dropout)  # 5. Construct model with char and token inputs model_4 = tf.keras.Model(inputs=[token_model.input, char_model.input],                          outputs=output_layer,                          name=\"model_4_token_and_char_embeddings\") <p>Woah... There's a lot going on here, let's get a summary and plot our model to visualize what's happening.</p> In\u00a0[75]: Copied! <pre># Get summary of token and character model\nmodel_4.summary()\n</pre> # Get summary of token and character model model_4.summary() <pre>Model: \"model_4_token_and_char_embeddings\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n char_input (InputLayer)        [(None, 1)]          0           []                               \n                                                                                                  \n token_input (InputLayer)       [(None,)]            0           []                               \n                                                                                                  \n char_vectorizer (TextVectoriza  (None, 290)         0           ['char_input[0][0]']             \n tion)                                                                                            \n                                                                                                  \n universal_sentence_encoder (Ke  (None, 512)         256797824   ['token_input[0][0]']            \n rasLayer)                                                                                        \n                                                                                                  \n char_embed (Embedding)         (None, 290, 25)      1750        ['char_vectorizer[1][0]']        \n                                                                                                  \n dense_4 (Dense)                (None, 128)          65664       ['universal_sentence_encoder[1][0\n                                                                 ]']                              \n                                                                                                  \n bidirectional (Bidirectional)  (None, 50)           10200       ['char_embed[1][0]']             \n                                                                                                  \n token_char_hybrid (Concatenate  (None, 178)         0           ['dense_4[0][0]',                \n )                                                                'bidirectional[0][0]']          \n                                                                                                  \n dropout (Dropout)              (None, 178)          0           ['token_char_hybrid[0][0]']      \n                                                                                                  \n dense_5 (Dense)                (None, 200)          35800       ['dropout[0][0]']                \n                                                                                                  \n dropout_1 (Dropout)            (None, 200)          0           ['dense_5[0][0]']                \n                                                                                                  \n dense_6 (Dense)                (None, 5)            1005        ['dropout_1[0][0]']              \n                                                                                                  \n==================================================================================================\nTotal params: 256,912,243\nTrainable params: 114,419\nNon-trainable params: 256,797,824\n__________________________________________________________________________________________________\n</pre> In\u00a0[76]: Copied! <pre># Plot hybrid token and character model\nfrom tensorflow.keras.utils import plot_model\nplot_model(model_4)\n</pre> # Plot hybrid token and character model from tensorflow.keras.utils import plot_model plot_model(model_4) Out[76]: <p>Now that's a good looking model. Let's compile it just as we have the rest of our models.</p> <p>\ud83d\udd11 Note: Section 4.2 of Neural Networks for Joint Sentence Classification in Medical Paper Abstracts mentions using the SGD (stochastic gradient descent) optimizer, however, to stay consistent with our other models, we're going to use the Adam optimizer. As an exercise, you could try using <code>tf.keras.optimizers.SGD</code> instead of <code>tf.keras.optimizers.Adam</code> and compare the results.</p> In\u00a0[77]: Copied! <pre># Compile token char model\nmodel_4.compile(loss=\"categorical_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(), # section 4.2 of https://arxiv.org/pdf/1612.05251.pdf mentions using SGD but we'll stick with Adam\n                metrics=[\"accuracy\"])\n</pre> # Compile token char model model_4.compile(loss=\"categorical_crossentropy\",                 optimizer=tf.keras.optimizers.Adam(), # section 4.2 of https://arxiv.org/pdf/1612.05251.pdf mentions using SGD but we'll stick with Adam                 metrics=[\"accuracy\"]) <p>And again, to keep our experiments fast, we'll fit our token-character-hybrid model on 10% of training and validate on 10% of validation batches. However, the difference with this model is that it requires two inputs, token-level sequences and character-level sequences.</p> <p>We can do this by create a <code>tf.data.Dataset</code> with a tuple as it's first input, for example:</p> <ul> <li><code>((token_data, char_data), (label))</code></li> </ul> <p>Let's see it in action.</p> In\u00a0[78]: Copied! <pre># Combine chars and tokens into a dataset\ntrain_char_token_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars)) # make data\ntrain_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot) # make labels\ntrain_char_token_dataset = tf.data.Dataset.zip((train_char_token_data, train_char_token_labels)) # combine data and labels\n\n# Prefetch and batch train data\ntrain_char_token_dataset = train_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) \n\n# Repeat same steps validation data\nval_char_token_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars))\nval_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\nval_char_token_dataset = tf.data.Dataset.zip((val_char_token_data, val_char_token_labels))\nval_char_token_dataset = val_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n</pre> # Combine chars and tokens into a dataset train_char_token_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars)) # make data train_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot) # make labels train_char_token_dataset = tf.data.Dataset.zip((train_char_token_data, train_char_token_labels)) # combine data and labels  # Prefetch and batch train data train_char_token_dataset = train_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)   # Repeat same steps validation data val_char_token_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars)) val_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot) val_char_token_dataset = tf.data.Dataset.zip((val_char_token_data, val_char_token_labels)) val_char_token_dataset = val_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) In\u00a0[79]: Copied! <pre># Check out training char and token embedding dataset\ntrain_char_token_dataset, val_char_token_dataset\n</pre> # Check out training char and token embedding dataset train_char_token_dataset, val_char_token_dataset Out[79]: <pre>(&lt;_PrefetchDataset element_spec=((TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None)), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))&gt;,\n &lt;_PrefetchDataset element_spec=((TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None)), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))&gt;)</pre> In\u00a0[80]: Copied! <pre># Fit the model on tokens and chars\nmodel_4_history = model_4.fit(train_char_token_dataset, # train on dataset of token and characters\n                              steps_per_epoch=int(0.1 * len(train_char_token_dataset)),\n                              epochs=3,\n                              validation_data=val_char_token_dataset,\n                              validation_steps=int(0.1 * len(val_char_token_dataset)))\n</pre> # Fit the model on tokens and chars model_4_history = model_4.fit(train_char_token_dataset, # train on dataset of token and characters                               steps_per_epoch=int(0.1 * len(train_char_token_dataset)),                               epochs=3,                               validation_data=val_char_token_dataset,                               validation_steps=int(0.1 * len(val_char_token_dataset))) <pre>Epoch 1/3\n562/562 [==============================] - 22s 28ms/step - loss: 0.9718 - accuracy: 0.6148 - val_loss: 0.7762 - val_accuracy: 0.7015\nEpoch 2/3\n562/562 [==============================] - 14s 25ms/step - loss: 0.7930 - accuracy: 0.6936 - val_loss: 0.7155 - val_accuracy: 0.7301\nEpoch 3/3\n562/562 [==============================] - 14s 25ms/step - loss: 0.7680 - accuracy: 0.7040 - val_loss: 0.6885 - val_accuracy: 0.7374\n</pre> In\u00a0[81]: Copied! <pre># Evaluate on the whole validation dataset\nmodel_4.evaluate(val_char_token_dataset)\n</pre> # Evaluate on the whole validation dataset model_4.evaluate(val_char_token_dataset) <pre>945/945 [==============================] - 13s 13ms/step - loss: 0.6905 - accuracy: 0.7350\n</pre> Out[81]: <pre>[0.6904993653297424, 0.7350390553474426]</pre> <p>Nice! Our token-character hybrid model has come to life!</p> <p>To make predictions with it, since it takes multiplie inputs, we can pass the <code>predict()</code> method a tuple of token-level sequences and character-level sequences.</p> <p>We can then evaluate the predictions as we've done before.</p> In\u00a0[82]: Copied! <pre># Make predictions using the token-character model hybrid\nmodel_4_pred_probs = model_4.predict(val_char_token_dataset)\nmodel_4_pred_probs\n</pre> # Make predictions using the token-character model hybrid model_4_pred_probs = model_4.predict(val_char_token_dataset) model_4_pred_probs <pre>945/945 [==============================] - 13s 13ms/step\n</pre> Out[82]: <pre>array([[3.50879818e-01, 4.52533990e-01, 1.95086596e-03, 1.89795882e-01,\n        4.83940262e-03],\n       [2.65543371e-01, 5.95941365e-01, 3.22520477e-03, 1.32380798e-01,\n        2.90922332e-03],\n       [2.61086196e-01, 1.19279526e-01, 5.44379167e-02, 5.30372798e-01,\n        3.48235779e-02],\n       ...,\n       [3.58209974e-04, 5.53221209e-03, 6.14999533e-02, 1.56954877e-04,\n        9.32452679e-01],\n       [5.82700176e-03, 6.14563487e-02, 2.22319663e-01, 2.89280107e-03,\n        7.07504213e-01],\n       [2.78134555e-01, 2.37935275e-01, 3.98698568e-01, 2.44974717e-02,\n        6.07340895e-02]], dtype=float32)</pre> In\u00a0[83]: Copied! <pre># Turn prediction probabilities into prediction classes\nmodel_4_preds = tf.argmax(model_4_pred_probs, axis=1)\nmodel_4_preds\n</pre> # Turn prediction probabilities into prediction classes model_4_preds = tf.argmax(model_4_pred_probs, axis=1) model_4_preds Out[83]: <pre>&lt;tf.Tensor: shape=(30212,), dtype=int64, numpy=array([1, 1, 3, ..., 4, 4, 2])&gt;</pre> In\u00a0[84]: Copied! <pre># Get results of token-char-hybrid model\nmodel_4_results = calculate_results(y_true=val_labels_encoded,\n                                    y_pred=model_4_preds)\nmodel_4_results\n</pre> # Get results of token-char-hybrid model model_4_results = calculate_results(y_true=val_labels_encoded,                                     y_pred=model_4_preds) model_4_results Out[84]: <pre>{'accuracy': 73.5039057328214,\n 'precision': 0.737718289862149,\n 'recall': 0.735039057328214,\n 'f1': 0.7310235703293549}</pre> In\u00a0[85]: Copied! <pre># Inspect training dataframe\ntrain_df.head()\n</pre> # Inspect training dataframe train_df.head() Out[85]: target text line_number total_lines 0 OBJECTIVE to investigate the efficacy of @ weeks of dail... 0 11 1 METHODS a total of @ patients with primary knee oa wer... 1 11 2 METHODS outcome measures included pain reduction and i... 2 11 3 METHODS pain was assessed using the visual analog pain... 3 11 4 METHODS secondary outcome measures included the wester... 4 11 <p>The <code>\"line_number\"</code> and <code>\"total_lines\"</code> columns are features which didn't necessarily come with the training data but can be passed to our model as a positional embedding. In other words, the positional embedding is where the sentence appears in an abstract.</p> <p>We can use these features because they will be available at test time.</p> <p> Since abstracts typically have a sequential order about them (for example, background, objective, methods, results, conclusion), it makes sense to add the line number of where a particular sentence occurs to our model. The beautiful thing is, these features will be available at test time (we can just count the number of sentences in an abstract and the number of each one).</p> <p>Meaning, if we were to predict the labels of sequences in an abstract our model had never seen, we could count the number of lines and the track the position of each individual line and pass it to our model.</p> <p>\ud83d\udee0 Exercise: Another way of creating our positional embedding feature would be to combine the <code>\"line_number\"</code> and <code>\"total_lines\"</code> columns into one, for example a <code>\"line_position\"</code> column may contain values like <code>1_of_11</code>, <code>2_of_11</code>, etc. Where <code>1_of_11</code> would be the first line in an abstract 11 sentences long. After going through the following steps, you might want to revisit this positional embedding stage and see how a combined column of <code>\"line_position\"</code> goes against two separate columns.</p> In\u00a0[86]: Copied! <pre># How many different line numbers are there?\ntrain_df[\"line_number\"].value_counts()\n</pre> # How many different line numbers are there? train_df[\"line_number\"].value_counts() Out[86]: <pre>0     15000\n1     15000\n2     15000\n3     15000\n4     14992\n5     14949\n6     14758\n7     14279\n8     13346\n9     11981\n10    10041\n11     7892\n12     5853\n13     4152\n14     2835\n15     1861\n16     1188\n17      751\n18      462\n19      286\n20      162\n21      101\n22       66\n23       33\n24       22\n25       14\n26        7\n27        4\n28        3\n29        1\n30        1\nName: line_number, dtype: int64</pre> In\u00a0[87]: Copied! <pre># Check the distribution of \"line_number\" column\ntrain_df.line_number.plot.hist()\n</pre> # Check the distribution of \"line_number\" column train_df.line_number.plot.hist() Out[87]: <pre>&lt;Axes: ylabel='Frequency'&gt;</pre> <p>Looking at the distribution of the <code>\"line_number\"</code> column, it looks like the majority of lines have a position of 15 or less.</p> <p>Knowing this, let's set the <code>depth</code> parameter of <code>tf.one_hot</code> to 15.</p> In\u00a0[88]: Copied! <pre># Use TensorFlow to create one-hot-encoded tensors of our \"line_number\" column \ntrain_line_numbers_one_hot = tf.one_hot(train_df[\"line_number\"].to_numpy(), depth=15)\nval_line_numbers_one_hot = tf.one_hot(val_df[\"line_number\"].to_numpy(), depth=15)\ntest_line_numbers_one_hot = tf.one_hot(test_df[\"line_number\"].to_numpy(), depth=15)\n</pre> # Use TensorFlow to create one-hot-encoded tensors of our \"line_number\" column  train_line_numbers_one_hot = tf.one_hot(train_df[\"line_number\"].to_numpy(), depth=15) val_line_numbers_one_hot = tf.one_hot(val_df[\"line_number\"].to_numpy(), depth=15) test_line_numbers_one_hot = tf.one_hot(test_df[\"line_number\"].to_numpy(), depth=15) <p>Setting the <code>depth</code> parameter of <code>tf.one_hot</code> to 15 means any sample with a <code>\"line_number\"</code> value of over 15 gets set to a tensor of all 0's, where as any sample with a <code>\"line_number\"</code> of under 15 gets turned into a tensor of all 0's but with a 1 at the index equal to the <code>\"line_number\"</code> value.</p> <p>\ud83d\udd11 Note: We could create a one-hot tensor which has room for all of the potential values of <code>\"line_number\"</code> (<code>depth=30</code>), however, this would end up in a tensor of double the size of our current one (<code>depth=15</code>) where the vast majority of values are 0. Plus, only ~2,000/180,000 samples have a <code>\"line_number\"</code> value of over 15. So we would not be gaining much information about our data for doubling our feature space. This kind of problem is called the curse of dimensionality. However, since this we're working with deep models, it might be worth trying to throw as much information at the model as possible and seeing what happens. I'll leave exploring values of the <code>depth</code> parameter as an extension.</p> In\u00a0[89]: Copied! <pre># Check one-hot encoded \"line_number\" feature samples\ntrain_line_numbers_one_hot.shape, train_line_numbers_one_hot[:20]\n</pre> # Check one-hot encoded \"line_number\" feature samples train_line_numbers_one_hot.shape, train_line_numbers_one_hot[:20] Out[89]: <pre>(TensorShape([180040, 15]),\n &lt;tf.Tensor: shape=(20, 15), dtype=float32, numpy=\n array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]],\n       dtype=float32)&gt;)</pre> <p>We can do the same as we've done for our <code>\"line_number\"</code> column witht he <code>\"total_lines\"</code> column. First, let's find an appropriate value for the <code>depth</code> parameter of <code>tf.one_hot</code>.</p> In\u00a0[90]: Copied! <pre># How many different numbers of lines are there?\ntrain_df[\"total_lines\"].value_counts()\n</pre> # How many different numbers of lines are there? train_df[\"total_lines\"].value_counts() Out[90]: <pre>11    24468\n10    23639\n12    22113\n9     19400\n13    18438\n14    14610\n8     12285\n15    10768\n7      7464\n16     7429\n17     5202\n6      3353\n18     3344\n19     2480\n20     1281\n5      1146\n21      770\n22      759\n23      264\n4       215\n24      200\n25      182\n26       81\n28       58\n3        32\n30       31\n27       28\nName: total_lines, dtype: int64</pre> In\u00a0[91]: Copied! <pre># Check the distribution of total lines\ntrain_df.total_lines.plot.hist();\n</pre> # Check the distribution of total lines train_df.total_lines.plot.hist(); <p>Looking at the distribution of our <code>\"total_lines\"</code> column, a value of 20 looks like it covers the majority of samples.</p> <p>We can confirm this with <code>np.percentile()</code>.</p> In\u00a0[92]: Copied! <pre># Check the coverage of a \"total_lines\" value of 20\nnp.percentile(train_df.total_lines, 98) # a value of 20 covers 98% of samples\n</pre> # Check the coverage of a \"total_lines\" value of 20 np.percentile(train_df.total_lines, 98) # a value of 20 covers 98% of samples Out[92]: <pre>20.0</pre> <p>Beautiful! Plenty of converage. Let's one-hot-encode our <code>\"total_lines\"</code> column just as we did our <code>\"line_number\"</code> column.</p> In\u00a0[93]: Copied! <pre># Use TensorFlow to create one-hot-encoded tensors of our \"total_lines\" column \ntrain_total_lines_one_hot = tf.one_hot(train_df[\"total_lines\"].to_numpy(), depth=20)\nval_total_lines_one_hot = tf.one_hot(val_df[\"total_lines\"].to_numpy(), depth=20)\ntest_total_lines_one_hot = tf.one_hot(test_df[\"total_lines\"].to_numpy(), depth=20)\n\n# Check shape and samples of total lines one-hot tensor\ntrain_total_lines_one_hot.shape, train_total_lines_one_hot[:10]\n</pre> # Use TensorFlow to create one-hot-encoded tensors of our \"total_lines\" column  train_total_lines_one_hot = tf.one_hot(train_df[\"total_lines\"].to_numpy(), depth=20) val_total_lines_one_hot = tf.one_hot(val_df[\"total_lines\"].to_numpy(), depth=20) test_total_lines_one_hot = tf.one_hot(test_df[\"total_lines\"].to_numpy(), depth=20)  # Check shape and samples of total lines one-hot tensor train_total_lines_one_hot.shape, train_total_lines_one_hot[:10] Out[93]: <pre>(TensorShape([180040, 20]),\n &lt;tf.Tensor: shape=(10, 20), dtype=float32, numpy=\n array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n         0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n         0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n         0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n         0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n         0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n         0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n         0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n         0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n         0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n         0., 0., 0., 0.]], dtype=float32)&gt;)</pre> In\u00a0[94]: Copied! <pre># 1. Token inputs\ntoken_inputs = layers.Input(shape=[], dtype=\"string\", name=\"token_inputs\")\ntoken_embeddings = tf_hub_embedding_layer(token_inputs)\ntoken_outputs = layers.Dense(128, activation=\"relu\")(token_embeddings)\ntoken_model = tf.keras.Model(inputs=token_inputs,\n                             outputs=token_outputs)\n\n# 2. Char inputs\nchar_inputs = layers.Input(shape=(1,), dtype=\"string\", name=\"char_inputs\")\nchar_vectors = char_vectorizer(char_inputs)\nchar_embeddings = char_embed(char_vectors)\nchar_bi_lstm = layers.Bidirectional(layers.LSTM(32))(char_embeddings)\nchar_model = tf.keras.Model(inputs=char_inputs,\n                            outputs=char_bi_lstm)\n\n# 3. Line numbers inputs\nline_number_inputs = layers.Input(shape=(15,), dtype=tf.int32, name=\"line_number_input\")\nx = layers.Dense(32, activation=\"relu\")(line_number_inputs)\nline_number_model = tf.keras.Model(inputs=line_number_inputs,\n                                   outputs=x)\n\n# 4. Total lines inputs\ntotal_lines_inputs = layers.Input(shape=(20,), dtype=tf.int32, name=\"total_lines_input\")\ny = layers.Dense(32, activation=\"relu\")(total_lines_inputs)\ntotal_line_model = tf.keras.Model(inputs=total_lines_inputs,\n                                  outputs=y)\n\n# 5. Combine token and char embeddings into a hybrid embedding\ncombined_embeddings = layers.Concatenate(name=\"token_char_hybrid_embedding\")([token_model.output, \n                                                                              char_model.output])\nz = layers.Dense(256, activation=\"relu\")(combined_embeddings)\nz = layers.Dropout(0.5)(z)\n\n# 6. Combine positional embeddings with combined token and char embeddings into a tribrid embedding\nz = layers.Concatenate(name=\"token_char_positional_embedding\")([line_number_model.output,\n                                                                total_line_model.output,\n                                                                z])\n\n# 7. Create output layer\noutput_layer = layers.Dense(5, activation=\"softmax\", name=\"output_layer\")(z)\n\n# 8. Put together model\nmodel_5 = tf.keras.Model(inputs=[line_number_model.input,\n                                 total_line_model.input,\n                                 token_model.input, \n                                 char_model.input],\n                         outputs=output_layer)\n</pre> # 1. Token inputs token_inputs = layers.Input(shape=[], dtype=\"string\", name=\"token_inputs\") token_embeddings = tf_hub_embedding_layer(token_inputs) token_outputs = layers.Dense(128, activation=\"relu\")(token_embeddings) token_model = tf.keras.Model(inputs=token_inputs,                              outputs=token_outputs)  # 2. Char inputs char_inputs = layers.Input(shape=(1,), dtype=\"string\", name=\"char_inputs\") char_vectors = char_vectorizer(char_inputs) char_embeddings = char_embed(char_vectors) char_bi_lstm = layers.Bidirectional(layers.LSTM(32))(char_embeddings) char_model = tf.keras.Model(inputs=char_inputs,                             outputs=char_bi_lstm)  # 3. Line numbers inputs line_number_inputs = layers.Input(shape=(15,), dtype=tf.int32, name=\"line_number_input\") x = layers.Dense(32, activation=\"relu\")(line_number_inputs) line_number_model = tf.keras.Model(inputs=line_number_inputs,                                    outputs=x)  # 4. Total lines inputs total_lines_inputs = layers.Input(shape=(20,), dtype=tf.int32, name=\"total_lines_input\") y = layers.Dense(32, activation=\"relu\")(total_lines_inputs) total_line_model = tf.keras.Model(inputs=total_lines_inputs,                                   outputs=y)  # 5. Combine token and char embeddings into a hybrid embedding combined_embeddings = layers.Concatenate(name=\"token_char_hybrid_embedding\")([token_model.output,                                                                                char_model.output]) z = layers.Dense(256, activation=\"relu\")(combined_embeddings) z = layers.Dropout(0.5)(z)  # 6. Combine positional embeddings with combined token and char embeddings into a tribrid embedding z = layers.Concatenate(name=\"token_char_positional_embedding\")([line_number_model.output,                                                                 total_line_model.output,                                                                 z])  # 7. Create output layer output_layer = layers.Dense(5, activation=\"softmax\", name=\"output_layer\")(z)  # 8. Put together model model_5 = tf.keras.Model(inputs=[line_number_model.input,                                  total_line_model.input,                                  token_model.input,                                   char_model.input],                          outputs=output_layer) <p>There's a lot going on here... let's visualize what's happening with a summary by plotting our model.</p> In\u00a0[95]: Copied! <pre># Get a summary of our token, char and positional embedding model\nmodel_5.summary()\n</pre> # Get a summary of our token, char and positional embedding model model_5.summary() <pre>Model: \"model_8\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n char_inputs (InputLayer)       [(None, 1)]          0           []                               \n                                                                                                  \n token_inputs (InputLayer)      [(None,)]            0           []                               \n                                                                                                  \n char_vectorizer (TextVectoriza  (None, 290)         0           ['char_inputs[0][0]']            \n tion)                                                                                            \n                                                                                                  \n universal_sentence_encoder (Ke  (None, 512)         256797824   ['token_inputs[0][0]']           \n rasLayer)                                                                                        \n                                                                                                  \n char_embed (Embedding)         (None, 290, 25)      1750        ['char_vectorizer[2][0]']        \n                                                                                                  \n dense_7 (Dense)                (None, 128)          65664       ['universal_sentence_encoder[2][0\n                                                                 ]']                              \n                                                                                                  \n bidirectional_1 (Bidirectional  (None, 64)          14848       ['char_embed[2][0]']             \n )                                                                                                \n                                                                                                  \n token_char_hybrid_embedding (C  (None, 192)         0           ['dense_7[0][0]',                \n oncatenate)                                                      'bidirectional_1[0][0]']        \n                                                                                                  \n line_number_input (InputLayer)  [(None, 15)]        0           []                               \n                                                                                                  \n total_lines_input (InputLayer)  [(None, 20)]        0           []                               \n                                                                                                  \n dense_10 (Dense)               (None, 256)          49408       ['token_char_hybrid_embedding[0][\n                                                                 0]']                             \n                                                                                                  \n dense_8 (Dense)                (None, 32)           512         ['line_number_input[0][0]']      \n                                                                                                  \n dense_9 (Dense)                (None, 32)           672         ['total_lines_input[0][0]']      \n                                                                                                  \n dropout_2 (Dropout)            (None, 256)          0           ['dense_10[0][0]']               \n                                                                                                  \n token_char_positional_embeddin  (None, 320)         0           ['dense_8[0][0]',                \n g (Concatenate)                                                  'dense_9[0][0]',                \n                                                                  'dropout_2[0][0]']              \n                                                                                                  \n output_layer (Dense)           (None, 5)            1605        ['token_char_positional_embedding\n                                                                 [0][0]']                         \n                                                                                                  \n==================================================================================================\nTotal params: 256,932,283\nTrainable params: 134,459\nNon-trainable params: 256,797,824\n__________________________________________________________________________________________________\n</pre> In\u00a0[96]: Copied! <pre># Plot the token, char, positional embedding model\nfrom tensorflow.keras.utils import plot_model\nplot_model(model_5)\n</pre> # Plot the token, char, positional embedding model from tensorflow.keras.utils import plot_model plot_model(model_5) Out[96]: <p>Visualizing the model makes it much easier to understand.</p> <p>Essentially what we're doing is trying to encode as much information about our sequences as possible into various embeddings (the inputs to our model) so our model has the best chance to figure out what label belongs to a sequence (the outputs of our model).</p> <p>You'll notice our model is looking very similar to the model shown in Figure 1 of Neural Networks for Joint Sentence Classification in Medical Paper Abstracts. However, a few differences still remain:</p> <ul> <li>We're using pretrained TensorFlow Hub token embeddings instead of GloVe emebddings.</li> <li>We're using a Dense layer on top of our token-character hybrid embeddings instead of a bi-LSTM layer.</li> <li>Section 3.1.3 of the paper mentions a label sequence optimization layer (which helps to make sure sequence labels come out in a respectable order) but it isn't shown in Figure 1. To makeup for the lack of this layer in our model, we've created the positional embeddings layers.</li> <li>Section 4.2 of the paper mentions the token and character embeddings are updated during training, our pretrained TensorFlow Hub embeddings remain frozen.</li> <li>The paper uses the <code>SGD</code> optimizer, we're going to stick with <code>Adam</code>.</li> </ul> <p>All of the differences above are potential extensions of this project.</p> In\u00a0[97]: Copied! <pre># Check which layers of our model are trainable or not\nfor layer in model_5.layers:\n  print(layer, layer.trainable)\n</pre> # Check which layers of our model are trainable or not for layer in model_5.layers:   print(layer, layer.trainable) <pre>&lt;keras.engine.input_layer.InputLayer object at 0x7efa4933cd60&gt; True\n&lt;keras.engine.input_layer.InputLayer object at 0x7efa4933d120&gt; True\n&lt;keras.layers.preprocessing.text_vectorization.TextVectorization object at 0x7efa49737cd0&gt; True\n&lt;tensorflow_hub.keras_layer.KerasLayer object at 0x7efb801dc610&gt; False\n&lt;keras.layers.core.embedding.Embedding object at 0x7efa4976d240&gt; True\n&lt;keras.layers.core.dense.Dense object at 0x7efa4933d960&gt; True\n&lt;keras.layers.rnn.bidirectional.Bidirectional object at 0x7efa493b9600&gt; True\n&lt;keras.layers.merging.concatenate.Concatenate object at 0x7efa49222260&gt; True\n&lt;keras.engine.input_layer.InputLayer object at 0x7efa493934f0&gt; True\n&lt;keras.engine.input_layer.InputLayer object at 0x7efa493bac50&gt; True\n&lt;keras.layers.core.dense.Dense object at 0x7efa49237190&gt; True\n&lt;keras.layers.core.dense.Dense object at 0x7efa49737c70&gt; True\n&lt;keras.layers.core.dense.Dense object at 0x7efa493d4130&gt; True\n&lt;keras.layers.regularization.dropout.Dropout object at 0x7efa493bb220&gt; True\n&lt;keras.layers.merging.concatenate.Concatenate object at 0x7efa492abfd0&gt; True\n&lt;keras.layers.core.dense.Dense object at 0x7efa492c9420&gt; True\n</pre> <p>Now our model is constructed, let's compile it.</p> <p>This time, we're going to introduce a new parameter to our loss function called <code>label_smoothing</code>. Label smoothing helps to regularize our model (prevent overfitting) by making sure it doesn't get too focused on applying one particular label to a sample.</p> <p>For example, instead of having an output prediction of:</p> <ul> <li><code>[0.0, 0.0, 1.0, 0.0, 0.0]</code> for a sample (the model is very confident the right label is index 2).</li> </ul> <p>It's predictions will get smoothed to be something like:</p> <ul> <li><code>[0.01, 0.01, 0.096, 0.01, 0.01]</code> giving a small activation to each of the other labels, in turn, hopefully improving generalization.</li> </ul> <p>\ud83d\udcd6 Resource: For more on label smoothing, see the great blog post by PyImageSearch, Label smoothing with Keras, TensorFlow, and Deep Learning.</p> In\u00a0[98]: Copied! <pre># Compile token, char, positional embedding model\nmodel_5.compile(loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2), # add label smoothing (examples which are really confident get smoothed a little)\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])\n</pre> # Compile token, char, positional embedding model model_5.compile(loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2), # add label smoothing (examples which are really confident get smoothed a little)                 optimizer=tf.keras.optimizers.Adam(),                 metrics=[\"accuracy\"]) In\u00a0[99]: Copied! <pre># Create training and validation datasets (all four kinds of inputs)\ntrain_pos_char_token_data = tf.data.Dataset.from_tensor_slices((train_line_numbers_one_hot, # line numbers\n                                                                train_total_lines_one_hot, # total lines\n                                                                train_sentences, # train tokens\n                                                                train_chars)) # train chars\ntrain_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot) # train labels\ntrain_pos_char_token_dataset = tf.data.Dataset.zip((train_pos_char_token_data, train_pos_char_token_labels)) # combine data and labels\ntrain_pos_char_token_dataset = train_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) # turn into batches and prefetch appropriately\n\n# Validation dataset\nval_pos_char_token_data = tf.data.Dataset.from_tensor_slices((val_line_numbers_one_hot,\n                                                              val_total_lines_one_hot,\n                                                              val_sentences,\n                                                              val_chars))\nval_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\nval_pos_char_token_dataset = tf.data.Dataset.zip((val_pos_char_token_data, val_pos_char_token_labels))\nval_pos_char_token_dataset = val_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) # turn into batches and prefetch appropriately\n\n# Check input shapes\ntrain_pos_char_token_dataset, val_pos_char_token_dataset\n</pre> # Create training and validation datasets (all four kinds of inputs) train_pos_char_token_data = tf.data.Dataset.from_tensor_slices((train_line_numbers_one_hot, # line numbers                                                                 train_total_lines_one_hot, # total lines                                                                 train_sentences, # train tokens                                                                 train_chars)) # train chars train_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot) # train labels train_pos_char_token_dataset = tf.data.Dataset.zip((train_pos_char_token_data, train_pos_char_token_labels)) # combine data and labels train_pos_char_token_dataset = train_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) # turn into batches and prefetch appropriately  # Validation dataset val_pos_char_token_data = tf.data.Dataset.from_tensor_slices((val_line_numbers_one_hot,                                                               val_total_lines_one_hot,                                                               val_sentences,                                                               val_chars)) val_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot) val_pos_char_token_dataset = tf.data.Dataset.zip((val_pos_char_token_data, val_pos_char_token_labels)) val_pos_char_token_dataset = val_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) # turn into batches and prefetch appropriately  # Check input shapes train_pos_char_token_dataset, val_pos_char_token_dataset Out[99]: <pre>(&lt;_PrefetchDataset element_spec=((TensorSpec(shape=(None, 15), dtype=tf.float32, name=None), TensorSpec(shape=(None, 20), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None)), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))&gt;,\n &lt;_PrefetchDataset element_spec=((TensorSpec(shape=(None, 15), dtype=tf.float32, name=None), TensorSpec(shape=(None, 20), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None)), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))&gt;)</pre> In\u00a0[100]: Copied! <pre># Fit the token, char and positional embedding model\nhistory_model_5 = model_5.fit(train_pos_char_token_dataset,\n                              steps_per_epoch=int(0.1 * len(train_pos_char_token_dataset)),\n                              epochs=3,\n                              validation_data=val_pos_char_token_dataset,\n                              validation_steps=int(0.1 * len(val_pos_char_token_dataset)))\n</pre> # Fit the token, char and positional embedding model history_model_5 = model_5.fit(train_pos_char_token_dataset,                               steps_per_epoch=int(0.1 * len(train_pos_char_token_dataset)),                               epochs=3,                               validation_data=val_pos_char_token_dataset,                               validation_steps=int(0.1 * len(val_pos_char_token_dataset))) <pre>Epoch 1/3\n562/562 [==============================] - 22s 30ms/step - loss: 1.0996 - accuracy: 0.7180 - val_loss: 0.9867 - val_accuracy: 0.8022\nEpoch 2/3\n562/562 [==============================] - 15s 26ms/step - loss: 0.9700 - accuracy: 0.8145 - val_loss: 0.9517 - val_accuracy: 0.8278\nEpoch 3/3\n562/562 [==============================] - 15s 26ms/step - loss: 0.9507 - accuracy: 0.8228 - val_loss: 0.9403 - val_accuracy: 0.8321\n</pre> <p>Tribrid model trained! Time to make some predictions with it and evaluate them just as we've done before.</p> In\u00a0[101]: Copied! <pre># Make predictions with token-char-positional hybrid model\nmodel_5_pred_probs = model_5.predict(val_pos_char_token_dataset, verbose=1)\nmodel_5_pred_probs\n</pre> # Make predictions with token-char-positional hybrid model model_5_pred_probs = model_5.predict(val_pos_char_token_dataset, verbose=1) model_5_pred_probs <pre>945/945 [==============================] - 13s 13ms/step\n</pre> Out[101]: <pre>array([[0.47378692, 0.11392056, 0.01195277, 0.37507388, 0.02526585],\n       [0.50771445, 0.12806556, 0.04414373, 0.31101325, 0.00906302],\n       [0.26883593, 0.11979864, 0.13117212, 0.4081741 , 0.07201922],\n       ...,\n       [0.03447666, 0.10706001, 0.04599329, 0.03351728, 0.7789528 ],\n       [0.03275388, 0.31916243, 0.09343442, 0.02761328, 0.527036  ],\n       [0.25131708, 0.54850894, 0.1116215 , 0.04071589, 0.04783657]],\n      dtype=float32)</pre> In\u00a0[102]: Copied! <pre># Turn prediction probabilities into prediction classes\nmodel_5_preds = tf.argmax(model_5_pred_probs, axis=1)\nmodel_5_preds\n</pre> # Turn prediction probabilities into prediction classes model_5_preds = tf.argmax(model_5_pred_probs, axis=1) model_5_preds Out[102]: <pre>&lt;tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 0, 3, ..., 4, 4, 1])&gt;</pre> In\u00a0[103]: Copied! <pre># Calculate results of token-char-positional hybrid model\nmodel_5_results = calculate_results(y_true=val_labels_encoded,\n                                    y_pred=model_5_preds)\nmodel_5_results\n</pre> # Calculate results of token-char-positional hybrid model model_5_results = calculate_results(y_true=val_labels_encoded,                                     y_pred=model_5_preds) model_5_results Out[103]: <pre>{'accuracy': 83.20203892493049,\n 'precision': 0.8307701956480288,\n 'recall': 0.8320203892493049,\n 'f1': 0.8311131749119387}</pre> In\u00a0[104]: Copied! <pre># Combine model results into a DataFrame\nall_model_results = pd.DataFrame({\"baseline\": baseline_results,\n                                  \"custom_token_embed_conv1d\": model_1_results,\n                                  \"pretrained_token_embed\": model_2_results,\n                                  \"custom_char_embed_conv1d\": model_3_results,\n                                  \"hybrid_char_token_embed\": model_4_results,\n                                  \"tribrid_pos_char_token_embed\": model_5_results})\nall_model_results = all_model_results.transpose()\nall_model_results\n</pre> # Combine model results into a DataFrame all_model_results = pd.DataFrame({\"baseline\": baseline_results,                                   \"custom_token_embed_conv1d\": model_1_results,                                   \"pretrained_token_embed\": model_2_results,                                   \"custom_char_embed_conv1d\": model_3_results,                                   \"hybrid_char_token_embed\": model_4_results,                                   \"tribrid_pos_char_token_embed\": model_5_results}) all_model_results = all_model_results.transpose() all_model_results Out[104]: accuracy precision recall f1 baseline 72.183238 0.718647 0.721832 0.698925 custom_token_embed_conv1d 78.564809 0.782409 0.785648 0.782994 pretrained_token_embed 71.610618 0.716161 0.716106 0.712831 custom_char_embed_conv1d 65.430955 0.647562 0.654310 0.642853 hybrid_char_token_embed 73.503906 0.737718 0.735039 0.731024 tribrid_pos_char_token_embed 83.202039 0.830770 0.832020 0.831113 In\u00a0[105]: Copied! <pre># Reduce the accuracy to same scale as other metrics\nall_model_results[\"accuracy\"] = all_model_results[\"accuracy\"]/100\n</pre> # Reduce the accuracy to same scale as other metrics all_model_results[\"accuracy\"] = all_model_results[\"accuracy\"]/100 In\u00a0[106]: Copied! <pre># Plot and compare all of the model results\nall_model_results.plot(kind=\"bar\", figsize=(10, 7)).legend(bbox_to_anchor=(1.0, 1.0));\n</pre> # Plot and compare all of the model results all_model_results.plot(kind=\"bar\", figsize=(10, 7)).legend(bbox_to_anchor=(1.0, 1.0)); <p>Since the PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts paper compares their tested model's F1-scores on the test dataset, let's take at our model's F1-scores.</p> <p>\ud83d\udd11 Note: We could've also made these comparisons in TensorBoard using the <code>TensorBoard</code> callback during training.</p> In\u00a0[107]: Copied! <pre># Sort model results by f1-score\nall_model_results.sort_values(\"f1\", ascending=False)[\"f1\"].plot(kind=\"bar\", figsize=(10, 7));\n</pre> # Sort model results by f1-score all_model_results.sort_values(\"f1\", ascending=False)[\"f1\"].plot(kind=\"bar\", figsize=(10, 7)); <p>Nice! Based on F1-scores, it looks like our tribrid embedding model performs the best by a fair margin.</p> <p>Though, in comparison to the results reported in Table 3 of the PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts paper, our model's F1-score is still underperforming (the authors model achieves an F1-score of 90.0 on the 20k RCT dataset versus our F1-score of ~82.6).</p> <p>There are some things to note about this difference:</p> <ul> <li>Our models (with an exception for the baseline) have been trained on ~18,000 (10% of batches) samples of sequences and labels rather than the full ~180,000 in the 20k RCT dataset.<ul> <li>This is often the case in machine learning experiments though, make sure training works on a smaller number of samples, then upscale when needed (an extension to this project will be training a model on the full dataset).</li> </ul> </li> <li>Our model's prediction performance levels have been evaluated on the validation dataset not the test dataset (we'll evaluate our best model on the test dataset shortly).</li> </ul> In\u00a0[108]: Copied! <pre># Save best performing model to SavedModel format (default)\nmodel_5.save(\"skimlit_tribrid_model\") # model will be saved to path specified by string\n</pre> # Save best performing model to SavedModel format (default) model_5.save(\"skimlit_tribrid_model\") # model will be saved to path specified by string <pre>WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n</pre> <p>Optional: If you're using Google Colab, you might want to copy your saved model to Google Drive (or download it) for more permanent storage (Google Colab files disappear after you disconnect).</p> In\u00a0[109]: Copied! <pre># Example of copying saved model from Google Colab to Drive (requires Google Drive to be mounted)\n# !cp skimlit_best_model -r /content/drive/MyDrive/tensorflow_course/skim_lit\n</pre> # Example of copying saved model from Google Colab to Drive (requires Google Drive to be mounted) # !cp skimlit_best_model -r /content/drive/MyDrive/tensorflow_course/skim_lit <p>Like all good cooking shows, we've got a pretrained model (exactly the same kind of model we built for <code>model_5</code> saved and stored on Google Drive and Google Storage).</p> <p>So to make sure we're all using the same model for evaluation, we'll download it and load it in.</p> <p>And when loading in our model, since it uses a couple of custom objects (our TensorFlow Hub layer and <code>TextVectorization</code> layer), we'll have to load it in by specifying them in the <code>custom_objects</code> parameter of <code>tf.keras.models.load_model()</code>.</p> In\u00a0[110]: Copied! <pre># Download pretrained model from Google Storage\n!wget https://storage.googleapis.com/ztm_tf_course/skimlit/skimlit_tribrid_model.zip\n!mkdir skimlit_gs_model\n!unzip skimlit_tribrid_model.zip -d skimlit_gs_model\n</pre> # Download pretrained model from Google Storage !wget https://storage.googleapis.com/ztm_tf_course/skimlit/skimlit_tribrid_model.zip !mkdir skimlit_gs_model !unzip skimlit_tribrid_model.zip -d skimlit_gs_model <pre>--2023-05-26 03:53:53--  https://storage.googleapis.com/ztm_tf_course/skimlit/skimlit_tribrid_model.zip\nResolving storage.googleapis.com (storage.googleapis.com)... 74.125.24.128, 142.250.4.128, 142.251.10.128, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|74.125.24.128|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 962182847 (918M) [application/zip]\nSaving to: \u2018skimlit_tribrid_model.zip\u2019\n\nskimlit_tribrid_mod 100%[===================&gt;] 917.61M  23.0MB/s    in 42s     \n\n2023-05-26 03:54:35 (21.9 MB/s) - \u2018skimlit_tribrid_model.zip\u2019 saved [962182847/962182847]\n\nArchive:  skimlit_tribrid_model.zip\n   creating: skimlit_gs_model/skimlit_tribrid_model/\n  inflating: skimlit_gs_model/skimlit_tribrid_model/keras_metadata.pb  \n   creating: skimlit_gs_model/skimlit_tribrid_model/assets/\n extracting: skimlit_gs_model/skimlit_tribrid_model/fingerprint.pb  \n   creating: skimlit_gs_model/skimlit_tribrid_model/variables/\n  inflating: skimlit_gs_model/skimlit_tribrid_model/variables/variables.index  \n  inflating: skimlit_gs_model/skimlit_tribrid_model/variables/variables.data-00000-of-00001  \n  inflating: skimlit_gs_model/skimlit_tribrid_model/saved_model.pb  \n</pre> In\u00a0[111]: Copied! <pre># Import TensorFlow model dependencies (if needed) - https://github.com/tensorflow/tensorflow/issues/38250 \nimport tensorflow_hub as hub\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization\n\nmodel_path = \"skimlit_gs_model/skimlit_tribrid_model/\"\n\n# Load downloaded model from Google Storage\nloaded_model = tf.keras.models.load_model(model_path)#,\n                                          # Note: with TensorFlow 2.5+ if your SavedModel has a keras_metadata.pb file \n                                          # (created when using model.save()), you shouldn't need the custom_objects\n                                          # parameter. I'm leaving the code below here in case you do.\n                                          # custom_objects={\"TextVectorization\": TextVectorization, # required for char vectorization\n                                          #                 \"KerasLayer\": hub.KerasLayer}) # required for token embedding\n</pre> # Import TensorFlow model dependencies (if needed) - https://github.com/tensorflow/tensorflow/issues/38250  import tensorflow_hub as hub import tensorflow as tf from tensorflow.keras.layers import TextVectorization  model_path = \"skimlit_gs_model/skimlit_tribrid_model/\"  # Load downloaded model from Google Storage loaded_model = tf.keras.models.load_model(model_path)#,                                           # Note: with TensorFlow 2.5+ if your SavedModel has a keras_metadata.pb file                                            # (created when using model.save()), you shouldn't need the custom_objects                                           # parameter. I'm leaving the code below here in case you do.                                           # custom_objects={\"TextVectorization\": TextVectorization, # required for char vectorization                                           #                 \"KerasLayer\": hub.KerasLayer}) # required for token embedding In\u00a0[112]: Copied! <pre># Make predictions with the loaded model on the validation set\nloaded_pred_probs = loaded_model.predict(val_pos_char_token_dataset, verbose=1)\nloaded_preds = tf.argmax(loaded_pred_probs, axis=1)\nloaded_preds[:10]\n</pre> # Make predictions with the loaded model on the validation set loaded_pred_probs = loaded_model.predict(val_pos_char_token_dataset, verbose=1) loaded_preds = tf.argmax(loaded_pred_probs, axis=1) loaded_preds[:10] <pre>945/945 [==============================] - 14s 13ms/step\n</pre> Out[112]: <pre>&lt;tf.Tensor: shape=(10,), dtype=int64, numpy=array([0, 0, 3, 2, 2, 4, 4, 4, 4, 1])&gt;</pre> In\u00a0[113]: Copied! <pre># Evaluate loaded model's predictions\nloaded_model_results = calculate_results(val_labels_encoded,\n                                         loaded_preds)\nloaded_model_results\n</pre> # Evaluate loaded model's predictions loaded_model_results = calculate_results(val_labels_encoded,                                          loaded_preds) loaded_model_results Out[113]: <pre>{'accuracy': 83.03654177148154,\n 'precision': 0.8290877655167075,\n 'recall': 0.8303654177148153,\n 'f1': 0.8294588444242452}</pre> <p>Now let's compare our loaded model's predictions with the prediction results we obtained before saving our model.</p> In\u00a0[114]: Copied! <pre># Compare loaded model results with original trained model results (should be quite close)\nnp.isclose(list(model_5_results.values()), list(loaded_model_results.values()), rtol=1e-02)\n</pre> # Compare loaded model results with original trained model results (should be quite close) np.isclose(list(model_5_results.values()), list(loaded_model_results.values()), rtol=1e-02) Out[114]: <pre>array([ True,  True,  True,  True])</pre> <p>It's worth noting that loading in a SavedModel unfreezes all layers (makes them all trainable). So if you want to freeze any layers, you'll have to set their trainable attribute to <code>False</code>.</p> In\u00a0[115]: Copied! <pre># Check loaded model summary (note the number of trainable parameters)\nloaded_model.summary()\n</pre> # Check loaded model summary (note the number of trainable parameters) loaded_model.summary() <pre>Model: \"model_8\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n char_inputs (InputLayer)       [(None, 1)]          0           []                               \n                                                                                                  \n token_inputs (InputLayer)      [(None,)]            0           []                               \n                                                                                                  \n char_vectorizer (TextVectoriza  (None, 290)         0           ['char_inputs[0][0]']            \n tion)                                                                                            \n                                                                                                  \n universal_sentence_encoder (Ke  (None, 512)         256797824   ['token_inputs[0][0]']           \n rasLayer)                                                                                        \n                                                                                                  \n char_embed (Embedding)         (None, 290, 25)      1750        ['char_vectorizer[0][0]']        \n                                                                                                  \n dense_7 (Dense)                (None, 128)          65664       ['universal_sentence_encoder[0][0\n                                                                 ]']                              \n                                                                                                  \n bidirectional_1 (Bidirectional  (None, 64)          14848       ['char_embed[0][0]']             \n )                                                                                                \n                                                                                                  \n token_char_hybrid_embedding (C  (None, 192)         0           ['dense_7[0][0]',                \n oncatenate)                                                      'bidirectional_1[0][0]']        \n                                                                                                  \n line_number_input (InputLayer)  [(None, 15)]        0           []                               \n                                                                                                  \n total_lines_input (InputLayer)  [(None, 20)]        0           []                               \n                                                                                                  \n dense_10 (Dense)               (None, 256)          49408       ['token_char_hybrid_embedding[0][\n                                                                 0]']                             \n                                                                                                  \n dense_8 (Dense)                (None, 32)           512         ['line_number_input[0][0]']      \n                                                                                                  \n dense_9 (Dense)                (None, 32)           672         ['total_lines_input[0][0]']      \n                                                                                                  \n dropout_2 (Dropout)            (None, 256)          0           ['dense_10[0][0]']               \n                                                                                                  \n token_char_positional_embeddin  (None, 320)         0           ['dense_8[0][0]',                \n g (Concatenate)                                                  'dense_9[0][0]',                \n                                                                  'dropout_2[0][0]']              \n                                                                                                  \n output_layer (Dense)           (None, 5)            1605        ['token_char_positional_embedding\n                                                                 [0][0]']                         \n                                                                                                  \n==================================================================================================\nTotal params: 256,932,283\nTrainable params: 134,459\nNon-trainable params: 256,797,824\n__________________________________________________________________________________________________\n</pre> In\u00a0[116]: Copied! <pre># Create test dataset batch and prefetched\ntest_pos_char_token_data = tf.data.Dataset.from_tensor_slices((test_line_numbers_one_hot,\n                                                               test_total_lines_one_hot,\n                                                               test_sentences,\n                                                               test_chars))\ntest_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\ntest_pos_char_token_dataset = tf.data.Dataset.zip((test_pos_char_token_data, test_pos_char_token_labels))\ntest_pos_char_token_dataset = test_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n\n# Check shapes\ntest_pos_char_token_dataset\n</pre> # Create test dataset batch and prefetched test_pos_char_token_data = tf.data.Dataset.from_tensor_slices((test_line_numbers_one_hot,                                                                test_total_lines_one_hot,                                                                test_sentences,                                                                test_chars)) test_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot) test_pos_char_token_dataset = tf.data.Dataset.zip((test_pos_char_token_data, test_pos_char_token_labels)) test_pos_char_token_dataset = test_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)  # Check shapes test_pos_char_token_dataset Out[116]: <pre>&lt;_PrefetchDataset element_spec=((TensorSpec(shape=(None, 15), dtype=tf.float32, name=None), TensorSpec(shape=(None, 20), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None)), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))&gt;</pre> In\u00a0[117]: Copied! <pre># Make predictions on the test dataset\ntest_pred_probs = loaded_model.predict(test_pos_char_token_dataset,\n                                       verbose=1)\ntest_preds = tf.argmax(test_pred_probs, axis=1)\ntest_preds[:10]\n</pre> # Make predictions on the test dataset test_pred_probs = loaded_model.predict(test_pos_char_token_dataset,                                        verbose=1) test_preds = tf.argmax(test_pred_probs, axis=1) test_preds[:10] <pre>942/942 [==============================] - 12s 13ms/step\n</pre> Out[117]: <pre>&lt;tf.Tensor: shape=(10,), dtype=int64, numpy=array([3, 3, 2, 2, 4, 4, 4, 1, 1, 0])&gt;</pre> In\u00a0[118]: Copied! <pre># Evaluate loaded model test predictions\nloaded_model_test_results = calculate_results(y_true=test_labels_encoded,\n                                              y_pred=test_preds)\nloaded_model_test_results\n</pre> # Evaluate loaded model test predictions loaded_model_test_results = calculate_results(y_true=test_labels_encoded,                                               y_pred=test_preds) loaded_model_test_results Out[118]: <pre>{'accuracy': 82.57176041148166,\n 'precision': 0.8242615240280317,\n 'recall': 0.8257176041148167,\n 'f1': 0.8247089023051047}</pre> <p>It seems our best model (so far) still has some ways to go to match the performance of the results in the paper (their model gets 90.0 F1-score on the test dataset, where as ours gets ~82.1 F1-score).</p> <p>However, as we discussed before our model has only been trained on 20,000 out of the total ~180,000 sequences in the RCT 20k dataset. We also haven't fine-tuned our pretrained embeddings (the paper fine-tunes GloVe embeddings). So there's a couple of extensions we could try to improve our results.</p> In\u00a0[119]: Copied! <pre>%%time\n# Get list of class names of test predictions\ntest_pred_classes = [label_encoder.classes_[pred] for pred in test_preds]\ntest_pred_classes\n</pre> %%time # Get list of class names of test predictions test_pred_classes = [label_encoder.classes_[pred] for pred in test_preds] test_pred_classes <pre>CPU times: user 15 s, sys: 553 ms, total: 15.6 s\nWall time: 14 s\n</pre> Out[119]: <pre>['OBJECTIVE',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'OBJECTIVE',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'METHODS',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'BACKGROUND',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'METHODS',\n 'CONCLUSIONS',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'OBJECTIVE',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'BACKGROUND',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'RESULTS',\n 'BACKGROUND',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'OBJECTIVE',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'OBJECTIVE',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'BACKGROUND',\n 'RESULTS',\n 'RESULTS',\n 'BACKGROUND',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'RESULTS',\n 'RESULTS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'OBJECTIVE',\n 'BACKGROUND',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'METHODS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'METHODS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'OBJECTIVE',\n 'RESULTS',\n 'RESULTS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'OBJECTIVE',\n 'OBJECTIVE',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'BACKGROUND',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'METHODS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'BACKGROUND',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'BACKGROUND',\n 'BACKGROUND',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'OBJECTIVE',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'METHODS',\n 'CONCLUSIONS',\n 'METHODS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'RESULTS',\n 'BACKGROUND',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'OBJECTIVE',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'CONCLUSIONS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'OBJECTIVE',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'METHODS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'OBJECTIVE',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'METHODS',\n 'BACKGROUND',\n 'OBJECTIVE',\n 'METHODS',\n 'RESULTS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'RESULTS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'BACKGROUND',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'BACKGROUND',\n 'BACKGROUND',\n 'BACKGROUND',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'METHODS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'OBJECTIVE',\n 'RESULTS',\n 'METHODS',\n 'RESULTS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'RESULTS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'RESULTS',\n 'METHODS',\n 'OBJECTIVE',\n 'METHODS',\n 'RESULTS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'BACKGROUND',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'BACKGROUND',\n 'BACKGROUND',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS',\n 'RESULTS',\n 'CONCLUSIONS',\n 'CONCLUSIONS',\n ...]</pre> <p>Now we'll enrich our test DataFame with a few values:</p> <ul> <li>A <code>\"prediction\"</code> (string) column containing our model's prediction for a given sample.</li> <li>A <code>\"pred_prob\"</code> (float) column containing the model's maximum prediction probabiliy for a given sample.</li> <li>A <code>\"correct\"</code> (bool) column to indicate whether or not the model's prediction matches the sample's target label.</li> </ul> In\u00a0[120]: Copied! <pre># Create prediction-enriched test dataframe\ntest_df[\"prediction\"] = test_pred_classes # create column with test prediction class names\ntest_df[\"pred_prob\"] = tf.reduce_max(test_pred_probs, axis=1).numpy() # get the maximum prediction probability\ntest_df[\"correct\"] = test_df[\"prediction\"] == test_df[\"target\"] # create binary column for whether the prediction is right or not\ntest_df.head(20)\n</pre> # Create prediction-enriched test dataframe test_df[\"prediction\"] = test_pred_classes # create column with test prediction class names test_df[\"pred_prob\"] = tf.reduce_max(test_pred_probs, axis=1).numpy() # get the maximum prediction probability test_df[\"correct\"] = test_df[\"prediction\"] == test_df[\"target\"] # create binary column for whether the prediction is right or not test_df.head(20) Out[120]: target text line_number total_lines prediction pred_prob correct 0 BACKGROUND this study analyzed liver function abnormaliti... 0 8 OBJECTIVE 0.523977 False 1 RESULTS a post hoc analysis was conducted with the use... 1 8 OBJECTIVE 0.351961 False 2 RESULTS liver function tests ( lfts ) were measured at... 2 8 METHODS 0.802363 False 3 RESULTS survival analyses were used to assess the asso... 3 8 METHODS 0.652682 False 4 RESULTS the percentage of patients with abnormal lfts ... 4 8 RESULTS 0.665139 True 5 RESULTS when mean hemodynamic profiles were compared i... 5 8 RESULTS 0.881854 True 6 RESULTS multivariable analyses revealed that patients ... 6 8 RESULTS 0.529604 True 7 CONCLUSIONS abnormal lfts are common in the adhf populatio... 7 8 CONCLUSIONS 0.568309 True 8 CONCLUSIONS elevated meld-xi scores are associated with po... 8 8 CONCLUSIONS 0.461624 True 9 BACKGROUND minimally invasive endovascular aneurysm repai... 0 12 BACKGROUND 0.537818 True 10 BACKGROUND the aim of this study was to analyse the cost-... 1 12 BACKGROUND 0.438266 True 11 METHODS resource use was determined from the amsterdam... 2 12 METHODS 0.655806 True 12 METHODS the analysis was performed from a provider per... 3 12 METHODS 0.855124 True 13 METHODS all costs were calculated as if all patients h... 4 12 METHODS 0.551576 True 14 RESULTS a total of @ patients were randomized . 5 12 RESULTS 0.712471 True 15 RESULTS the @-day mortality rate was @ per cent after ... 6 12 RESULTS 0.677854 True 16 RESULTS at @months , the total mortality rate for evar... 7 12 RESULTS 0.895368 True 17 RESULTS the mean cost difference between evar and or w... 8 12 RESULTS 0.848430 True 18 RESULTS the incremental cost-effectiveness ratio per p... 9 12 RESULTS 0.797845 True 19 RESULTS there was no significant difference in quality... 10 12 RESULTS 0.726324 True <p>Looking good! Having our data like this, makes it very easy to manipulate and view in different ways.</p> <p>How about we sort our DataFrame to find the samples with the highest <code>\"pred_prob\"</code> and where the prediction was wrong (<code>\"correct\" == False</code>)?</p> In\u00a0[121]: Copied! <pre># Find top 100 most wrong samples (note: 100 is an abitrary number, you could go through all of them if you wanted)\ntop_100_wrong = test_df[test_df[\"correct\"] == False].sort_values(\"pred_prob\", ascending=False)[:100]\ntop_100_wrong\n</pre> # Find top 100 most wrong samples (note: 100 is an abitrary number, you could go through all of them if you wanted) top_100_wrong = test_df[test_df[\"correct\"] == False].sort_values(\"pred_prob\", ascending=False)[:100] top_100_wrong Out[121]: target text line_number total_lines prediction pred_prob correct 13874 CONCLUSIONS symptom outcomes will be assessed and estimate... 4 6 METHODS 0.946729 False 8545 METHODS pretest-posttest . 1 11 BACKGROUND 0.934970 False 16347 BACKGROUND to evaluate the effects of the lactic acid bac... 0 12 OBJECTIVE 0.930999 False 2388 RESULTS the primary endpoint is the cumulative three-y... 4 13 METHODS 0.928258 False 10452 BACKGROUND to validate the association between accommodat... 0 10 OBJECTIVE 0.918861 False ... ... ... ... ... ... ... ... 12134 RESULTS we conducted a population-based case-control s... 2 16 METHODS 0.840864 False 2605 RESULTS circulating epc ( cells positive for cd@ , cd@... 4 10 METHODS 0.840451 False 19223 METHODS of the @ dogs receiving placebo , @ ( @ % ) vo... 7 10 RESULTS 0.840258 False 20003 CONCLUSIONS most subjects with bronchospasm responded to r... 9 11 RESULTS 0.839862 False 26098 CONCLUSIONS clinicaltrials.gov identifier nct@ . 10 10 BACKGROUND 0.839650 False <p>100 rows \u00d7 7 columns</p> <p>Great (or not so great)! Now we've got a subset of our model's most wrong predictions, let's write some code to visualize them.</p> In\u00a0[122]: Copied! <pre># Investigate top wrong preds\nfor row in top_100_wrong[0:10].itertuples(): # adjust indexes to view different samples\n  _, target, text, line_number, total_lines, prediction, pred_prob, _ = row\n  print(f\"Target: {target}, Pred: {prediction}, Prob: {pred_prob}, Line number: {line_number}, Total lines: {total_lines}\\n\")\n  print(f\"Text:\\n{text}\\n\")\n  print(\"-----\\n\")\n</pre> # Investigate top wrong preds for row in top_100_wrong[0:10].itertuples(): # adjust indexes to view different samples   _, target, text, line_number, total_lines, prediction, pred_prob, _ = row   print(f\"Target: {target}, Pred: {prediction}, Prob: {pred_prob}, Line number: {line_number}, Total lines: {total_lines}\\n\")   print(f\"Text:\\n{text}\\n\")   print(\"-----\\n\") <pre>Target: CONCLUSIONS, Pred: METHODS, Prob: 0.9467289447784424, Line number: 4, Total lines: 6\n\nText:\nsymptom outcomes will be assessed and estimates of cost-effectiveness made .\n\n-----\n\nTarget: METHODS, Pred: BACKGROUND, Prob: 0.9349697828292847, Line number: 1, Total lines: 11\n\nText:\npretest-posttest .\n\n-----\n\nTarget: BACKGROUND, Pred: OBJECTIVE, Prob: 0.9309993982315063, Line number: 0, Total lines: 12\n\nText:\nto evaluate the effects of the lactic acid bacterium lactobacillus salivarius on caries risk factors .\n\n-----\n\nTarget: RESULTS, Pred: METHODS, Prob: 0.9282575845718384, Line number: 4, Total lines: 13\n\nText:\nthe primary endpoint is the cumulative three-year hiv incidence .\n\n-----\n\nTarget: BACKGROUND, Pred: OBJECTIVE, Prob: 0.9188613891601562, Line number: 0, Total lines: 10\n\nText:\nto validate the association between accommodation and visual asthenopia by measuring objective accommodative amplitude with the optical quality analysis system ( oqas , visiometrics , terrassa , spain ) , and to investigate associations among accommodation , ocular surface instability , and visual asthenopia while viewing @d displays .\n\n-----\n\nTarget: METHODS, Pred: RESULTS, Prob: 0.9168779253959656, Line number: 6, Total lines: 9\n\nText:\n-@ % vs. fish : -@ % vs. fish + s : -@ % ; p &lt; @ ) but there were no significant differences between groups .\n\n-----\n\nTarget: RESULTS, Pred: METHODS, Prob: 0.9150147438049316, Line number: 6, Total lines: 14\n\nText:\nthe primary outcome was to evaluate changes in abdominal and shoulder-tip pain via a @-mm visual analog scale at @ , @ , and @hours postoperatively .\n\n-----\n\nTarget: RESULTS, Pred: METHODS, Prob: 0.9143044352531433, Line number: 3, Total lines: 16\n\nText:\na cluster randomised trial was implemented with @,@ children in @ government primary schools on the south coast of kenya in @-@ .\n\n-----\n\nTarget: CONCLUSIONS, Pred: BACKGROUND, Prob: 0.9103274941444397, Line number: 19, Total lines: 19\n\nText:\nclinicaltrials.gov identifier : nct@ .\n\n-----\n\nTarget: CONCLUSIONS, Pred: BACKGROUND, Prob: 0.9066908955574036, Line number: 18, Total lines: 18\n\nText:\nnct@ ( clinicaltrials.gov ) .\n\n-----\n\n</pre> <p>What do you notice about the most wrong predictions? Does the model make silly mistakes? Or are some of the labels incorrect/ambiguous (e.g. a line in an abstract could potentially be labelled <code>OBJECTIVE</code> or <code>BACKGROUND</code> and make sense).</p> <p>A next step here would be if there are a fair few samples with inconsistent labels, you could go through your training dataset, update the labels and then retrain a model. The process of using a model to help improve/investigate your dataset's labels is often referred to as active learning.</p> In\u00a0[123]: Copied! <pre>import json\n# Download and open example abstracts (copy and pasted from PubMed)\n!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/skimlit_example_abstracts.json\n\nwith open(\"skimlit_example_abstracts.json\", \"r\") as f:\n  example_abstracts = json.load(f)\n\nexample_abstracts\n</pre> import json # Download and open example abstracts (copy and pasted from PubMed) !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/skimlit_example_abstracts.json  with open(\"skimlit_example_abstracts.json\", \"r\") as f:   example_abstracts = json.load(f)  example_abstracts <pre>--2023-05-26 03:55:38--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/skimlit_example_abstracts.json\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 6737 (6.6K) [text/plain]\nSaving to: \u2018skimlit_example_abstracts.json\u2019\n\nskimlit_example_abs 100%[===================&gt;]   6.58K  --.-KB/s    in 0s      \n\n2023-05-26 03:55:38 (83.7 MB/s) - \u2018skimlit_example_abstracts.json\u2019 saved [6737/6737]\n\n</pre> Out[123]: <pre>[{'abstract': 'This RCT examined the efficacy of a manualized social intervention for children with HFASDs. Participants were randomly assigned to treatment or wait-list conditions. Treatment included instruction and therapeutic activities targeting social skills, face-emotion recognition, interest expansion, and interpretation of non-literal language. A response-cost program was applied to reduce problem behaviors and foster skills acquisition. Significant treatment effects were found for five of seven primary outcome measures (parent ratings and direct child measures). Secondary measures based on staff ratings (treatment group only) corroborated gains reported by parents. High levels of parent, child and staff satisfaction were reported, along with high levels of treatment fidelity. Standardized effect size estimates were primarily in the medium and large ranges and favored the treatment group.',\n  'source': 'https://pubmed.ncbi.nlm.nih.gov/20232240/',\n  'details': 'RCT of a manualized social treatment for high-functioning autism spectrum disorders'},\n {'abstract': \"Postpartum depression (PPD) is the most prevalent mood disorder associated with childbirth. No single cause of PPD has been identified, however the increased risk of nutritional deficiencies incurred through the high nutritional requirements of pregnancy may play a role in the pathology of depressive symptoms. Three nutritional interventions have drawn particular interest as possible non-invasive and cost-effective prevention and/or treatment strategies for PPD; omega-3 (n-3) long chain polyunsaturated fatty acids (LCPUFA), vitamin D and overall diet. We searched for meta-analyses of randomised controlled trials (RCT's) of nutritional interventions during the perinatal period with PPD as an outcome, and checked for any trials published subsequently to the meta-analyses. Fish oil: Eleven RCT's of prenatal fish oil supplementation RCT's show null and positive effects on PPD symptoms. Vitamin D: no relevant RCT's were identified, however seven observational studies of maternal vitamin D levels with PPD outcomes showed inconsistent associations. Diet: Two Australian RCT's with dietary advice interventions in pregnancy had a positive and null result on PPD. With the exception of fish oil, few RCT's with nutritional interventions during pregnancy assess PPD. Further research is needed to determine whether nutritional intervention strategies during pregnancy can protect against symptoms of PPD. Given the prevalence of PPD and ease of administering PPD measures, we recommend future prenatal nutritional RCT's include PPD as an outcome.\",\n  'source': 'https://pubmed.ncbi.nlm.nih.gov/28012571/',\n  'details': 'Formatting removed (can be used to compare model to actual example)'},\n {'abstract': 'Mental illness, including depression, anxiety and bipolar disorder, accounts for a significant proportion of global disability and poses a substantial social, economic and heath burden. Treatment is presently dominated by pharmacotherapy, such as antidepressants, and psychotherapy, such as cognitive behavioural therapy; however, such treatments avert less than half of the disease burden, suggesting that additional strategies are needed to prevent and treat mental disorders. There are now consistent mechanistic, observational and interventional data to suggest diet quality may be a modifiable risk factor for mental illness. This review provides an overview of the nutritional psychiatry field. It includes a discussion of the neurobiological mechanisms likely modulated by diet, the use of dietary and nutraceutical interventions in mental disorders, and recommendations for further research. Potential biological pathways related to mental disorders include inflammation, oxidative stress, the gut microbiome, epigenetic modifications and neuroplasticity. Consistent epidemiological evidence, particularly for depression, suggests an association between measures of diet quality and mental health, across multiple populations and age groups; these do not appear to be explained by other demographic, lifestyle factors or reverse causality. Our recently published intervention trial provides preliminary clinical evidence that dietary interventions in clinically diagnosed populations are feasible and can provide significant clinical benefit. Furthermore, nutraceuticals including n-3 fatty acids, folate, S-adenosylmethionine, N-acetyl cysteine and probiotics, among others, are promising avenues for future research. Continued research is now required to investigate the efficacy of intervention studies in large cohorts and within clinically relevant populations, particularly in patients with schizophrenia, bipolar and anxiety disorders.',\n  'source': 'https://pubmed.ncbi.nlm.nih.gov/28942748/',\n  'details': 'Effect of nutrition on mental health'},\n {'abstract': \"Hepatitis C virus (HCV) and alcoholic liver disease (ALD), either alone or in combination, count for more than two thirds of all liver diseases in the Western world. There is no safe level of drinking in HCV-infected patients and the most effective goal for these patients is total abstinence. Baclofen, a GABA(B) receptor agonist, represents a promising pharmacotherapy for alcohol dependence (AD). Previously, we performed a randomized clinical trial (RCT), which demonstrated the safety and efficacy of baclofen in patients affected by AD and cirrhosis. The goal of this post-hoc analysis was to explore baclofen's effect in a subgroup of alcohol-dependent HCV-infected cirrhotic patients. Any patient with HCV infection was selected for this analysis. Among the 84 subjects randomized in the main trial, 24 alcohol-dependent cirrhotic patients had a HCV infection; 12 received baclofen 10mg t.i.d. and 12 received placebo for 12-weeks. With respect to the placebo group (3/12, 25.0%), a significantly higher number of patients who achieved and maintained total alcohol abstinence was found in the baclofen group (10/12, 83.3%; p=0.0123). Furthermore, in the baclofen group, compared to placebo, there was a significantly higher increase in albumin values from baseline (p=0.0132) and a trend toward a significant reduction in INR levels from baseline (p=0.0716). In conclusion, baclofen was safe and significantly more effective than placebo in promoting alcohol abstinence, and improving some Liver Function Tests (LFTs) (i.e. albumin, INR) in alcohol-dependent HCV-infected cirrhotic patients. Baclofen may represent a clinically relevant alcohol pharmacotherapy for these patients.\",\n  'source': 'https://pubmed.ncbi.nlm.nih.gov/22244707/',\n  'details': 'Baclofen promotes alcohol abstinence in alcohol dependent cirrhotic patients with hepatitis C virus (HCV) infection'}]</pre> In\u00a0[124]: Copied! <pre># See what our example abstracts look like\nabstracts = pd.DataFrame(example_abstracts)\nabstracts\n</pre> # See what our example abstracts look like abstracts = pd.DataFrame(example_abstracts) abstracts Out[124]: abstract source details 0 This RCT examined the efficacy of a manualized... https://pubmed.ncbi.nlm.nih.gov/20232240/ RCT of a manualized social treatment for high-... 1 Postpartum depression (PPD) is the most preval... https://pubmed.ncbi.nlm.nih.gov/28012571/ Formatting removed (can be used to compare mod... 2 Mental illness, including depression, anxiety ... https://pubmed.ncbi.nlm.nih.gov/28942748/ Effect of nutrition on mental health 3 Hepatitis C virus (HCV) and alcoholic liver di... https://pubmed.ncbi.nlm.nih.gov/22244707/ Baclofen promotes alcohol abstinence in alcoho... <p>Now we've downloaded some example abstracts, let's see how one of them goes with our trained model.</p> <p>First, we'll need to parse it using spaCy to turn it from a big chunk of text into sentences.</p> In\u00a0[125]: Copied! <pre># Create sentencizer - Source: https://spacy.io/usage/linguistic-features#sbd \nfrom spacy.lang.en import English\nnlp = English() # setup English sentence parser\n\n# New version of spaCy\nsentencizer = nlp.add_pipe(\"sentencizer\") # create sentence splitting pipeline object\n\n# Old version of spaCy\n# sentencizer = nlp.create_pipe(\"sentencizer\") # create sentence splitting pipeline object \n# nlp.add_pipe(sentencizer) # add sentence splitting pipeline object to sentence parser\n\n# Create \"doc\" of parsed sequences, change index for a different abstract\ndoc = nlp(example_abstracts[0][\"abstract\"]) \nabstract_lines = [str(sent) for sent in list(doc.sents)] # return detected sentences from doc in string type (not spaCy token type)\nabstract_lines\n</pre> # Create sentencizer - Source: https://spacy.io/usage/linguistic-features#sbd  from spacy.lang.en import English nlp = English() # setup English sentence parser  # New version of spaCy sentencizer = nlp.add_pipe(\"sentencizer\") # create sentence splitting pipeline object  # Old version of spaCy # sentencizer = nlp.create_pipe(\"sentencizer\") # create sentence splitting pipeline object  # nlp.add_pipe(sentencizer) # add sentence splitting pipeline object to sentence parser  # Create \"doc\" of parsed sequences, change index for a different abstract doc = nlp(example_abstracts[0][\"abstract\"])  abstract_lines = [str(sent) for sent in list(doc.sents)] # return detected sentences from doc in string type (not spaCy token type) abstract_lines Out[125]: <pre>['This RCT examined the efficacy of a manualized social intervention for children with HFASDs.',\n 'Participants were randomly assigned to treatment or wait-list conditions.',\n 'Treatment included instruction and therapeutic activities targeting social skills, face-emotion recognition, interest expansion, and interpretation of non-literal language.',\n 'A response-cost program was applied to reduce problem behaviors and foster skills acquisition.',\n 'Significant treatment effects were found for five of seven primary outcome measures (parent ratings and direct child measures).',\n 'Secondary measures based on staff ratings (treatment group only) corroborated gains reported by parents.',\n 'High levels of parent, child and staff satisfaction were reported, along with high levels of treatment fidelity.',\n 'Standardized effect size estimates were primarily in the medium and large ranges and favored the treatment group.']</pre> <p>Beautiful! It looks like spaCy has split the sentences in the abstract correctly. However, it should be noted, there may be more complex abstracts which don't get split perfectly into separate sentences (such as the example in Baclofen promotes alcohol abstinence in alcohol dependent cirrhotic patients with hepatitis C virus (HCV) infection), in this case, more custom splitting techniques would have to be investigated.</p> <p>Now our abstract has been split into sentences, how about we write some code to count line numbers as well as total lines.</p> <p>To do so, we can leverage some of the functionality of our <code>preprocess_text_with_line_numbers()</code> function.</p> In\u00a0[126]: Copied! <pre># Get total number of lines\ntotal_lines_in_sample = len(abstract_lines)\n\n# Go through each line in abstract and create a list of dictionaries containing features for each line\nsample_lines = []\nfor i, line in enumerate(abstract_lines):\n  sample_dict = {}\n  sample_dict[\"text\"] = str(line)\n  sample_dict[\"line_number\"] = i\n  sample_dict[\"total_lines\"] = total_lines_in_sample - 1\n  sample_lines.append(sample_dict)\nsample_lines\n</pre> # Get total number of lines total_lines_in_sample = len(abstract_lines)  # Go through each line in abstract and create a list of dictionaries containing features for each line sample_lines = [] for i, line in enumerate(abstract_lines):   sample_dict = {}   sample_dict[\"text\"] = str(line)   sample_dict[\"line_number\"] = i   sample_dict[\"total_lines\"] = total_lines_in_sample - 1   sample_lines.append(sample_dict) sample_lines Out[126]: <pre>[{'text': 'This RCT examined the efficacy of a manualized social intervention for children with HFASDs.',\n  'line_number': 0,\n  'total_lines': 7},\n {'text': 'Participants were randomly assigned to treatment or wait-list conditions.',\n  'line_number': 1,\n  'total_lines': 7},\n {'text': 'Treatment included instruction and therapeutic activities targeting social skills, face-emotion recognition, interest expansion, and interpretation of non-literal language.',\n  'line_number': 2,\n  'total_lines': 7},\n {'text': 'A response-cost program was applied to reduce problem behaviors and foster skills acquisition.',\n  'line_number': 3,\n  'total_lines': 7},\n {'text': 'Significant treatment effects were found for five of seven primary outcome measures (parent ratings and direct child measures).',\n  'line_number': 4,\n  'total_lines': 7},\n {'text': 'Secondary measures based on staff ratings (treatment group only) corroborated gains reported by parents.',\n  'line_number': 5,\n  'total_lines': 7},\n {'text': 'High levels of parent, child and staff satisfaction were reported, along with high levels of treatment fidelity.',\n  'line_number': 6,\n  'total_lines': 7},\n {'text': 'Standardized effect size estimates were primarily in the medium and large ranges and favored the treatment group.',\n  'line_number': 7,\n  'total_lines': 7}]</pre> <p>Now we've got <code>\"line_number\"</code> and <code>\"total_lines\"</code> values, we can one-hot encode them with <code>tf.one_hot</code> just like we did with our training dataset (using the same values for the <code>depth</code> parameter).</p> In\u00a0[127]: Copied! <pre># Get all line_number values from sample abstract\ntest_abstract_line_numbers = [line[\"line_number\"] for line in sample_lines]\n# One-hot encode to same depth as training data, so model accepts right input shape\ntest_abstract_line_numbers_one_hot = tf.one_hot(test_abstract_line_numbers, depth=15) \ntest_abstract_line_numbers_one_hot\n</pre> # Get all line_number values from sample abstract test_abstract_line_numbers = [line[\"line_number\"] for line in sample_lines] # One-hot encode to same depth as training data, so model accepts right input shape test_abstract_line_numbers_one_hot = tf.one_hot(test_abstract_line_numbers, depth=15)  test_abstract_line_numbers_one_hot Out[127]: <pre>&lt;tf.Tensor: shape=(8, 15), dtype=float32, numpy=\narray([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]],\n      dtype=float32)&gt;</pre> In\u00a0[128]: Copied! <pre># Get all total_lines values from sample abstract\ntest_abstract_total_lines = [line[\"total_lines\"] for line in sample_lines]\n# One-hot encode to same depth as training data, so model accepts right input shape\ntest_abstract_total_lines_one_hot = tf.one_hot(test_abstract_total_lines, depth=20)\ntest_abstract_total_lines_one_hot\n</pre> # Get all total_lines values from sample abstract test_abstract_total_lines = [line[\"total_lines\"] for line in sample_lines] # One-hot encode to same depth as training data, so model accepts right input shape test_abstract_total_lines_one_hot = tf.one_hot(test_abstract_total_lines, depth=20) test_abstract_total_lines_one_hot Out[128]: <pre>&lt;tf.Tensor: shape=(8, 20), dtype=float32, numpy=\narray([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0.]], dtype=float32)&gt;</pre> <p>We can also use our <code>split_chars()</code> function to split our abstract lines into characters.</p> In\u00a0[129]: Copied! <pre># Split abstract lines into characters\nabstract_chars = [split_chars(sentence) for sentence in abstract_lines]\nabstract_chars\n</pre> # Split abstract lines into characters abstract_chars = [split_chars(sentence) for sentence in abstract_lines] abstract_chars Out[129]: <pre>['T h i s   R C T   e x a m i n e d   t h e   e f f i c a c y   o f   a   m a n u a l i z e d   s o c i a l   i n t e r v e n t i o n   f o r   c h i l d r e n   w i t h   H F A S D s .',\n 'P a r t i c i p a n t s   w e r e   r a n d o m l y   a s s i g n e d   t o   t r e a t m e n t   o r   w a i t - l i s t   c o n d i t i o n s .',\n 'T r e a t m e n t   i n c l u d e d   i n s t r u c t i o n   a n d   t h e r a p e u t i c   a c t i v i t i e s   t a r g e t i n g   s o c i a l   s k i l l s ,   f a c e - e m o t i o n   r e c o g n i t i o n ,   i n t e r e s t   e x p a n s i o n ,   a n d   i n t e r p r e t a t i o n   o f   n o n - l i t e r a l   l a n g u a g e .',\n 'A   r e s p o n s e - c o s t   p r o g r a m   w a s   a p p l i e d   t o   r e d u c e   p r o b l e m   b e h a v i o r s   a n d   f o s t e r   s k i l l s   a c q u i s i t i o n .',\n 'S i g n i f i c a n t   t r e a t m e n t   e f f e c t s   w e r e   f o u n d   f o r   f i v e   o f   s e v e n   p r i m a r y   o u t c o m e   m e a s u r e s   ( p a r e n t   r a t i n g s   a n d   d i r e c t   c h i l d   m e a s u r e s ) .',\n 'S e c o n d a r y   m e a s u r e s   b a s e d   o n   s t a f f   r a t i n g s   ( t r e a t m e n t   g r o u p   o n l y )   c o r r o b o r a t e d   g a i n s   r e p o r t e d   b y   p a r e n t s .',\n 'H i g h   l e v e l s   o f   p a r e n t ,   c h i l d   a n d   s t a f f   s a t i s f a c t i o n   w e r e   r e p o r t e d ,   a l o n g   w i t h   h i g h   l e v e l s   o f   t r e a t m e n t   f i d e l i t y .',\n 'S t a n d a r d i z e d   e f f e c t   s i z e   e s t i m a t e s   w e r e   p r i m a r i l y   i n   t h e   m e d i u m   a n d   l a r g e   r a n g e s   a n d   f a v o r e d   t h e   t r e a t m e n t   g r o u p .']</pre> <p>Alright, now we've preprocessed our wild RCT abstract into all of the same features our model was trained on, we can pass these features to our model and make sequence label predictions!</p> In\u00a0[130]: Copied! <pre># Make predictions on sample abstract features\n%%time\ntest_abstract_pred_probs = loaded_model.predict(x=(test_abstract_line_numbers_one_hot,\n                                                   test_abstract_total_lines_one_hot,\n                                                   tf.constant(abstract_lines),\n                                                   tf.constant(abstract_chars)))\ntest_abstract_pred_probs\n</pre> # Make predictions on sample abstract features %%time test_abstract_pred_probs = loaded_model.predict(x=(test_abstract_line_numbers_one_hot,                                                    test_abstract_total_lines_one_hot,                                                    tf.constant(abstract_lines),                                                    tf.constant(abstract_chars))) test_abstract_pred_probs <pre>1/1 [==============================] - 1s 1s/step\nCPU times: user 1.24 s, sys: 16 ms, total: 1.26 s\nWall time: 1.21 s\n</pre> Out[130]: <pre>array([[0.26442432, 0.11057511, 0.01761848, 0.57510155, 0.03228046],\n       [0.06930789, 0.03808675, 0.70767295, 0.09548164, 0.08945073],\n       [0.13771403, 0.04937523, 0.5468998 , 0.16338749, 0.10262337],\n       [0.09653305, 0.17096138, 0.52601266, 0.07597268, 0.13052028],\n       [0.04460111, 0.0888952 , 0.45653552, 0.04623139, 0.36373675],\n       [0.0326678 , 0.14643149, 0.5172315 , 0.04089198, 0.26277718],\n       [0.03291797, 0.17809066, 0.07167646, 0.02872156, 0.6885933 ],\n       [0.01844072, 0.13500078, 0.32094944, 0.03390973, 0.49169934]],\n      dtype=float32)</pre> In\u00a0[131]: Copied! <pre># Turn prediction probabilities into prediction classes\ntest_abstract_preds = tf.argmax(test_abstract_pred_probs, axis=1)\ntest_abstract_preds\n</pre> # Turn prediction probabilities into prediction classes test_abstract_preds = tf.argmax(test_abstract_pred_probs, axis=1) test_abstract_preds Out[131]: <pre>&lt;tf.Tensor: shape=(8,), dtype=int64, numpy=array([3, 2, 2, 2, 2, 2, 4, 4])&gt;</pre> <p>Now we've got the predicted sequence label for each line in our sample abstract, let's write some code to visualize each sentence with its predicted label.</p> In\u00a0[132]: Copied! <pre># Turn prediction class integers into string class names\ntest_abstract_pred_classes = [label_encoder.classes_[i] for i in test_abstract_preds]\ntest_abstract_pred_classes\n</pre> # Turn prediction class integers into string class names test_abstract_pred_classes = [label_encoder.classes_[i] for i in test_abstract_preds] test_abstract_pred_classes Out[132]: <pre>['OBJECTIVE',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'METHODS',\n 'RESULTS',\n 'RESULTS']</pre> In\u00a0[133]: Copied! <pre># Visualize abstract lines and predicted sequence labels\nfor i, line in enumerate(abstract_lines):\n  print(f\"{test_abstract_pred_classes[i]}: {line}\")\n</pre> # Visualize abstract lines and predicted sequence labels for i, line in enumerate(abstract_lines):   print(f\"{test_abstract_pred_classes[i]}: {line}\") <pre>OBJECTIVE: This RCT examined the efficacy of a manualized social intervention for children with HFASDs.\nMETHODS: Participants were randomly assigned to treatment or wait-list conditions.\nMETHODS: Treatment included instruction and therapeutic activities targeting social skills, face-emotion recognition, interest expansion, and interpretation of non-literal language.\nMETHODS: A response-cost program was applied to reduce problem behaviors and foster skills acquisition.\nMETHODS: Significant treatment effects were found for five of seven primary outcome measures (parent ratings and direct child measures).\nMETHODS: Secondary measures based on staff ratings (treatment group only) corroborated gains reported by parents.\nRESULTS: High levels of parent, child and staff satisfaction were reported, along with high levels of treatment fidelity.\nRESULTS: Standardized effect size estimates were primarily in the medium and large ranges and favored the treatment group.\n</pre> <p>Nice! Isn't that much easier to read? I mean, it looks like our model's predictions could be improved, but how cool is that?</p> <p>Imagine implementing our model to the backend of the PubMed website to format any unstructured RCT abstract on the site.</p> <p>Or there could even be a browser extension, called \"SkimLit\" which would add structure (powered by our model) to any unstructured RCT abtract.</p> <p>And if showed your medical researcher friend, and they thought the predictions weren't up to standard, there could be a button saying \"is this label correct?... if not, what should it be?\". That way the dataset, along with our model's future predictions, could be improved over time.</p> <p>Of course, there are many more ways we could go to improve the model, the usuability, the preprocessing functionality (e.g. functionizing our sample abstract preprocessing pipeline) but I'll leave these for the exercises/extensions.</p> <p>\ud83e\udd14 Question: How can we be sure the results of our test example from the wild are truly wild? Is there something we should check about the sample we're testing on?</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#09-milestone-project-2-skimlit","title":"09. Milestone Project 2: SkimLit \ud83d\udcc4\ud83d\udd25\u00b6","text":"<p>In the previous notebook (NLP fundamentals in TensorFlow), we went through some fundamental natural lanuage processing concepts. The main ones being tokenzation (turning words into numbers) and creating embeddings (creating a numerical representation of words).</p> <p>In this project, we're going to be putting what we've learned into practice.</p> <p>More specificially, we're going to be replicating the deep learning model behind the 2017 paper PubMed 200k RCT: a Dataset for Sequenctial Sentence Classification in Medical Abstracts.</p> <p>When it was released, the paper presented a new dataset called PubMed 200k RCT which consists of ~200,000 labelled Randomized Controlled Trial (RCT) abstracts.</p> <p>The goal of the dataset was to explore the ability for NLP models to classify sentences which appear in sequential order.</p> <p>In other words, given the abstract of a RCT, what role does each sentence serve in the abstract?</p> <p></p> <p>Example inputs (harder to read abstract from PubMed) and outputs (easier to read abstract) of the model we're going to build. The model will take an abstract wall of text and predict the section label each sentence should have.</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#model-input","title":"Model Input\u00b6","text":"<p>For example, can we train an NLP model which takes the following input (note: the following sample has had all numerical symbols replaced with \"@\"):</p> <p>To investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( OA ). A total of @ patients with primary knee OA were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks. Outcome measures included pain reduction and improvement in function scores and systemic inflammation markers. Pain was assessed using the visual analog pain scale ( @-@ mm ). Secondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and @-min walk distance ( @MWD )., Serum levels of interleukin @ ( IL-@ ) , IL-@ , tumor necrosis factor ( TNF ) - , and high-sensitivity C-reactive protein ( hsCRP ) were measured. There was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , PGA , and @MWD at @ weeks. The mean difference between treatment arms ( @ % CI ) was @ ( @-@ @ ) , p &lt; @ ; @ ( @-@ @ ) , p &lt; @ ; @ ( @-@ @ ) , p &lt; @ ; and @ ( @-@ @ ) , p &lt; @ , respectively. Further , there was a clinically relevant reduction in the serum levels of IL-@ , IL-@ , TNF - , and hsCRP at @ weeks in the intervention group when compared to the placebo group. These differences remained significant at @ weeks. The Outcome Measures in Rheumatology Clinical Trials-Osteoarthritis Research Society International responder rate was @ % in the intervention group and @ % in the placebo group ( p &lt; @ ). Low-dose oral prednisolone had both a short-term and a longer sustained effect resulting in less knee pain , better physical function , and attenuation of systemic inflammation in older patients with knee OA ( ClinicalTrials.gov identifier NCT@ ).</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#model-output","title":"Model output\u00b6","text":"<p>And returns the following output:</p> <pre><code>['###24293578\\n',\n 'OBJECTIVE\\tTo investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .\\n',\n 'METHODS\\tA total of @ patients with primary knee OA were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .\\n',\n 'METHODS\\tOutcome measures included pain reduction and improvement in function scores and systemic inflammation markers .\\n',\n 'METHODS\\tPain was assessed using the visual analog pain scale ( @-@ mm ) .\\n',\n 'METHODS\\tSecondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and @-min walk distance ( @MWD ) .\\n',\n 'METHODS\\tSerum levels of interleukin @ ( IL-@ ) , IL-@ , tumor necrosis factor ( TNF ) - , and high-sensitivity C-reactive protein ( hsCRP ) were measured .\\n',\n 'RESULTS\\tThere was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , PGA , and @MWD at @ weeks .\\n',\n 'RESULTS\\tThe mean difference between treatment arms ( @ % CI ) was @ ( @-@ @ ) , p &lt; @ ; @ ( @-@ @ ) , p &lt; @ ; @ ( @-@ @ ) , p &lt; @ ; and @ ( @-@ @ ) , p &lt; @ , respectively .\\n',\n 'RESULTS\\tFurther , there was a clinically relevant reduction in the serum levels of IL-@ , IL-@ , TNF - , and hsCRP at @ weeks in the intervention group when compared to the placebo group .\\n',\n 'RESULTS\\tThese differences remained significant at @ weeks .\\n',\n 'RESULTS\\tThe Outcome Measures in Rheumatology Clinical Trials-Osteoarthritis Research Society International responder rate was @ % in the intervention group and @ % in the placebo group ( p &lt; @ ) .\\n',\n 'CONCLUSIONS\\tLow-dose oral prednisolone had both a short-term and a longer sustained effect resulting in less knee pain , better physical function , and attenuation of systemic inflammation in older patients with knee OA ( ClinicalTrials.gov identifier NCT@ ) .\\n',\n '\\n']\n</code></pre>"},{"location":"09_SkimLit_nlp_milestone_project_2/#problem-in-a-sentence","title":"Problem in a sentence\u00b6","text":"<p>The number of RCT papers released is continuing to increase, those without structured abstracts can be hard to read and in turn slow down researchers moving through the literature.</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#solution-in-a-sentence","title":"Solution in a sentence\u00b6","text":"<p>Create an NLP model to classify abstract sentences into the role they play (e.g. objective, methods, results, etc)  to enable researchers to skim through the literature (hence SkimLit \ud83e\udd13\ud83d\udd25) and dive deeper when necessary.</p> <ol> <li>Where our data is coming from: PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts</li> <li>Where our model is coming from: Neural networks for joint sentence classification in medical paper abstracts.</li> </ol> <p>\ud83d\udcd6 Resources: Before going through the code in this notebook, you might want to get a background of what we're going to be doing. To do so, spend an hour (or two) going through the following papers and then return to this notebook:</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>Time to take what we've learned in the NLP fundmentals notebook and build our biggest NLP model yet:</p> <ul> <li>Downloading a text dataset (PubMed RCT200k from GitHub)</li> <li>Writing a preprocessing function to prepare our data for modelling</li> <li>Setting up a series of modelling experiments<ul> <li>Making a baseline (TF-IDF classifier)</li> <li>Deep models with different combinations of: token embeddings, character embeddings, pretrained embeddings, positional embeddings</li> </ul> </li> <li>Building our first multimodal model (taking multiple types of data inputs)<ul> <li>Replicating the model architecture from https://arxiv.org/abs/1612.05251</li> </ul> </li> <li>Find the most wrong predictions</li> <li>Making predictions on PubMed abstracts from the wild</li> </ul>"},{"location":"09_SkimLit_nlp_milestone_project_2/#how-you-should-approach-this-notebook","title":"How you should approach this notebook\u00b6","text":"<p>You can read through the descriptions and the code (it should all run, except for the cells which error on purpose), but there's a better option.</p> <p>Write all of the code yourself.</p> <p>Yes. I'm serious. Create a new notebook, and rewrite each line by yourself. Investigate it, see if you can break it, why does it break?</p> <p>You don't have to write the text descriptions but writing the code yourself is a great way to get hands-on experience.</p> <p>Don't worry if you make mistakes, we all do. The way to get better and make less mistakes is to write more code.</p> <p>\ud83d\udcd6 Resources:</p> <ul> <li>See the full set of course materials on GitHub: https://github.com/mrdbourke/tensorflow-deep-learning</li> <li>See code updates to this notebook on GitHub discussions: https://github.com/mrdbourke/tensorflow-deep-learning/discussions/557</li> </ul>"},{"location":"09_SkimLit_nlp_milestone_project_2/#confirm-access-to-a-gpu","title":"Confirm access to a GPU\u00b6","text":"<p>Since we're going to be building deep learning models, let's make sure we have a GPU.</p> <p>In Google Colab, you can set this up by going to Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU.</p> <p>If you don't have access to a GPU, the models we're building here will likely take up to 10x longer to run.</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#get-data","title":"Get data\u00b6","text":"<p>Before we can start building a model, we've got to download the PubMed 200k RCT dataset.</p> <p>In a phenomenal act of kindness, the authors of the paper have made the data they used for their research availably publically and for free in the form of .txt files on GitHub.</p> <p>We can copy them to our local directory using <code>git clone https://github.com/Franck-Dernoncourt/pubmed-rct</code>.</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#preprocess-data","title":"Preprocess data\u00b6","text":"<p>Okay, now we've downloaded some text data, do you think we're ready to model it?</p> <p>Wait...</p> <p>We've downloaded the data but we haven't even looked at it yet.</p> <p>What's the motto for getting familiar with any new dataset?</p> <p>I'll give you a clue, the word begins with \"v\" and we say it three times.</p> <p>Vibe, vibe, vibe?</p> <p>Sort of... we've definitely got to the feel the vibe of our data.</p> <p>Values, values, values?</p> <p>Right again, we want to see lots of values but not quite what we're looking for.</p> <p>Visualize, visualize, visualize?</p> <p>Boom! That's it. To get familiar and understand how we have to prepare our data for our deep learning models, we've got to visualize it.</p> <p>Because our data is in the form of text files, let's write some code to read each of the lines in a target file.</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#get-lists-of-sentences","title":"Get lists of sentences\u00b6","text":"<p>When we build our deep learning model, one of its main inputs will be a list of strings (the lines of an abstract).</p> <p>We can get these easily from our DataFrames by calling the <code>tolist()</code> method on our <code>\"text\"</code> columns.</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#make-numeric-labels-ml-models-require-numeric-labels","title":"Make numeric labels (ML models require numeric labels)\u00b6","text":"<p>We're going to create one hot and label encoded labels.</p> <p>We could get away with just making label encoded labels, however, TensorFlow's CategoricalCrossentropy loss function likes to have one hot encoded labels (this will enable us to use label smoothing later on).</p> <p>To numerically encode labels we'll use Scikit-Learn's <code>OneHotEncoder</code> and <code>LabelEncoder</code> classes.</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#label-encode-labels","title":"Label encode labels\u00b6","text":""},{"location":"09_SkimLit_nlp_milestone_project_2/#creating-a-series-of-model-experiments","title":"Creating a series of model experiments\u00b6","text":"<p>We've proprocessed our data so now, in true machine learning fashion, it's time to setup a series of modelling experiments.</p> <p>We'll start by creating a simple baseline model to obtain a score we'll try to beat by building more and more complex models as we move towards replicating the sequence model outlined in Neural networks for joint sentence classification in medical paper abstracts.</p> <p>For each model, we'll train it on the training data and evaluate it on the validation data.</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#model-0-getting-a-baseline","title":"Model 0: Getting a baseline\u00b6","text":"<p>Our first model we'll be a TF-IDF Multinomial Naive Bayes as recommended by Scikit-Learn's machine learning map.</p> <p>To build it, we'll create a Scikit-Learn <code>Pipeline</code> which uses the <code>TfidfVectorizer</code> class to convert our abstract sentences to numbers using the TF-IDF (term frequency-inverse document frequecy) algorithm and then learns to classify our sentences using the <code>MultinomialNB</code> aglorithm.</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#download-helper-functions-script","title":"Download helper functions script\u00b6","text":"<p>Let's get our <code>helper_functions.py</code> script we've been using to store helper functions we've created in previous notebooks.</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#preparing-our-data-for-deep-sequence-models","title":"Preparing our data for deep sequence models\u00b6","text":"<p>Excellent! We've got a working baseline to try and improve upon.</p> <p>But before we start building deeper models, we've got to create vectorization and embedding layers.</p> <p>The vectorization layer will convert our text to numbers and the embedding layer will capture the relationships between those numbers.</p> <p>To start creating our vectorization and embedding layers, we'll need to import the appropriate libraries (namely TensorFlow and NumPy).</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#create-text-vectorizer","title":"Create text vectorizer\u00b6","text":"<p>Now we've got a little more information about our texts, let's create a way to turn it into numbers.</p> <p>To do so, we'll use the <code>TextVectorization</code> layer from TensorFlow.</p> <p>We'll keep all the parameters default except for <code>max_tokens</code> (the number of unique words in our dataset) and <code>output_sequence_length</code> (our desired output length for each vectorized sentence).</p> <p>Section 3.2 of the PubMed 200k RCT paper states the vocabulary size of the PubMed 20k dataset as 68,000. So we'll use that as our <code>max_tokens</code> parameter.</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#create-custom-text-embedding","title":"Create custom text embedding\u00b6","text":"<p>Our <code>token_vectorization</code> layer maps the words in our text directly to numbers. However, this doesn't necessarily capture the relationships between those numbers.</p> <p>To create a richer numerical representation of our text, we can use an embedding.</p> <p>As our model learns (by going through many different examples of abstract sentences and their labels), it'll update its embedding to better represent the relationships between tokens in our corpus.</p> <p>We can create a trainable embedding layer using TensorFlow's <code>Embedding</code> layer.</p> <p>Once again, the main parameters we're concerned with here are the inputs and outputs of our <code>Embedding</code> layer.</p> <p>The <code>input_dim</code> parameter defines the size of our vocabulary. And the <code>output_dim</code> parameter defines the dimension of the embedding output.</p> <p>Once created, our embedding layer will take the integer outputs of our <code>text_vectorization</code> layer as inputs and convert them to feature vectors of size <code>output_dim</code>.</p> <p>Let's see it in action.</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#create-datasets-as-fast-as-possible","title":"Create datasets (as fast as possible)\u00b6","text":"<p>We've gone through all the trouble of preprocessing our datasets to be used with a machine learning model, however, there are still a few steps we can use to make them work faster with our models.</p> <p>Namely, the <code>tf.data</code> API provides methods which enable faster data loading.</p> <p>\ud83d\udcd6 Resource: For best practices on data loading in TensorFlow, check out the following:</p> <ul> <li>tf.data: Build TensorFlow input pipelines</li> <li>Better performance with the tf.data API</li> </ul> <p>The main steps we'll want to use with our data is to turn it into a <code>PrefetchDataset</code> of batches.</p> <p>Doing so we'll ensure TensorFlow loads our data onto the GPU as fast as possible, in turn leading to faster training time.</p> <p>To create a batched <code>PrefetchDataset</code> we can use the methods <code>batch()</code> and <code>prefetch()</code>, the parameter <code>tf.data.AUTOTUNE</code> will also allow TensorFlow to determine the optimal amount of compute to use to prepare datasets.</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#model-1-conv1d-with-token-embeddings","title":"Model 1: Conv1D with token embeddings\u00b6","text":"<p>Alright, we've now got a way to numerically represent our text and labels, time to build a series of deep models to try and improve upon our baseline.</p> <p>All of our deep models will follow a similar structure:</p> <pre><code>Input (text) -&gt; Tokenize -&gt; Embedding -&gt; Layers -&gt; Output (label probability)\n</code></pre> <p>The main component we'll be changing throughout is the <code>Layers</code> component. Because any modern deep NLP model requires text to be converted into an embedding before meaningful patterns can be discovered within.</p> <p>The first model we're going to build is a 1-dimensional Convolutional Neural Network.</p> <p>We're also going to be following the standard machine learning workflow of:</p> <ul> <li>Build model</li> <li>Train model</li> <li>Evaluate model (make predictions and compare to ground truth)</li> </ul>"},{"location":"09_SkimLit_nlp_milestone_project_2/#model-2-feature-extraction-with-pretrained-token-embeddings","title":"Model 2: Feature extraction with pretrained token embeddings\u00b6","text":"<p>Training our own embeddings took a little while to run, slowing our experiments down.</p> <p>Since we're moving towards replicating the model architecture in Neural Networks for Joint Sentence Classification in Medical Paper Abstracts, it mentions they used a pretrained GloVe embedding as a way to initialise their token embeddings.</p> <p>To emulate this, let's see what results we can get with the pretrained Universal Sentence Encoder embeddings from TensorFlow Hub.</p> <p>\ud83d\udd11 Note: We could use GloVe embeddings as per the paper but since we're working with TensorFlow, we'll use what's available from TensorFlow Hub (GloVe embeddings aren't). We'll save using pretrained GloVe embeddings as an extension.</p> <p>The model structure will look like:</p> <pre><code>Inputs (string) -&gt; Pretrained embeddings from TensorFlow Hub (Universal Sentence Encoder) -&gt; Layers -&gt; Output (prediction probabilities)\n</code></pre> <p>You'll notice the lack of tokenization layer we've used in a previous model. This is because the Universal Sentence Encoder (USE) takes care of tokenization for us.</p> <p>This type of model is called transfer learning, or more specifically, feature extraction transfer learning. In other words, taking the patterns a model has learned elsewhere and applying it to our own problem.</p> <p> The feature extractor model we're building using a pretrained embedding from TensorFlow Hub.</p> <p>To download the pretrained USE into a layer we can use in our model, we can use the <code>hub.KerasLayer</code> class.</p> <p>We'll keep the pretrained embeddings frozen (by setting <code>trainable=False</code>) and add a trainable couple of layers on the top to tailor the model outputs to our own data.</p> <p>\ud83d\udd11 Note: Due to having to download a relatively large model (~916MB), the cell below may take a little while to run.</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#building-and-fitting-an-nlp-feature-extraction-model-from-tensorflow-hub","title":"Building and fitting an NLP feature extraction model from TensorFlow Hub\u00b6","text":""},{"location":"09_SkimLit_nlp_milestone_project_2/#model-3-conv1d-with-character-embeddings","title":"Model 3: Conv1D with character embeddings\u00b6","text":""},{"location":"09_SkimLit_nlp_milestone_project_2/#creating-a-character-level-tokenizer","title":"Creating a character-level tokenizer\u00b6","text":"<p>The Neural Networks for Joint Sentence Classification in Medical Paper Abstracts paper mentions their model uses a hybrid of token and character embeddings.</p> <p>We've built models with a custom token embedding and a pretrained token embedding, how about we build one using a character embedding?</p> <p>The difference between a character and token embedding is that the character embedding is created using sequences split into characters (e.g. <code>hello</code> -&gt; [<code>h</code>, <code>e</code>, <code>l</code>, <code>l</code>, <code>o</code>]) where as a token embedding is created on sequences split into tokens.</p> <p> Token level embeddings split sequences into tokens (words) and embeddings each of them, character embeddings split sequences into characters and creates a feature vector for each.</p> <p>We can create a character-level embedding by first vectorizing our sequences (after they've been split into characters) using the <code>TextVectorization</code> class and then passing those vectorized sequences through an <code>Embedding</code> layer.</p> <p>Before we can vectorize our sequences on a character-level we'll need to split them into characters. Let's write a function to do so.</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#creating-a-character-level-embedding","title":"Creating a character-level embedding\u00b6","text":"<p>We've got a way to vectorize our character-level sequences, now's time to create a character-level embedding.</p> <p>Just like our custom token embedding, we can do so using the <code>tensorflow.keras.layers.Embedding</code> class.</p> <p>Our character-level embedding layer requires an input dimension and output dimension.</p> <p>The input dimension (<code>input_dim</code>) will be equal to the number of different characters in our <code>char_vocab</code> (28). And since we're following the structure of the model in Figure 1 of Neural Networks for Joint Sentence Classification in Medical Paper Abstracts, the output dimension of the character embedding (<code>output_dim</code>) will be 25.</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#building-a-conv1d-model-to-fit-on-character-embeddings","title":"Building a Conv1D model to fit on character embeddings\u00b6","text":"<p>Now we've got a way to turn our character-level sequences into numbers (<code>char_vectorizer</code>) as well as numerically represent them as an embedding (<code>char_embed</code>) let's test how effective they are at encoding the information in our sequences by creating a character-level sequence model.</p> <p>The model will have the same structure as our custom token embedding model (<code>model_1</code>) except it'll take character-level sequences as input instead of token-level sequences.</p> <pre><code>Input (character-level text) -&gt; Tokenize -&gt; Embedding -&gt; Layers (Conv1D, GlobalMaxPool1D) -&gt; Output (label probability)\n</code></pre>"},{"location":"09_SkimLit_nlp_milestone_project_2/#model-4-combining-pretrained-token-embeddings-character-embeddings-hybrid-embedding-layer","title":"Model 4: Combining pretrained token embeddings + character embeddings (hybrid embedding layer)\u00b6","text":"<p>Alright, now things are going to get spicy.</p> <p>In moving closer to build a model similar to the one in Figure 1 of Neural Networks for Joint Sentence Classification in Medical Paper Abstracts, it's time we tackled the hybrid token embedding layer they speak of.</p> <p>This hybrid token embedding layer is a combination of token embeddings and character embeddings. In other words, they create a stacked embedding to represent sequences before passing them to the sequence label prediction layer.</p> <p>So far we've built two models which have used token and character-level embeddings, however, these two models have used each of these embeddings exclusively.</p> <p>To start replicating (or getting close to replicating) the model in Figure 1, we're going to go through the following steps:</p> <ol> <li>Create a token-level model (similar to <code>model_1</code>)</li> <li>Create a character-level model (similar to <code>model_3</code> with a slight modification to reflect the paper)</li> <li>Combine (using <code>layers.Concatenate</code>) the outputs of 1 and 2</li> <li>Build a series of output layers on top of 3 similar to Figure 1 and section 4.2 of Neural Networks for Joint Sentence Classification in Medical Paper Abstracts</li> <li>Construct a model which takes token and character-level sequences as input and produces sequence label probabilities as output</li> </ol>"},{"location":"09_SkimLit_nlp_milestone_project_2/#combining-token-and-character-data-into-a-tfdata-dataset","title":"Combining token and character data into a <code>tf.data</code> dataset\u00b6","text":""},{"location":"09_SkimLit_nlp_milestone_project_2/#fitting-a-model-on-token-and-character-level-sequences","title":"Fitting a model on token and character-level sequences\u00b6","text":""},{"location":"09_SkimLit_nlp_milestone_project_2/#model-5-transfer-learning-with-pretrained-token-embeddings-character-embeddings-positional-embeddings","title":"Model 5: Transfer Learning with pretrained token embeddings + character embeddings + positional embeddings\u00b6","text":"<p>It seems like combining token embeddings and character embeddings gave our model a little performance boost.</p> <p>But there's one more piece of the puzzle we can add in.</p> <p>What if we engineered our own features into the model?</p> <p>Meaning, what if we took our own knowledge about the data and encoded it in a numerical way to give our model more information about our samples?</p> <p>The process of applying your own knowledge to build features as input to a model is called feature engineering.</p> <p>Can you think of something important about the sequences we're trying to classify?</p> <p>If you were to look at an abstract, would you expect the sentences to appear in order? Or does it make sense if they were to appear sequentially? For example, sequences labelled <code>CONCLUSIONS</code> at the beggining and sequences labelled <code>OBJECTIVE</code> at the end?</p> <p>Abstracts typically come in a sequential order, such as:</p> <ul> <li><code>OBJECTIVE</code> ...</li> <li><code>METHODS</code> ...</li> <li><code>METHODS</code> ...</li> <li><code>METHODS</code> ...</li> <li><code>RESULTS</code> ...</li> <li><code>CONCLUSIONS</code> ...</li> </ul> <p>Or</p> <ul> <li><code>BACKGROUND</code> ...</li> <li><code>OBJECTIVE</code> ...</li> <li><code>METHODS</code> ...</li> <li><code>METHODS</code> ...</li> <li><code>RESULTS</code> ...</li> <li><code>RESULTS</code> ...</li> <li><code>CONCLUSIONS</code> ...</li> <li><code>CONCLUSIONS</code> ...</li> </ul> <p>Of course, we can't engineer the sequence labels themselves into the training data (we don't have these at test time), but we can encode the order of a set of sequences in an abstract.</p> <p>For example,</p> <ul> <li><code>Sentence 1 of 10</code> ...</li> <li><code>Sentence 2 of 10</code> ...</li> <li><code>Sentence 3 of 10</code> ...</li> <li><code>Sentence 4 of 10</code> ...</li> <li>...</li> </ul> <p>You might've noticed this when we created our <code>preprocess_text_with_line_numbers()</code> function. When we read in a text file of abstracts, we counted the number of lines in an abstract as well as the number of each line itself.</p> <p>Doing this led to the <code>\"line_number\"</code> and <code>\"total_lines\"</code> columns of our DataFrames.</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#create-positional-embeddings","title":"Create positional embeddings\u00b6","text":"<p>Okay, enough talk about positional embeddings, let's create them.</p> <p>Since our <code>\"line_number\"</code> and <code>\"total_line\"</code> columns are already numerical, we could pass them as they are to our model.</p> <p>But to avoid our model thinking a line with <code>\"line_number\"=5</code> is five times greater than a line with <code>\"line_number\"=1</code>, we'll use one-hot-encoding to encode our <code>\"line_number\"</code> and <code>\"total_lines\"</code> features.</p> <p>To do this, we can use the <code>tf.one_hot</code> utility.</p> <p><code>tf.one_hot</code> returns a one-hot-encoded tensor. It accepts an array (or tensor) as input and the <code>depth</code> parameter determines the dimension of the returned tensor.</p> <p>To figure out what we should set the <code>depth</code> parameter to, let's investigate the distribution of the <code>\"line_number\"</code> column.</p> <p>\ud83d\udd11 Note: When it comes to one-hot-encoding our features, Scikit-Learn's <code>OneHotEncoder</code> class is another viable option here.</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#building-a-tribrid-embedding-model","title":"Building a tribrid embedding model\u00b6","text":"<p>Woohoo! Positional embedding tensors ready.</p> <p>It's time to build the biggest model we've built yet. One which incorporates token embeddings, character embeddings and our newly crafted positional embeddings.</p> <p>We'll be venturing into uncovered territory but there will be nothing here you haven't practiced before.</p> <p>More specifically we're going to go through the following steps:</p> <ol> <li>Create a token-level model (similar to <code>model_1</code>)</li> <li>Create a character-level model (similar to <code>model_3</code> with a slight modification to reflect the paper)</li> <li>Create a <code>\"line_number\"</code> model (takes in one-hot-encoded <code>\"line_number\"</code> tensor and passes it through a non-linear layer)</li> <li>Create a <code>\"total_lines\"</code> model (takes in one-hot-encoded <code>\"total_lines\"</code> tensor and passes it through a non-linear layer)</li> <li>Combine (using <code>layers.Concatenate</code>) the outputs of 1 and 2 into a token-character-hybrid embedding and pass it series of output to Figure 1 and section 4.2 of Neural Networks for Joint Sentence Classification in Medical Paper Abstracts</li> <li>Combine (using <code>layers.Concatenate</code>) the outputs of 3, 4 and 5 into a token-character-positional tribrid embedding</li> <li>Create an output layer to accept the tribrid embedding and output predicted label probabilities</li> <li>Combine the inputs of 1, 2, 3, 4 and outputs of 7 into a <code>tf.keras.Model</code></li> </ol> <p>Woah! That's alot... but nothing we're not capable of. Let's code it.</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#create-tribrid-embedding-datasets-and-fit-tribrid-model","title":"Create tribrid embedding datasets and fit tribrid model\u00b6","text":"<p>Model compiled!</p> <p>Again, to keep our experiments swift, let's fit on 20,000 examples for 3 epochs.</p> <p>This time our model requires four feature inputs:</p> <ol> <li>Train line numbers one-hot tensor (<code>train_line_numbers_one_hot</code>)</li> <li>Train total lines one-hot tensor (<code>train_total_lines_one_hot</code>)</li> <li>Token-level sequences tensor (<code>train_sentences</code>)</li> <li>Char-level sequences tensor (<code>train_chars</code>)</li> </ol> <p>We can pass these as tuples to our <code>tf.data.Dataset.from_tensor_slices()</code> method to create appropriately shaped and batched <code>PrefetchedDataset</code>'s.</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#compare-model-results","title":"Compare model results\u00b6","text":"<p>Far out, we've come a long way. From a baseline model to training a model containing three different kinds of embeddings.</p> <p>Now it's time to compare each model's performance against each other.</p> <p>We'll also be able to compare our model's to the PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts paper.</p> <p>Since all of our model results are in dictionaries, let's combine them into a pandas DataFrame to visualize them.</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#save-and-load-best-performing-model","title":"Save and load best performing model\u00b6","text":"<p>Since we've been through a fair few experiments, it's a good idea to save our best performing model so we can reuse it without having to retrain it.</p> <p>We can save our best performing model by calling the <code>save()</code> method on it.</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#make-predictions-and-evalaute-them-against-the-truth-labels","title":"Make predictions and evalaute them against the truth labels\u00b6","text":"<p>To make sure our model saved and loaded correctly, let's make predictions with it, evaluate them and then compare them to the prediction results we calculated earlier.</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#evaluate-model-on-test-dataset","title":"Evaluate model on test dataset\u00b6","text":"<p>To make our model's performance more comparable with the results reported in Table 3 of the PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts paper, let's make predictions on the test dataset and evaluate them.</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#find-most-wrong","title":"Find most wrong\u00b6","text":"<p>One of the best ways to investigate where your model is going wrong (or potentially where your data is wrong) is to visualize the \"most wrong\" predictions.</p> <p>The most wrong predictions are samples where the model has made a prediction with a high probability but has gotten it wrong (the model's prediction disagreess with the ground truth label).</p> <p>Looking at the most wrong predictions can give us valuable information on how to improve further models or fix the labels in our data.</p> <p>Let's write some code to help us visualize the most wrong predictions from the test dataset.</p> <p>First we'll convert all of our integer-based test predictions into their string-based class names.</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#make-example-predictions","title":"Make example predictions\u00b6","text":"<p>Okay, we've made some predictions on the test dataset, now's time to really test our model out.</p> <p>To do so, we're going to get some data from the wild and see how our model performs.</p> <p>In other words, were going to find an RCT abstract from PubMed, preprocess the text so it works with our model, then pass each sequence in the wild abstract through our model to see what label it predicts.</p> <p>For an appropriate sample, we'll need to search PubMed for RCT's (randomized controlled trials) without abstracts which have been split up (on exploring PubMed you'll notice many of the abstracts are already preformatted into separate sections, this helps dramatically with readability).</p> <p>Going through various PubMed studies, I managed to find the following unstructured abstract from RCT of a manualized social treatment for high-functioning autism spectrum disorders:</p> <p>This RCT examined the efficacy of a manualized social intervention for children with HFASDs. Participants were randomly assigned to treatment or wait-list conditions. Treatment included instruction and therapeutic activities targeting social skills, face-emotion recognition, interest expansion, and interpretation of non-literal language. A response-cost program was applied to reduce problem behaviors and foster skills acquisition. Significant treatment effects were found for five of seven primary outcome measures (parent ratings and direct child measures). Secondary measures based on staff ratings (treatment group only) corroborated gains reported by parents. High levels of parent, child and staff satisfaction were reported, along with high levels of treatment fidelity. Standardized effect size estimates were primarily in the medium and large ranges and favored the treatment group.</p> <p>Looking at the large chunk of text can seem quite intimidating. Now imagine you're a medical researcher trying to skim through the literature to find a study relevant to your work.</p> <p>Sounds like quite the challenge right?</p> <p>Enter SkimLit \ud83e\udd13\ud83d\udd25!</p> <p>Let's see what our best model so far (<code>model_5</code>) makes of the above abstract.</p> <p>But wait...</p> <p>As you might've guessed the above abstract hasn't been formatted in the same structure as the data our model has been trained on. Therefore, before we can make a prediction on it, we need to preprocess it just as we have our other sequences.</p> <p>More specifically, for each abstract, we'll need to:</p> <ol> <li>Split it into sentences (lines).</li> <li>Split it into characters.</li> <li>Find the number of each line.</li> <li>Find the total number of lines.</li> </ol> <p>Starting with number 1, there are a couple of ways to split our abstracts into actual sentences. A simple one would be to use Python's in-built <code>split()</code> string method, splitting the abstract wherever a fullstop appears. However, can you imagine where this might go wrong?</p> <p>Another more advanced option would be to leverage spaCy's (a very powerful NLP library) <code>sentencizer</code> class. Which is an easy to use sentence splitter based on spaCy's English language model.</p> <p>I've prepared some abstracts from PubMed RCT papers to try our model on, we can download them from GitHub.</p>"},{"location":"09_SkimLit_nlp_milestone_project_2/#exercises","title":"\ud83d\udee0 Exercises\u00b6","text":"<ol> <li>Train <code>model_5</code> on all of the data in the training dataset for as many epochs until it stops improving. Since this might take a while, you might want to use:</li> </ol> <ul> <li><code>tf.keras.callbacks.ModelCheckpoint</code> to save the model's best weights only.</li> <li><code>tf.keras.callbacks.EarlyStopping</code> to stop the model from training once the validation loss has stopped improving for ~3 epochs.</li> </ul> <ol> <li>Checkout the Keras guide on using pretrained GloVe embeddings. Can you get this working with one of our models?</li> </ol> <ul> <li>Hint: You'll want to incorporate it with a custom token Embedding layer.</li> <li>It's up to you whether or not you fine-tune the GloVe embeddings or leave them frozen.</li> </ul> <ol> <li>Try replacing the TensorFlow Hub Universal Sentence Encoder pretrained  embedding for the TensorFlow Hub BERT PubMed expert (a language model pretrained on PubMed texts) pretrained embedding. Does this effect results?</li> </ol> <ul> <li>Note: Using the BERT PubMed expert pretrained embedding requires an extra preprocessing step for sequences (as detailed in the TensorFlow Hub guide).</li> <li>Does the BERT model beat the results mentioned in this paper? https://arxiv.org/pdf/1710.06071.pdf</li> </ul> <ol> <li>What happens if you were to merge our <code>line_number</code> and <code>total_lines</code> features for each sequence? For example, created a <code>X_of_Y</code> feature instead? Does this effect model performance?</li> </ol> <ul> <li>Another example: <code>line_number=1</code> and <code>total_lines=11</code> turns into <code>line_of_X=1_of_11</code>.</li> </ul> <ol> <li>Write a function (or series of functions) to take a sample abstract string, preprocess it (in the same way our model has been trained), make a prediction on each sequence in the abstract and return the abstract in the format:</li> </ol> <ul> <li><code>PREDICTED_LABEL</code>: <code>SEQUENCE</code></li> <li><code>PREDICTED_LABEL</code>: <code>SEQUENCE</code></li> <li><code>PREDICTED_LABEL</code>: <code>SEQUENCE</code></li> <li><code>PREDICTED_LABEL</code>: <code>SEQUENCE</code></li> <li>...<ul> <li>You can find your own unstrcutured RCT abstract from PubMed or try this one from: Baclofen promotes alcohol abstinence in alcohol dependent cirrhotic patients with hepatitis C virus (HCV) infection.</li> </ul> </li> </ul>"},{"location":"09_SkimLit_nlp_milestone_project_2/#extra-curriculum","title":"\ud83d\udcd6 Extra-curriculum\u00b6","text":"<ul> <li>For more on working with text/spaCy, see spaCy's advanced NLP course. If you're going to be working on production-level NLP problems, you'll probably end up using spaCy.</li> <li>For another look at how to approach a text classification problem like the one we've just gone through, I'd suggest going through Google's Machine Learning Course for text classification.</li> <li>Since our dataset has imbalanced classes (as with many real-world datasets), so it might be worth looking into the TensorFlow guide for different methods to training a model with imbalanced classes.</li> </ul>"},{"location":"10_time_series_forecasting_in_tensorflow/","title":"10. Milestone Project 3: Time series forecasting in TensorFlow (BitPredict \ud83d\udcb0\ud83d\udcc8)","text":"In\u00a0[\u00a0]: Copied! <pre># Check for GPU\n!nvidia-smi -L\n</pre> # Check for GPU !nvidia-smi -L <pre>GPU 0: Tesla K80 (UUID: GPU-c7456639-4229-1150-8316-e4197bf2c93e)\n</pre> In\u00a0[\u00a0]: Copied! <pre># Download Bitcoin historical data from GitHub \n# Note: you'll need to select \"Raw\" to download the data in the correct format\n!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\n</pre> # Download Bitcoin historical data from GitHub  # Note: you'll need to select \"Raw\" to download the data in the correct format !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv  <pre>--2021-09-27 03:40:22--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 178509 (174K) [text/plain]\nSaving to: \u2018BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\u2019\n\nBTC_USD_2013-10-01_ 100%[===================&gt;] 174.33K  --.-KB/s    in 0.01s   \n\n2021-09-27 03:40:22 (16.5 MB/s) - \u2018BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\u2019 saved [178509/178509]\n\n</pre> In\u00a0[\u00a0]: Copied! <pre># Import with pandas \nimport pandas as pd\n# Parse dates and set date column to index\ndf = pd.read_csv(\"/content/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\", \n                 parse_dates=[\"Date\"], \n                 index_col=[\"Date\"]) # parse the date column (tell pandas column 1 is a datetime)\ndf.head()\n</pre> # Import with pandas  import pandas as pd # Parse dates and set date column to index df = pd.read_csv(\"/content/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\",                   parse_dates=[\"Date\"],                   index_col=[\"Date\"]) # parse the date column (tell pandas column 1 is a datetime) df.head() Out[\u00a0]: Currency Closing Price (USD) 24h Open (USD) 24h High (USD) 24h Low (USD) Date 2013-10-01 BTC 123.65499 124.30466 124.75166 122.56349 2013-10-02 BTC 125.45500 123.65499 125.75850 123.63383 2013-10-03 BTC 108.58483 125.45500 125.66566 83.32833 2013-10-04 BTC 118.67466 108.58483 118.67500 107.05816 2013-10-05 BTC 121.33866 118.67466 121.93633 118.00566 <p>Looking good! Let's get some more info.</p> In\u00a0[\u00a0]: Copied! <pre>df.info()\n</pre> df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 2787 entries, 2013-10-01 to 2021-05-18\nData columns (total 5 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   Currency             2787 non-null   object \n 1   Closing Price (USD)  2787 non-null   float64\n 2   24h Open (USD)       2787 non-null   float64\n 3   24h High (USD)       2787 non-null   float64\n 4   24h Low (USD)        2787 non-null   float64\ndtypes: float64(4), object(1)\nmemory usage: 130.6+ KB\n</pre> <p>Because we told pandas to parse the date column and set it as the index, its not in the list of columns.</p> <p>You can also see there isn't many samples.</p> In\u00a0[\u00a0]: Copied! <pre># How many samples do we have?\nlen(df)\n</pre> # How many samples do we have? len(df) Out[\u00a0]: <pre>2787</pre> <p>We've collected the historical price of Bitcoin for the past ~8 years but there's only 2787 total samples.</p> <p>This is something you'll run into with time series data problems. Often, the number of samples isn't as large as other kinds of data.</p> <p>For example, collecting one sample at different time frames results in:</p> 1 sample per timeframe Number of samples per year Second 31,536,000 Hour 8,760 Day 365 Week 52 Month 12 <p>\ud83d\udd11 Note: The frequency at which a time series value is collected is often referred to as seasonality. This is usually mesaured in number of samples per year. For example, collecting the price of Bitcoin once per day would result in a time series with a seasonality of 365. Time series data collected with different seasonality values often exhibit seasonal patterns (e.g. electricity demand behing higher in Summer months for air conditioning than Winter months). For more on different time series patterns, see Forecasting: Principles and Practice Chapter 2.3.</p> <p> Example of different kinds of patterns you'll see in time series data. Notice the bottom right time series (Google stock price changes) has little to no patterns, making it difficult to predict. See Forecasting: Principles and Practice Chapter 2.3 for full graphic.</p> <p>Deep learning algorithms usually flourish with lots of data, in the range of thousands to millions of samples.</p> <p>In our case, we've got the daily prices of Bitcoin, a max of 365 samples per year.</p> <p>But that doesn't we can't try them with our data.</p> <p>To simplify, let's remove some of the columns from our data so we're only left with a date index and the closing price.</p> In\u00a0[\u00a0]: Copied! <pre># Only want closing price for each day \nbitcoin_prices = pd.DataFrame(df[\"Closing Price (USD)\"]).rename(columns={\"Closing Price (USD)\": \"Price\"})\nbitcoin_prices.head()\n</pre> # Only want closing price for each day  bitcoin_prices = pd.DataFrame(df[\"Closing Price (USD)\"]).rename(columns={\"Closing Price (USD)\": \"Price\"}) bitcoin_prices.head() Out[\u00a0]: Price Date 2013-10-01 123.65499 2013-10-02 125.45500 2013-10-03 108.58483 2013-10-04 118.67466 2013-10-05 121.33866 <p>Much better!</p> <p>But that's only five days worth of Bitcoin prices, let's plot everything we've got.</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nbitcoin_prices.plot(figsize=(10, 7))\nplt.ylabel(\"BTC Price\")\nplt.title(\"Price of Bitcoin from 1 Oct 2013 to 18 May 2021\", fontsize=16)\nplt.legend(fontsize=14);\n</pre> import matplotlib.pyplot as plt bitcoin_prices.plot(figsize=(10, 7)) plt.ylabel(\"BTC Price\") plt.title(\"Price of Bitcoin from 1 Oct 2013 to 18 May 2021\", fontsize=16) plt.legend(fontsize=14); <p>Woah, looks like it would've been a good idea to buy Bitcoin back in 2014.</p> In\u00a0[\u00a0]: Copied! <pre># Importing and formatting historical Bitcoin data with Python\nimport csv\nfrom datetime import datetime\n\ntimesteps = []\nbtc_price = []\nwith open(\"/content/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\", \"r\") as f:\n  csv_reader = csv.reader(f, delimiter=\",\") # read in the target CSV\n  next(csv_reader) # skip first line (this gets rid of the column titles)\n  for line in csv_reader:\n    timesteps.append(datetime.strptime(line[1], \"%Y-%m-%d\")) # get the dates as dates (not strings), strptime = string parse time\n    btc_price.append(float(line[2])) # get the closing price as float\n\n# View first 10 of each\ntimesteps[:10], btc_price[:10]\n</pre> # Importing and formatting historical Bitcoin data with Python import csv from datetime import datetime  timesteps = [] btc_price = [] with open(\"/content/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\", \"r\") as f:   csv_reader = csv.reader(f, delimiter=\",\") # read in the target CSV   next(csv_reader) # skip first line (this gets rid of the column titles)   for line in csv_reader:     timesteps.append(datetime.strptime(line[1], \"%Y-%m-%d\")) # get the dates as dates (not strings), strptime = string parse time     btc_price.append(float(line[2])) # get the closing price as float  # View first 10 of each timesteps[:10], btc_price[:10] Out[\u00a0]: <pre>([datetime.datetime(2013, 10, 1, 0, 0),\n  datetime.datetime(2013, 10, 2, 0, 0),\n  datetime.datetime(2013, 10, 3, 0, 0),\n  datetime.datetime(2013, 10, 4, 0, 0),\n  datetime.datetime(2013, 10, 5, 0, 0),\n  datetime.datetime(2013, 10, 6, 0, 0),\n  datetime.datetime(2013, 10, 7, 0, 0),\n  datetime.datetime(2013, 10, 8, 0, 0),\n  datetime.datetime(2013, 10, 9, 0, 0),\n  datetime.datetime(2013, 10, 10, 0, 0)],\n [123.65499,\n  125.455,\n  108.58483,\n  118.67466,\n  121.33866,\n  120.65533,\n  121.795,\n  123.033,\n  124.049,\n  125.96116])</pre> <p>Beautiful! Now, let's see how things look.</p> In\u00a0[\u00a0]: Copied! <pre># Plot from CSV\nimport matplotlib.pyplot as plt\nimport numpy as np\nplt.figure(figsize=(10, 7))\nplt.plot(timesteps, btc_price)\nplt.title(\"Price of Bitcoin from 1 Oct 2013 to 18 May 2021\", fontsize=16)\nplt.xlabel(\"Date\")\nplt.ylabel(\"BTC Price\");\n</pre> # Plot from CSV import matplotlib.pyplot as plt import numpy as np plt.figure(figsize=(10, 7)) plt.plot(timesteps, btc_price) plt.title(\"Price of Bitcoin from 1 Oct 2013 to 18 May 2021\", fontsize=16) plt.xlabel(\"Date\") plt.ylabel(\"BTC Price\"); <p>Ho ho! Would you look at that! Just like the pandas plot. And because we formatted the <code>timesteps</code> to be <code>datetime</code> objects, <code>matplotlib</code> displays a fantastic looking date axis.</p> In\u00a0[\u00a0]: Copied! <pre># Get bitcoin date array\ntimesteps = bitcoin_prices.index.to_numpy()\nprices = bitcoin_prices[\"Price\"].to_numpy()\n\ntimesteps[:10], prices[:10]\n</pre> # Get bitcoin date array timesteps = bitcoin_prices.index.to_numpy() prices = bitcoin_prices[\"Price\"].to_numpy()  timesteps[:10], prices[:10] Out[\u00a0]: <pre>(array(['2013-10-01T00:00:00.000000000', '2013-10-02T00:00:00.000000000',\n        '2013-10-03T00:00:00.000000000', '2013-10-04T00:00:00.000000000',\n        '2013-10-05T00:00:00.000000000', '2013-10-06T00:00:00.000000000',\n        '2013-10-07T00:00:00.000000000', '2013-10-08T00:00:00.000000000',\n        '2013-10-09T00:00:00.000000000', '2013-10-10T00:00:00.000000000'],\n       dtype='datetime64[ns]'),\n array([123.65499, 125.455  , 108.58483, 118.67466, 121.33866, 120.65533,\n        121.795  , 123.033  , 124.049  , 125.96116]))</pre> <p>And now we'll use the ever faithful <code>train_test_split</code> from Scikit-Learn to create our train and test sets.</p> In\u00a0[\u00a0]: Copied! <pre># Wrong way to make train/test sets for time series\nfrom sklearn.model_selection import train_test_split \n\nX_train, X_test, y_train, y_test = train_test_split(timesteps, # dates\n                                                    prices, # prices\n                                                    test_size=0.2,\n                                                    random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n</pre> # Wrong way to make train/test sets for time series from sklearn.model_selection import train_test_split   X_train, X_test, y_train, y_test = train_test_split(timesteps, # dates                                                     prices, # prices                                                     test_size=0.2,                                                     random_state=42) X_train.shape, X_test.shape, y_train.shape, y_test.shape  Out[\u00a0]: <pre>((2229,), (558,), (2229,), (558,))</pre> <p>Looks like the splits worked well, but let's not trust numbers on a page, let's visualize, visualize, visualize!</p> In\u00a0[\u00a0]: Copied! <pre># Let's plot wrong train and test splits\nplt.figure(figsize=(10, 7))\nplt.scatter(X_train, y_train, s=5, label=\"Train data\")\nplt.scatter(X_test, y_test, s=5, label=\"Test data\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"BTC Price\")\nplt.legend(fontsize=14)\nplt.show();\n</pre> # Let's plot wrong train and test splits plt.figure(figsize=(10, 7)) plt.scatter(X_train, y_train, s=5, label=\"Train data\") plt.scatter(X_test, y_test, s=5, label=\"Test data\") plt.xlabel(\"Date\") plt.ylabel(\"BTC Price\") plt.legend(fontsize=14) plt.show(); <p>Hmmm... what's wrong with this plot?</p> <p>Well, let's remind ourselves of what we're trying to do.</p> <p>We're trying to use the historical price of Bitcoin to predict future prices of Bitcoin.</p> <p>With this in mind, our seen data (training set) is what?</p> <p>Prices of Bitcoin in the past.</p> <p>And our unseen data (test set) is?</p> <p>Prices of Bitcoin in the future.</p> <p>Does the plot above reflect this?</p> <p>No.</p> <p>Our test data is scattered all throughout the training data.</p> <p>This kind of random split is okay for datasets without a time component (such as images or passages of text for classification problems) but for time series, we've got to take the time factor into account.</p> <p>To fix this, we've got to split our data in a way that reflects what we're actually trying to do.</p> <p>We need to split our historical Bitcoin data to have a dataset that reflects the past (train set) and a dataset that reflects the future (test set).</p> In\u00a0[\u00a0]: Copied! <pre># Create train and test splits the right way for time series data\nsplit_size = int(0.8 * len(prices)) # 80% train, 20% test\n\n# Create train data splits (everything before the split)\nX_train, y_train = timesteps[:split_size], prices[:split_size]\n\n# Create test data splits (everything after the split)\nX_test, y_test = timesteps[split_size:], prices[split_size:]\n\nlen(X_train), len(X_test), len(y_train), len(y_test)\n</pre> # Create train and test splits the right way for time series data split_size = int(0.8 * len(prices)) # 80% train, 20% test  # Create train data splits (everything before the split) X_train, y_train = timesteps[:split_size], prices[:split_size]  # Create test data splits (everything after the split) X_test, y_test = timesteps[split_size:], prices[split_size:]  len(X_train), len(X_test), len(y_train), len(y_test) Out[\u00a0]: <pre>(2229, 558, 2229, 558)</pre> <p>Okay, looks like our custom made splits are the same lengths as the splits we made with <code>train_test_split</code>.</p> <p>But again, these are numbers on a page.</p> <p>And you know how the saying goes, trust one eye more than two ears.</p> <p>Let's visualize.</p> In\u00a0[\u00a0]: Copied! <pre># Plot correctly made splits\nplt.figure(figsize=(10, 7))\nplt.scatter(X_train, y_train, s=5, label=\"Train data\")\nplt.scatter(X_test, y_test, s=5, label=\"Test data\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"BTC Price\")\nplt.legend(fontsize=14)\nplt.show();\n</pre> # Plot correctly made splits plt.figure(figsize=(10, 7)) plt.scatter(X_train, y_train, s=5, label=\"Train data\") plt.scatter(X_test, y_test, s=5, label=\"Test data\") plt.xlabel(\"Date\") plt.ylabel(\"BTC Price\") plt.legend(fontsize=14) plt.show(); <p>That looks much better!</p> <p>Do you see what's happened here?</p> <p>We're going to be using the training set (past) to train a model to try and predict values on the test set (future).</p> <p>Because the test set is an artificial future, we can guage how our model might perform on actual future data.</p> <p>\ud83d\udd11 Note: The amount of data you reserve for your test set not set in stone. You could have 80/20, 90/10, 95/5 splits or in some cases, you might not even have enough data to split into train and test sets (see the resource below). The point is to remember the test set is a pseudofuture and not the actual future, it is only meant to give you an indication of how the models you're building are performing.</p> <p>\ud83d\udcd6 Resource: Working with time series data can be tricky compared to other kinds of data. And there are a few pitfalls to watch out for, such as how much data to use for a test set. The article 3 facts about time series forecasting that surprise experienced machine learning practitioners talks about different things to watch out for when working with time series data, I'd recommend reading it.</p> In\u00a0[\u00a0]: Copied! <pre># Create a function to plot time series data\ndef plot_time_series(timesteps, values, format='.', start=0, end=None, label=None):\n  \"\"\"\n  Plots a timesteps (a series of points in time) against values (a series of values across timesteps).\n  \n  Parameters\n  ---------\n  timesteps : array of timesteps\n  values : array of values across time\n  format : style of plot, default \".\"\n  start : where to start the plot (setting a value will index from start of timesteps &amp; values)\n  end : where to end the plot (setting a value will index from end of timesteps &amp; values)\n  label : label to show on plot of values\n  \"\"\"\n  # Plot the series\n  plt.plot(timesteps[start:end], values[start:end], format, label=label)\n  plt.xlabel(\"Time\")\n  plt.ylabel(\"BTC Price\")\n  if label:\n    plt.legend(fontsize=14) # make label bigger\n  plt.grid(True)\n</pre> # Create a function to plot time series data def plot_time_series(timesteps, values, format='.', start=0, end=None, label=None):   \"\"\"   Plots a timesteps (a series of points in time) against values (a series of values across timesteps).      Parameters   ---------   timesteps : array of timesteps   values : array of values across time   format : style of plot, default \".\"   start : where to start the plot (setting a value will index from start of timesteps &amp; values)   end : where to end the plot (setting a value will index from end of timesteps &amp; values)   label : label to show on plot of values   \"\"\"   # Plot the series   plt.plot(timesteps[start:end], values[start:end], format, label=label)   plt.xlabel(\"Time\")   plt.ylabel(\"BTC Price\")   if label:     plt.legend(fontsize=14) # make label bigger   plt.grid(True) In\u00a0[\u00a0]: Copied! <pre># Try out our plotting function\nplt.figure(figsize=(10, 7))\nplot_time_series(timesteps=X_train, values=y_train, label=\"Train data\")\nplot_time_series(timesteps=X_test, values=y_test, label=\"Test data\")\n</pre> # Try out our plotting function plt.figure(figsize=(10, 7)) plot_time_series(timesteps=X_train, values=y_train, label=\"Train data\") plot_time_series(timesteps=X_test, values=y_test, label=\"Test data\") <p>Looking good!</p> <p>Time for some modelling experiments.</p> In\u00a0[\u00a0]: Copied! <pre># Create a na\u00efve forecast\nnaive_forecast = y_test[:-1] # Na\u00efve forecast equals every value excluding the last value\nnaive_forecast[:10], naive_forecast[-10:] # View frist 10 and last 10\n</pre> # Create a na\u00efve forecast naive_forecast = y_test[:-1] # Na\u00efve forecast equals every value excluding the last value naive_forecast[:10], naive_forecast[-10:] # View frist 10 and last 10  Out[\u00a0]: <pre>(array([9226.48582088, 8794.35864452, 8798.04205463, 9081.18687849,\n        8711.53433917, 8760.89271814, 8749.52059102, 8656.97092235,\n        8500.64355816, 8469.2608989 ]),\n array([57107.12067189, 58788.20967893, 58102.19142623, 55715.54665129,\n        56573.5554719 , 52147.82118698, 49764.1320816 , 50032.69313676,\n        47885.62525472, 45604.61575361]))</pre> In\u00a0[\u00a0]: Copied! <pre># Plot naive forecast\nplt.figure(figsize=(10, 7))\nplot_time_series(timesteps=X_train, values=y_train, label=\"Train data\")\nplot_time_series(timesteps=X_test, values=y_test, label=\"Test data\")\nplot_time_series(timesteps=X_test[1:], values=naive_forecast, format=\"-\", label=\"Naive forecast\");\n</pre> # Plot naive forecast plt.figure(figsize=(10, 7)) plot_time_series(timesteps=X_train, values=y_train, label=\"Train data\") plot_time_series(timesteps=X_test, values=y_test, label=\"Test data\") plot_time_series(timesteps=X_test[1:], values=naive_forecast, format=\"-\", label=\"Naive forecast\"); <p>The naive forecast looks like it's following the data well.</p> <p>Let's zoom in to take a better look.</p> <p>We can do so by creating an offset value and passing it to the <code>start</code> parameter of our <code>plot_time_series()</code> function.</p> In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(10, 7))\noffset = 300 # offset the values by 300 timesteps \nplot_time_series(timesteps=X_test, values=y_test, start=offset, label=\"Test data\")\nplot_time_series(timesteps=X_test[1:], values=naive_forecast, format=\"-\", start=offset, label=\"Naive forecast\");\n</pre> plt.figure(figsize=(10, 7)) offset = 300 # offset the values by 300 timesteps  plot_time_series(timesteps=X_test, values=y_test, start=offset, label=\"Test data\") plot_time_series(timesteps=X_test[1:], values=naive_forecast, format=\"-\", start=offset, label=\"Naive forecast\"); <p>When we zoom in we see the na\u00efve forecast comes slightly after the test data. This makes sense because the naive forecast uses the previous timestep value to predict the next timestep value.</p> <p>Forecast made. Time to evaluate it.</p> In\u00a0[\u00a0]: Copied! <pre># Let's get TensorFlow! \nimport tensorflow as tf\n</pre> # Let's get TensorFlow!  import tensorflow as tf <p>And since TensorFlow doesn't have a ready made version of MASE (mean aboslute scaled error), how about we create our own?</p> <p>We'll take inspiration from sktime's (Scikit-Learn for time series) <code>MeanAbsoluteScaledError</code> class which calculates the MASE.</p> In\u00a0[\u00a0]: Copied! <pre># MASE implemented courtesy of sktime - https://github.com/alan-turing-institute/sktime/blob/ee7a06843a44f4aaec7582d847e36073a9ab0566/sktime/performance_metrics/forecasting/_functions.py#L16\ndef mean_absolute_scaled_error(y_true, y_pred):\n  \"\"\"\n  Implement MASE (assuming no seasonality of data).\n  \"\"\"\n  mae = tf.reduce_mean(tf.abs(y_true - y_pred))\n\n  # Find MAE of naive forecast (no seasonality)\n  mae_naive_no_season = tf.reduce_mean(tf.abs(y_true[1:] - y_true[:-1])) # our seasonality is 1 day (hence the shifting of 1 day)\n\n  return mae / mae_naive_no_season\n</pre> # MASE implemented courtesy of sktime - https://github.com/alan-turing-institute/sktime/blob/ee7a06843a44f4aaec7582d847e36073a9ab0566/sktime/performance_metrics/forecasting/_functions.py#L16 def mean_absolute_scaled_error(y_true, y_pred):   \"\"\"   Implement MASE (assuming no seasonality of data).   \"\"\"   mae = tf.reduce_mean(tf.abs(y_true - y_pred))    # Find MAE of naive forecast (no seasonality)   mae_naive_no_season = tf.reduce_mean(tf.abs(y_true[1:] - y_true[:-1])) # our seasonality is 1 day (hence the shifting of 1 day)    return mae / mae_naive_no_season <p>You'll notice the version of MASE above doesn't take in the training values like sktime's <code>mae_loss()</code>. In our case, we're comparing the MAE of our predictions on the test to the MAE of the na\u00efve forecast on the test set.</p> <p>In practice, if we've created the function correctly, the na\u00efve model should achieve an MASE of 1 (or very close to 1). Any model worse than the na\u00efve forecast will achieve an MASE of &gt;1 and any model better than the na\u00efve forecast will achieve an MASE of &lt;1.</p> <p>Let's put each of our different evaluation metrics together into a function.</p> In\u00a0[\u00a0]: Copied! <pre>def evaluate_preds(y_true, y_pred):\n  # Make sure float32 (for metric calculations)\n  y_true = tf.cast(y_true, dtype=tf.float32)\n  y_pred = tf.cast(y_pred, dtype=tf.float32)\n\n  # Calculate various metrics\n  mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n  mse = tf.keras.metrics.mean_squared_error(y_true, y_pred) # puts and emphasis on outliers (all errors get squared)\n  rmse = tf.sqrt(mse)\n  mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n  mase = mean_absolute_scaled_error(y_true, y_pred)\n  \n  return {\"mae\": mae.numpy(),\n          \"mse\": mse.numpy(),\n          \"rmse\": rmse.numpy(),\n          \"mape\": mape.numpy(),\n          \"mase\": mase.numpy()}\n</pre> def evaluate_preds(y_true, y_pred):   # Make sure float32 (for metric calculations)   y_true = tf.cast(y_true, dtype=tf.float32)   y_pred = tf.cast(y_pred, dtype=tf.float32)    # Calculate various metrics   mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)   mse = tf.keras.metrics.mean_squared_error(y_true, y_pred) # puts and emphasis on outliers (all errors get squared)   rmse = tf.sqrt(mse)   mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)   mase = mean_absolute_scaled_error(y_true, y_pred)      return {\"mae\": mae.numpy(),           \"mse\": mse.numpy(),           \"rmse\": rmse.numpy(),           \"mape\": mape.numpy(),           \"mase\": mase.numpy()} <p>Looking good! How about we test our function on the naive forecast?</p> In\u00a0[\u00a0]: Copied! <pre>naive_results = evaluate_preds(y_true=y_test[1:],\n                               y_pred=naive_forecast)\nnaive_results\n</pre> naive_results = evaluate_preds(y_true=y_test[1:],                                y_pred=naive_forecast) naive_results Out[\u00a0]: <pre>{'mae': 567.9802,\n 'mape': 2.516525,\n 'mase': 0.99957,\n 'mse': 1147547.0,\n 'rmse': 1071.2362}</pre> <p>Alright, looks like we've got some baselines to beat.</p> <p>Taking a look at the na\u00efve forecast's MAE, it seems on average each forecast is ~$567 different than the actual Bitcoin price.</p> <p>How does this compare to the average price of Bitcoin in the test dataset?</p> In\u00a0[\u00a0]: Copied! <pre># Find average price of Bitcoin in test dataset\ntf.reduce_mean(y_test).numpy()\n</pre> # Find average price of Bitcoin in test dataset tf.reduce_mean(y_test).numpy() Out[\u00a0]: <pre>20056.632963737226</pre> <p>Okay, looking at these two values is starting to give us an idea of how our model is performing:</p> <ul> <li><p>The average price of Bitcoin in the test dataset is: $20,056 (note: average may not be the best measure here, since the highest price is over 3x this value and the lowest price is over 4x lower)</p> </li> <li><p>Each prediction in naive forecast is on average off by: $567</p> </li> </ul> <p>Is this enough to say it's a good model?</p> <p>That's up your own interpretation. Personally, I'd prefer a model which was closer to the mark.</p> <p>How about we try and build one?</p> In\u00a0[\u00a0]: Copied! <pre>HORIZON = 1 # predict 1 step at a time\nWINDOW_SIZE = 7 # use a week worth of timesteps to predict the horizon\n</pre> HORIZON = 1 # predict 1 step at a time WINDOW_SIZE = 7 # use a week worth of timesteps to predict the horizon <p>Now we'll write a function to take in an array and turn it into a window and horizon.</p> In\u00a0[\u00a0]: Copied! <pre># Create function to label windowed data\ndef get_labelled_windows(x, horizon=1):\n  \"\"\"\n  Creates labels for windowed dataset.\n\n  E.g. if horizon=1 (default)\n  Input: [1, 2, 3, 4, 5, 6] -&gt; Output: ([1, 2, 3, 4, 5], [6])\n  \"\"\"\n  return x[:, :-horizon], x[:, -horizon:]\n</pre> # Create function to label windowed data def get_labelled_windows(x, horizon=1):   \"\"\"   Creates labels for windowed dataset.    E.g. if horizon=1 (default)   Input: [1, 2, 3, 4, 5, 6] -&gt; Output: ([1, 2, 3, 4, 5], [6])   \"\"\"   return x[:, :-horizon], x[:, -horizon:] In\u00a0[\u00a0]: Copied! <pre># Test out the window labelling function\ntest_window, test_label = get_labelled_windows(tf.expand_dims(tf.range(8)+1, axis=0), horizon=HORIZON)\nprint(f\"Window: {tf.squeeze(test_window).numpy()} -&gt; Label: {tf.squeeze(test_label).numpy()}\")\n</pre> # Test out the window labelling function test_window, test_label = get_labelled_windows(tf.expand_dims(tf.range(8)+1, axis=0), horizon=HORIZON) print(f\"Window: {tf.squeeze(test_window).numpy()} -&gt; Label: {tf.squeeze(test_label).numpy()}\") <pre>Window: [1 2 3 4 5 6 7] -&gt; Label: 8\n</pre> <p>Oh yeah, that's what I'm talking about!</p> <p>Now we need a way to make windows for an entire time series.</p> <p>We could do this with Python for loops, however, for large time series, that'd be quite slow.</p> <p>To speed things up, we'll leverage NumPy's array indexing.</p> <p>Let's write a function which:</p> <ol> <li>Creates a window step of specific window size, for example: <code>[[0, 1, 2, 3, 4, 5, 6, 7]]</code></li> <li>Uses NumPy indexing to create a 2D of multiple window steps, for example:</li> </ol> <pre><code>[[0, 1, 2, 3, 4, 5, 6, 7],\n [1, 2, 3, 4, 5, 6, 7, 8],\n [2, 3, 4, 5, 6, 7, 8, 9]]\n</code></pre> <ol> <li>Uses the 2D array of multuple window steps to index on a target series</li> <li>Uses the <code>get_labelled_windows()</code> function we created above to turn the window steps into windows with a specified horizon</li> </ol> <p>\ud83d\udcd6 Resource: The function created below has been adapted from Syafiq Kamarul Azman's article Fast and Robust Sliding Window Vectorization with NumPy.</p> In\u00a0[\u00a0]: Copied! <pre># Create function to view NumPy arrays as windows \ndef make_windows(x, window_size=7, horizon=1):\n  \"\"\"\n  Turns a 1D array into a 2D array of sequential windows of window_size.\n  \"\"\"\n  # 1. Create a window of specific window_size (add the horizon on the end for later labelling)\n  window_step = np.expand_dims(np.arange(window_size+horizon), axis=0)\n  # print(f\"Window step:\\n {window_step}\")\n\n  # 2. Create a 2D array of multiple window steps (minus 1 to account for 0 indexing)\n  window_indexes = window_step + np.expand_dims(np.arange(len(x)-(window_size+horizon-1)), axis=0).T # create 2D array of windows of size window_size\n  # print(f\"Window indexes:\\n {window_indexes[:3], window_indexes[-3:], window_indexes.shape}\")\n\n  # 3. Index on the target array (time series) with 2D array of multiple window steps\n  windowed_array = x[window_indexes]\n\n  # 4. Get the labelled windows\n  windows, labels = get_labelled_windows(windowed_array, horizon=horizon)\n\n  return windows, labels\n</pre> # Create function to view NumPy arrays as windows  def make_windows(x, window_size=7, horizon=1):   \"\"\"   Turns a 1D array into a 2D array of sequential windows of window_size.   \"\"\"   # 1. Create a window of specific window_size (add the horizon on the end for later labelling)   window_step = np.expand_dims(np.arange(window_size+horizon), axis=0)   # print(f\"Window step:\\n {window_step}\")    # 2. Create a 2D array of multiple window steps (minus 1 to account for 0 indexing)   window_indexes = window_step + np.expand_dims(np.arange(len(x)-(window_size+horizon-1)), axis=0).T # create 2D array of windows of size window_size   # print(f\"Window indexes:\\n {window_indexes[:3], window_indexes[-3:], window_indexes.shape}\")    # 3. Index on the target array (time series) with 2D array of multiple window steps   windowed_array = x[window_indexes]    # 4. Get the labelled windows   windows, labels = get_labelled_windows(windowed_array, horizon=horizon)    return windows, labels <p>Phew! A few steps there... let's see how it goes.</p> In\u00a0[\u00a0]: Copied! <pre>full_windows, full_labels = make_windows(prices, window_size=WINDOW_SIZE, horizon=HORIZON)\nlen(full_windows), len(full_labels)\n</pre> full_windows, full_labels = make_windows(prices, window_size=WINDOW_SIZE, horizon=HORIZON) len(full_windows), len(full_labels) Out[\u00a0]: <pre>(2780, 2780)</pre> <p>Of course we have to visualize, visualize, visualize!</p> In\u00a0[\u00a0]: Copied! <pre># View the first 3 windows/labels\nfor i in range(3):\n  print(f\"Window: {full_windows[i]} -&gt; Label: {full_labels[i]}\")\n</pre> # View the first 3 windows/labels for i in range(3):   print(f\"Window: {full_windows[i]} -&gt; Label: {full_labels[i]}\") <pre>Window: [123.65499 125.455   108.58483 118.67466 121.33866 120.65533 121.795  ] -&gt; Label: [123.033]\nWindow: [125.455   108.58483 118.67466 121.33866 120.65533 121.795   123.033  ] -&gt; Label: [124.049]\nWindow: [108.58483 118.67466 121.33866 120.65533 121.795   123.033   124.049  ] -&gt; Label: [125.96116]\n</pre> In\u00a0[\u00a0]: Copied! <pre># View the last 3 windows/labels\nfor i in range(3):\n  print(f\"Window: {full_windows[i-3]} -&gt; Label: {full_labels[i-3]}\")\n</pre> # View the last 3 windows/labels for i in range(3):   print(f\"Window: {full_windows[i-3]} -&gt; Label: {full_labels[i-3]}\") <pre>Window: [58788.20967893 58102.19142623 55715.54665129 56573.5554719\n 52147.82118698 49764.1320816  50032.69313676] -&gt; Label: [47885.62525472]\nWindow: [58102.19142623 55715.54665129 56573.5554719  52147.82118698\n 49764.1320816  50032.69313676 47885.62525472] -&gt; Label: [45604.61575361]\nWindow: [55715.54665129 56573.5554719  52147.82118698 49764.1320816\n 50032.69313676 47885.62525472 45604.61575361] -&gt; Label: [43144.47129086]\n</pre> <p>\ud83d\udd11 Note: You can find a function which achieves similar results to the ones we implemented above at <code>tf.keras.preprocessing.timeseries_dataset_from_array()</code>. Just like ours, it takes in an array and returns a windowed dataset. It has the benefit of returning data in the form of a tf.data.Dataset instance (we'll see how to do this with our own data later).</p> In\u00a0[\u00a0]: Copied! <pre># Make the train/test splits\ndef make_train_test_splits(windows, labels, test_split=0.2):\n  \"\"\"\n  Splits matching pairs of windows and labels into train and test splits.\n  \"\"\"\n  split_size = int(len(windows) * (1-test_split)) # this will default to 80% train/20% test\n  train_windows = windows[:split_size]\n  train_labels = labels[:split_size]\n  test_windows = windows[split_size:]\n  test_labels = labels[split_size:]\n  return train_windows, test_windows, train_labels, test_labels\n</pre> # Make the train/test splits def make_train_test_splits(windows, labels, test_split=0.2):   \"\"\"   Splits matching pairs of windows and labels into train and test splits.   \"\"\"   split_size = int(len(windows) * (1-test_split)) # this will default to 80% train/20% test   train_windows = windows[:split_size]   train_labels = labels[:split_size]   test_windows = windows[split_size:]   test_labels = labels[split_size:]   return train_windows, test_windows, train_labels, test_labels <p>Look at that amazing function, lets test it.</p> In\u00a0[\u00a0]: Copied! <pre>train_windows, test_windows, train_labels, test_labels = make_train_test_splits(full_windows, full_labels)\nlen(train_windows), len(test_windows), len(train_labels), len(test_labels)\n</pre> train_windows, test_windows, train_labels, test_labels = make_train_test_splits(full_windows, full_labels) len(train_windows), len(test_windows), len(train_labels), len(test_labels) Out[\u00a0]: <pre>(2224, 556, 2224, 556)</pre> <p>Notice the default split of 80% training data and 20% testing data (this split can be adjusted if needed).</p> <p>How do the first 5 samples of the training windows and labels looks?</p> In\u00a0[\u00a0]: Copied! <pre>train_windows[:5], train_labels[:5]\n</pre> train_windows[:5], train_labels[:5] Out[\u00a0]: <pre>(array([[123.65499, 125.455  , 108.58483, 118.67466, 121.33866, 120.65533,\n         121.795  ],\n        [125.455  , 108.58483, 118.67466, 121.33866, 120.65533, 121.795  ,\n         123.033  ],\n        [108.58483, 118.67466, 121.33866, 120.65533, 121.795  , 123.033  ,\n         124.049  ],\n        [118.67466, 121.33866, 120.65533, 121.795  , 123.033  , 124.049  ,\n         125.96116],\n        [121.33866, 120.65533, 121.795  , 123.033  , 124.049  , 125.96116,\n         125.27966]]), array([[123.033  ],\n        [124.049  ],\n        [125.96116],\n        [125.27966],\n        [125.9275 ]]))</pre> In\u00a0[\u00a0]: Copied! <pre># Check to see if same (accounting for horizon and window size)\nnp.array_equal(np.squeeze(train_labels[:-HORIZON-1]), y_train[WINDOW_SIZE:])\n</pre> # Check to see if same (accounting for horizon and window size) np.array_equal(np.squeeze(train_labels[:-HORIZON-1]), y_train[WINDOW_SIZE:]) Out[\u00a0]: <pre>True</pre> In\u00a0[\u00a0]: Copied! <pre>import os\n\n# Create a function to implement a ModelCheckpoint callback with a specific filename \ndef create_model_checkpoint(model_name, save_path=\"model_experiments\"):\n  return tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(save_path, model_name), # create filepath to save model\n                                            verbose=0, # only output a limited amount of text\n                                            save_best_only=True) # save only the best model to file\n</pre> import os  # Create a function to implement a ModelCheckpoint callback with a specific filename  def create_model_checkpoint(model_name, save_path=\"model_experiments\"):   return tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(save_path, model_name), # create filepath to save model                                             verbose=0, # only output a limited amount of text                                             save_best_only=True) # save only the best model to file In\u00a0[\u00a0]: Copied! <pre>import tensorflow as tf\nfrom tensorflow.keras import layers\n\n# Set random seed for as reproducible results as possible\ntf.random.set_seed(42)\n\n# Construct model\nmodel_1 = tf.keras.Sequential([\n  layers.Dense(128, activation=\"relu\"),\n  layers.Dense(HORIZON, activation=\"linear\") # linear activation is the same as having no activation                        \n], name=\"model_1_dense\") # give the model a name so we can save it\n\n# Compile model\nmodel_1.compile(loss=\"mae\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"mae\"]) # we don't necessarily need this when the loss function is already MAE\n\n# Fit model\nmodel_1.fit(x=train_windows, # train windows of 7 timesteps of Bitcoin prices\n            y=train_labels, # horizon value of 1 (using the previous 7 timesteps to predict next day)\n            epochs=100,\n            verbose=1,\n            batch_size=128,\n            validation_data=(test_windows, test_labels),\n            callbacks=[create_model_checkpoint(model_name=model_1.name)]) # create ModelCheckpoint callback to save best model\n</pre> import tensorflow as tf from tensorflow.keras import layers  # Set random seed for as reproducible results as possible tf.random.set_seed(42)  # Construct model model_1 = tf.keras.Sequential([   layers.Dense(128, activation=\"relu\"),   layers.Dense(HORIZON, activation=\"linear\") # linear activation is the same as having no activation                         ], name=\"model_1_dense\") # give the model a name so we can save it  # Compile model model_1.compile(loss=\"mae\",                 optimizer=tf.keras.optimizers.Adam(),                 metrics=[\"mae\"]) # we don't necessarily need this when the loss function is already MAE  # Fit model model_1.fit(x=train_windows, # train windows of 7 timesteps of Bitcoin prices             y=train_labels, # horizon value of 1 (using the previous 7 timesteps to predict next day)             epochs=100,             verbose=1,             batch_size=128,             validation_data=(test_windows, test_labels),             callbacks=[create_model_checkpoint(model_name=model_1.name)]) # create ModelCheckpoint callback to save best model <pre>Epoch 1/100\n18/18 [==============================] - 3s 12ms/step - loss: 780.3455 - mae: 780.3455 - val_loss: 2279.6526 - val_mae: 2279.6526\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 2/100\n18/18 [==============================] - 0s 4ms/step - loss: 247.6756 - mae: 247.6756 - val_loss: 1005.9991 - val_mae: 1005.9991\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 3/100\n18/18 [==============================] - 0s 4ms/step - loss: 188.4116 - mae: 188.4116 - val_loss: 923.2862 - val_mae: 923.2862\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 4/100\n18/18 [==============================] - 0s 4ms/step - loss: 169.4340 - mae: 169.4340 - val_loss: 900.5872 - val_mae: 900.5872\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 5/100\n18/18 [==============================] - 0s 4ms/step - loss: 165.0895 - mae: 165.0895 - val_loss: 895.2238 - val_mae: 895.2238\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 6/100\n18/18 [==============================] - 0s 4ms/step - loss: 158.5210 - mae: 158.5210 - val_loss: 855.1982 - val_mae: 855.1982\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 7/100\n18/18 [==============================] - 0s 5ms/step - loss: 151.3566 - mae: 151.3566 - val_loss: 840.9168 - val_mae: 840.9168\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 8/100\n18/18 [==============================] - 0s 5ms/step - loss: 145.2560 - mae: 145.2560 - val_loss: 803.5956 - val_mae: 803.5956\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 9/100\n18/18 [==============================] - 0s 4ms/step - loss: 144.3546 - mae: 144.3546 - val_loss: 799.5455 - val_mae: 799.5455\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 10/100\n18/18 [==============================] - 0s 4ms/step - loss: 141.2943 - mae: 141.2943 - val_loss: 763.5010 - val_mae: 763.5010\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 11/100\n18/18 [==============================] - 0s 4ms/step - loss: 135.6595 - mae: 135.6595 - val_loss: 771.3357 - val_mae: 771.3357\nEpoch 12/100\n18/18 [==============================] - 0s 5ms/step - loss: 134.1700 - mae: 134.1700 - val_loss: 782.8079 - val_mae: 782.8079\nEpoch 13/100\n18/18 [==============================] - 0s 4ms/step - loss: 134.6015 - mae: 134.6015 - val_loss: 784.4449 - val_mae: 784.4449\nEpoch 14/100\n18/18 [==============================] - 0s 4ms/step - loss: 130.6127 - mae: 130.6127 - val_loss: 751.3234 - val_mae: 751.3234\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 15/100\n18/18 [==============================] - 0s 5ms/step - loss: 128.8347 - mae: 128.8347 - val_loss: 696.5757 - val_mae: 696.5757\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 16/100\n18/18 [==============================] - 0s 5ms/step - loss: 124.7739 - mae: 124.7739 - val_loss: 702.4698 - val_mae: 702.4698\nEpoch 17/100\n18/18 [==============================] - 0s 4ms/step - loss: 123.4474 - mae: 123.4474 - val_loss: 704.9241 - val_mae: 704.9241\nEpoch 18/100\n18/18 [==============================] - 0s 5ms/step - loss: 122.2105 - mae: 122.2105 - val_loss: 667.9725 - val_mae: 667.9725\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 19/100\n18/18 [==============================] - 0s 4ms/step - loss: 121.7263 - mae: 121.7263 - val_loss: 718.8797 - val_mae: 718.8797\nEpoch 20/100\n18/18 [==============================] - 0s 4ms/step - loss: 119.2420 - mae: 119.2420 - val_loss: 657.0667 - val_mae: 657.0667\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 21/100\n18/18 [==============================] - 0s 4ms/step - loss: 121.2275 - mae: 121.2275 - val_loss: 637.0330 - val_mae: 637.0330\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 22/100\n18/18 [==============================] - 0s 4ms/step - loss: 119.9544 - mae: 119.9544 - val_loss: 671.2490 - val_mae: 671.2490\nEpoch 23/100\n18/18 [==============================] - 0s 5ms/step - loss: 121.9248 - mae: 121.9248 - val_loss: 633.3593 - val_mae: 633.3593\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 24/100\n18/18 [==============================] - 0s 5ms/step - loss: 116.3666 - mae: 116.3666 - val_loss: 624.4852 - val_mae: 624.4852\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 25/100\n18/18 [==============================] - 0s 4ms/step - loss: 114.6816 - mae: 114.6816 - val_loss: 619.7571 - val_mae: 619.7571\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 26/100\n18/18 [==============================] - 0s 4ms/step - loss: 116.4455 - mae: 116.4455 - val_loss: 615.6364 - val_mae: 615.6364\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 27/100\n18/18 [==============================] - 0s 5ms/step - loss: 116.5868 - mae: 116.5868 - val_loss: 615.9631 - val_mae: 615.9631\nEpoch 28/100\n18/18 [==============================] - 0s 4ms/step - loss: 113.4691 - mae: 113.4691 - val_loss: 608.0920 - val_mae: 608.0920\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 29/100\n18/18 [==============================] - 0s 5ms/step - loss: 113.7598 - mae: 113.7598 - val_loss: 621.9306 - val_mae: 621.9306\nEpoch 30/100\n18/18 [==============================] - 0s 4ms/step - loss: 116.8613 - mae: 116.8613 - val_loss: 604.4056 - val_mae: 604.4056\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 31/100\n18/18 [==============================] - 0s 4ms/step - loss: 111.9375 - mae: 111.9375 - val_loss: 609.3882 - val_mae: 609.3882\nEpoch 32/100\n18/18 [==============================] - 0s 4ms/step - loss: 112.4175 - mae: 112.4175 - val_loss: 603.0588 - val_mae: 603.0588\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 33/100\n18/18 [==============================] - 0s 4ms/step - loss: 112.6697 - mae: 112.6697 - val_loss: 645.6975 - val_mae: 645.6975\nEpoch 34/100\n18/18 [==============================] - 0s 4ms/step - loss: 111.9867 - mae: 111.9867 - val_loss: 604.7632 - val_mae: 604.7632\nEpoch 35/100\n18/18 [==============================] - 0s 4ms/step - loss: 110.9451 - mae: 110.9451 - val_loss: 593.4648 - val_mae: 593.4648\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 36/100\n18/18 [==============================] - 0s 5ms/step - loss: 114.4816 - mae: 114.4816 - val_loss: 608.0073 - val_mae: 608.0073\nEpoch 37/100\n18/18 [==============================] - 0s 4ms/step - loss: 110.2017 - mae: 110.2017 - val_loss: 597.2309 - val_mae: 597.2309\nEpoch 38/100\n18/18 [==============================] - 0s 5ms/step - loss: 112.2372 - mae: 112.2372 - val_loss: 637.9797 - val_mae: 637.9797\nEpoch 39/100\n18/18 [==============================] - 0s 4ms/step - loss: 115.1289 - mae: 115.1289 - val_loss: 587.4679 - val_mae: 587.4679\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 40/100\n18/18 [==============================] - 0s 5ms/step - loss: 110.0854 - mae: 110.0854 - val_loss: 592.7117 - val_mae: 592.7117\nEpoch 41/100\n18/18 [==============================] - 0s 4ms/step - loss: 110.6343 - mae: 110.6343 - val_loss: 593.8997 - val_mae: 593.8997\nEpoch 42/100\n18/18 [==============================] - 0s 4ms/step - loss: 113.5762 - mae: 113.5762 - val_loss: 636.3674 - val_mae: 636.3674\nEpoch 43/100\n18/18 [==============================] - 0s 5ms/step - loss: 116.2286 - mae: 116.2286 - val_loss: 662.9264 - val_mae: 662.9264\nEpoch 44/100\n18/18 [==============================] - 0s 4ms/step - loss: 120.0192 - mae: 120.0192 - val_loss: 635.6360 - val_mae: 635.6360\nEpoch 45/100\n18/18 [==============================] - 0s 4ms/step - loss: 110.9675 - mae: 110.9675 - val_loss: 601.9926 - val_mae: 601.9926\nEpoch 46/100\n18/18 [==============================] - 0s 4ms/step - loss: 111.6012 - mae: 111.6012 - val_loss: 593.3531 - val_mae: 593.3531\nEpoch 47/100\n18/18 [==============================] - 0s 6ms/step - loss: 109.6161 - mae: 109.6161 - val_loss: 637.0014 - val_mae: 637.0014\nEpoch 48/100\n18/18 [==============================] - 0s 5ms/step - loss: 109.1368 - mae: 109.1368 - val_loss: 598.4199 - val_mae: 598.4199\nEpoch 49/100\n18/18 [==============================] - 0s 5ms/step - loss: 112.4355 - mae: 112.4355 - val_loss: 579.7040 - val_mae: 579.7040\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 50/100\n18/18 [==============================] - 0s 4ms/step - loss: 110.2108 - mae: 110.2108 - val_loss: 639.2326 - val_mae: 639.2326\nEpoch 51/100\n18/18 [==============================] - 0s 5ms/step - loss: 111.0958 - mae: 111.0958 - val_loss: 597.3575 - val_mae: 597.3575\nEpoch 52/100\n18/18 [==============================] - 0s 4ms/step - loss: 110.7351 - mae: 110.7351 - val_loss: 580.7227 - val_mae: 580.7227\nEpoch 53/100\n18/18 [==============================] - 0s 5ms/step - loss: 111.1785 - mae: 111.1785 - val_loss: 648.3588 - val_mae: 648.3588\nEpoch 54/100\n18/18 [==============================] - 0s 4ms/step - loss: 114.0832 - mae: 114.0832 - val_loss: 593.2007 - val_mae: 593.2007\nEpoch 55/100\n18/18 [==============================] - 0s 4ms/step - loss: 110.4910 - mae: 110.4910 - val_loss: 579.5065 - val_mae: 579.5065\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 56/100\n18/18 [==============================] - 0s 5ms/step - loss: 108.0489 - mae: 108.0489 - val_loss: 807.3851 - val_mae: 807.3851\nEpoch 57/100\n18/18 [==============================] - 0s 4ms/step - loss: 125.0614 - mae: 125.0614 - val_loss: 674.1654 - val_mae: 674.1654\nEpoch 58/100\n18/18 [==============================] - 0s 4ms/step - loss: 115.4340 - mae: 115.4340 - val_loss: 582.2698 - val_mae: 582.2698\nEpoch 59/100\n18/18 [==============================] - 0s 5ms/step - loss: 110.0881 - mae: 110.0881 - val_loss: 606.7637 - val_mae: 606.7637\nEpoch 60/100\n18/18 [==============================] - 0s 4ms/step - loss: 108.7156 - mae: 108.7156 - val_loss: 602.3102 - val_mae: 602.3102\nEpoch 61/100\n18/18 [==============================] - 0s 4ms/step - loss: 108.1525 - mae: 108.1525 - val_loss: 573.9990 - val_mae: 573.9990\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 62/100\n18/18 [==============================] - 0s 4ms/step - loss: 107.3727 - mae: 107.3727 - val_loss: 581.7012 - val_mae: 581.7012\nEpoch 63/100\n18/18 [==============================] - 0s 4ms/step - loss: 110.7667 - mae: 110.7667 - val_loss: 637.5252 - val_mae: 637.5252\nEpoch 64/100\n18/18 [==============================] - 0s 4ms/step - loss: 110.1539 - mae: 110.1539 - val_loss: 586.6601 - val_mae: 586.6601\nEpoch 65/100\n18/18 [==============================] - 0s 5ms/step - loss: 108.2325 - mae: 108.2325 - val_loss: 573.5620 - val_mae: 573.5620\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 66/100\n18/18 [==============================] - 0s 5ms/step - loss: 108.6825 - mae: 108.6825 - val_loss: 572.2206 - val_mae: 572.2206\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 67/100\n18/18 [==============================] - 0s 4ms/step - loss: 106.6371 - mae: 106.6371 - val_loss: 646.6349 - val_mae: 646.6349\nEpoch 68/100\n18/18 [==============================] - 0s 4ms/step - loss: 114.1603 - mae: 114.1603 - val_loss: 681.8561 - val_mae: 681.8561\nEpoch 69/100\n18/18 [==============================] - 0s 5ms/step - loss: 124.5514 - mae: 124.5514 - val_loss: 655.9885 - val_mae: 655.9885\nEpoch 70/100\n18/18 [==============================] - 0s 4ms/step - loss: 125.0235 - mae: 125.0235 - val_loss: 601.0032 - val_mae: 601.0032\nEpoch 71/100\n18/18 [==============================] - 0s 6ms/step - loss: 110.3652 - mae: 110.3652 - val_loss: 595.3962 - val_mae: 595.3962\nEpoch 72/100\n18/18 [==============================] - 0s 5ms/step - loss: 107.9285 - mae: 107.9285 - val_loss: 573.7085 - val_mae: 573.7085\nEpoch 73/100\n18/18 [==============================] - 0s 4ms/step - loss: 109.5085 - mae: 109.5085 - val_loss: 580.4180 - val_mae: 580.4180\nEpoch 74/100\n18/18 [==============================] - 0s 4ms/step - loss: 108.7380 - mae: 108.7380 - val_loss: 576.1211 - val_mae: 576.1211\nEpoch 75/100\n18/18 [==============================] - 0s 5ms/step - loss: 107.9404 - mae: 107.9404 - val_loss: 591.1477 - val_mae: 591.1477\nEpoch 76/100\n18/18 [==============================] - 0s 5ms/step - loss: 109.4232 - mae: 109.4232 - val_loss: 597.8605 - val_mae: 597.8605\nEpoch 77/100\n18/18 [==============================] - 0s 4ms/step - loss: 107.5879 - mae: 107.5879 - val_loss: 571.9299 - val_mae: 571.9299\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 78/100\n18/18 [==============================] - 0s 4ms/step - loss: 108.1598 - mae: 108.1598 - val_loss: 575.2383 - val_mae: 575.2383\nEpoch 79/100\n18/18 [==============================] - 0s 4ms/step - loss: 107.9175 - mae: 107.9175 - val_loss: 617.3071 - val_mae: 617.3071\nEpoch 80/100\n18/18 [==============================] - 0s 4ms/step - loss: 108.9510 - mae: 108.9510 - val_loss: 583.4847 - val_mae: 583.4847\nEpoch 81/100\n18/18 [==============================] - 0s 5ms/step - loss: 106.0505 - mae: 106.0505 - val_loss: 570.0802 - val_mae: 570.0802\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 82/100\n18/18 [==============================] - 0s 4ms/step - loss: 115.6827 - mae: 115.6827 - val_loss: 575.7382 - val_mae: 575.7382\nEpoch 83/100\n18/18 [==============================] - 0s 5ms/step - loss: 110.9379 - mae: 110.9379 - val_loss: 659.6570 - val_mae: 659.6570\nEpoch 84/100\n18/18 [==============================] - 0s 5ms/step - loss: 111.4836 - mae: 111.4836 - val_loss: 570.1959 - val_mae: 570.1959\nEpoch 85/100\n18/18 [==============================] - 0s 4ms/step - loss: 107.5948 - mae: 107.5948 - val_loss: 601.5945 - val_mae: 601.5945\nEpoch 86/100\n18/18 [==============================] - 0s 4ms/step - loss: 108.9426 - mae: 108.9426 - val_loss: 592.8107 - val_mae: 592.8107\nEpoch 87/100\n18/18 [==============================] - 0s 5ms/step - loss: 105.7717 - mae: 105.7717 - val_loss: 603.6169 - val_mae: 603.6169\nEpoch 88/100\n18/18 [==============================] - 0s 5ms/step - loss: 107.9217 - mae: 107.9217 - val_loss: 569.0500 - val_mae: 569.0500\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 89/100\n18/18 [==============================] - 0s 5ms/step - loss: 106.0344 - mae: 106.0344 - val_loss: 568.9512 - val_mae: 568.9512\nINFO:tensorflow:Assets written to: model_experiments/model_1_dense/assets\nEpoch 90/100\n18/18 [==============================] - 0s 5ms/step - loss: 105.4977 - mae: 105.4977 - val_loss: 581.7681 - val_mae: 581.7681\nEpoch 91/100\n18/18 [==============================] - 0s 5ms/step - loss: 108.8468 - mae: 108.8468 - val_loss: 573.6023 - val_mae: 573.6023\nEpoch 92/100\n18/18 [==============================] - 0s 5ms/step - loss: 110.8884 - mae: 110.8884 - val_loss: 576.8247 - val_mae: 576.8247\nEpoch 93/100\n18/18 [==============================] - 0s 5ms/step - loss: 113.8781 - mae: 113.8781 - val_loss: 608.3018 - val_mae: 608.3018\nEpoch 94/100\n18/18 [==============================] - 0s 5ms/step - loss: 110.5763 - mae: 110.5763 - val_loss: 601.6047 - val_mae: 601.6047\nEpoch 95/100\n18/18 [==============================] - 0s 4ms/step - loss: 106.5906 - mae: 106.5906 - val_loss: 570.3652 - val_mae: 570.3652\nEpoch 96/100\n18/18 [==============================] - 0s 4ms/step - loss: 116.9515 - mae: 116.9515 - val_loss: 615.2581 - val_mae: 615.2581\nEpoch 97/100\n18/18 [==============================] - 0s 5ms/step - loss: 108.0739 - mae: 108.0739 - val_loss: 580.3073 - val_mae: 580.3073\nEpoch 98/100\n18/18 [==============================] - 0s 4ms/step - loss: 108.7102 - mae: 108.7102 - val_loss: 586.6512 - val_mae: 586.6512\nEpoch 99/100\n18/18 [==============================] - 0s 5ms/step - loss: 109.0488 - mae: 109.0488 - val_loss: 570.0629 - val_mae: 570.0629\nEpoch 100/100\n18/18 [==============================] - 0s 4ms/step - loss: 106.1845 - mae: 106.1845 - val_loss: 585.9763 - val_mae: 585.9763\n</pre> Out[\u00a0]: <pre>&lt;keras.callbacks.History at 0x7fdcf48bd110&gt;</pre> <p>Because of the small size of our data (less than 3000 total samples), the model trains very fast.</p> <p>Let's evaluate it.</p> In\u00a0[\u00a0]: Copied! <pre># Evaluate model on test data\nmodel_1.evaluate(test_windows, test_labels)\n</pre> # Evaluate model on test data model_1.evaluate(test_windows, test_labels) <pre>18/18 [==============================] - 0s 2ms/step - loss: 585.9762 - mae: 585.9762\n</pre> Out[\u00a0]: <pre>[585.9761962890625, 585.9761962890625]</pre> <p>You'll notice the model achieves the same <code>val_loss</code> (in this case, this is MAE) as the last epoch.</p> <p>But if we load in the version of <code>model_1</code> which was saved to file using the <code>ModelCheckpoint</code> callback, we should see an improvement in results.</p> In\u00a0[\u00a0]: Copied! <pre># Load in saved best performing model_1 and evaluate on test data\nmodel_1 = tf.keras.models.load_model(\"model_experiments/model_1_dense\")\nmodel_1.evaluate(test_windows, test_labels)\n</pre> # Load in saved best performing model_1 and evaluate on test data model_1 = tf.keras.models.load_model(\"model_experiments/model_1_dense\") model_1.evaluate(test_windows, test_labels) <pre>18/18 [==============================] - 0s 3ms/step - loss: 568.9512 - mae: 568.9512\n</pre> Out[\u00a0]: <pre>[568.951171875, 568.951171875]</pre> <p>Much better! Due to the fluctuating performance of the model during training, loading back in the best performing model see's a sizeable improvement in MAE.</p> In\u00a0[\u00a0]: Copied! <pre>def make_preds(model, input_data):\n  \"\"\"\n  Uses model to make predictions on input_data.\n\n  Parameters\n  ----------\n  model: trained model \n  input_data: windowed input data (same kind of data model was trained on)\n\n  Returns model predictions on input_data.\n  \"\"\"\n  forecast = model.predict(input_data)\n  return tf.squeeze(forecast) # return 1D array of predictions\n</pre> def make_preds(model, input_data):   \"\"\"   Uses model to make predictions on input_data.    Parameters   ----------   model: trained model    input_data: windowed input data (same kind of data model was trained on)    Returns model predictions on input_data.   \"\"\"   forecast = model.predict(input_data)   return tf.squeeze(forecast) # return 1D array of predictions <p>Nice!</p> <p>Now let's use our <code>make_preds()</code> and see how it goes.</p> In\u00a0[\u00a0]: Copied! <pre># Make predictions using model_1 on the test dataset and view the results\nmodel_1_preds = make_preds(model_1, test_windows)\nlen(model_1_preds), model_1_preds[:10]\n</pre> # Make predictions using model_1 on the test dataset and view the results model_1_preds = make_preds(model_1, test_windows) len(model_1_preds), model_1_preds[:10] Out[\u00a0]: <pre>(556, &lt;tf.Tensor: shape=(10,), dtype=float32, numpy=\n array([8861.711, 8769.886, 9015.71 , 8795.517, 8723.809, 8730.11 ,\n        8691.95 , 8502.054, 8460.961, 8516.547], dtype=float32)&gt;)</pre> <p>\ud83d\udd11 Note: With these outputs, our model isn't forecasting yet. It's only making predictions on the test dataset. Forecasting would involve a model making predictions into the future, however, the test dataset is only a pseudofuture.</p> <p>Excellent! Now we've got some prediction values, let's use the <code>evaluate_preds()</code> we created before to compare them to the ground truth.</p> In\u00a0[\u00a0]: Copied! <pre># Evaluate preds\nmodel_1_results = evaluate_preds(y_true=tf.squeeze(test_labels), # reduce to right shape\n                                 y_pred=model_1_preds)\nmodel_1_results\n</pre> # Evaluate preds model_1_results = evaluate_preds(y_true=tf.squeeze(test_labels), # reduce to right shape                                  y_pred=model_1_preds) model_1_results Out[\u00a0]: <pre>{'mae': 568.95123,\n 'mape': 2.5448983,\n 'mase': 0.9994897,\n 'mse': 1171744.0,\n 'rmse': 1082.4713}</pre> <p>How did our model go? Did it beat the na\u00efve forecast?</p> In\u00a0[\u00a0]: Copied! <pre>naive_results\n</pre> naive_results Out[\u00a0]: <pre>{'mae': 567.9802,\n 'mape': 2.516525,\n 'mase': 0.99957,\n 'mse': 1147547.0,\n 'rmse': 1071.2362}</pre> <p>It looks like our na\u00efve model beats our first deep model on nearly every metric.</p> <p>That goes to show the power of the na\u00efve model and the reason for having a baseline for any machine learning project.</p> <p>And of course, no evaluation would be finished without visualizing the results.</p> <p>Let's use the <code>plot_time_series()</code> function to plot <code>model_1_preds</code> against the test data.</p> In\u00a0[\u00a0]: Copied! <pre>offset = 300\nplt.figure(figsize=(10, 7))\n# Account for the test_window offset and index into test_labels to ensure correct plotting\nplot_time_series(timesteps=X_test[-len(test_windows):], values=test_labels[:, 0], start=offset, label=\"Test_data\")\nplot_time_series(timesteps=X_test[-len(test_windows):], values=model_1_preds, start=offset, format=\"-\", label=\"model_1_preds\")\n</pre> offset = 300 plt.figure(figsize=(10, 7)) # Account for the test_window offset and index into test_labels to ensure correct plotting plot_time_series(timesteps=X_test[-len(test_windows):], values=test_labels[:, 0], start=offset, label=\"Test_data\") plot_time_series(timesteps=X_test[-len(test_windows):], values=model_1_preds, start=offset, format=\"-\", label=\"model_1_preds\") <p>What's wrong with these predictions?</p> <p>As mentioned before, they're on the test dataset. So they're not actual forecasts.</p> <p>With our current model setup, how do you think we'd make forecasts for the future?</p> <p>Have a think about it for now, we'll cover this later on.</p> In\u00a0[\u00a0]: Copied! <pre>HORIZON = 1 # predict one step at a time\nWINDOW_SIZE = 30 # use 30 timesteps in the past\n</pre> HORIZON = 1 # predict one step at a time WINDOW_SIZE = 30 # use 30 timesteps in the past In\u00a0[\u00a0]: Copied! <pre># Make windowed data with appropriate horizon and window sizes\nfull_windows, full_labels = make_windows(prices, window_size=WINDOW_SIZE, horizon=HORIZON)\nlen(full_windows), len(full_labels)\n</pre> # Make windowed data with appropriate horizon and window sizes full_windows, full_labels = make_windows(prices, window_size=WINDOW_SIZE, horizon=HORIZON) len(full_windows), len(full_labels) Out[\u00a0]: <pre>(2757, 2757)</pre> In\u00a0[\u00a0]: Copied! <pre># Make train and testing windows\ntrain_windows, test_windows, train_labels, test_labels = make_train_test_splits(windows=full_windows, labels=full_labels)\nlen(train_windows), len(test_windows), len(train_labels), len(test_labels)\n</pre> # Make train and testing windows train_windows, test_windows, train_labels, test_labels = make_train_test_splits(windows=full_windows, labels=full_labels) len(train_windows), len(test_windows), len(train_labels), len(test_labels) Out[\u00a0]: <pre>(2205, 552, 2205, 552)</pre> <p>Data prepared!</p> <p>Now let's construct <code>model_2</code>, a model with the same architecture as <code>model_1</code> as well as the same training routine.</p> In\u00a0[\u00a0]: Copied! <pre>tf.random.set_seed(42)\n\n# Create model (same model as model 1 but data input will be different)\nmodel_2 = tf.keras.Sequential([\n  layers.Dense(128, activation=\"relu\"),\n  layers.Dense(HORIZON) # need to predict horizon number of steps into the future\n], name=\"model_2_dense\")\n\nmodel_2.compile(loss=\"mae\",\n                optimizer=tf.keras.optimizers.Adam())\n\nmodel_2.fit(train_windows,\n            train_labels,\n            epochs=100,\n            batch_size=128,\n            verbose=0,\n            validation_data=(test_windows, test_labels),\n            callbacks=[create_model_checkpoint(model_name=model_2.name)])\n</pre> tf.random.set_seed(42)  # Create model (same model as model 1 but data input will be different) model_2 = tf.keras.Sequential([   layers.Dense(128, activation=\"relu\"),   layers.Dense(HORIZON) # need to predict horizon number of steps into the future ], name=\"model_2_dense\")  model_2.compile(loss=\"mae\",                 optimizer=tf.keras.optimizers.Adam())  model_2.fit(train_windows,             train_labels,             epochs=100,             batch_size=128,             verbose=0,             validation_data=(test_windows, test_labels),             callbacks=[create_model_checkpoint(model_name=model_2.name)]) <pre>INFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_2_dense/assets\n</pre> Out[\u00a0]: <pre>&lt;keras.callbacks.History at 0x7fdcf1094290&gt;</pre> <p>Once again, training goes nice and fast.</p> <p>Let's evaluate our model's performance.</p> In\u00a0[\u00a0]: Copied! <pre># Evaluate model 2 preds\nmodel_2.evaluate(test_windows, test_labels)\n</pre> # Evaluate model 2 preds model_2.evaluate(test_windows, test_labels) <pre>18/18 [==============================] - 0s 2ms/step - loss: 608.9615\n</pre> Out[\u00a0]: <pre>608.9614868164062</pre> <p>Hmmm... is that the best it did?</p> <p>How about we try loading in the best performing <code>model_2</code> which was saved to file thanks to our <code>ModelCheckpoint</code> callback.</p> In\u00a0[\u00a0]: Copied! <pre># Load in best performing model\nmodel_2 = tf.keras.models.load_model(\"model_experiments/model_2_dense/\")\nmodel_2.evaluate(test_windows, test_labels)\n</pre> # Load in best performing model model_2 = tf.keras.models.load_model(\"model_experiments/model_2_dense/\") model_2.evaluate(test_windows, test_labels) <pre>18/18 [==============================] - 0s 2ms/step - loss: 608.9615\n</pre> Out[\u00a0]: <pre>608.9614868164062</pre> <p>Excellent! Loading back in the best performing model see's a performance boost.</p> <p>But let's not stop there, let's make some predictions with <code>model_2</code> and then evaluate them just as we did before.</p> In\u00a0[\u00a0]: Copied! <pre># Get forecast predictions\nmodel_2_preds = make_preds(model_2,\n                           input_data=test_windows)\n</pre> # Get forecast predictions model_2_preds = make_preds(model_2,                            input_data=test_windows) In\u00a0[\u00a0]: Copied! <pre># Evaluate results for model 2 predictions\nmodel_2_results = evaluate_preds(y_true=tf.squeeze(test_labels), # remove 1 dimension of test labels\n                                 y_pred=model_2_preds)\nmodel_2_results\n</pre> # Evaluate results for model 2 predictions model_2_results = evaluate_preds(y_true=tf.squeeze(test_labels), # remove 1 dimension of test labels                                  y_pred=model_2_preds) model_2_results Out[\u00a0]: <pre>{'mae': 608.9615,\n 'mape': 2.7693386,\n 'mase': 1.0644706,\n 'mse': 1281438.8,\n 'rmse': 1132.0065}</pre> <p>It looks like <code>model_2</code> performs worse than the na\u00efve model as well as <code>model_1</code>!</p> <p>Does this mean a smaller window size is better? (I'll leave this as a challenge you can experiment with)</p> <p>How do the predictions look?</p> In\u00a0[\u00a0]: Copied! <pre>offset = 300\nplt.figure(figsize=(10, 7))\n# Account for the test_window offset\nplot_time_series(timesteps=X_test[-len(test_windows):], values=test_labels[:, 0], start=offset, label=\"test_data\")\nplot_time_series(timesteps=X_test[-len(test_windows):], values=model_2_preds, start=offset, format=\"-\", label=\"model_2_preds\")\n</pre> offset = 300 plt.figure(figsize=(10, 7)) # Account for the test_window offset plot_time_series(timesteps=X_test[-len(test_windows):], values=test_labels[:, 0], start=offset, label=\"test_data\") plot_time_series(timesteps=X_test[-len(test_windows):], values=model_2_preds, start=offset, format=\"-\", label=\"model_2_preds\")  In\u00a0[\u00a0]: Copied! <pre>HORIZON = 7\nWINDOW_SIZE = 30\n\nfull_windows, full_labels = make_windows(prices, window_size=WINDOW_SIZE, horizon=HORIZON)\nlen(full_windows), len(full_labels)\n</pre> HORIZON = 7 WINDOW_SIZE = 30  full_windows, full_labels = make_windows(prices, window_size=WINDOW_SIZE, horizon=HORIZON) len(full_windows), len(full_labels) Out[\u00a0]: <pre>(2751, 2751)</pre> <p>And we'll split the full dataset windows into training and test sets.</p> In\u00a0[\u00a0]: Copied! <pre>train_windows, test_windows, train_labels, test_labels = make_train_test_splits(windows=full_windows, labels=full_labels, test_split=0.2)\nlen(train_windows), len(test_windows), len(train_labels), len(test_labels)\n</pre> train_windows, test_windows, train_labels, test_labels = make_train_test_splits(windows=full_windows, labels=full_labels, test_split=0.2) len(train_windows), len(test_windows), len(train_labels), len(test_labels) Out[\u00a0]: <pre>(2200, 551, 2200, 551)</pre> <p>Now let's build, compile, fit and evaluate a model.</p> In\u00a0[\u00a0]: Copied! <pre>tf.random.set_seed(42)\n\n# Create model (same as model_1 except with different data input size)\nmodel_3 = tf.keras.Sequential([\n  layers.Dense(128, activation=\"relu\"),\n  layers.Dense(HORIZON)\n], name=\"model_3_dense\")\n\nmodel_3.compile(loss=\"mae\",\n                optimizer=tf.keras.optimizers.Adam())\n\nmodel_3.fit(train_windows,\n            train_labels,\n            batch_size=128,\n            epochs=100,\n            verbose=0,\n            validation_data=(test_windows, test_labels),\n            callbacks=[create_model_checkpoint(model_name=model_3.name)])\n</pre> tf.random.set_seed(42)  # Create model (same as model_1 except with different data input size) model_3 = tf.keras.Sequential([   layers.Dense(128, activation=\"relu\"),   layers.Dense(HORIZON) ], name=\"model_3_dense\")  model_3.compile(loss=\"mae\",                 optimizer=tf.keras.optimizers.Adam())  model_3.fit(train_windows,             train_labels,             batch_size=128,             epochs=100,             verbose=0,             validation_data=(test_windows, test_labels),             callbacks=[create_model_checkpoint(model_name=model_3.name)]) <pre>INFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\nINFO:tensorflow:Assets written to: model_experiments/model_3_dense/assets\n</pre> Out[\u00a0]: <pre>&lt;keras.callbacks.History at 0x7fdcf1d36350&gt;</pre> In\u00a0[\u00a0]: Copied! <pre># How did our model with a larger window size and horizon go?\nmodel_3.evaluate(test_windows, test_labels)\n</pre> # How did our model with a larger window size and horizon go? model_3.evaluate(test_windows, test_labels) <pre>18/18 [==============================] - 0s 2ms/step - loss: 1321.5201\n</pre> Out[\u00a0]: <pre>1321.5201416015625</pre> <p>To compare apples to apples (best performing model to best performing model), we've got to load in the best version of <code>model_3</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Load in best version of model_3 and evaluate\nmodel_3 = tf.keras.models.load_model(\"model_experiments/model_3_dense/\")\nmodel_3.evaluate(test_windows, test_labels)\n</pre> # Load in best version of model_3 and evaluate model_3 = tf.keras.models.load_model(\"model_experiments/model_3_dense/\") model_3.evaluate(test_windows, test_labels) <pre>18/18 [==============================] - 0s 2ms/step - loss: 1237.5063\n</pre> Out[\u00a0]: <pre>1237.50634765625</pre> <p>In this case, the error will be higher because we're predicting 7 steps at a time.</p> <p>This makes sense though because the further you try and predict, the larger your error will be (think of trying to predict the weather 7 days in advance).</p> <p>Let's make predictions with our model using the <code>make_preds()</code> function and evaluate them using the <code>evaluate_preds()</code> function.</p> In\u00a0[\u00a0]: Copied! <pre># The predictions are going to be 7 steps at a time (this is the HORIZON size)\nmodel_3_preds = make_preds(model_3,\n                           input_data=test_windows)\nmodel_3_preds[:5]\n</pre> # The predictions are going to be 7 steps at a time (this is the HORIZON size) model_3_preds = make_preds(model_3,                            input_data=test_windows) model_3_preds[:5] Out[\u00a0]: <pre>&lt;tf.Tensor: shape=(5, 7), dtype=float32, numpy=\narray([[9004.693 , 9048.1   , 9425.088 , 9258.258 , 9495.798 , 9558.451 ,\n        9357.354 ],\n       [8735.507 , 8840.304 , 9247.793 , 8885.6   , 9097.188 , 9174.328 ,\n        9156.819 ],\n       [8672.508 , 8782.388 , 9123.8545, 8770.37  , 9007.13  , 9003.87  ,\n        9042.724 ],\n       [8874.398 , 8784.737 , 9043.901 , 8943.051 , 9033.479 , 9176.488 ,\n        9039.676 ],\n       [8825.891 , 8777.4375, 8926.779 , 8870.178 , 9213.232 , 9268.156 ,\n        8942.485 ]], dtype=float32)&gt;</pre> In\u00a0[\u00a0]: Copied! <pre># Calculate model_3 results - these are going to be multi-dimensional because\n# we're trying to predict more than one step at a time.\nmodel_3_results = evaluate_preds(y_true=tf.squeeze(test_labels),\n                                 y_pred=model_3_preds)\nmodel_3_results\n</pre> # Calculate model_3 results - these are going to be multi-dimensional because # we're trying to predict more than one step at a time. model_3_results = evaluate_preds(y_true=tf.squeeze(test_labels),                                  y_pred=model_3_preds) model_3_results Out[\u00a0]: <pre>{'mae': array([ 513.60516 ,  355.08356 ,  327.17035 ,  358.50977 ,  420.53207 ,\n         537.8539  ,  545.6606  ,  485.92307 ,  584.4969  ,  687.3814  ,\n         836.22675 ,  755.1571  ,  731.4959  ,  775.3398  ,  567.9548  ,\n         266.80865 ,  188.80217 ,  188.1077  ,  253.09521 ,  301.4336  ,\n         151.10742 ,  196.81424 ,  191.46184 ,  231.65067 ,  143.6114  ,\n         122.59033 ,  132.78844 ,  190.8116  ,  179.1598  ,  228.25949 ,\n         314.44016 ,  379.093   ,  278.3254  ,  295.34604 ,  299.38525 ,\n         248.64963 ,  299.75635 ,  259.6937  ,  180.30559 ,  206.72887 ,\n         374.62906 ,  144.85129 ,  142.33607 ,  131.1158  ,   93.94057 ,\n          54.825542,   73.7943  ,  103.59989 ,  121.3337  ,  168.67209 ,\n         183.90945 ,  152.25314 ,  186.57137 ,  146.91309 ,  240.42955 ,\n         351.00668 ,  540.9516  ,  549.15674 ,  521.2422  ,  526.8553  ,\n         453.36313 ,  257.98166 ,  277.29492 ,  301.82465 ,  455.71756 ,\n         458.96002 ,  503.44427 ,  522.3119  ,  223.07687 ,  250.09473 ,\n         297.14468 ,  400.56976 ,  495.79785 ,  364.33664 ,  283.3654  ,\n         325.59457 ,  238.21178 ,  318.9777  ,  460.77246 ,  651.0755  ,\n         835.88074 ,  669.9654  ,  319.5452  ,  261.99496 ,  142.39217 ,\n         136.62834 ,  154.75252 ,  221.32631 ,  290.50446 ,  503.8846  ,\n         414.2602  ,  434.35727 ,  377.98926 ,  251.7899  ,  204.28418 ,\n         388.22684 ,  360.65945 ,  493.80902 ,  614.86035 ,  754.7017  ,\n         533.7708  ,  378.98898 ,  280.49484 ,  339.48062 ,  413.12875 ,\n         452.87933 ,  550.53906 ,  634.57214 ,  935.57227 ,  931.6003  ,\n         881.2804  ,  426.40094 ,  179.45885 ,  121.66225 ,  160.43806 ,\n         372.07037 ,  341.85776 ,  476.52475 ,  618.3239  , 1038.8976  ,\n        1569.5022  , 2157.1196  , 1987.6074  , 2158.6108  , 2303.5603  ,\n        2662.9421  , 1405.5017  ,  728.30145 ,  351.70047 ,  322.03168 ,\n         493.94742 ,  435.48492 ,  565.55615 ,  350.1165  ,  289.0203  ,\n         251.21645 ,  409.05566 ,  342.2103  ,  320.83594 ,  330.88596 ,\n         357.8457  ,  335.9481  ,  206.1031  ,  544.3837  ,  700.0093  ,\n         468.91965 ,  404.4963  ,  172.80176 ,  308.33664 ,  210.47566 ,\n         318.95444 ,  486.1319  ,  428.87982 ,  533.91095 ,  433.7813  ,\n         396.89447 ,  138.40346 ,  189.96617 ,  170.39133 ,  181.54387 ,\n         282.8902  ,  264.3889  ,  250.62172 ,  240.33713 ,  276.9541  ,\n         326.0306  ,  489.61572 ,  686.2451  ,  526.9798  ,  603.0119  ,\n         825.38275 ,  871.04694 ,  990.06903 , 1090.0593  ,  560.2842  ,\n         310.5219  ,  371.39035 ,  348.45996 ,  355.73465 ,  429.56473 ,\n         581.2839  ,  550.5218  ,  635.4312  ,  913.40375 ,  840.71844 ,\n         305.03488 ,  493.93414 ,  751.3177  ,  410.63434 ,  220.62459 ,\n         282.25473 ,  291.85352 ,  422.50293 ,  458.65375 ,  637.2345  ,\n         647.82367 ,  417.24442 ,  220.23717 ,  246.72168 ,  200.22935 ,\n         455.14926 ,  719.6058  ,  696.3037  ,  485.09836 ,  294.9534  ,\n         170.52371 ,  211.82463 ,  270.56488 ,  189.89355 ,  171.21721 ,\n         366.4471  ,  231.96303 ,  318.78726 ,  273.79352 ,  358.55582 ,\n         412.22797 ,  512.31573 ,  185.43848 ,  196.38113 ,  200.00586 ,\n         224.58678 ,  213.41965 ,  186.23926 ,  113.37096 ,  172.23117 ,\n         168.89885 ,  236.09584 ,  307.59683 ,  328.93903 ,  566.5961  ,\n         285.04282 ,  300.4495  ,  125.45201 ,  168.94322 ,  137.25754 ,\n         143.50404 ,  145.27776 ,  107.340126,   77.16044 ,  131.87096 ,\n         134.01953 ,  167.45871 ,  137.92188 ,  148.91281 ,  204.95467 ,\n         157.89732 ,  196.95201 ,  167.93861 ,  156.45578 ,  188.6808  ,\n         161.44113 ,   90.997765,  136.84096 ,  198.51799 ,  230.13881 ,\n         294.7839  ,  594.24286 ,  699.64856 ,  815.137   ,  905.33887 ,\n        1127.2999  , 1342.6952  , 1317.0686  ,  590.1699  ,  296.34543 ,\n         243.4368  ,  256.3987  ,  222.28488 ,  323.8387  ,   82.51339 ,\n         120.14453 ,  249.14857 ,  205.73674 ,  243.45451 ,  250.64857 ,\n         287.27567 ,  224.3803  ,  266.55972 ,  221.50935 ,  218.49539 ,\n         272.42242 ,  279.5964  ,  252.58775 ,  381.56473 ,  417.97028 ,\n         624.5562  ,  368.0364  ,  327.25473 ,  263.4396  ,  349.95242 ,\n         398.62485 ,  297.07465 ,  147.04924 ,  164.54367 ,  313.36246 ,\n         477.7486  ,  675.042   ,  897.269   , 1094.7098  , 1460.177   ,\n        1398.5858  ,  952.72375 ,  645.9594  ,  166.17912 ,  144.66867 ,\n         189.1822  ,  304.81375 ,  435.10742 ,  449.64426 ,  425.244   ,\n         441.93637 ,  407.29605 ,  252.4036  ,  248.36928 ,  336.17062 ,\n         482.8111  ,  437.53043 ,  533.0855  ,  346.98047 ,  127.68722 ,\n         110.208565,  301.75223 ,  195.50697 ,  174.65248 ,  238.4707  ,\n         302.3711  ,  313.51703 ,  310.1289  ,  200.29701 ,  172.47209 ,\n         140.2302  ,  252.48479 ,  289.32407 ,  343.62222 ,  504.11816 ,\n         635.1339  ,  602.131   ,  519.0612  ,  214.90291 ,  195.91197 ,\n         265.03683 ,  198.9008  ,  345.51227 ,  517.22235 ,  631.02997 ,\n         988.4249  , 1174.2136  , 1196.2849  , 1253.93    ,  526.06573 ,\n         210.31306 ,  215.27205 ,  169.86008 ,  283.78543 ,  269.76117 ,\n         228.63477 ,  186.64719 ,  410.5434  ,  601.2659  ,  618.12164 ,\n         768.7538  , 1158.5449  , 1232.8798  , 1254.429   ,  423.02524 ,\n         390.03613 ,  367.23926 ,  209.55803 ,  530.2231  ,  821.38293 ,\n         812.37195 ,  741.19464 ,  953.48285 , 1258.9928  , 1844.9332  ,\n        1605.2852  , 1112.7673  ,  594.18945 ,  549.7676  ,  632.5438  ,\n         829.2656  , 1103.2213  , 1130.1024  , 1033.3527  ,  878.5851  ,\n         595.4096  , 1115.6515  , 1371.1725  , 1385.25    ,  387.52484 ,\n         303.74945 ,  495.12305 ,  719.5804  ,  648.9032  ,  766.1624  ,\n         683.23157 ,  596.4121  ,  523.09235 ,  577.34015 , 1337.6241  ,\n        2454.597   , 2759.2363  , 3135.8757  , 3407.3806  , 3602.6362  ,\n        2527.1584  , 1158.1016  ,  679.84375 ,  748.5586  , 1073.4896  ,\n        1217.7305  , 1459.5664  , 2502.7336  , 3075.8264  , 3090.2869  ,\n        2564.758   , 2660.5317  , 2928.9348  , 3690.5566  , 3525.1433  ,\n        4183.6704  , 5144.0693  , 4384.6533  , 4164.9614  , 4431.0967  ,\n        3335.121   , 3017.6858  , 2589.5403  , 4103.6313  , 5582.089   ,\n        5108.456   , 1382.9648  ,  953.5435  , 1081.6842  , 2483.1992  ,\n        2992.056   , 3127.0942  , 2972.2717  , 3054.6477  , 3283.7107  ,\n        3617.3652  , 1170.2556  , 1074.1886  , 1059.3181  , 1044.5385  ,\n        1060.6202  , 2115.7234  , 3569.8901  , 3474.3647  , 2448.0173  ,\n        2641.4202  , 3188.6016  , 4899.913   , 5516.293   , 4635.4883  ,\n        4677.036   , 5732.8804  , 5866.7344  , 8083.341   , 5049.8237  ,\n        1476.2316  , 1873.3314  , 1219.1033  , 2324.0698  , 2467.0044  ,\n        3005.0864  , 2805.0486  , 3688.2556  , 2962.7131  , 3664.236   ,\n        5618.6836  , 6925.0913  , 9751.091   , 8790.822   , 5057.624   ,\n        2971.6863  , 1715.495   , 1125.7689  , 2201.432   , 3969.5005  ,\n        2988.5112  , 2911.9336  , 2796.869   , 3937.8755  , 5449.5913  ,\n        6395.02    , 6148.2524  , 5437.706   , 4291.8267  , 2362.3962  ,\n        1657.3444  , 1426.63    , 1647.7142  , 2730.3962  , 1813.716   ,\n        2061.149   , 3095.72    , 4312.8096  , 5500.7227  , 5258.029   ,\n        5152.029   , 1331.4045  , 1634.4017  , 2493.4336  , 3957.9548  ,\n        4499.2153  , 2215.8533  , 2587.4136  , 1622.4392  ,  591.8778  ,\n        1297.4688  , 1284.5931  ,  915.22656 ,  940.4565  , 1084.6562  ,\n         875.89954 , 1041.3527  , 2299.2512  , 3152.6038  , 3518.3923  ,\n        2531.5212  , 2682.5737  , 3094.2947  , 3829.3286  , 5550.609   ,\n        7159.343   , 8820.526   , 8587.338   , 6777.814   , 5193.2866  ,\n        4793.332   , 2605.2517  , 1668.3544  , 2510.7244  , 4105.6865  ,\n        6278.3906  , 4109.759   , 1711.1211  , 2261.9878  , 2384.754   ,\n        1246.6602  , 1672.8494  , 1103.1322  , 1743.3594  , 1608.452   ,\n        1933.8995  , 2742.3455  , 3945.3633  , 5765.9414  , 6955.8384  ,\n        8107.707   ], dtype=float32),\n 'mape': array([ 5.8601456 ,  4.087344  ,  3.7758112 ,  4.187648  ,  4.9781313 ,\n         6.4483633 ,  6.614449  ,  6.0616755 ,  7.547588  ,  9.095916  ,\n        11.300213  , 10.396466  , 10.109091  , 10.697323  ,  7.8628383 ,\n         3.6872823 ,  2.585636  ,  2.505     ,  3.3504546 ,  4.013595  ,\n         2.0036254 ,  2.6397336 ,  2.6087892 ,  3.160931  ,  1.9612147 ,\n         1.6679796 ,  1.7952247 ,  2.5901198 ,  2.436256  ,  3.1327312 ,\n         4.3595414 ,  5.2856445 ,  3.9403565 ,  4.3142104 ,  4.3733163 ,\n         3.6376827 ,  4.362974  ,  3.7757344 ,  2.6192985 ,  2.9007592 ,\n         5.1501136 ,  2.014742  ,  1.9706616 ,  1.8104095 ,  1.3007166 ,\n         0.75351447,  1.0192169 ,  1.4282547 ,  1.6741827 ,  2.3589606 ,\n         2.563099  ,  2.1327207 ,  2.5974777 ,  2.017332  ,  3.1387668 ,\n         4.502132  ,  6.959625  ,  6.9602156 ,  6.5331526 ,  6.5757833 ,\n         5.610295  ,  3.0642097 ,  3.2474015 ,  3.4847279 ,  5.222716  ,\n         5.229475  ,  5.727943  ,  5.9273834 ,  2.5284538 ,  2.8663971 ,\n         3.4309018 ,  4.6874056 ,  5.841506  ,  4.30981   ,  3.3552744 ,\n         3.8162014 ,  2.7363582 ,  3.4623704 ,  4.971694  ,  7.0260944 ,\n         8.974986  ,  7.167823  ,  3.401335  ,  2.7761257 ,  1.522323  ,\n         1.4535459 ,  1.6354691 ,  2.252781  ,  2.9475648 ,  5.073901  ,\n         4.1032834 ,  4.295298  ,  3.7052956 ,  2.4620814 ,  2.013232  ,\n         3.8428285 ,  3.6306674 ,  5.0162754 ,  6.2794676 ,  7.757465  ,\n         5.495671  ,  3.8907118 ,  2.8546908 ,  3.531113  ,  4.464891  ,\n         5.0162473 ,  6.1257253 ,  7.1294317 , 10.711446  , 10.66408   ,\n        10.087247  ,  4.893642  ,  2.071729  ,  1.362841  ,  1.8011061 ,\n         4.219873  ,  4.055696  ,  5.744903  ,  7.492872  , 15.032968  ,\n        24.38846   , 35.42812   , 34.702423  , 39.86109   , 42.83657   ,\n        49.6046    , 26.081123  , 13.669959  ,  6.360867  ,  5.4986367 ,\n         7.9925113 ,  6.8575478 ,  8.82061   ,  5.311197  ,  4.5279207 ,\n         3.870413  ,  6.2283797 ,  5.2470355 ,  5.1045485 ,  5.329424  ,\n         5.7789664 ,  5.358487  ,  3.2211263 ,  8.118441  , 10.231978  ,\n         6.726861  ,  5.757309  ,  2.4041245 ,  4.2864537 ,  2.977692  ,\n         4.454852  ,  6.9261975 ,  6.1590724 ,  7.7638254 ,  6.308201  ,\n         5.7371497 ,  1.9719449 ,  2.7191157 ,  2.3998728 ,  2.568591  ,\n         3.953004  ,  3.6584775 ,  3.453558  ,  3.301869  ,  3.7942226 ,\n         4.310324  ,  6.4319835 ,  8.567106  ,  6.293662  ,  7.0238814 ,\n         9.469167  ,  9.915066  , 11.217036  , 12.28066   ,  6.209284  ,\n         3.2802734 ,  3.8508573 ,  3.6124747 ,  3.7172396 ,  4.6127787 ,\n         6.2338514 ,  6.037696  ,  7.1359625 , 10.132258  ,  9.337389  ,\n         3.379447  ,  5.190343  ,  7.822919  ,  4.2644997 ,  2.328503  ,\n         3.0320923 ,  3.122556  ,  4.5721846 ,  5.013454  ,  7.0593185 ,\n         7.180876  ,  4.6288185 ,  2.4294462 ,  2.6893413 ,  2.1346936 ,\n         4.6703887 ,  7.429678  ,  7.177754  ,  4.9274387 ,  2.969763  ,\n         1.7036035 ,  2.1020818 ,  2.7900844 ,  1.9541025 ,  1.7562655 ,\n         3.8170404 ,  2.4557195 ,  3.335657  ,  2.8912652 ,  3.8006802 ,\n         4.3685193 ,  5.4371476 ,  1.96905   ,  2.086854  ,  2.1329875 ,\n         2.4014482 ,  2.2829742 ,  1.9878384 ,  1.2127163 ,  1.8306142 ,\n         1.8156711 ,  2.5207567 ,  3.2923858 ,  3.5837193 ,  6.1877694 ,\n         3.1183949 ,  3.2884538 ,  1.3740205 ,  1.8472785 ,  1.5017135 ,\n         1.5671413 ,  1.5889224 ,  1.1630745 ,  0.83710796,  1.4223018 ,\n         1.4407477 ,  1.8030001 ,  1.4797007 ,  1.599079  ,  2.2175813 ,\n         1.7130904 ,  2.1422603 ,  1.8232663 ,  1.7015574 ,  2.0563016 ,\n         1.7596645 ,  0.98229814,  1.4560648 ,  2.1070251 ,  2.4114208 ,\n         3.0499995 ,  5.7738442 ,  6.618569  ,  7.495159  ,  8.227665  ,\n        10.186895  , 11.928129  , 11.676125  ,  5.199727  ,  2.5929737 ,\n         2.1249924 ,  2.2335236 ,  1.9233094 ,  2.846623  ,  0.7076666 ,\n         1.0350616 ,  2.132228  ,  1.778991  ,  2.1161108 ,  2.1635604 ,\n         2.4792838 ,  1.9270526 ,  2.2551262 ,  1.8457135 ,  1.8112589 ,\n         2.262672  ,  2.3515143 ,  2.1173332 ,  3.2111585 ,  3.5736024 ,\n         5.3715696 ,  3.18694   ,  2.858538  ,  2.295805  ,  3.0580947 ,\n         3.4806054 ,  2.5900245 ,  1.2625037 ,  1.4050376 ,  2.7622259 ,\n         4.3049536 ,  6.2830725 ,  8.513568  , 10.517086  , 14.161078  ,\n        13.640608  ,  9.320237  ,  6.3080745 ,  1.6300542 ,  1.3983166 ,\n         1.8191801 ,  2.8875985 ,  4.068832  ,  4.1736484 ,  3.92286   ,\n         4.0472727 ,  3.7387483 ,  2.3069727 ,  2.3063238 ,  3.1941137 ,\n         4.5895944 ,  4.157942  ,  5.0558047 ,  3.3005743 ,  1.2102847 ,\n         1.037109  ,  2.8088598 ,  1.8151615 ,  1.6353209 ,  2.2353542 ,\n         2.8412042 ,  2.952003  ,  2.9187465 ,  1.8913074 ,  1.6178632 ,\n         1.2982845 ,  2.3039308 ,  2.6034784 ,  3.017652  ,  4.444261  ,\n         5.5989385 ,  5.26695   ,  4.535601  ,  1.8671715 ,  1.7055075 ,\n         2.3196962 ,  1.7318225 ,  2.7897227 ,  4.062603  ,  4.8944497 ,\n         7.6816416 ,  9.107798  ,  9.207181  ,  9.502263  ,  3.9754786 ,\n         1.5744586 ,  1.5997065 ,  1.242373  ,  2.0770247 ,  1.974714  ,\n         1.6764196 ,  1.3502706 ,  2.7858412 ,  3.9915662 ,  4.0835853 ,\n         5.016299  ,  7.6388373 ,  8.078356  ,  8.1091385 ,  2.6612654 ,\n         2.4994845 ,  2.2963924 ,  1.304031  ,  3.2585588 ,  4.9145164 ,\n         4.7377896 ,  4.253874  ,  5.353144  ,  6.9800787 , 10.170477  ,\n         8.772201  ,  6.0037975 ,  3.184015  ,  3.0090022 ,  3.637018  ,\n         4.7740936 ,  6.289234  ,  6.4191875 ,  5.904088  ,  4.9963098 ,\n         3.1906037 ,  5.8562336 ,  7.18103   ,  7.211652  ,  2.0219958 ,\n         1.5912417 ,  2.624319  ,  3.8534112 ,  3.5140653 ,  4.1405296 ,\n         3.6843975 ,  3.215591  ,  2.8228219 ,  2.8898468 ,  6.2849083 ,\n        11.458009  , 12.342688  , 13.764669  , 14.824157  , 15.566204  ,\n        10.829556  ,  4.9278235 ,  2.8345613 ,  3.0226748 ,  4.311801  ,\n         4.7200484 ,  5.516797  ,  9.303045  , 11.212711  , 11.037065  ,\n         8.631595  ,  8.57773   ,  9.389048  , 11.497772  , 10.463535  ,\n        11.683612  , 13.823776  , 11.273857  , 10.805961  , 11.400717  ,\n         8.4966135 ,  7.8893795 ,  7.169477  , 11.423848  , 15.540573  ,\n        14.24638   ,  3.8430922 ,  2.545029  ,  2.926236  ,  7.288314  ,\n         8.999365  ,  9.586316  ,  9.271836  ,  9.423251  , 10.242945  ,\n        11.396603  ,  3.6557918 ,  3.3454742 ,  3.2290876 ,  3.159098  ,\n         3.1852577 ,  6.1412444 , 10.163153  ,  9.673946  ,  6.657339  ,\n         6.975828  ,  8.377453  , 12.244584  , 13.00238   , 10.547294  ,\n        10.269914  , 12.4054    , 12.598526  , 17.187769  , 10.61679   ,\n         3.0717697 ,  3.7662785 ,  2.3915377 ,  4.4263554 ,  4.6217527 ,\n         5.4785805 ,  5.1152287 ,  6.826434  ,  5.61515   ,  7.0909567 ,\n        11.599258  , 14.51015   , 20.678543  , 18.670784  , 10.813841  ,\n         6.395925  ,  3.68131   ,  2.323965  ,  4.4593787 ,  8.030951  ,\n         5.9532943 ,  5.6428704 ,  5.1379447 ,  7.1625338 ,  9.831415  ,\n        11.209798  , 10.556711  ,  9.350725  ,  7.3507624 ,  3.9944    ,\n         2.8233469 ,  2.3929296 ,  2.8540497 ,  4.729086  ,  3.236382  ,\n         3.6810744 ,  5.6685753 ,  8.005048  , 10.247115  ,  9.747856  ,\n         9.5460415 ,  2.4771707 ,  2.9854116 ,  4.3493814 ,  6.8806467 ,\n         7.758685  ,  3.8011405 ,  4.4251776 ,  2.7610898 ,  1.0093751 ,\n         2.2422    ,  2.2289147 ,  1.58688   ,  1.6239213 ,  1.8814766 ,\n         1.5162641 ,  1.7166423 ,  3.7594385 ,  5.0978    ,  5.664717  ,\n         4.0754285 ,  4.3486743 ,  5.1419683 ,  6.618967  ,  9.823991  ,\n        12.98137   , 16.388018  , 16.260675  , 13.249782  , 10.180592  ,\n         9.383685  ,  5.1218452 ,  3.2893803 ,  4.634893  ,  7.4068375 ,\n        11.284324  ,  7.287351  ,  3.0196326 ,  4.006139  ,  4.172643  ,\n         2.233759  ,  3.0054173 ,  1.9702865 ,  3.1210938 ,  2.7774746 ,\n         3.4924886 ,  5.1419373 ,  7.6572022 , 11.470654  , 14.311543  ,\n        17.28017   ], dtype=float32),\n 'mase': 2.2020736,\n 'mse': array([3.15562156e+05, 1.69165547e+05, 1.44131812e+05, 1.76002922e+05,\n        2.63519750e+05, 3.91517188e+05, 3.99524688e+05, 3.44422312e+05,\n        4.93892156e+05, 6.88785625e+05, 9.18703562e+05, 8.00988125e+05,\n        6.35508625e+05, 6.45535125e+05, 3.57740000e+05, 1.03007117e+05,\n        5.33132578e+04, 4.65829805e+04, 1.10349188e+05, 1.11647695e+05,\n        4.35359297e+04, 5.45387773e+04, 4.55849375e+04, 7.64886406e+04,\n        4.03074453e+04, 2.71072188e+04, 1.90268887e+04, 4.55625000e+04,\n        4.25345820e+04, 6.63246172e+04, 1.16741711e+05, 1.69440984e+05,\n        1.10486977e+05, 1.59707406e+05, 1.61631141e+05, 1.23861828e+05,\n        1.50757328e+05, 1.31196297e+05, 5.50281289e+04, 6.28155391e+04,\n        1.77474391e+05, 2.68843926e+04, 2.53033750e+04, 2.58044258e+04,\n        1.34592480e+04, 3.88203076e+03, 9.22772559e+03, 1.22399551e+04,\n        2.20868379e+04, 3.60825352e+04, 4.87962109e+04, 3.74652812e+04,\n        4.10560898e+04, 4.17418750e+04, 1.10490836e+05, 2.00762031e+05,\n        3.48149000e+05, 3.58060812e+05, 3.28375281e+05, 2.96505844e+05,\n        2.33354141e+05, 1.54047203e+05, 1.43171328e+05, 1.62586469e+05,\n        2.96452719e+05, 2.66880719e+05, 2.94189062e+05, 3.10067656e+05,\n        6.27178750e+04, 8.63868672e+04, 9.69545391e+04, 2.01814359e+05,\n        2.97053594e+05, 2.00674891e+05, 1.27305148e+05, 1.32509578e+05,\n        8.46020391e+04, 1.74817453e+05, 2.95115844e+05, 4.75888719e+05,\n        7.21894500e+05, 4.62711281e+05, 1.33154750e+05, 8.13640156e+04,\n        3.43108672e+04, 3.45204180e+04, 5.36851641e+04, 7.42413281e+04,\n        1.09339375e+05, 2.79682844e+05, 2.19119422e+05, 1.99934359e+05,\n        1.65903391e+05, 8.16738047e+04, 4.82395195e+04, 1.65301000e+05,\n        1.81699391e+05, 3.30645000e+05, 4.63625750e+05, 6.75044000e+05,\n        3.34911500e+05, 1.69343750e+05, 1.03219234e+05, 1.48347609e+05,\n        2.98684844e+05, 3.71952656e+05, 4.43306688e+05, 5.27904688e+05,\n        1.12945275e+06, 1.00652938e+06, 8.03246875e+05, 2.21277969e+05,\n        6.64886406e+04, 2.66182090e+04, 3.29688867e+04, 1.68663250e+05,\n        1.76257219e+05, 2.99365562e+05, 4.31793438e+05, 1.91346712e+06,\n        3.90353825e+06, 6.29502500e+06, 5.49042400e+06, 6.25676850e+06,\n        6.28618000e+06, 7.23876400e+06, 2.08143525e+06, 6.66786500e+05,\n        1.44639844e+05, 1.83413594e+05, 3.41026562e+05, 3.02069719e+05,\n        3.71339750e+05, 2.02418219e+05, 1.36151141e+05, 8.24758203e+04,\n        1.96006719e+05, 1.61595656e+05, 1.71401391e+05, 2.00606359e+05,\n        2.37198891e+05, 1.58720172e+05, 6.36030352e+04, 3.38904031e+05,\n        5.23524156e+05, 2.86212188e+05, 1.92326125e+05, 3.73765547e+04,\n        1.31962000e+05, 9.33163047e+04, 1.18443672e+05, 3.00598719e+05,\n        2.28996984e+05, 3.57954281e+05, 2.36219078e+05, 1.71530359e+05,\n        2.68482539e+04, 4.79163125e+04, 4.00419297e+04, 5.41031875e+04,\n        1.07641391e+05, 9.80020000e+04, 7.08350078e+04, 6.55985312e+04,\n        8.53740000e+04, 1.33837578e+05, 2.72516312e+05, 6.54133500e+05,\n        4.97212750e+05, 6.11190062e+05, 9.92907688e+05, 9.56692312e+05,\n        1.09541412e+06, 1.19283125e+06, 3.75002219e+05, 2.22053797e+05,\n        2.60620031e+05, 1.88109625e+05, 1.87927500e+05, 2.49058609e+05,\n        3.79192656e+05, 4.07706781e+05, 6.24783062e+05, 9.62079375e+05,\n        7.95344562e+05, 1.40557531e+05, 2.65227406e+05, 6.26269500e+05,\n        2.05804234e+05, 6.53089570e+04, 1.33562969e+05, 1.14692070e+05,\n        2.26913016e+05, 2.61226500e+05, 5.08516250e+05, 4.90744656e+05,\n        2.38721828e+05, 6.85555703e+04, 7.30576641e+04, 5.36113750e+04,\n        3.28223562e+05, 6.26447375e+05, 5.17735438e+05, 3.32536062e+05,\n        1.72657266e+05, 8.28138125e+04, 1.43392656e+05, 8.83169844e+04,\n        5.15846680e+04, 4.20798125e+04, 1.78049141e+05, 1.17351570e+05,\n        1.16870422e+05, 1.02416836e+05, 1.67488781e+05, 2.05509953e+05,\n        2.73221469e+05, 4.73881641e+04, 4.60577500e+04, 5.33725703e+04,\n        7.87071094e+04, 7.06486719e+04, 5.23131758e+04, 2.50280723e+04,\n        4.52024609e+04, 4.14732461e+04, 7.00107344e+04, 1.24132148e+05,\n        1.34168141e+05, 3.38401562e+05, 9.75599844e+04, 1.10023000e+05,\n        1.99194824e+04, 3.32738164e+04, 2.43711914e+04, 2.76617012e+04,\n        2.70386660e+04, 1.58115801e+04, 1.16166348e+04, 2.74852148e+04,\n        2.95266426e+04, 3.95874922e+04, 2.81846035e+04, 3.19155312e+04,\n        5.60368477e+04, 3.98338398e+04, 5.02201602e+04, 3.19759512e+04,\n        2.96142285e+04, 4.48333711e+04, 4.32265664e+04, 1.52425576e+04,\n        2.86077324e+04, 4.23488125e+04, 7.25032578e+04, 1.21924688e+05,\n        6.41490688e+05, 8.50461562e+05, 1.11135362e+06, 1.22003100e+06,\n        1.54110162e+06, 2.03452138e+06, 1.76440575e+06, 4.22100781e+05,\n        1.30513797e+05, 7.30581250e+04, 8.56110391e+04, 7.42401094e+04,\n        1.49466766e+05, 1.25623496e+04, 2.13100293e+04, 9.81795703e+04,\n        7.28018203e+04, 1.26064742e+05, 1.01337961e+05, 1.24553164e+05,\n        7.13400000e+04, 8.41950469e+04, 6.25289375e+04, 8.18789453e+04,\n        1.27303195e+05, 8.94661250e+04, 1.04100523e+05, 1.66147031e+05,\n        2.32076422e+05, 4.41965062e+05, 1.75838281e+05, 1.76445328e+05,\n        9.12169375e+04, 1.67975469e+05, 1.87876422e+05, 1.07636930e+05,\n        3.26578105e+04, 5.35412461e+04, 1.38143531e+05, 3.28638219e+05,\n        6.38893688e+05, 1.08279325e+06, 1.55802525e+06, 2.42136925e+06,\n        2.08980088e+06, 9.83019312e+05, 4.27435500e+05, 5.84014961e+04,\n        4.02855312e+04, 5.27582578e+04, 1.33694297e+05, 2.41687594e+05,\n        2.54570500e+05, 2.18475578e+05, 2.39844719e+05, 1.72920359e+05,\n        9.46134531e+04, 8.14810703e+04, 1.83791891e+05, 3.41631531e+05,\n        2.67090906e+05, 3.54819906e+05, 1.69607641e+05, 2.63053145e+04,\n        2.11339043e+04, 1.11686047e+05, 6.08852578e+04, 4.64014297e+04,\n        6.97661953e+04, 1.16936102e+05, 1.44631031e+05, 1.14626898e+05,\n        7.92795234e+04, 5.69158320e+04, 3.03999824e+04, 9.47752891e+04,\n        1.19674977e+05, 2.15400000e+05, 3.15528031e+05, 4.17611781e+05,\n        3.91754000e+05, 2.83129906e+05, 8.15816328e+04, 5.00839805e+04,\n        1.01595930e+05, 6.17696953e+04, 3.16726438e+05, 5.76748000e+05,\n        7.41103312e+05, 1.33218000e+06, 1.56516188e+06, 1.55553338e+06,\n        1.68517900e+06, 3.21850031e+05, 6.30693242e+04, 7.78374766e+04,\n        5.12543047e+04, 1.14444250e+05, 1.13520102e+05, 7.03014531e+04,\n        4.71108398e+04, 4.52435781e+05, 7.79108188e+05, 7.32536312e+05,\n        1.03909088e+06, 1.56208738e+06, 1.63956762e+06, 1.72789975e+06,\n        4.00525438e+05, 2.34608641e+05, 1.76724859e+05, 6.78014609e+04,\n        3.90382062e+05, 9.80141562e+05, 1.03537131e+06, 8.44669812e+05,\n        1.38893975e+06, 1.98341138e+06, 3.62835425e+06, 2.66920750e+06,\n        1.42117000e+06, 4.54602219e+05, 3.65967062e+05, 9.46589188e+05,\n        1.45184738e+06, 1.85900112e+06, 1.89930475e+06, 1.82373112e+06,\n        1.21286088e+06, 4.94641594e+05, 1.49650312e+06, 2.03091800e+06,\n        2.25657350e+06, 2.11182750e+05, 1.21396250e+05, 3.04308844e+05,\n        7.11903438e+05, 6.20312000e+05, 8.05386562e+05, 6.26902875e+05,\n        5.66389875e+05, 4.27517438e+05, 7.83491562e+05, 3.63025775e+06,\n        8.18037650e+06, 1.07676250e+07, 1.26563580e+07, 1.33500370e+07,\n        1.33957390e+07, 6.52511100e+06, 1.60696775e+06, 8.56758062e+05,\n        1.15616712e+06, 1.69269000e+06, 2.17122950e+06, 3.34218200e+06,\n        7.84994850e+06, 1.08378930e+07, 1.05726990e+07, 1.01602540e+07,\n        1.14205900e+07, 1.16179750e+07, 1.73510000e+07, 1.79301800e+07,\n        2.70626460e+07, 3.72665640e+07, 2.97687340e+07, 2.14580120e+07,\n        2.44066140e+07, 1.56006580e+07, 1.03771360e+07, 9.93532800e+06,\n        2.18889460e+07, 3.45001800e+07, 2.72495080e+07, 2.51545225e+06,\n        2.11410550e+06, 1.65101288e+06, 8.78065000e+06, 1.42263970e+07,\n        1.50277600e+07, 1.42628750e+07, 1.11520980e+07, 1.31385740e+07,\n        1.44270080e+07, 2.10807200e+06, 2.37278650e+06, 2.06521262e+06,\n        1.51271400e+06, 1.63591925e+06, 6.01038950e+06, 1.48181850e+07,\n        1.51462730e+07, 8.88826900e+06, 1.03656340e+07, 1.24236510e+07,\n        3.12300180e+07, 4.44122480e+07, 3.35616920e+07, 3.37026000e+07,\n        4.33024720e+07, 4.04647440e+07, 6.68120760e+07, 2.68142400e+07,\n        3.37228800e+06, 6.18199650e+06, 3.33604150e+06, 9.76003100e+06,\n        9.50538200e+06, 1.48300340e+07, 1.08088990e+07, 1.52812380e+07,\n        1.21298770e+07, 1.47110990e+07, 4.46196920e+07, 6.19225800e+07,\n        1.05739048e+08, 8.06071760e+07, 3.00356620e+07, 1.30779130e+07,\n        4.55988750e+06, 1.67929975e+06, 6.99432300e+06, 1.63487150e+07,\n        1.21498980e+07, 1.21798540e+07, 1.68979740e+07, 2.39387020e+07,\n        3.73065680e+07, 5.09610480e+07, 4.59523240e+07, 3.20003840e+07,\n        2.06371540e+07, 8.14507050e+06, 4.14795750e+06, 4.23499100e+06,\n        3.09784650e+06, 8.37917500e+06, 7.36108300e+06, 7.06094350e+06,\n        1.57310800e+07, 2.78767160e+07, 4.07339880e+07, 3.28521600e+07,\n        2.89016500e+07, 3.40228600e+06, 3.84223250e+06, 8.10806000e+06,\n        1.74778820e+07, 2.13824600e+07, 5.77506950e+06, 7.30700800e+06,\n        3.70581275e+06, 6.14524500e+05, 1.92765088e+06, 2.29573075e+06,\n        1.09031975e+06, 1.19423488e+06, 2.01535425e+06, 1.38583700e+06,\n        2.60915325e+06, 7.48127550e+06, 1.25728690e+07, 1.49188240e+07,\n        7.64712100e+06, 9.50861100e+06, 1.16537410e+07, 2.15388360e+07,\n        4.32744960e+07, 6.52254160e+07, 9.30125040e+07, 8.49970400e+07,\n        5.87007080e+07, 3.43532560e+07, 2.82685780e+07, 9.30442400e+06,\n        4.66932300e+06, 8.00189950e+06, 2.05975580e+07, 4.07234120e+07,\n        1.93118840e+07, 4.04353375e+06, 5.92508700e+06, 7.37346950e+06,\n        2.68488000e+06, 5.27761500e+06, 1.84823362e+06, 4.87232050e+06,\n        4.87888400e+06, 5.77534400e+06, 1.21702240e+07, 2.70199400e+07,\n        4.94704840e+07, 6.90300960e+07, 8.50554880e+07], dtype=float32),\n 'rmse': array([  561.7492  ,   411.2974  ,   379.64694 ,   419.52707 ,\n          513.34174 ,   625.7134  ,   632.07965 ,   586.87506 ,\n          702.7746  ,   829.9311  ,   958.4903  ,   894.97943 ,\n          797.1879  ,   803.452   ,   598.1137  ,   320.94724 ,\n          230.89664 ,   215.8309  ,   332.18848 ,   334.13724 ,\n          208.65266 ,   233.53539 ,   213.50632 ,   276.5658  ,\n          200.76715 ,   164.6427  ,   137.93799 ,   213.45375 ,\n          206.23912 ,   257.53568 ,   341.67487 ,   411.6321  ,\n          332.3958  ,   399.6341  ,   402.03375 ,   351.9401  ,\n          388.27478 ,   362.2103  ,   234.58073 ,   250.63026 ,\n          421.2771  ,   163.96461 ,   159.07036 ,   160.63757 ,\n          116.014   ,    62.305946,    96.06105 ,   110.63433 ,\n          148.61641 ,   189.95403 ,   220.89862 ,   193.55951 ,\n          202.62302 ,   204.30829 ,   332.4016  ,   448.06476 ,\n          590.04156 ,   598.38184 ,   573.0404  ,   544.5235  ,\n          483.0674  ,   392.4885  ,   378.37988 ,   403.22012 ,\n          544.47473 ,   516.605   ,   542.39197 ,   556.8372  ,\n          250.43535 ,   293.91644 ,   311.3752  ,   449.23752 ,\n          545.02625 ,   447.9675  ,   356.79846 ,   364.01865 ,\n          290.8643  ,   418.11176 ,   543.24567 ,   689.84686 ,\n          849.64374 ,   680.2288  ,   364.9038  ,   285.2438  ,\n          185.23193 ,   185.7967  ,   231.7006  ,   272.47263 ,\n          330.66504 ,   528.85046 ,   468.10193 ,   447.1402  ,\n          407.3124  ,   285.7863  ,   219.63496 ,   406.57227 ,\n          426.26212 ,   575.0174  ,   680.9007  ,   821.6106  ,\n          578.7154  ,   411.51398 ,   321.27753 ,   385.1592  ,\n          546.5207  ,   609.8793  ,   665.8128  ,   726.57056 ,\n         1062.7572  ,  1003.25934 ,   896.2404  ,   470.40195 ,\n          257.8539  ,   163.15088 ,   181.57336 ,   410.6863  ,\n          419.83    ,   547.14307 ,   657.1099  ,  1383.2812  ,\n         1975.7374  ,  2508.9888  ,  2343.1653  ,  2501.3535  ,\n         2507.2256  ,  2690.495   ,  1442.718   ,   816.56995 ,\n          380.3155  ,   428.26813 ,   583.9748  ,   549.6087  ,\n          609.3766  ,   449.90912 ,   368.98663 ,   287.18607 ,\n          442.72644 ,   401.98962 ,   414.00653 ,   447.891   ,\n          487.03067 ,   398.39697 ,   252.1964  ,   582.15466 ,\n          723.54974 ,   534.98804 ,   438.55002 ,   193.33018 ,\n          363.26575 ,   305.47717 ,   344.15646 ,   548.26886 ,\n          478.53625 ,   598.29285 ,   486.0237  ,   414.16226 ,\n          163.85437 ,   218.89795 ,   200.1048  ,   232.60092 ,\n          328.0875  ,   313.0527  ,   266.14847 ,   256.1221  ,\n          292.1883  ,   365.8382  ,   522.03094 ,   808.7852  ,\n          705.1331  ,   781.7865  ,   996.4475  ,   978.1065  ,\n         1046.6204  ,  1092.1682  ,   612.37427 ,   471.22586 ,\n          510.50952 ,   433.7161  ,   433.50607 ,   499.05774 ,\n          615.78625 ,   638.5192  ,   790.4321  ,   980.85645 ,\n          891.8209  ,   374.91006 ,   515.0024  ,   791.3719  ,\n          453.65652 ,   255.55615 ,   365.4627  ,   338.66217 ,\n          476.35388 ,   511.10318 ,   713.1032  ,   700.5317  ,\n          488.5917  ,   261.8312  ,   270.2918  ,   231.54132 ,\n          572.908   ,   791.48425 ,   719.5384  ,   576.6594  ,\n          415.52045 ,   287.7739  ,   378.6722  ,   297.18173 ,\n          227.1226  ,   205.13364 ,   421.95868 ,   342.56616 ,\n          341.86316 ,   320.02628 ,   409.25394 ,   453.33206 ,\n          522.70593 ,   217.68823 ,   214.61069 ,   231.02502 ,\n          280.54785 ,   265.7982  ,   228.72073 ,   158.20264 ,\n          212.60869 ,   203.64981 ,   264.5954  ,   352.3239  ,\n          366.2897  ,   581.72296 ,   312.34595 ,   331.69714 ,\n          141.1364  ,   182.4111  ,   156.11276 ,   166.31807 ,\n          164.43439 ,   125.7441  ,   107.780495,   165.78665 ,\n          171.83319 ,   198.96605 ,   167.8827  ,   178.64919 ,\n          236.72102 ,   199.58418 ,   224.09854 ,   178.8182  ,\n          172.08786 ,   211.73892 ,   207.91    ,   123.46075 ,\n          169.1382  ,   205.78828 ,   269.2643  ,   349.1772  ,\n          800.93115 ,   922.2048  ,  1054.2076  ,  1104.5502  ,\n         1241.4113  ,  1426.3665  ,  1328.3093  ,   649.6928  ,\n          361.26694 ,   270.29266 ,   292.59366 ,   272.4704  ,\n          386.6093  ,   112.081894,   145.97955 ,   313.3362  ,\n          269.8181  ,   355.05597 ,   318.33624 ,   352.9209  ,\n          267.0955  ,   290.16382 ,   250.05786 ,   286.145   ,\n          356.79572 ,   299.1089  ,   322.64612 ,   407.6114  ,\n          481.7431  ,   664.8045  ,   419.33078 ,   420.05396 ,\n          302.0214  ,   409.84808 ,   433.44717 ,   328.0807  ,\n          180.71472 ,   231.3898  ,   371.67667 ,   573.2698  ,\n          799.3082  ,  1040.5736  ,  1248.2089  ,  1556.075   ,\n         1445.6144  ,   991.4733  ,   653.7855  ,   241.664   ,\n          200.71255 ,   229.69164 ,   365.6423  ,   491.61737 ,\n          504.54974 ,   467.4137  ,   489.7394  ,   415.83694 ,\n          307.593   ,   285.44888 ,   428.70956 ,   584.49255 ,\n          516.80835 ,   595.6676  ,   411.8345  ,   162.18913 ,\n          145.37505 ,   334.1946  ,   246.74936 ,   215.40991 ,\n          264.1329  ,   341.95923 ,   380.3039  ,   338.56595 ,\n          281.5662  ,   238.5704  ,   174.35591 ,   307.85596 ,\n          345.94073 ,   464.11203 ,   561.7188  ,   646.22894 ,\n          625.9026  ,   532.09955 ,   285.625   ,   223.7945  ,\n          318.74115 ,   248.5351  ,   562.78455 ,   759.4393  ,\n          860.8736  ,  1154.201   ,  1251.0643  ,  1247.2102  ,\n         1298.1444  ,   567.3183  ,   251.13608 ,   278.9937  ,\n          226.39412 ,   338.29608 ,   336.92746 ,   265.14423 ,\n          217.05034 ,   672.6335  ,   882.671   ,   855.88336 ,\n         1019.35803 ,  1249.835   ,  1280.4559  ,  1314.4961  ,\n          632.8708  ,   484.36414 ,   420.38657 ,   260.38715 ,\n          624.8056  ,   990.02106 ,  1017.5319  ,   919.0592  ,\n         1178.533   ,  1408.3364  ,  1904.8239  ,  1633.771   ,\n         1192.1284  ,   674.242   ,   604.9521  ,   972.9281  ,\n         1204.9263  ,  1363.4519  ,  1378.1527  ,  1350.4559  ,\n         1101.2997  ,   703.3076  ,  1223.3164  ,  1425.1029  ,\n         1502.1896  ,   459.54623 ,   348.41965 ,   551.64197 ,\n          843.7437  ,   787.5989  ,   897.4333  ,   791.77203 ,\n          752.5888  ,   653.84814 ,   885.1505  ,  1905.3235  ,\n         2860.1357  ,  3281.406   ,  3557.5776  ,  3653.7703  ,\n         3660.019   ,  2554.4297  ,  1267.6624  ,   925.6123  ,\n         1075.2522  ,  1301.0342  ,  1473.5092  ,  1828.1636  ,\n         2801.7761  ,  3292.0957  ,  3251.5688  ,  3187.5154  ,\n         3379.4363  ,  3408.5151  ,  4165.453   ,  4234.4043  ,\n         5202.1772  ,  6104.635   ,  5456.073   ,  4632.28    ,\n         4940.305   ,  3949.767   ,  3221.3562  ,  3152.0356  ,\n         4678.5625  ,  5873.6855  ,  5220.106   ,  1586.0177  ,\n         1453.9965  ,  1284.9175  ,  2963.2163  ,  3771.7898  ,\n         3876.5652  ,  3776.622   ,  3339.4758  ,  3624.7173  ,\n         3798.29    ,  1451.92    ,  1540.3851  ,  1437.0847  ,\n         1229.9243  ,  1279.0306  ,  2451.6096  ,  3849.44    ,\n         3891.8213  ,  2981.3203  ,  3219.5703  ,  3524.72    ,\n         5588.383   ,  6664.252   ,  5793.246   ,  5805.394   ,\n         6580.4614  ,  6361.1904  ,  8173.865   ,  5178.2466  ,\n         1836.379   ,  2486.362   ,  1826.4835  ,  3124.1047  ,\n         3083.0798  ,  3850.9783  ,  3287.689   ,  3909.1228  ,\n         3482.797   ,  3835.5054  ,  6679.7974  ,  7869.091   ,\n        10282.95    ,  8978.149   ,  5480.48    ,  3616.3398  ,\n         2135.3894  ,  1295.878   ,  2644.6782  ,  4043.3542  ,\n         3485.6704  ,  3489.9646  ,  4110.7144  ,  4892.719   ,\n         6107.91    ,  7138.7007  ,  6778.814   ,  5656.8877  ,\n         4542.8135  ,  2853.957   ,  2036.6534  ,  2057.9094  ,\n         1760.0701  ,  2894.6804  ,  2713.1316  ,  2657.2437  ,\n         3966.2427  ,  5279.841   ,  6382.3184  ,  5731.68    ,\n         5376.0254  ,  1844.5288  ,  1960.1613  ,  2847.4656  ,\n         4180.656   ,  4624.117   ,  2403.1375  ,  2703.1477  ,\n         1925.0488  ,   783.9162  ,  1388.3987  ,  1515.1669  ,\n         1044.1838  ,  1092.8105  ,  1419.6317  ,  1177.2158  ,\n         1615.2874  ,  2735.192   ,  3545.8242  ,  3862.4895  ,\n         2765.3428  ,  3083.6038  ,  3413.7578  ,  4640.995   ,\n         6578.3354  ,  8076.225   ,  9644.3     ,  9219.384   ,\n         7661.638   ,  5861.165   ,  5316.8203  ,  3050.3152  ,\n         2160.8616  ,  2828.763   ,  4538.453   ,  6381.4897  ,\n         4394.529   ,  2010.854   ,  2434.1501  ,  2715.4133  ,\n         1638.5604  ,  2297.3062  ,  1359.4976  ,  2207.3335  ,\n         2208.8198  ,  2403.1946  ,  3488.5847  ,  5198.071   ,\n         7033.5254  ,  8308.436   ,  9222.554   ], dtype=float32)}</pre> In\u00a0[\u00a0]: Copied! <pre>def evaluate_preds(y_true, y_pred):\n  # Make sure float32 (for metric calculations)\n  y_true = tf.cast(y_true, dtype=tf.float32)\n  y_pred = tf.cast(y_pred, dtype=tf.float32)\n\n  # Calculate various metrics\n  mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n  mse = tf.keras.metrics.mean_squared_error(y_true, y_pred)\n  rmse = tf.sqrt(mse)\n  mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n  mase = mean_absolute_scaled_error(y_true, y_pred)\n\n  # Account for different sized metrics (for longer horizons, reduce to single number)\n  if mae.ndim &gt; 0: # if mae isn't already a scalar, reduce it to one by aggregating tensors to mean\n    mae = tf.reduce_mean(mae)\n    mse = tf.reduce_mean(mse)\n    rmse = tf.reduce_mean(rmse)\n    mape = tf.reduce_mean(mape)\n    mase = tf.reduce_mean(mase)\n\n  return {\"mae\": mae.numpy(),\n          \"mse\": mse.numpy(),\n          \"rmse\": rmse.numpy(),\n          \"mape\": mape.numpy(),\n          \"mase\": mase.numpy()}\n</pre> def evaluate_preds(y_true, y_pred):   # Make sure float32 (for metric calculations)   y_true = tf.cast(y_true, dtype=tf.float32)   y_pred = tf.cast(y_pred, dtype=tf.float32)    # Calculate various metrics   mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)   mse = tf.keras.metrics.mean_squared_error(y_true, y_pred)   rmse = tf.sqrt(mse)   mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)   mase = mean_absolute_scaled_error(y_true, y_pred)    # Account for different sized metrics (for longer horizons, reduce to single number)   if mae.ndim &gt; 0: # if mae isn't already a scalar, reduce it to one by aggregating tensors to mean     mae = tf.reduce_mean(mae)     mse = tf.reduce_mean(mse)     rmse = tf.reduce_mean(rmse)     mape = tf.reduce_mean(mape)     mase = tf.reduce_mean(mase)    return {\"mae\": mae.numpy(),           \"mse\": mse.numpy(),           \"rmse\": rmse.numpy(),           \"mape\": mape.numpy(),           \"mase\": mase.numpy()} <p>Now we've updated <code>evaluate_preds()</code> to work with multiple shapes, how does it look?</p> In\u00a0[\u00a0]: Copied! <pre># Get model_3 results aggregated to single values\nmodel_3_results = evaluate_preds(y_true=tf.squeeze(test_labels),\n                                 y_pred=model_3_preds)\nmodel_3_results\n</pre> # Get model_3 results aggregated to single values model_3_results = evaluate_preds(y_true=tf.squeeze(test_labels),                                  y_pred=model_3_preds) model_3_results Out[\u00a0]: <pre>{'mae': 1237.5063,\n 'mape': 5.5588784,\n 'mase': 2.2020736,\n 'mse': 5405198.5,\n 'rmse': 1425.7477}</pre> <p>Time to visualize.</p> <p>If our prediction evaluation metrics were mutli-dimensional, how do you think the predictions will look like if we plot them?</p> In\u00a0[\u00a0]: Copied! <pre>offset = 300\nplt.figure(figsize=(10, 7))\nplot_time_series(timesteps=X_test[-len(test_windows):], values=test_labels[:, 0], start=offset, label=\"Test_data\")\n# Checking the shape of model_3_preds results in [n_test_samples, HORIZON] (this will screw up the plot)\nplot_time_series(timesteps=X_test[-len(test_windows):], values=model_3_preds, start=offset, label=\"model_3_preds\")\n</pre> offset = 300 plt.figure(figsize=(10, 7)) plot_time_series(timesteps=X_test[-len(test_windows):], values=test_labels[:, 0], start=offset, label=\"Test_data\") # Checking the shape of model_3_preds results in [n_test_samples, HORIZON] (this will screw up the plot) plot_time_series(timesteps=X_test[-len(test_windows):], values=model_3_preds, start=offset, label=\"model_3_preds\") <p>When we try to plot our multi-horizon predicts, we get a funky looking plot.</p> <p>Again, we can fix this by aggregating our model's predictions.</p> <p>\ud83d\udd11 Note: Aggregating the predictions (e.g. reducing a 7-day horizon to one value such as the mean) loses information from the original prediction. As in, the model predictions were trained to be made for 7-days but by reducing them to one, we gain the ability to plot them visually but we lose the extra information contained across multiple days.</p> In\u00a0[\u00a0]: Copied! <pre>offset = 300\nplt.figure(figsize=(10, 7))\n# Plot model_3_preds by aggregating them (note: this condenses information so the preds will look fruther ahead than the test data)\nplot_time_series(timesteps=X_test[-len(test_windows):], \n                 values=test_labels[:, 0], \n                 start=offset, \n                 label=\"Test_data\")\nplot_time_series(timesteps=X_test[-len(test_windows):], \n                 values=tf.reduce_mean(model_3_preds, axis=1), \n                 format=\"-\",\n                 start=offset, \n                 label=\"model_3_preds\")\n</pre> offset = 300 plt.figure(figsize=(10, 7)) # Plot model_3_preds by aggregating them (note: this condenses information so the preds will look fruther ahead than the test data) plot_time_series(timesteps=X_test[-len(test_windows):],                   values=test_labels[:, 0],                   start=offset,                   label=\"Test_data\") plot_time_series(timesteps=X_test[-len(test_windows):],                   values=tf.reduce_mean(model_3_preds, axis=1),                   format=\"-\",                  start=offset,                   label=\"model_3_preds\") In\u00a0[\u00a0]: Copied! <pre>pd.DataFrame({\"naive\": naive_results[\"mae\"], \n              \"horizon_1_window_7\": model_1_results[\"mae\"], \n              \"horizon_1_window_30\": model_2_results[\"mae\"], \n              \"horizon_7_window_30\": model_3_results[\"mae\"]}, index=[\"mae\"]).plot(figsize=(10, 7), kind=\"bar\");\n</pre> pd.DataFrame({\"naive\": naive_results[\"mae\"],                \"horizon_1_window_7\": model_1_results[\"mae\"],                \"horizon_1_window_30\": model_2_results[\"mae\"],                \"horizon_7_window_30\": model_3_results[\"mae\"]}, index=[\"mae\"]).plot(figsize=(10, 7), kind=\"bar\"); <p>Woah, our na\u00efve model is performing best (it's very hard to beat a na\u00efve model in open systems) but the dense model with a horizon of 1 and a window size of 7 looks to be performing cloest.</p> <p>Because of this, let's use <code>HORIZON=1</code> and <code>WINDOW_SIZE=7</code> for our next series of modelling experiments (in other words, we'll use the previous week of Bitcoin prices to try and predict the next day).</p> <p>\ud83d\udd11 Note: You might be wondering, why are the na\u00efve results so good? One of the reasons could be due the presence of autocorrelation in the data. If a time series has autocorrelation it means the value at <code>t+1</code> (the next timestep) is typically close to the value at <code>t</code> (the current timestep). In other words, today's value is probably pretty close to yesterday's value. Of course, this isn't always the case but when it is, a na\u00efve model will often get fairly good results.</p> <p>\ud83d\udcd6 Resource: For more on how autocorrelation influences a model's predictions, see the article How (not) to use Machine Learning for time series forecasting: Avoiding the pitfalls by Vegard Flovik</p> In\u00a0[\u00a0]: Copied! <pre>HORIZON = 1 # predict next day\nWINDOW_SIZE = 7 # use previous week worth of data\n</pre> HORIZON = 1 # predict next day WINDOW_SIZE = 7 # use previous week worth of data In\u00a0[\u00a0]: Copied! <pre># Create windowed dataset\nfull_windows, full_labels = make_windows(prices, window_size=WINDOW_SIZE, horizon=HORIZON)\nlen(full_windows), len(full_labels)\n</pre> # Create windowed dataset full_windows, full_labels = make_windows(prices, window_size=WINDOW_SIZE, horizon=HORIZON) len(full_windows), len(full_labels) Out[\u00a0]: <pre>(2780, 2780)</pre> In\u00a0[\u00a0]: Copied! <pre># Create train/test splits\ntrain_windows, test_windows, train_labels, test_labels = make_train_test_splits(full_windows, full_labels)\nlen(train_windows), len(test_windows), len(train_labels), len(test_labels)\n</pre> # Create train/test splits train_windows, test_windows, train_labels, test_labels = make_train_test_splits(full_windows, full_labels) len(train_windows), len(test_windows), len(train_labels), len(test_labels) Out[\u00a0]: <pre>(2224, 556, 2224, 556)</pre> <p>Data windowed!</p> <p>Now, since we're going to be using Conv1D layers, we need to make sure our input shapes are correct.</p> <p>The Conv1D layer in TensorFlow takes an input of: <code>(batch_size, timesteps, input_dim)</code>.</p> <p>In our case, the <code>batch_size</code> (by default this is 32 but we can change it) is handled for us but the other values will be:</p> <ul> <li><code>timesteps = WINDOW_SIZE</code> - the <code>timesteps</code> is also often referred to as <code>features</code>, our features are the previous <code>WINDOW_SIZE</code> values of Bitcoin</li> <li><code>input_dim = HORIZON</code> - our model views <code>WINDOW_SIZE</code> (one week) worth of data at a time to predict <code>HORIZON</code> (one day)</li> </ul> <p>Right now, our data has the <code>timesteps</code> dimension ready but we'll have to adjust it to have the <code>input_dim</code> dimension.</p> In\u00a0[\u00a0]: Copied! <pre># Check data sample shapes\ntrain_windows[0].shape # returns (WINDOW_SIZE, )\n</pre> # Check data sample shapes train_windows[0].shape # returns (WINDOW_SIZE, ) Out[\u00a0]: <pre>(7,)</pre> <p>To fix this, we could adjust the shape of all of our <code>train_windows</code> or we could use a <code>tf.keras.layers.Lamdba</code> (called a Lambda layer) to do this for us in our model.</p> <p>The Lambda layer wraps a function into a layer which can be used with a model.</p> <p>Let's try it out.</p> In\u00a0[\u00a0]: Copied! <pre># Before we pass our data to the Conv1D layer, we have to reshape it in order to make sure it works\nx = tf.constant(train_windows[0])\nexpand_dims_layer = layers.Lambda(lambda x: tf.expand_dims(x, axis=1)) # add an extra dimension for timesteps\nprint(f\"Original shape: {x.shape}\") # (WINDOW_SIZE)\nprint(f\"Expanded shape: {expand_dims_layer(x).shape}\") # (WINDOW_SIZE, input_dim) \nprint(f\"Original values with expanded shape:\\n {expand_dims_layer(x)}\")\n</pre> # Before we pass our data to the Conv1D layer, we have to reshape it in order to make sure it works x = tf.constant(train_windows[0]) expand_dims_layer = layers.Lambda(lambda x: tf.expand_dims(x, axis=1)) # add an extra dimension for timesteps print(f\"Original shape: {x.shape}\") # (WINDOW_SIZE) print(f\"Expanded shape: {expand_dims_layer(x).shape}\") # (WINDOW_SIZE, input_dim)  print(f\"Original values with expanded shape:\\n {expand_dims_layer(x)}\") <pre>Original shape: (7,)\nExpanded shape: (7, 1)\nOriginal values with expanded shape:\n [[123.65499]\n [125.455  ]\n [108.58483]\n [118.67466]\n [121.33866]\n [120.65533]\n [121.795  ]]\n</pre> <p>Looking good!</p> <p>Now we've got a Lambda layer, let's build, compile, fit and evaluate a Conv1D model on our data.</p> <p>\ud83d\udd11 Note: If you run the model below without the Lambda layer, you'll get an input shape error (one of the most common errors when building neural networks).</p> In\u00a0[\u00a0]: Copied! <pre>tf.random.set_seed(42)\n\n# Create model\nmodel_4 = tf.keras.Sequential([\n  # Create Lambda layer to reshape inputs, without this layer, the model will error\n  layers.Lambda(lambda x: tf.expand_dims(x, axis=1)), # resize the inputs to adjust for window size / Conv1D 3D input requirements\n  layers.Conv1D(filters=128, kernel_size=5, padding=\"causal\", activation=\"relu\"),\n  layers.Dense(HORIZON)\n], name=\"model_4_conv1D\")\n\n# Compile model\nmodel_4.compile(loss=\"mae\",\n                optimizer=tf.keras.optimizers.Adam())\n\n# Fit model\nmodel_4.fit(train_windows,\n            train_labels,\n            batch_size=128, \n            epochs=100,\n            verbose=0,\n            validation_data=(test_windows, test_labels),\n            callbacks=[create_model_checkpoint(model_name=model_4.name)])\n</pre> tf.random.set_seed(42)  # Create model model_4 = tf.keras.Sequential([   # Create Lambda layer to reshape inputs, without this layer, the model will error   layers.Lambda(lambda x: tf.expand_dims(x, axis=1)), # resize the inputs to adjust for window size / Conv1D 3D input requirements   layers.Conv1D(filters=128, kernel_size=5, padding=\"causal\", activation=\"relu\"),   layers.Dense(HORIZON) ], name=\"model_4_conv1D\")  # Compile model model_4.compile(loss=\"mae\",                 optimizer=tf.keras.optimizers.Adam())  # Fit model model_4.fit(train_windows,             train_labels,             batch_size=128,              epochs=100,             verbose=0,             validation_data=(test_windows, test_labels),             callbacks=[create_model_checkpoint(model_name=model_4.name)]) <pre>INFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\nINFO:tensorflow:Assets written to: model_experiments/model_4_conv1D/assets\n</pre> Out[\u00a0]: <pre>&lt;keras.callbacks.History at 0x7fdcf1dfba50&gt;</pre> <p>What does the Lambda layer look like in a summary?</p> In\u00a0[\u00a0]: Copied! <pre>model_4.summary()\n</pre> model_4.summary() <pre>Model: \"model_4_conv1D\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlambda_1 (Lambda)            (None, 1, 7)              0         \n_________________________________________________________________\nconv1d (Conv1D)              (None, 1, 128)            4608      \n_________________________________________________________________\ndense_6 (Dense)              (None, 1, 1)              129       \n=================================================================\nTotal params: 4,737\nTrainable params: 4,737\nNon-trainable params: 0\n_________________________________________________________________\n</pre> <p>The Lambda layer appears the same as any other regular layer.</p> <p>Time to evaluate the Conv1D model.</p> In\u00a0[\u00a0]: Copied! <pre># Load in best performing Conv1D model and evaluate it on the test data\nmodel_4 = tf.keras.models.load_model(\"model_experiments/model_4_conv1D\")\nmodel_4.evaluate(test_windows, test_labels)\n</pre> # Load in best performing Conv1D model and evaluate it on the test data model_4 = tf.keras.models.load_model(\"model_experiments/model_4_conv1D\") model_4.evaluate(test_windows, test_labels) <pre>18/18 [==============================] - 0s 3ms/step - loss: 570.8283\n</pre> Out[\u00a0]: <pre>570.8283081054688</pre> In\u00a0[\u00a0]: Copied! <pre># Make predictions\nmodel_4_preds = make_preds(model_4, test_windows)\nmodel_4_preds[:10]\n</pre> # Make predictions model_4_preds = make_preds(model_4, test_windows) model_4_preds[:10] Out[\u00a0]: <pre>&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=\narray([8851.464, 8754.471, 8983.928, 8759.672, 8703.627, 8708.295,\n       8661.667, 8494.839, 8435.317, 8492.115], dtype=float32)&gt;</pre> In\u00a0[\u00a0]: Copied! <pre># Evaluate predictions\nmodel_4_results = evaluate_preds(y_true=tf.squeeze(test_labels),\n                                 y_pred=model_4_preds)\nmodel_4_results\n</pre> # Evaluate predictions model_4_results = evaluate_preds(y_true=tf.squeeze(test_labels),                                  y_pred=model_4_preds) model_4_results Out[\u00a0]: <pre>{'mae': 570.8283,\n 'mape': 2.5593357,\n 'mase': 1.0027872,\n 'mse': 1176671.1,\n 'rmse': 1084.7448}</pre> In\u00a0[\u00a0]: Copied! <pre>tf.random.set_seed(42)\n\n# Let's build an LSTM model with the Functional API\ninputs = layers.Input(shape=(WINDOW_SIZE))\nx = layers.Lambda(lambda x: tf.expand_dims(x, axis=1))(inputs) # expand input dimension to be compatible with LSTM\n# print(x.shape)\n# x = layers.LSTM(128, activation=\"relu\", return_sequences=True)(x) # this layer will error if the inputs are not the right shape\nx = layers.LSTM(128, activation=\"relu\")(x) # using the tanh loss function results in a massive error\n# print(x.shape)\n# Add another optional dense layer (you could add more of these to see if they improve model performance)\n# x = layers.Dense(32, activation=\"relu\")(x)\noutput = layers.Dense(HORIZON)(x)\nmodel_5 = tf.keras.Model(inputs=inputs, outputs=output, name=\"model_5_lstm\")\n\n# Compile model\nmodel_5.compile(loss=\"mae\",\n                optimizer=tf.keras.optimizers.Adam())\n\n# Seems when saving the model several warnings are appearing: https://github.com/tensorflow/tensorflow/issues/47554 \nmodel_5.fit(train_windows,\n            train_labels,\n            epochs=100,\n            verbose=0,\n            batch_size=128,\n            validation_data=(test_windows, test_labels),\n            callbacks=[create_model_checkpoint(model_name=model_5.name)])\n</pre> tf.random.set_seed(42)  # Let's build an LSTM model with the Functional API inputs = layers.Input(shape=(WINDOW_SIZE)) x = layers.Lambda(lambda x: tf.expand_dims(x, axis=1))(inputs) # expand input dimension to be compatible with LSTM # print(x.shape) # x = layers.LSTM(128, activation=\"relu\", return_sequences=True)(x) # this layer will error if the inputs are not the right shape x = layers.LSTM(128, activation=\"relu\")(x) # using the tanh loss function results in a massive error # print(x.shape) # Add another optional dense layer (you could add more of these to see if they improve model performance) # x = layers.Dense(32, activation=\"relu\")(x) output = layers.Dense(HORIZON)(x) model_5 = tf.keras.Model(inputs=inputs, outputs=output, name=\"model_5_lstm\")  # Compile model model_5.compile(loss=\"mae\",                 optimizer=tf.keras.optimizers.Adam())  # Seems when saving the model several warnings are appearing: https://github.com/tensorflow/tensorflow/issues/47554  model_5.fit(train_windows,             train_labels,             epochs=100,             verbose=0,             batch_size=128,             validation_data=(test_windows, test_labels),             callbacks=[create_model_checkpoint(model_name=model_5.name)]) <pre>WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\nINFO:tensorflow:Assets written to: model_experiments/model_5_lstm/assets\n</pre> Out[\u00a0]: <pre>&lt;keras.callbacks.History at 0x7fdcf17b5190&gt;</pre> In\u00a0[\u00a0]: Copied! <pre># Load in best version of model 5 and evaluate on the test data\nmodel_5 = tf.keras.models.load_model(\"model_experiments/model_5_lstm/\")\nmodel_5.evaluate(test_windows, test_labels)\n</pre> # Load in best version of model 5 and evaluate on the test data model_5 = tf.keras.models.load_model(\"model_experiments/model_5_lstm/\") model_5.evaluate(test_windows, test_labels) <pre>WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n18/18 [==============================] - 0s 2ms/step - loss: 596.6447\n</pre> Out[\u00a0]: <pre>596.6446533203125</pre> <p>Now we've got the best performing LSTM model loaded in, let's make predictions with it and evaluate them.</p> In\u00a0[\u00a0]: Copied! <pre># Make predictions with our LSTM model\nmodel_5_preds = make_preds(model_5, test_windows)\nmodel_5_preds[:10]\n</pre> # Make predictions with our LSTM model model_5_preds = make_preds(model_5, test_windows) model_5_preds[:10] Out[\u00a0]: <pre>&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=\narray([8991.225, 8823.2  , 9009.359, 8847.859, 8742.254, 8788.655,\n       8744.746, 8552.568, 8514.823, 8542.873], dtype=float32)&gt;</pre> In\u00a0[\u00a0]: Copied! <pre># Evaluate model 5 preds\nmodel_5_results = evaluate_preds(y_true=tf.squeeze(test_labels),\n                                 y_pred=model_5_preds)\nmodel_5_results\n</pre> # Evaluate model 5 preds model_5_results = evaluate_preds(y_true=tf.squeeze(test_labels),                                  y_pred=model_5_preds) model_5_results Out[\u00a0]: <pre>{'mae': 596.64465,\n 'mape': 2.6838453,\n 'mase': 1.0481395,\n 'mse': 1273486.9,\n 'rmse': 1128.4888}</pre> <p>Hmmm... it seems even with an LSTM-powered RNN we weren't able to beat our na\u00efve models results.</p> <p>Perhaps adding another variable will help?</p> <p>\ud83d\udd11 Note: I'm putting this here again as a reminder that because neural networks are such powerful algorithms, they can be used for almost any problem, however, that doesn't mean they'll achieve performant or usable results. You're probably starting to clue onto this now.</p> In\u00a0[\u00a0]: Copied! <pre># Let's make a multivariate time series\nbitcoin_prices.head()\n</pre> # Let's make a multivariate time series bitcoin_prices.head() Out[\u00a0]: Price Date 2013-10-01 123.65499 2013-10-02 125.45500 2013-10-03 108.58483 2013-10-04 118.67466 2013-10-05 121.33866 <p>Alright, time to add another feature column, the block reward size.</p> <p>First, we'll need to create variables for the different block reward sizes as well as the dates they came into play.</p> <p>The following block rewards and dates were sourced from cmcmarkets.com.</p> Block Reward Start Date 50 3 January 2009 (2009-01-03) 25 28 November 2012 12.5 9 July 2016 6.25 11 May 2020 3.125 TBA (expected 2024) 1.5625 TBA (expected 2028) <p>\ud83d\udd11 Note: Since our Bitcoin historical data starts from 01 October 2013, none of the timesteps in our multivariate time series will have a block reward of 50.</p> In\u00a0[\u00a0]: Copied! <pre># Block reward values\nblock_reward_1 = 50 # 3 January 2009 (2009-01-03) - this block reward isn't in our dataset (it starts from 01 October 2013)\nblock_reward_2 = 25 # 28 November 2012 \nblock_reward_3 = 12.5 # 9 July 2016\nblock_reward_4 = 6.25 # 11 May 2020\n\n# Block reward dates (datetime form of the above date stamps)\nblock_reward_2_datetime = np.datetime64(\"2012-11-28\")\nblock_reward_3_datetime = np.datetime64(\"2016-07-09\")\nblock_reward_4_datetime = np.datetime64(\"2020-05-11\")\n</pre> # Block reward values block_reward_1 = 50 # 3 January 2009 (2009-01-03) - this block reward isn't in our dataset (it starts from 01 October 2013) block_reward_2 = 25 # 28 November 2012  block_reward_3 = 12.5 # 9 July 2016 block_reward_4 = 6.25 # 11 May 2020  # Block reward dates (datetime form of the above date stamps) block_reward_2_datetime = np.datetime64(\"2012-11-28\") block_reward_3_datetime = np.datetime64(\"2016-07-09\") block_reward_4_datetime = np.datetime64(\"2020-05-11\") <p>We're going to get the days (indexes) for different block reward values.</p> <p>This is important because if we're going to use multiple variables for our time series, they have to the same frequency as our original variable. For example, if our Bitcoin prices are daily, we need the block reward values to be daily as well.</p> <p>\ud83d\udd11 Note: For using multiple variables, make sure they're the same frequency as each other. If your variables aren't at the same frequency (e.g. Bitcoin prices are daily but block rewards are weekly), you may need to transform them in a way that they can be used with your model.</p> In\u00a0[\u00a0]: Copied! <pre># Get date indexes for when to add in different block dates\nblock_reward_2_days = (block_reward_3_datetime - bitcoin_prices.index[0]).days\nblock_reward_3_days = (block_reward_4_datetime - bitcoin_prices.index[0]).days\nblock_reward_2_days, block_reward_3_days\n</pre> # Get date indexes for when to add in different block dates block_reward_2_days = (block_reward_3_datetime - bitcoin_prices.index[0]).days block_reward_3_days = (block_reward_4_datetime - bitcoin_prices.index[0]).days block_reward_2_days, block_reward_3_days Out[\u00a0]: <pre>(1012, 2414)</pre> <p>Now we can add another feature to our dataset <code>block_reward</code> (this gets lower over time so it may lead to increasing prices of Bitcoin).</p> In\u00a0[\u00a0]: Copied! <pre># Add block_reward column\nbitcoin_prices_block = bitcoin_prices.copy()\nbitcoin_prices_block[\"block_reward\"] = None\n\n# Set values of block_reward column (it's the last column hence -1 indexing on iloc)\nbitcoin_prices_block.iloc[:block_reward_2_days, -1] = block_reward_2\nbitcoin_prices_block.iloc[block_reward_2_days:block_reward_3_days, -1] = block_reward_3\nbitcoin_prices_block.iloc[block_reward_3_days:, -1] = block_reward_4\nbitcoin_prices_block.head()\n</pre> # Add block_reward column bitcoin_prices_block = bitcoin_prices.copy() bitcoin_prices_block[\"block_reward\"] = None  # Set values of block_reward column (it's the last column hence -1 indexing on iloc) bitcoin_prices_block.iloc[:block_reward_2_days, -1] = block_reward_2 bitcoin_prices_block.iloc[block_reward_2_days:block_reward_3_days, -1] = block_reward_3 bitcoin_prices_block.iloc[block_reward_3_days:, -1] = block_reward_4 bitcoin_prices_block.head() Out[\u00a0]: Price block_reward Date 2013-10-01 123.65499 25 2013-10-02 125.45500 25 2013-10-03 108.58483 25 2013-10-04 118.67466 25 2013-10-05 121.33866 25 <p>Woohoo! We've officially added another variable to our time series data.</p> <p>Let's see what it looks like.</p> In\u00a0[\u00a0]: Copied! <pre># Plot the block reward/price over time\n# Note: Because of the different scales of our values we'll scale them to be between 0 and 1.\nfrom sklearn.preprocessing import minmax_scale\nscaled_price_block_df = pd.DataFrame(minmax_scale(bitcoin_prices_block[[\"Price\", \"block_reward\"]]), # we need to scale the data first\n                                     columns=bitcoin_prices_block.columns,\n                                     index=bitcoin_prices_block.index)\nscaled_price_block_df.plot(figsize=(10, 7));\n</pre> # Plot the block reward/price over time # Note: Because of the different scales of our values we'll scale them to be between 0 and 1. from sklearn.preprocessing import minmax_scale scaled_price_block_df = pd.DataFrame(minmax_scale(bitcoin_prices_block[[\"Price\", \"block_reward\"]]), # we need to scale the data first                                      columns=bitcoin_prices_block.columns,                                      index=bitcoin_prices_block.index) scaled_price_block_df.plot(figsize=(10, 7)); <p>When we scale the block reward and the Bitcoin price, we can see the price goes up as the block reward goes down, perhaps this information will be helpful to our model's performance.</p> In\u00a0[\u00a0]: Copied! <pre># Setup dataset hyperparameters\nHORIZON = 1\nWINDOW_SIZE = 7\n</pre> # Setup dataset hyperparameters HORIZON = 1 WINDOW_SIZE = 7 In\u00a0[\u00a0]: Copied! <pre># Make a copy of the Bitcoin historical data with block reward feature\nbitcoin_prices_windowed = bitcoin_prices_block.copy()\n\n# Add windowed columns\nfor i in range(WINDOW_SIZE): # Shift values for each step in WINDOW_SIZE\n  bitcoin_prices_windowed[f\"Price+{i+1}\"] = bitcoin_prices_windowed[\"Price\"].shift(periods=i+1)\nbitcoin_prices_windowed.head(10)\n</pre> # Make a copy of the Bitcoin historical data with block reward feature bitcoin_prices_windowed = bitcoin_prices_block.copy()  # Add windowed columns for i in range(WINDOW_SIZE): # Shift values for each step in WINDOW_SIZE   bitcoin_prices_windowed[f\"Price+{i+1}\"] = bitcoin_prices_windowed[\"Price\"].shift(periods=i+1) bitcoin_prices_windowed.head(10) Out[\u00a0]: Price block_reward Price+1 Price+2 Price+3 Price+4 Price+5 Price+6 Price+7 Date 2013-10-01 123.65499 25 NaN NaN NaN NaN NaN NaN NaN 2013-10-02 125.45500 25 123.65499 NaN NaN NaN NaN NaN NaN 2013-10-03 108.58483 25 125.45500 123.65499 NaN NaN NaN NaN NaN 2013-10-04 118.67466 25 108.58483 125.45500 123.65499 NaN NaN NaN NaN 2013-10-05 121.33866 25 118.67466 108.58483 125.45500 123.65499 NaN NaN NaN 2013-10-06 120.65533 25 121.33866 118.67466 108.58483 125.45500 123.65499 NaN NaN 2013-10-07 121.79500 25 120.65533 121.33866 118.67466 108.58483 125.45500 123.65499 NaN 2013-10-08 123.03300 25 121.79500 120.65533 121.33866 118.67466 108.58483 125.45500 123.65499 2013-10-09 124.04900 25 123.03300 121.79500 120.65533 121.33866 118.67466 108.58483 125.45500 2013-10-10 125.96116 25 124.04900 123.03300 121.79500 120.65533 121.33866 118.67466 108.58483 <p>Now that we've got a windowed dataset, let's separate features (<code>X</code>) from labels (<code>y</code>).</p> <p>Remember in our windowed dataset, we're trying to use the previous <code>WINDOW_SIZE</code> steps to predict <code>HORIZON</code> steps.</p> <pre><code>Window for a week (7) to predict a horizon of 1 (multivariate time series)\nWINDOW_SIZE &amp; block_reward -&gt; HORIZON\n\n[0, 1, 2, 3, 4, 5, 6, block_reward] -&gt; [7]\n[1, 2, 3, 4, 5, 6, 7, block_reward] -&gt; [8]\n[2, 3, 4, 5, 6, 7, 8, block_reward] -&gt; [9]\n</code></pre> <p>We'll also remove the <code>NaN</code> values using pandas <code>dropna()</code> method, this equivalent to starting our windowing function at <code>sample 0 (the first sample) + WINDOW_SIZE</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Let's create X &amp; y, remove the NaN's and convert to float32 to prevent TensorFlow errors \nX = bitcoin_prices_windowed.dropna().drop(\"Price\", axis=1).astype(np.float32) \ny = bitcoin_prices_windowed.dropna()[\"Price\"].astype(np.float32)\nX.head()\n</pre> # Let's create X &amp; y, remove the NaN's and convert to float32 to prevent TensorFlow errors  X = bitcoin_prices_windowed.dropna().drop(\"Price\", axis=1).astype(np.float32)  y = bitcoin_prices_windowed.dropna()[\"Price\"].astype(np.float32) X.head() Out[\u00a0]: block_reward Price+1 Price+2 Price+3 Price+4 Price+5 Price+6 Price+7 Date 2013-10-08 25.0 121.794998 120.655327 121.338661 118.674660 108.584831 125.455002 123.654991 2013-10-09 25.0 123.032997 121.794998 120.655327 121.338661 118.674660 108.584831 125.455002 2013-10-10 25.0 124.049004 123.032997 121.794998 120.655327 121.338661 118.674660 108.584831 2013-10-11 25.0 125.961159 124.049004 123.032997 121.794998 120.655327 121.338661 118.674660 2013-10-12 25.0 125.279663 125.961159 124.049004 123.032997 121.794998 120.655327 121.338661 In\u00a0[\u00a0]: Copied! <pre># View labels\ny.head()\n</pre> # View labels y.head() Out[\u00a0]: <pre>Date\n2013-10-08    123.032997\n2013-10-09    124.049004\n2013-10-10    125.961159\n2013-10-11    125.279663\n2013-10-12    125.927498\nName: Price, dtype: float32</pre> <p>What a good looking dataset, let's split it into train and test sets using an 80/20 split just as we've done before.</p> In\u00a0[\u00a0]: Copied! <pre># Make train and test sets\nsplit_size = int(len(X) * 0.8)\nX_train, y_train = X[:split_size], y[:split_size]\nX_test, y_test = X[split_size:], y[split_size:]\nlen(X_train), len(y_train), len(X_test), len(y_test)\n</pre> # Make train and test sets split_size = int(len(X) * 0.8) X_train, y_train = X[:split_size], y[:split_size] X_test, y_test = X[split_size:], y[split_size:] len(X_train), len(y_train), len(X_test), len(y_test) Out[\u00a0]: <pre>(2224, 2224, 556, 556)</pre> <p>Training and test multivariate time series datasets made! Time to build a model.</p> In\u00a0[\u00a0]: Copied! <pre>tf.random.set_seed(42)\n\n# Make multivariate time series model\nmodel_6 = tf.keras.Sequential([\n  layers.Dense(128, activation=\"relu\"),\n  # layers.Dense(128, activation=\"relu\"), # adding an extra layer here should lead to beating the naive model\n  layers.Dense(HORIZON)\n], name=\"model_6_dense_multivariate\")\n\n# Compile\nmodel_6.compile(loss=\"mae\",\n                optimizer=tf.keras.optimizers.Adam())\n\n# Fit\nmodel_6.fit(X_train, y_train,\n            epochs=100,\n            batch_size=128,\n            verbose=0, # only print 1 line per epoch\n            validation_data=(X_test, y_test),\n            callbacks=[create_model_checkpoint(model_name=model_6.name)])\n</pre> tf.random.set_seed(42)  # Make multivariate time series model model_6 = tf.keras.Sequential([   layers.Dense(128, activation=\"relu\"),   # layers.Dense(128, activation=\"relu\"), # adding an extra layer here should lead to beating the naive model   layers.Dense(HORIZON) ], name=\"model_6_dense_multivariate\")  # Compile model_6.compile(loss=\"mae\",                 optimizer=tf.keras.optimizers.Adam())  # Fit model_6.fit(X_train, y_train,             epochs=100,             batch_size=128,             verbose=0, # only print 1 line per epoch             validation_data=(X_test, y_test),             callbacks=[create_model_checkpoint(model_name=model_6.name)]) <pre>INFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\nINFO:tensorflow:Assets written to: model_experiments/model_6_dense_multivariate/assets\n</pre> Out[\u00a0]: <pre>&lt;keras.callbacks.History at 0x7fdceed05590&gt;</pre> <p>Multivariate model fit!</p> <p>You might've noticed that the model inferred the input shape of our data automatically (the data now has an extra feature). Often this will be the case, however, if you're running into shape issues, you can always explicitly define the input shape using <code>input_shape</code> parameter of the first layer in a model.</p> <p>Time to evaluate our multivariate model.</p> In\u00a0[\u00a0]: Copied! <pre># Make sure best model is loaded and evaluate\nmodel_6 = tf.keras.models.load_model(\"model_experiments/model_6_dense_multivariate\")\nmodel_6.evaluate(X_test, y_test)\n</pre> # Make sure best model is loaded and evaluate model_6 = tf.keras.models.load_model(\"model_experiments/model_6_dense_multivariate\") model_6.evaluate(X_test, y_test) <pre>18/18 [==============================] - 0s 2ms/step - loss: 567.5873\n</pre> Out[\u00a0]: <pre>567.5873413085938</pre> In\u00a0[\u00a0]: Copied! <pre># Make predictions on multivariate data\nmodel_6_preds = tf.squeeze(model_6.predict(X_test))\nmodel_6_preds[:10]\n</pre> # Make predictions on multivariate data model_6_preds = tf.squeeze(model_6.predict(X_test)) model_6_preds[:10] Out[\u00a0]: <pre>&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=\narray([8836.276, 8763.8  , 9040.486, 8741.225, 8719.326, 8765.071,\n       8661.102, 8496.891, 8463.231, 8521.585], dtype=float32)&gt;</pre> In\u00a0[\u00a0]: Copied! <pre># Evaluate preds\nmodel_6_results = evaluate_preds(y_true=y_test,\n                                 y_pred=model_6_preds)\nmodel_6_results\n</pre> # Evaluate preds model_6_results = evaluate_preds(y_true=y_test,                                  y_pred=model_6_preds) model_6_results Out[\u00a0]: <pre>{'mae': 567.5874,\n 'mape': 2.541387,\n 'mase': 0.99709386,\n 'mse': 1161688.4,\n 'rmse': 1077.8165}</pre> <p>Hmmm... how do these results compare to <code>model_1</code> (same window size and horizon but without the block reward feature)?</p> In\u00a0[\u00a0]: Copied! <pre>model_1_results\n</pre> model_1_results Out[\u00a0]: <pre>{'mae': 568.95123,\n 'mape': 2.5448983,\n 'mase': 0.9994897,\n 'mse': 1171744.0,\n 'rmse': 1082.4713}</pre> <p>It looks like the adding in the block reward may have helped our model slightly.</p> <p>But there a few more things we could try.</p> <p>\ud83d\udcd6 Resource: For different ideas on how to improve a neural network model (from a model perspective), refer to the Improving a model section in notebook 02.</p> <p>\ud83d\udee0 Exercise(s):</p> <ol> <li>Try adding an extra <code>tf.keras.layers.Dense()</code> layer with 128 hidden units to <code>model_6</code>, how does this effect model performance?</li> <li>Is there a better way to create this model? As in, should the <code>block_reward</code> feature be bundled in with the Bitcoin historical price feature? Perhaps you could test whether building a multi-input model (e.g. one model input for Bitcoin price history and one model input for <code>block_reward</code>)  works better? See Model 4: Hybrid embedding section of notebook 09 for an idea on how to create a multi-input model.</li> </ol> In\u00a0[\u00a0]: Copied! <pre># Create NBeatsBlock custom layer \nclass NBeatsBlock(tf.keras.layers.Layer):\n  def __init__(self, # the constructor takes all the hyperparameters for the layer\n               input_size: int,\n               theta_size: int,\n               horizon: int,\n               n_neurons: int,\n               n_layers: int,\n               **kwargs): # the **kwargs argument takes care of all of the arguments for the parent class (input_shape, trainable, name)\n    super().__init__(**kwargs)\n    self.input_size = input_size\n    self.theta_size = theta_size\n    self.horizon = horizon\n    self.n_neurons = n_neurons\n    self.n_layers = n_layers\n\n    # Block contains stack of 4 fully connected layers each has ReLU activation\n    self.hidden = [tf.keras.layers.Dense(n_neurons, activation=\"relu\") for _ in range(n_layers)]\n    # Output of block is a theta layer with linear activation\n    self.theta_layer = tf.keras.layers.Dense(theta_size, activation=\"linear\", name=\"theta\")\n\n  def call(self, inputs): # the call method is what runs when the layer is called \n    x = inputs \n    for layer in self.hidden: # pass inputs through each hidden layer \n      x = layer(x)\n    theta = self.theta_layer(x) \n    # Output the backcast and forecast from theta\n    backcast, forecast = theta[:, :self.input_size], theta[:, -self.horizon:]\n    return backcast, forecast\n</pre> # Create NBeatsBlock custom layer  class NBeatsBlock(tf.keras.layers.Layer):   def __init__(self, # the constructor takes all the hyperparameters for the layer                input_size: int,                theta_size: int,                horizon: int,                n_neurons: int,                n_layers: int,                **kwargs): # the **kwargs argument takes care of all of the arguments for the parent class (input_shape, trainable, name)     super().__init__(**kwargs)     self.input_size = input_size     self.theta_size = theta_size     self.horizon = horizon     self.n_neurons = n_neurons     self.n_layers = n_layers      # Block contains stack of 4 fully connected layers each has ReLU activation     self.hidden = [tf.keras.layers.Dense(n_neurons, activation=\"relu\") for _ in range(n_layers)]     # Output of block is a theta layer with linear activation     self.theta_layer = tf.keras.layers.Dense(theta_size, activation=\"linear\", name=\"theta\")    def call(self, inputs): # the call method is what runs when the layer is called      x = inputs      for layer in self.hidden: # pass inputs through each hidden layer        x = layer(x)     theta = self.theta_layer(x)      # Output the backcast and forecast from theta     backcast, forecast = theta[:, :self.input_size], theta[:, -self.horizon:]     return backcast, forecast <p>Setting up the <code>NBeatsBlock</code> custom layer we see:</p> <ul> <li>The class inherits from <code>tf.keras.layers.Layer</code> (this gives it all of the methods assosciated with <code>tf.keras.layers.Layer</code>)</li> <li>The constructor (<code>def __init__(...)</code>) takes all of the layer hyperparameters as well as the <code>**kwargs</code> argument<ul> <li>The <code>**kwargs</code> argument takes care of all of the hyperparameters which aren't mentioned in the constructor such as, <code>input_shape</code>, <code>trainable</code> and <code>name</code></li> </ul> </li> <li>In the constructor, the block architecture layers are created:<ul> <li>The hidden layers are created as a stack of fully connected with <code>n_nuerons</code> hidden units layers with ReLU activation</li> <li>The theta layer uses <code>theta_size</code> hidden units as well as linear activation</li> </ul> </li> <li>The <code>call()</code> method is what is run when the layer is called:<ul> <li>It first passes the inputs (the historical Bitcoin data) through each of the hidden layers (a stack of fully connected layers with ReLU activation)</li> <li>After the inputs have been through each of the fully connected layers, they get passed through the theta layer where the backcast (backwards predictions, shape: <code>input_size</code>) and forecast (forward predictions, shape: <code>horizon</code>) are returned</li> </ul> </li> </ul> <p> Using TensorFlow layer subclassing to replicate the N-BEATS basic block. See section 3.1 of N-BEATS paper for details.</p> <p>Let's see our block replica in action by together by creating a toy version of <code>NBeatsBlock</code>.</p> <p>\ud83d\udcd6  Resource: Much of the creation of the time series materials (the ones you're going through now), including replicating the N-BEATS algorithm were streamed live on Twitch. If you'd like to see replays of how the algorithm was replicated, check out the Time series research and TensorFlow course material creation playlist on the Daniel Bourke arXiv YouTube channel.</p> In\u00a0[\u00a0]: Copied! <pre># Set up dummy NBeatsBlock layer to represent inputs and outputs\ndummy_nbeats_block_layer = NBeatsBlock(input_size=WINDOW_SIZE, \n                                       theta_size=WINDOW_SIZE+HORIZON, # backcast + forecast \n                                       horizon=HORIZON,\n                                       n_neurons=128,\n                                       n_layers=4)\n</pre> # Set up dummy NBeatsBlock layer to represent inputs and outputs dummy_nbeats_block_layer = NBeatsBlock(input_size=WINDOW_SIZE,                                         theta_size=WINDOW_SIZE+HORIZON, # backcast + forecast                                         horizon=HORIZON,                                        n_neurons=128,                                        n_layers=4) In\u00a0[\u00a0]: Copied! <pre># Create dummy inputs (have to be same size as input_size)\ndummy_inputs = tf.expand_dims(tf.range(WINDOW_SIZE) + 1, axis=0) # input shape to the model has to reflect Dense layer input requirements (ndim=2)\ndummy_inputs\n</pre> # Create dummy inputs (have to be same size as input_size) dummy_inputs = tf.expand_dims(tf.range(WINDOW_SIZE) + 1, axis=0) # input shape to the model has to reflect Dense layer input requirements (ndim=2) dummy_inputs Out[\u00a0]: <pre>&lt;tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[1, 2, 3, 4, 5, 6, 7]], dtype=int32)&gt;</pre> In\u00a0[\u00a0]: Copied! <pre># Pass dummy inputs to dummy NBeatsBlock layer\nbackcast, forecast = dummy_nbeats_block_layer(dummy_inputs)\n# These are the activation outputs of the theta layer (they'll be random due to no training of the model)\nprint(f\"Backcast: {tf.squeeze(backcast.numpy())}\")\nprint(f\"Forecast: {tf.squeeze(forecast.numpy())}\")\n</pre> # Pass dummy inputs to dummy NBeatsBlock layer backcast, forecast = dummy_nbeats_block_layer(dummy_inputs) # These are the activation outputs of the theta layer (they'll be random due to no training of the model) print(f\"Backcast: {tf.squeeze(backcast.numpy())}\") print(f\"Forecast: {tf.squeeze(forecast.numpy())}\") <pre>Backcast: [ 0.19014978  0.83798355 -0.32870018  0.25159916 -0.47540277 -0.77836645\n -0.5299447 ]\nForecast: -0.7554212808609009\n</pre> In\u00a0[\u00a0]: Copied! <pre>HORIZON = 1 # how far to predict forward\nWINDOW_SIZE = 7 # how far to lookback\n</pre> HORIZON = 1 # how far to predict forward WINDOW_SIZE = 7 # how far to lookback In\u00a0[\u00a0]: Copied! <pre># Create NBEATS data inputs (NBEATS works with univariate time series)\nbitcoin_prices.head()\n</pre> # Create NBEATS data inputs (NBEATS works with univariate time series) bitcoin_prices.head() Out[\u00a0]: Price Date 2013-10-01 123.65499 2013-10-02 125.45500 2013-10-03 108.58483 2013-10-04 118.67466 2013-10-05 121.33866 In\u00a0[\u00a0]: Copied! <pre># Add windowed columns\nbitcoin_prices_nbeats = bitcoin_prices.copy()\nfor i in range(WINDOW_SIZE):\n  bitcoin_prices_nbeats[f\"Price+{i+1}\"] = bitcoin_prices_nbeats[\"Price\"].shift(periods=i+1)\nbitcoin_prices_nbeats.dropna().head()\n</pre> # Add windowed columns bitcoin_prices_nbeats = bitcoin_prices.copy() for i in range(WINDOW_SIZE):   bitcoin_prices_nbeats[f\"Price+{i+1}\"] = bitcoin_prices_nbeats[\"Price\"].shift(periods=i+1) bitcoin_prices_nbeats.dropna().head() Out[\u00a0]: Price Price+1 Price+2 Price+3 Price+4 Price+5 Price+6 Price+7 Date 2013-10-08 123.03300 121.79500 120.65533 121.33866 118.67466 108.58483 125.45500 123.65499 2013-10-09 124.04900 123.03300 121.79500 120.65533 121.33866 118.67466 108.58483 125.45500 2013-10-10 125.96116 124.04900 123.03300 121.79500 120.65533 121.33866 118.67466 108.58483 2013-10-11 125.27966 125.96116 124.04900 123.03300 121.79500 120.65533 121.33866 118.67466 2013-10-12 125.92750 125.27966 125.96116 124.04900 123.03300 121.79500 120.65533 121.33866 In\u00a0[\u00a0]: Copied! <pre># Make features and labels\nX = bitcoin_prices_nbeats.dropna().drop(\"Price\", axis=1)\ny = bitcoin_prices_nbeats.dropna()[\"Price\"]\n\n# Make train and test sets\nsplit_size = int(len(X) * 0.8)\nX_train, y_train = X[:split_size], y[:split_size]\nX_test, y_test = X[split_size:], y[split_size:]\nlen(X_train), len(y_train), len(X_test), len(y_test)\n</pre> # Make features and labels X = bitcoin_prices_nbeats.dropna().drop(\"Price\", axis=1) y = bitcoin_prices_nbeats.dropna()[\"Price\"]  # Make train and test sets split_size = int(len(X) * 0.8) X_train, y_train = X[:split_size], y[:split_size] X_test, y_test = X[split_size:], y[split_size:] len(X_train), len(y_train), len(X_test), len(y_test) Out[\u00a0]: <pre>(2224, 2224, 556, 556)</pre> <p>Train and test sets ready to go!</p> <p>Now let's convert them into TensorFlow <code>tf.data.Dataset</code>'s to ensure they run as fast as possible whilst training.</p> <p>We'll do this by:</p> <ol> <li>Turning the arrays in tensor Datasets using <code>tf.data.Dataset.from_tensor_slices()</code></li> </ol> <ul> <li>Note: <code>from_tensor_slices()</code> works best when your data fits in memory, for extremely large datasets, you'll want to look into using the <code>TFRecord</code> format</li> </ul> <ol> <li>Combine the labels and features tensors into a Dataset using <code>tf.data.Dataset.zip()</code></li> <li>Batch and prefetch the Datasets using <code>batch()</code> and <code>prefetch()</code></li> </ol> <ul> <li>Batching and prefetching ensures the loading time from CPU (preparing data) to GPU (computing on data) is as small as possible</li> </ul> <p>\ud83d\udcd6 Resource: For more on building highly performant TensorFlow data pipelines, I'd recommend reading through the Better performance with the tf.data API guide.</p> In\u00a0[\u00a0]: Copied! <pre># 1. Turn train and test arrays into tensor Datasets\ntrain_features_dataset = tf.data.Dataset.from_tensor_slices(X_train)\ntrain_labels_dataset = tf.data.Dataset.from_tensor_slices(y_train)\n\ntest_features_dataset = tf.data.Dataset.from_tensor_slices(X_test)\ntest_labels_dataset = tf.data.Dataset.from_tensor_slices(y_test)\n\n# 2. Combine features &amp; labels\ntrain_dataset = tf.data.Dataset.zip((train_features_dataset, train_labels_dataset))\ntest_dataset = tf.data.Dataset.zip((test_features_dataset, test_labels_dataset))\n\n# 3. Batch and prefetch for optimal performance\nBATCH_SIZE = 1024 # taken from Appendix D in N-BEATS paper\ntrain_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\ntest_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\ntrain_dataset, test_dataset\n</pre> # 1. Turn train and test arrays into tensor Datasets train_features_dataset = tf.data.Dataset.from_tensor_slices(X_train) train_labels_dataset = tf.data.Dataset.from_tensor_slices(y_train)  test_features_dataset = tf.data.Dataset.from_tensor_slices(X_test) test_labels_dataset = tf.data.Dataset.from_tensor_slices(y_test)  # 2. Combine features &amp; labels train_dataset = tf.data.Dataset.zip((train_features_dataset, train_labels_dataset)) test_dataset = tf.data.Dataset.zip((test_features_dataset, test_labels_dataset))  # 3. Batch and prefetch for optimal performance BATCH_SIZE = 1024 # taken from Appendix D in N-BEATS paper train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE) test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)  train_dataset, test_dataset Out[\u00a0]: <pre>(&lt;PrefetchDataset shapes: ((None, 7), (None,)), types: (tf.float64, tf.float64)&gt;,\n &lt;PrefetchDataset shapes: ((None, 7), (None,)), types: (tf.float64, tf.float64)&gt;)</pre> <p>Data prepared! Notice the input shape for the features <code>(None, 7)</code>, the <code>None</code> leaves space for the batch size where as the <code>7</code> represents the <code>WINDOW_SIZE</code>.</p> <p>Time to get create the N-BEATS architecture.</p> In\u00a0[\u00a0]: Copied! <pre># Values from N-BEATS paper Figure 1 and Table 18/Appendix D\nN_EPOCHS = 5000 # called \"Iterations\" in Table 18\nN_NEURONS = 512 # called \"Width\" in Table 18\nN_LAYERS = 4\nN_STACKS = 30\n\nINPUT_SIZE = WINDOW_SIZE * HORIZON # called \"Lookback\" in Table 18\nTHETA_SIZE = INPUT_SIZE + HORIZON\n\nINPUT_SIZE, THETA_SIZE\n</pre> # Values from N-BEATS paper Figure 1 and Table 18/Appendix D N_EPOCHS = 5000 # called \"Iterations\" in Table 18 N_NEURONS = 512 # called \"Width\" in Table 18 N_LAYERS = 4 N_STACKS = 30  INPUT_SIZE = WINDOW_SIZE * HORIZON # called \"Lookback\" in Table 18 THETA_SIZE = INPUT_SIZE + HORIZON  INPUT_SIZE, THETA_SIZE Out[\u00a0]: <pre>(7, 8)</pre> In\u00a0[\u00a0]: Copied! <pre># Make tensors\ntensor_1 = tf.range(10) + 10\ntensor_2 = tf.range(10)\n\n# Subtract\nsubtracted = layers.subtract([tensor_1, tensor_2])\n\n# Add\nadded = layers.add([tensor_1, tensor_2])\n\nprint(f\"Input tensors: {tensor_1.numpy()} &amp; {tensor_2.numpy()}\")\nprint(f\"Subtracted: {subtracted.numpy()}\")\nprint(f\"Added: {added.numpy()}\")\n</pre> # Make tensors tensor_1 = tf.range(10) + 10 tensor_2 = tf.range(10)  # Subtract subtracted = layers.subtract([tensor_1, tensor_2])  # Add added = layers.add([tensor_1, tensor_2])  print(f\"Input tensors: {tensor_1.numpy()} &amp; {tensor_2.numpy()}\") print(f\"Subtracted: {subtracted.numpy()}\") print(f\"Added: {added.numpy()}\") <pre>Input tensors: [10 11 12 13 14 15 16 17 18 19] &amp; [0 1 2 3 4 5 6 7 8 9]\nSubtracted: [10 10 10 10 10 10 10 10 10 10]\nAdded: [10 12 14 16 18 20 22 24 26 28]\n</pre> <p>Both of these layer functions are straight-forward, subtract or add together their inputs.</p> <p>And as mentioned before, they're what powers N-BEATS double residual stacking.</p> <p>The power of residual stacking or residual connections was revealed in Deep Residual Learning for Image Recognition where the authors were able to build a deeper but less complex neural network (this is what introduced the popular ResNet architecture) than previous attempts.</p> <p>This deeper neural network led to state of the art results on the ImageNet challenge in 2015 and different versions of residual connections have been present in deep learning ever since.</p> <p>What is a residual connection?</p> <p>A residual connection (also called skip connections) involves a deeper neural network layer receiving the outputs as well as the inputs of a shallower neural network layer.</p> <p>In the case of N-BEATS, the architecture uses residual connections which:</p> <ul> <li>Subtract the backcast outputs from a previous block from the backcast inputs to the current block</li> <li>Add the forecast outputs from all blocks together in a stack</li> </ul> <p> Annotated version of Figure 1 from the N-BEATS paper highlighting the double residual stacking (section 3.2) of the architecture. Backcast residuals of each block are subtracted from each other and used as the input to the next block where as the forecasts of each block are added together to become the stack forecast.</p> <p>What are the benefits of residual connections?</p> <p>In practice, residual connections have been beneficial for training deeper models (N-BEATS reaches ~150 layers, also see \"These approaches provide clear advantages in improving the trainability of deep architectures\" in section 3.2 of the N-BEATS paper).</p> <p>It's thought that they help avoid the problem of vanishing gradients (patterns learned by a neural network not being passed through to deeper layers).</p> In\u00a0[\u00a0]: Copied! <pre>%%time\n\ntf.random.set_seed(42)\n\n# 1. Setup N-BEATS Block layer\nnbeats_block_layer = NBeatsBlock(input_size=INPUT_SIZE,\n                                 theta_size=THETA_SIZE,\n                                 horizon=HORIZON,\n                                 n_neurons=N_NEURONS,\n                                 n_layers=N_LAYERS,\n                                 name=\"InitialBlock\")\n\n# 2. Create input to stacks\nstack_input = layers.Input(shape=(INPUT_SIZE), name=\"stack_input\")\n\n# 3. Create initial backcast and forecast input (backwards predictions are referred to as residuals in the paper)\nbackcast, forecast = nbeats_block_layer(stack_input)\n# Add in subtraction residual link, thank you to: https://github.com/mrdbourke/tensorflow-deep-learning/discussions/174 \nresiduals = layers.subtract([stack_input, backcast], name=f\"subtract_00\") \n\n# 4. Create stacks of blocks\nfor i, _ in enumerate(range(N_STACKS-1)): # first stack is already creted in (3)\n\n  # 5. Use the NBeatsBlock to calculate the backcast as well as block forecast\n  backcast, block_forecast = NBeatsBlock(\n      input_size=INPUT_SIZE,\n      theta_size=THETA_SIZE,\n      horizon=HORIZON,\n      n_neurons=N_NEURONS,\n      n_layers=N_LAYERS,\n      name=f\"NBeatsBlock_{i}\"\n  )(residuals) # pass it in residuals (the backcast)\n\n  # 6. Create the double residual stacking\n  residuals = layers.subtract([residuals, backcast], name=f\"subtract_{i}\") \n  forecast = layers.add([forecast, block_forecast], name=f\"add_{i}\")\n\n# 7. Put the stack model together\nmodel_7 = tf.keras.Model(inputs=stack_input, \n                         outputs=forecast, \n                         name=\"model_7_N-BEATS\")\n\n# 8. Compile with MAE loss and Adam optimizer\nmodel_7.compile(loss=\"mae\",\n                optimizer=tf.keras.optimizers.Adam(0.001),\n                metrics=[\"mae\", \"mse\"])\n\n# 9. Fit the model with EarlyStopping and ReduceLROnPlateau callbacks\nmodel_7.fit(train_dataset,\n            epochs=N_EPOCHS,\n            validation_data=test_dataset,\n            verbose=0, # prevent large amounts of training outputs\n            # callbacks=[create_model_checkpoint(model_name=stack_model.name)] # saving model every epoch consumes far too much time\n            callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=200, restore_best_weights=True),\n                      tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=100, verbose=1)])\n</pre>  %%time  tf.random.set_seed(42)  # 1. Setup N-BEATS Block layer nbeats_block_layer = NBeatsBlock(input_size=INPUT_SIZE,                                  theta_size=THETA_SIZE,                                  horizon=HORIZON,                                  n_neurons=N_NEURONS,                                  n_layers=N_LAYERS,                                  name=\"InitialBlock\")  # 2. Create input to stacks stack_input = layers.Input(shape=(INPUT_SIZE), name=\"stack_input\")  # 3. Create initial backcast and forecast input (backwards predictions are referred to as residuals in the paper) backcast, forecast = nbeats_block_layer(stack_input) # Add in subtraction residual link, thank you to: https://github.com/mrdbourke/tensorflow-deep-learning/discussions/174  residuals = layers.subtract([stack_input, backcast], name=f\"subtract_00\")   # 4. Create stacks of blocks for i, _ in enumerate(range(N_STACKS-1)): # first stack is already creted in (3)    # 5. Use the NBeatsBlock to calculate the backcast as well as block forecast   backcast, block_forecast = NBeatsBlock(       input_size=INPUT_SIZE,       theta_size=THETA_SIZE,       horizon=HORIZON,       n_neurons=N_NEURONS,       n_layers=N_LAYERS,       name=f\"NBeatsBlock_{i}\"   )(residuals) # pass it in residuals (the backcast)    # 6. Create the double residual stacking   residuals = layers.subtract([residuals, backcast], name=f\"subtract_{i}\")    forecast = layers.add([forecast, block_forecast], name=f\"add_{i}\")  # 7. Put the stack model together model_7 = tf.keras.Model(inputs=stack_input,                           outputs=forecast,                           name=\"model_7_N-BEATS\")  # 8. Compile with MAE loss and Adam optimizer model_7.compile(loss=\"mae\",                 optimizer=tf.keras.optimizers.Adam(0.001),                 metrics=[\"mae\", \"mse\"])  # 9. Fit the model with EarlyStopping and ReduceLROnPlateau callbacks model_7.fit(train_dataset,             epochs=N_EPOCHS,             validation_data=test_dataset,             verbose=0, # prevent large amounts of training outputs             # callbacks=[create_model_checkpoint(model_name=stack_model.name)] # saving model every epoch consumes far too much time             callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=200, restore_best_weights=True),                       tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=100, verbose=1)]) <pre>\nEpoch 00328: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n\nEpoch 00428: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\nCPU times: user 1min 23s, sys: 4.58 s, total: 1min 28s\nWall time: 3min 44s\n</pre> <p>And would you look at that! N-BEATS algorithm fit to our Bitcoin historical data.</p> <p>How did it perform?</p> In\u00a0[\u00a0]: Copied! <pre># Evaluate N-BEATS model on the test dataset\nmodel_7.evaluate(test_dataset)\n</pre> # Evaluate N-BEATS model on the test dataset model_7.evaluate(test_dataset) <pre>1/1 [==============================] - 0s 46ms/step - loss: 585.4998 - mae: 585.4998 - mse: 1179491.5000\n</pre> Out[\u00a0]: <pre>[585.4998168945312, 585.4998168945312, 1179491.5]</pre> In\u00a0[\u00a0]: Copied! <pre># Make predictions with N-BEATS model\nmodel_7_preds = make_preds(model_7, test_dataset)\nmodel_7_preds[:10]\n</pre> # Make predictions with N-BEATS model model_7_preds = make_preds(model_7, test_dataset) model_7_preds[:10] Out[\u00a0]: <pre>&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=\narray([8908.059, 8854.672, 8990.933, 8759.821, 8819.711, 8774.012,\n       8604.187, 8547.038, 8495.928, 8489.514], dtype=float32)&gt;</pre> In\u00a0[\u00a0]: Copied! <pre># Evaluate N-BEATS model predictions\nmodel_7_results = evaluate_preds(y_true=y_test,\n                                 y_pred=model_7_preds)\nmodel_7_results\n</pre> # Evaluate N-BEATS model predictions model_7_results = evaluate_preds(y_true=y_test,                                  y_pred=model_7_preds) model_7_results Out[\u00a0]: <pre>{'mae': 585.4998,\n 'mape': 2.7445195,\n 'mase': 1.028561,\n 'mse': 1179491.5,\n 'rmse': 1086.044}</pre> <p>Woah... even with all of those special layers and hand-crafted network, it looks like the N-BEATS model doesn't perform as well as <code>model_1</code> or the original naive forecast.</p> <p>This goes to show the power of smaller networks as well as the fact not all larger models are better suited for a certain type of data.</p> In\u00a0[\u00a0]: Copied! <pre># Plot the N-BEATS model and inspect the architecture\nfrom tensorflow.keras.utils import plot_model\nplot_model(model_7)\n</pre> # Plot the N-BEATS model and inspect the architecture from tensorflow.keras.utils import plot_model plot_model(model_7) Out[\u00a0]: <p>Now that is one good looking model!</p> <p>It even looks similar to the model shown in Figure 1 of the N-BEATS paper.</p> <p> Comparison of <code>model_7</code> (N-BEATS replica model make with Keras Functional API) versus actual N-BEATS architecture diagram.</p> <p>Looks like our Functional API usage did the trick!</p> <p>\ud83d\udd11 Note: Our N-BEATS model replicates the N-BEATS generic architecture, the training setups are largely the same, except for the N-BEATS paper used an ensemble of models to make predictions (multiple different loss functions and multiple different lookback windows), see Table 18 of the N-BEATS paper for more. An extension could be to setup this kind of training regime and see if it improves performance.</p> <p>How about we try and save our version of the N-BEATS model?</p> In\u00a0[\u00a0]: Copied! <pre># This will error out unless a \"get_config()\" method is implemented - this could be extra curriculum\nmodel_7.save(model_7.name)\n</pre> # This will error out unless a \"get_config()\" method is implemented - this could be extra curriculum model_7.save(model_7.name) <pre>WARNING:absl:Found untraced functions such as theta_layer_call_and_return_conditional_losses, theta_layer_call_fn, theta_layer_call_and_return_conditional_losses, theta_layer_call_fn, theta_layer_call_and_return_conditional_losses while saving (showing 5 of 750). These functions will not be directly callable after loading.\n</pre> <pre>INFO:tensorflow:Assets written to: model_7_N-BEATS/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_7_N-BEATS/assets\n/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n  category=CustomMaskWarning)\n</pre> <p>You'll notice a warning appears telling us to fully save our model correctly we need to implement a <code>get_config()</code> method in our custom layer class.</p> <p>\ud83d\udcd6 Resource: If you would like to save and load the N-BEATS model or any other custom or subclassed layer/model configuration, you should overwrite the <code>get_config()</code> and optionally <code>from_config()</code> methods. See the TensorFlow Custom Objects documentation for more.</p> In\u00a0[\u00a0]: Copied! <pre>def get_ensemble_models(horizon=HORIZON, \n                        train_data=train_dataset,\n                        test_data=test_dataset,\n                        num_iter=10, \n                        num_epochs=100, \n                        loss_fns=[\"mae\", \"mse\", \"mape\"]):\n  \"\"\"\n  Returns a list of num_iter models each trained on MAE, MSE and MAPE loss.\n\n  For example, if num_iter=10, a list of 30 trained models will be returned:\n  10 * len([\"mae\", \"mse\", \"mape\"]).\n  \"\"\"\n  # Make empty list for trained ensemble models\n  ensemble_models = []\n\n  # Create num_iter number of models per loss function\n  for i in range(num_iter):\n    # Build and fit a new model with a different loss function\n    for loss_function in loss_fns:\n      print(f\"Optimizing model by reducing: {loss_function} for {num_epochs} epochs, model number: {i}\")\n\n      # Construct a simple model (similar to model_1)\n      model = tf.keras.Sequential([\n        # Initialize layers with normal (Gaussian) distribution so we can use the models for prediction\n        # interval estimation later: https://www.tensorflow.org/api_docs/python/tf/keras/initializers/HeNormal\n        layers.Dense(128, kernel_initializer=\"he_normal\", activation=\"relu\"), \n        layers.Dense(128, kernel_initializer=\"he_normal\", activation=\"relu\"),\n        layers.Dense(HORIZON)                                 \n      ])\n\n      # Compile simple model with current loss function\n      model.compile(loss=loss_function,\n                    optimizer=tf.keras.optimizers.Adam(),\n                    metrics=[\"mae\", \"mse\"])\n      \n      # Fit model\n      model.fit(train_data,\n                epochs=num_epochs,\n                verbose=0,\n                validation_data=test_data,\n                # Add callbacks to prevent training from going/stalling for too long\n                callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n                                                            patience=200,\n                                                            restore_best_weights=True),\n                           tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n                                                                patience=100,\n                                                                verbose=1)])\n      \n      # Append fitted model to list of ensemble models\n      ensemble_models.append(model)\n\n  return ensemble_models # return list of trained models\n</pre> def get_ensemble_models(horizon=HORIZON,                          train_data=train_dataset,                         test_data=test_dataset,                         num_iter=10,                          num_epochs=100,                          loss_fns=[\"mae\", \"mse\", \"mape\"]):   \"\"\"   Returns a list of num_iter models each trained on MAE, MSE and MAPE loss.    For example, if num_iter=10, a list of 30 trained models will be returned:   10 * len([\"mae\", \"mse\", \"mape\"]).   \"\"\"   # Make empty list for trained ensemble models   ensemble_models = []    # Create num_iter number of models per loss function   for i in range(num_iter):     # Build and fit a new model with a different loss function     for loss_function in loss_fns:       print(f\"Optimizing model by reducing: {loss_function} for {num_epochs} epochs, model number: {i}\")        # Construct a simple model (similar to model_1)       model = tf.keras.Sequential([         # Initialize layers with normal (Gaussian) distribution so we can use the models for prediction         # interval estimation later: https://www.tensorflow.org/api_docs/python/tf/keras/initializers/HeNormal         layers.Dense(128, kernel_initializer=\"he_normal\", activation=\"relu\"),          layers.Dense(128, kernel_initializer=\"he_normal\", activation=\"relu\"),         layers.Dense(HORIZON)                                        ])        # Compile simple model with current loss function       model.compile(loss=loss_function,                     optimizer=tf.keras.optimizers.Adam(),                     metrics=[\"mae\", \"mse\"])              # Fit model       model.fit(train_data,                 epochs=num_epochs,                 verbose=0,                 validation_data=test_data,                 # Add callbacks to prevent training from going/stalling for too long                 callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",                                                             patience=200,                                                             restore_best_weights=True),                            tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",                                                                 patience=100,                                                                 verbose=1)])              # Append fitted model to list of ensemble models       ensemble_models.append(model)    return ensemble_models # return list of trained models <p>Ensemble model creator function created!</p> <p>Let's try it out by running <code>num_iter=5</code> runs for 1000 epochs. This will result in 15 total models (5 for each different loss function).</p> <p>Of course, these numbers could be tweaked to create more models trained for longer.</p> <p>\ud83d\udd11 Note: With ensembles, you'll generally find more total models means better performance. However, this comes with the tradeoff of having to train more models (longer training time) and make predictions with more models (longer prediction time).</p> In\u00a0[\u00a0]: Copied! <pre>%%time\n# Get list of trained ensemble models\nensemble_models = get_ensemble_models(num_iter=5,\n                                      num_epochs=1000)\n</pre> %%time # Get list of trained ensemble models ensemble_models = get_ensemble_models(num_iter=5,                                       num_epochs=1000) <pre>Optimizing model by reducing: mae for 1000 epochs, model number: 0\n\nEpoch 00794: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n\nEpoch 00928: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\nOptimizing model by reducing: mse for 1000 epochs, model number: 0\n\nEpoch 00591: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n\nEpoch 00707: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n\nEpoch 00807: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\nOptimizing model by reducing: mape for 1000 epochs, model number: 0\n\nEpoch 00165: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n\nEpoch 00282: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n\nEpoch 00382: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\nOptimizing model by reducing: mae for 1000 epochs, model number: 1\nOptimizing model by reducing: mse for 1000 epochs, model number: 1\n\nEpoch 00409: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n\nEpoch 00509: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\nOptimizing model by reducing: mape for 1000 epochs, model number: 1\n\nEpoch 00185: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n\nEpoch 00726: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n\nEpoch 00826: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\nOptimizing model by reducing: mae for 1000 epochs, model number: 2\nOptimizing model by reducing: mse for 1000 epochs, model number: 2\nOptimizing model by reducing: mape for 1000 epochs, model number: 2\n\nEpoch 00241: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n\nEpoch 00341: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\nOptimizing model by reducing: mae for 1000 epochs, model number: 3\n\nEpoch 00572: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\nOptimizing model by reducing: mse for 1000 epochs, model number: 3\n\nEpoch 00304: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n\nEpoch 00607: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n\nEpoch 00707: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\nOptimizing model by reducing: mape for 1000 epochs, model number: 3\n\nEpoch 00301: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n\nEpoch 00401: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\nOptimizing model by reducing: mae for 1000 epochs, model number: 4\nOptimizing model by reducing: mse for 1000 epochs, model number: 4\n\nEpoch 00640: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n\nEpoch 00740: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\nOptimizing model by reducing: mape for 1000 epochs, model number: 4\n\nEpoch 00132: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n\nEpoch 00609: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n\nEpoch 00709: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\nCPU times: user 6min 24s, sys: 38.6 s, total: 7min 3s\nWall time: 7min 58s\n</pre> <p>Look at all of those models!</p> <p>How about we now write a function to use the list of trained ensemble models to make predictions and then return a list of predictions (one set of predictions per model)?</p> In\u00a0[\u00a0]: Copied! <pre># Create a function which uses a list of trained models to make and return a list of predictions\ndef make_ensemble_preds(ensemble_models, data):\n  ensemble_preds = []\n  for model in ensemble_models:\n    preds = model.predict(data) # make predictions with current ensemble model\n    ensemble_preds.append(preds)\n  return tf.constant(tf.squeeze(ensemble_preds))\n</pre> # Create a function which uses a list of trained models to make and return a list of predictions def make_ensemble_preds(ensemble_models, data):   ensemble_preds = []   for model in ensemble_models:     preds = model.predict(data) # make predictions with current ensemble model     ensemble_preds.append(preds)   return tf.constant(tf.squeeze(ensemble_preds)) In\u00a0[\u00a0]: Copied! <pre># Create a list of ensemble predictions\nensemble_preds = make_ensemble_preds(ensemble_models=ensemble_models,\n                                     data=test_dataset)\nensemble_preds\n</pre> # Create a list of ensemble predictions ensemble_preds = make_ensemble_preds(ensemble_models=ensemble_models,                                      data=test_dataset) ensemble_preds <pre>WARNING:tensorflow:5 out of the last 22 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7fdcef255d40&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n</pre> <pre>WARNING:tensorflow:5 out of the last 22 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7fdcef255d40&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n</pre> <pre>WARNING:tensorflow:6 out of the last 23 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7fdc77fed9e0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n</pre> <pre>WARNING:tensorflow:6 out of the last 23 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7fdc77fed9e0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n</pre> Out[\u00a0]: <pre>&lt;tf.Tensor: shape=(15, 556), dtype=float32, numpy=\narray([[ 8805.756,  8773.019,  9028.609, ..., 50112.656, 49132.555,\n        46455.695],\n       [ 8764.092,  8740.744,  9051.838, ..., 49355.098, 48502.336,\n        45333.934],\n       [ 8732.57 ,  8719.407,  9093.386, ..., 49921.9  , 47992.15 ,\n        45316.45 ],\n       ...,\n       [ 8938.421,  8773.84 ,  9045.577, ..., 49488.133, 49741.4  ,\n        46536.25 ],\n       [ 8724.761,  8805.311,  9094.972, ..., 49553.086, 48492.86 ,\n        45084.266],\n       [ 8823.311,  8768.297,  9047.492, ..., 49759.902, 48090.945,\n        45874.336]], dtype=float32)&gt;</pre> <p>Now we've got a set of ensemble predictions, we can evaluate them against the ground truth values.</p> <p>However, since we've trained 15 models, there's going to be 15 sets of predictions. Rather than comparing every set of predictions to the ground truth, let's take the median (you could also take the mean too but the median is usually more robust than the mean).</p> In\u00a0[\u00a0]: Copied! <pre># Evaluate ensemble model(s) predictions\nensemble_results = evaluate_preds(y_true=y_test,\n                                  y_pred=np.median(ensemble_preds, axis=0)) # take the median across all ensemble predictions\nensemble_results\n</pre> # Evaluate ensemble model(s) predictions ensemble_results = evaluate_preds(y_true=y_test,                                   y_pred=np.median(ensemble_preds, axis=0)) # take the median across all ensemble predictions ensemble_results Out[\u00a0]: <pre>{'mae': 567.4423,\n 'mape': 2.5843322,\n 'mase': 0.996839,\n 'mse': 1144512.9,\n 'rmse': 1069.8191}</pre> <p>Nice! Looks like the ensemble model is the best performing model on the MAE metric so far.</p> In\u00a0[\u00a0]: Copied! <pre># Find upper and lower bounds of ensemble predictions\ndef get_upper_lower(preds): # 1. Take the predictions of multiple randomly initialized deep learning neural networks\n  \n  # 2. Measure the standard deviation of the predictions\n  std = tf.math.reduce_std(preds, axis=0)\n  \n  # 3. Multiply the standard deviation by 1.96\n  interval = 1.96 * std # https://en.wikipedia.org/wiki/1.96 \n\n  # 4. Get the prediction interval upper and lower bounds\n  preds_mean = tf.reduce_mean(preds, axis=0)\n  lower, upper = preds_mean - interval, preds_mean + interval\n  return lower, upper\n\n# Get the upper and lower bounds of the 95% \nlower, upper = get_upper_lower(preds=ensemble_preds)\n</pre> # Find upper and lower bounds of ensemble predictions def get_upper_lower(preds): # 1. Take the predictions of multiple randomly initialized deep learning neural networks      # 2. Measure the standard deviation of the predictions   std = tf.math.reduce_std(preds, axis=0)      # 3. Multiply the standard deviation by 1.96   interval = 1.96 * std # https://en.wikipedia.org/wiki/1.96     # 4. Get the prediction interval upper and lower bounds   preds_mean = tf.reduce_mean(preds, axis=0)   lower, upper = preds_mean - interval, preds_mean + interval   return lower, upper  # Get the upper and lower bounds of the 95%  lower, upper = get_upper_lower(preds=ensemble_preds) <p>Wonderful, now we've got the upper and lower bounds for the the 95% prediction interval, let's plot them against our ensemble model's predictions.</p> <p>To do so, we can use our plotting function as well as the <code>matplotlib.pyplot.fill_between()</code> method to shade in the space between the upper and lower bounds.</p> In\u00a0[\u00a0]: Copied! <pre># Get the median values of our ensemble preds\nensemble_median = np.median(ensemble_preds, axis=0)\n\n# Plot the median of our ensemble preds along with the prediction intervals (where the predictions fall between)\noffset=500\nplt.figure(figsize=(10, 7))\nplt.plot(X_test.index[offset:], y_test[offset:], \"g\", label=\"Test Data\")\nplt.plot(X_test.index[offset:], ensemble_median[offset:], \"k-\", label=\"Ensemble Median\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"BTC Price\")\nplt.fill_between(X_test.index[offset:], \n                 (lower)[offset:], \n                 (upper)[offset:], label=\"Prediction Intervals\")\nplt.legend(loc=\"upper left\", fontsize=14);\n</pre> # Get the median values of our ensemble preds ensemble_median = np.median(ensemble_preds, axis=0)  # Plot the median of our ensemble preds along with the prediction intervals (where the predictions fall between) offset=500 plt.figure(figsize=(10, 7)) plt.plot(X_test.index[offset:], y_test[offset:], \"g\", label=\"Test Data\") plt.plot(X_test.index[offset:], ensemble_median[offset:], \"k-\", label=\"Ensemble Median\") plt.xlabel(\"Date\") plt.ylabel(\"BTC Price\") plt.fill_between(X_test.index[offset:],                   (lower)[offset:],                   (upper)[offset:], label=\"Prediction Intervals\") plt.legend(loc=\"upper left\", fontsize=14); <p>We've just plotted:</p> <ul> <li>The test data (the ground truth Bitcoin prices)</li> <li>The median of the ensemble predictions</li> <li>The 95% prediction intervals (assuming the data is Gaussian/normal, the model is saying that 95% of the time, predicted value should fall between this range)</li> </ul> <p>What can you tell about the ensemble model from the plot above?</p> <p>It looks like the ensemble predictions are lagging slightly behind the actual data.</p> <p>And the prediction intervals are fairly low throughout.</p> <p>The combination of lagging predictions as well as low prediction intervals indicates that our ensemble model may be overfitting the data, meaning it's basically replicating what a na\u00efve model would do and just predicting the previous timestep value for the next value.</p> <p>This would explain why previous attempts to beat the na\u00efve forecast have been futile.</p> <p>We can test this hypothesis of overfitting by creating a model to make predictions into the future and seeing what they look like.</p> <p>\ud83d\udd11 Note: Our prediction intervals assume that the data we're using come from a Gaussian/normal distribution (also called a bell curve), however, open systems rarely follow the Gaussian. We'll see this later on with the turkey problem \ud83e\udd83. For further reading on this topic, I'd recommend reading The Black Swan by Nassim Nicholas Taleb, especially Part 2 and Chapter 15.</p> In\u00a0[\u00a0]: Copied! <pre>bitcoin_prices_windowed.head()\n</pre> bitcoin_prices_windowed.head() Out[\u00a0]: Price block_reward Price+1 Price+2 Price+3 Price+4 Price+5 Price+6 Price+7 Date 2013-10-01 123.65499 25 NaN NaN NaN NaN NaN NaN NaN 2013-10-02 125.45500 25 123.65499 NaN NaN NaN NaN NaN NaN 2013-10-03 108.58483 25 125.45500 123.65499 NaN NaN NaN NaN NaN 2013-10-04 118.67466 25 108.58483 125.45500 123.65499 NaN NaN NaN NaN 2013-10-05 121.33866 25 118.67466 108.58483 125.45500 123.65499 NaN NaN NaN In\u00a0[\u00a0]: Copied! <pre># Train model on entire data to make prediction for the next day \nX_all = bitcoin_prices_windowed.drop([\"Price\", \"block_reward\"], axis=1).dropna().to_numpy() # only want prices, our future model can be a univariate model\ny_all = bitcoin_prices_windowed.dropna()[\"Price\"].to_numpy()\n</pre> # Train model on entire data to make prediction for the next day  X_all = bitcoin_prices_windowed.drop([\"Price\", \"block_reward\"], axis=1).dropna().to_numpy() # only want prices, our future model can be a univariate model y_all = bitcoin_prices_windowed.dropna()[\"Price\"].to_numpy() <p>Windows and labels ready! Let's turn them into performance optimized TensorFlow Datasets by:</p> <ol> <li>Turning <code>X_all</code> and <code>y_all</code> into tensor Datasets using <code>tf.data.Dataset.from_tensor_slices()</code></li> <li>Combining the features and labels into a Dataset tuple using <code>tf.data.Dataset.zip()</code></li> <li>Batch and prefetch the data using <code>tf.data.Dataset.batch()</code> and <code>tf.data.Dataset.prefetch()</code> respectively</li> </ol> In\u00a0[\u00a0]: Copied! <pre># 1. Turn X and y into tensor Datasets\nfeatures_dataset_all = tf.data.Dataset.from_tensor_slices(X_all)\nlabels_dataset_all = tf.data.Dataset.from_tensor_slices(y_all)\n\n# 2. Combine features &amp; labels\ndataset_all = tf.data.Dataset.zip((features_dataset_all, labels_dataset_all))\n\n# 3. Batch and prefetch for optimal performance\nBATCH_SIZE = 1024 # taken from Appendix D in N-BEATS paper\ndataset_all = dataset_all.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\ndataset_all\n</pre> # 1. Turn X and y into tensor Datasets features_dataset_all = tf.data.Dataset.from_tensor_slices(X_all) labels_dataset_all = tf.data.Dataset.from_tensor_slices(y_all)  # 2. Combine features &amp; labels dataset_all = tf.data.Dataset.zip((features_dataset_all, labels_dataset_all))  # 3. Batch and prefetch for optimal performance BATCH_SIZE = 1024 # taken from Appendix D in N-BEATS paper dataset_all = dataset_all.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)  dataset_all Out[\u00a0]: <pre>&lt;PrefetchDataset shapes: ((None, 7), (None,)), types: (tf.float64, tf.float64)&gt;</pre> <p>And now let's create a model similar to <code>model_1</code> except with an extra layer, we'll also fit it to the entire dataset for 100 epochs (feel free to play around with the number of epochs or callbacks here, you've got the skills to now).</p> In\u00a0[\u00a0]: Copied! <pre>tf.random.set_seed(42)\n\n# Create model (nice and simple, just to test)\nmodel_9 = tf.keras.Sequential([\n  layers.Dense(128, activation=\"relu\"),\n  layers.Dense(128, activation=\"relu\"),\n  layers.Dense(HORIZON)\n])\n\n# Compile\nmodel_9.compile(loss=tf.keras.losses.mae,\n                optimizer=tf.keras.optimizers.Adam())\n\n# Fit model on all of the data to make future forecasts\nmodel_9.fit(dataset_all,\n            epochs=100,\n            verbose=0) # don't print out anything, we've seen this all before\n</pre> tf.random.set_seed(42)  # Create model (nice and simple, just to test) model_9 = tf.keras.Sequential([   layers.Dense(128, activation=\"relu\"),   layers.Dense(128, activation=\"relu\"),   layers.Dense(HORIZON) ])  # Compile model_9.compile(loss=tf.keras.losses.mae,                 optimizer=tf.keras.optimizers.Adam())  # Fit model on all of the data to make future forecasts model_9.fit(dataset_all,             epochs=100,             verbose=0) # don't print out anything, we've seen this all before Out[\u00a0]: <pre>&lt;keras.callbacks.History at 0x7fdc77ae0dd0&gt;</pre> In\u00a0[\u00a0]: Copied! <pre># How many timesteps to predict into the future?\nINTO_FUTURE = 14 # since our Bitcoin data is daily, this is for 14 days\n</pre> # How many timesteps to predict into the future? INTO_FUTURE = 14 # since our Bitcoin data is daily, this is for 14 days <p> Example flow chart representing the loop we're about to create for making forecasts. Not pictured: retraining a forecasting model every time a forecast is made &amp; new data is acquired. For example, if you're predicting the price of Bitcoin daily, you'd want to retrain your model every day, since each day you're going to have a new data point to work with.</p> <p>Alright, let's create a function which returns <code>INTO_FUTURE</code> forecasted values using a trained model.</p> <p>To do so, we'll build the following steps:</p> <ol> <li>Function which takes as input:</li> </ol> <ul> <li>a list of values (the Bitcoin historical data)</li> <li>a trained model (such as <code>model_9</code>)</li> <li>a window into the future to predict (our <code>INTO_FUTURE</code> variable)</li> <li>the window size a model was trained on (<code>WINDOW_SIZE</code>) - the model can only predict on the same kind of data it was trained on</li> </ul> <ol> <li>Creates an empty list for future forecasts (this will be returned at the end of the function) and extracts the last <code>WINDOW_SIZE</code> values from the input values (predictions will start from the last <code>WINDOW_SIZE</code> values of the training data)</li> <li>Loop <code>INTO_FUTURE</code> times making a prediction on <code>WINDOW_SIZE</code> datasets which update to remove the first the value and append the latest prediction</li> </ol> <ul> <li>Eventually future predictions will be made using the model's own previous predictions as input</li> </ul> In\u00a0[\u00a0]: Copied! <pre># 1. Create function to make predictions into the future\ndef make_future_forecast(values, model, into_future, window_size=WINDOW_SIZE) -&gt; list:\n  \"\"\"\n  Makes future forecasts into_future steps after values ends.\n\n  Returns future forecasts as list of floats.\n  \"\"\"\n  # 2. Make an empty list for future forecasts/prepare data to forecast on\n  future_forecast = []\n  last_window = values[-WINDOW_SIZE:] # only want preds from the last window (this will get updated)\n\n  # 3. Make INTO_FUTURE number of predictions, altering the data which gets predicted on each time \n  for _ in range(into_future):\n    \n    # Predict on last window then append it again, again, again (model starts to make forecasts on its own forecasts)\n    future_pred = model.predict(tf.expand_dims(last_window, axis=0))\n    print(f\"Predicting on: \\n {last_window} -&gt; Prediction: {tf.squeeze(future_pred).numpy()}\\n\")\n    \n    # Append predictions to future_forecast\n    future_forecast.append(tf.squeeze(future_pred).numpy())\n    # print(future_forecast)\n\n    # Update last window with new pred and get WINDOW_SIZE most recent preds (model was trained on WINDOW_SIZE windows)\n    last_window = np.append(last_window, future_pred)[-WINDOW_SIZE:]\n  \n  return future_forecast\n</pre> # 1. Create function to make predictions into the future def make_future_forecast(values, model, into_future, window_size=WINDOW_SIZE) -&gt; list:   \"\"\"   Makes future forecasts into_future steps after values ends.    Returns future forecasts as list of floats.   \"\"\"   # 2. Make an empty list for future forecasts/prepare data to forecast on   future_forecast = []   last_window = values[-WINDOW_SIZE:] # only want preds from the last window (this will get updated)    # 3. Make INTO_FUTURE number of predictions, altering the data which gets predicted on each time    for _ in range(into_future):          # Predict on last window then append it again, again, again (model starts to make forecasts on its own forecasts)     future_pred = model.predict(tf.expand_dims(last_window, axis=0))     print(f\"Predicting on: \\n {last_window} -&gt; Prediction: {tf.squeeze(future_pred).numpy()}\\n\")          # Append predictions to future_forecast     future_forecast.append(tf.squeeze(future_pred).numpy())     # print(future_forecast)      # Update last window with new pred and get WINDOW_SIZE most recent preds (model was trained on WINDOW_SIZE windows)     last_window = np.append(last_window, future_pred)[-WINDOW_SIZE:]      return future_forecast <p>Nice! Time to bring BitPredict \ud83d\udcb0\ud83d\udcc8 to life and make future forecasts of the price of Bitcoin.</p> <p>\ud83d\udee0 Exercise: In terms of a forecasting model, what might another approach to our <code>make_future_forecasts()</code> function? Recall, that for making forecasts, you need to retrain a model each time you want to generate a new prediction.</p> <p>So perhaps you could try to: make a prediction (one timestep into the future), retrain a model with this new prediction appended to the data, make a prediction, append the prediction, retrain a model... etc.</p> <p>As it is, the <code>make_future_forecasts()</code> function skips the retraining of a model part.</p> In\u00a0[\u00a0]: Copied! <pre># Make forecasts into future of the price of Bitcoin\n# Note: if you're reading this at a later date, you may already be in the future, so the forecasts \n# we're making may not actually be forecasts, if that's the case, readjust the training data.\nfuture_forecast = make_future_forecast(values=y_all,\n                                       model=model_9,\n                                       into_future=INTO_FUTURE,\n                                       window_size=WINDOW_SIZE)\n</pre> # Make forecasts into future of the price of Bitcoin # Note: if you're reading this at a later date, you may already be in the future, so the forecasts  # we're making may not actually be forecasts, if that's the case, readjust the training data. future_forecast = make_future_forecast(values=y_all,                                        model=model_9,                                        into_future=INTO_FUTURE,                                        window_size=WINDOW_SIZE) <pre>Predicting on: \n [56573.5554719  52147.82118698 49764.1320816  50032.69313676\n 47885.62525472 45604.61575361 43144.47129086] -&gt; Prediction: 55764.46484375\n\nPredicting on: \n [52147.82118698 49764.1320816  50032.69313676 47885.62525472\n 45604.61575361 43144.47129086 55764.46484375] -&gt; Prediction: 50985.9453125\n\nPredicting on: \n [49764.1320816  50032.69313676 47885.62525472 45604.61575361\n 43144.47129086 55764.46484375 50985.9453125 ] -&gt; Prediction: 48522.96484375\n\nPredicting on: \n [50032.69313676 47885.62525472 45604.61575361 43144.47129086\n 55764.46484375 50985.9453125  48522.96484375] -&gt; Prediction: 48137.203125\n\nPredicting on: \n [47885.62525472 45604.61575361 43144.47129086 55764.46484375\n 50985.9453125  48522.96484375 48137.203125  ] -&gt; Prediction: 47880.63671875\n\nPredicting on: \n [45604.61575361 43144.47129086 55764.46484375 50985.9453125\n 48522.96484375 48137.203125   47880.63671875] -&gt; Prediction: 46879.71875\n\nPredicting on: \n [43144.47129086 55764.46484375 50985.9453125  48522.96484375\n 48137.203125   47880.63671875 46879.71875   ] -&gt; Prediction: 48227.6015625\n\nPredicting on: \n [55764.46484375 50985.9453125  48522.96484375 48137.203125\n 47880.63671875 46879.71875    48227.6015625 ] -&gt; Prediction: 53963.69140625\n\nPredicting on: \n [50985.9453125  48522.96484375 48137.203125   47880.63671875\n 46879.71875    48227.6015625  53963.69140625] -&gt; Prediction: 49685.55859375\n\nPredicting on: \n [48522.96484375 48137.203125   47880.63671875 46879.71875\n 48227.6015625  53963.69140625 49685.55859375] -&gt; Prediction: 47596.17578125\n\nPredicting on: \n [48137.203125   47880.63671875 46879.71875    48227.6015625\n 53963.69140625 49685.55859375 47596.17578125] -&gt; Prediction: 48114.4296875\n\nPredicting on: \n [47880.63671875 46879.71875    48227.6015625  53963.69140625\n 49685.55859375 47596.17578125 48114.4296875 ] -&gt; Prediction: 48808.0078125\n\nPredicting on: \n [46879.71875    48227.6015625  53963.69140625 49685.55859375\n 47596.17578125 48114.4296875  48808.0078125 ] -&gt; Prediction: 48623.85546875\n\nPredicting on: \n [48227.6015625  53963.69140625 49685.55859375 47596.17578125\n 48114.4296875  48808.0078125  48623.85546875] -&gt; Prediction: 50178.72265625\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>future_forecast[:10]\n</pre> future_forecast[:10] Out[\u00a0]: <pre>[55764.465,\n 50985.945,\n 48522.965,\n 48137.203,\n 47880.637,\n 46879.72,\n 48227.6,\n 53963.69,\n 49685.56,\n 47596.176]</pre> In\u00a0[\u00a0]: Copied! <pre>def get_future_dates(start_date, into_future, offset=1):\n  \"\"\"\n  Returns array of datetime values from ranging from start_date to start_date+horizon.\n\n  start_date: date to start range (np.datetime64)\n  into_future: number of days to add onto start date for range (int)\n  offset: number of days to offset start_date by (default 1)\n  \"\"\"\n  start_date = start_date + np.timedelta64(offset, \"D\") # specify start date, \"D\" stands for day\n  end_date = start_date + np.timedelta64(into_future, \"D\") # specify end date\n  return np.arange(start_date, end_date, dtype=\"datetime64[D]\") # return a date range between start date and end date\n</pre> def get_future_dates(start_date, into_future, offset=1):   \"\"\"   Returns array of datetime values from ranging from start_date to start_date+horizon.    start_date: date to start range (np.datetime64)   into_future: number of days to add onto start date for range (int)   offset: number of days to offset start_date by (default 1)   \"\"\"   start_date = start_date + np.timedelta64(offset, \"D\") # specify start date, \"D\" stands for day   end_date = start_date + np.timedelta64(into_future, \"D\") # specify end date   return np.arange(start_date, end_date, dtype=\"datetime64[D]\") # return a date range between start date and end date <p>The start date of our forecasted dates will be the last date of our dataset.</p> In\u00a0[\u00a0]: Copied! <pre># Last timestep of timesteps (currently in np.datetime64 format)\nlast_timestep = bitcoin_prices.index[-1]\nlast_timestep\n</pre> # Last timestep of timesteps (currently in np.datetime64 format) last_timestep = bitcoin_prices.index[-1] last_timestep Out[\u00a0]: <pre>Timestamp('2021-05-18 00:00:00')</pre> In\u00a0[\u00a0]: Copied! <pre># Get next two weeks of timesteps\nnext_time_steps = get_future_dates(start_date=last_timestep, \n                                   into_future=INTO_FUTURE)\nnext_time_steps\n</pre> # Get next two weeks of timesteps next_time_steps = get_future_dates(start_date=last_timestep,                                     into_future=INTO_FUTURE) next_time_steps Out[\u00a0]: <pre>array(['2021-05-19', '2021-05-20', '2021-05-21', '2021-05-22',\n       '2021-05-23', '2021-05-24', '2021-05-25', '2021-05-26',\n       '2021-05-27', '2021-05-28', '2021-05-29', '2021-05-30',\n       '2021-05-31', '2021-06-01'], dtype='datetime64[D]')</pre> <p>Look at that! We've now got a list of dates we can use to visualize our future Bitcoin predictions.</p> <p>But to make sure the lines of the plot connect (try not running the cell below and then plotting the data to see what I mean), let's insert the last timestep and Bitcoin price of our training data to the <code>next_time_steps</code> and <code>future_forecast</code> arrays.</p> In\u00a0[\u00a0]: Copied! <pre># Insert last timestep/final price so the graph doesn't look messed\nnext_time_steps = np.insert(next_time_steps, 0, last_timestep)\nfuture_forecast = np.insert(future_forecast, 0, btc_price[-1])\nnext_time_steps, future_forecast\n</pre> # Insert last timestep/final price so the graph doesn't look messed next_time_steps = np.insert(next_time_steps, 0, last_timestep) future_forecast = np.insert(future_forecast, 0, btc_price[-1]) next_time_steps, future_forecast Out[\u00a0]: <pre>(array(['2021-05-18', '2021-05-19', '2021-05-20', '2021-05-21',\n        '2021-05-22', '2021-05-23', '2021-05-24', '2021-05-25',\n        '2021-05-26', '2021-05-27', '2021-05-28', '2021-05-29',\n        '2021-05-30', '2021-05-31', '2021-06-01'], dtype='datetime64[D]'),\n array([43144.473, 55764.465, 50985.945, 48522.965, 48137.203, 47880.637,\n        46879.72 , 48227.6  , 53963.69 , 49685.56 , 47596.176, 48114.43 ,\n        48808.008, 48623.855, 50178.723], dtype=float32))</pre> <p>Time to plot!</p> In\u00a0[\u00a0]: Copied! <pre># Plot future price predictions of Bitcoin\nplt.figure(figsize=(10, 7))\nplot_time_series(bitcoin_prices.index, btc_price, start=2500, format=\"-\", label=\"Actual BTC Price\")\nplot_time_series(next_time_steps, future_forecast, format=\"-\", label=\"Predicted BTC Price\")\n</pre> # Plot future price predictions of Bitcoin plt.figure(figsize=(10, 7)) plot_time_series(bitcoin_prices.index, btc_price, start=2500, format=\"-\", label=\"Actual BTC Price\") plot_time_series(next_time_steps, future_forecast, format=\"-\", label=\"Predicted BTC Price\") <p>Hmmm... how did our model go?</p> <p>It looks like our predictions are starting to form a bit of a cyclic pattern (up and down in the same way).</p> <p>Perhaps that's due to our model overfitting the training data and not generalizing well for future data. Also, as you could imagine, the further you predict into the future, the higher your chance for error (try seeing what happens when you predict 100 days into the future).</p> <p>But of course, we can't measure these predictions as they are because after all, they're predictions into the actual-future (by the time you read this, the future might have already happened, if so, how did the model go?).</p> <p>\ud83d\udd11 Note: A reminder, the predictions we've made here are not financial advice. And by now, you should be well aware of just how poor machine learning models can be at forecasting values in an open system - anyone promising you a model which can \"beat the market\" is likely trying to scam you, oblivious to their errors or very lucky.</p> In\u00a0[\u00a0]: Copied! <pre># Let's introduce a Turkey problem to our BTC data (price BTC falls 100x in one day)\nbtc_price_turkey = btc_price.copy()\nbtc_price_turkey[-1] = btc_price_turkey[-1] / 100\n</pre> # Let's introduce a Turkey problem to our BTC data (price BTC falls 100x in one day) btc_price_turkey = btc_price.copy() btc_price_turkey[-1] = btc_price_turkey[-1] / 100 In\u00a0[\u00a0]: Copied! <pre># Manufacture an extra price on the end (to showcase the Turkey problem)\nbtc_price_turkey[-10:]\n</pre> # Manufacture an extra price on the end (to showcase the Turkey problem) btc_price_turkey[-10:] Out[\u00a0]: <pre>[58788.2096789273,\n 58102.1914262342,\n 55715.5466512869,\n 56573.5554719043,\n 52147.8211869823,\n 49764.1320815975,\n 50032.6931367648,\n 47885.6252547166,\n 45604.6157536131,\n 431.44471290860304]</pre> <p>Notice the last value is 100x lower than what it actually was (remember, this is not a real data point, its only to illustrate the effects of the turkey problem).</p> <p>Now we've got Bitcoin prices including a turkey problem data point, let's get the timesteps.</p> In\u00a0[\u00a0]: Copied! <pre># Get the timesteps for the turkey problem \nbtc_timesteps_turkey = np.array(bitcoin_prices.index)\nbtc_timesteps_turkey[-10:]\n</pre> # Get the timesteps for the turkey problem  btc_timesteps_turkey = np.array(bitcoin_prices.index) btc_timesteps_turkey[-10:] Out[\u00a0]: <pre>array(['2021-05-09T00:00:00.000000000', '2021-05-10T00:00:00.000000000',\n       '2021-05-11T00:00:00.000000000', '2021-05-12T00:00:00.000000000',\n       '2021-05-13T00:00:00.000000000', '2021-05-14T00:00:00.000000000',\n       '2021-05-15T00:00:00.000000000', '2021-05-16T00:00:00.000000000',\n       '2021-05-17T00:00:00.000000000', '2021-05-18T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre> <p>Beautiful! Let's see our artificially created turkey problem Bitcoin data.</p> In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(10, 7))\nplot_time_series(timesteps=btc_timesteps_turkey, \n                 values=btc_price_turkey, \n                 format=\"-\", \n                 label=\"BTC Price + Turkey Problem\", \n                 start=2500)\n</pre> plt.figure(figsize=(10, 7)) plot_time_series(timesteps=btc_timesteps_turkey,                   values=btc_price_turkey,                   format=\"-\",                   label=\"BTC Price + Turkey Problem\",                   start=2500) <p>How do you think building a model on this data will go?</p> <p>Remember, all we've changed is a single data point out of our entire dataset.</p> <p>Before we build a model, let's create some windowed datasets with our turkey data.</p> In\u00a0[\u00a0]: Copied! <pre># Create train and test sets for turkey problem data\nfull_windows, full_labels = make_windows(np.array(btc_price_turkey), window_size=WINDOW_SIZE, horizon=HORIZON)\nlen(full_windows), len(full_labels)\n\nX_train, X_test, y_train, y_test = make_train_test_splits(full_windows, full_labels)\nlen(X_train), len(X_test), len(y_train), len(y_test)\n</pre> # Create train and test sets for turkey problem data full_windows, full_labels = make_windows(np.array(btc_price_turkey), window_size=WINDOW_SIZE, horizon=HORIZON) len(full_windows), len(full_labels)  X_train, X_test, y_train, y_test = make_train_test_splits(full_windows, full_labels) len(X_train), len(X_test), len(y_train), len(y_test) Out[\u00a0]: <pre>(2224, 556, 2224, 556)</pre> In\u00a0[\u00a0]: Copied! <pre># Clone model 1 architecture for turkey model and fit the turkey model on the turkey data\nturkey_model = tf.keras.models.clone_model(model_1)\nturkey_model._name = \"Turkey_Model\"\nturkey_model.compile(loss=\"mae\",\n                     optimizer=tf.keras.optimizers.Adam())\nturkey_model.fit(X_train, y_train,\n                 epochs=100,\n                 verbose=0,\n                 validation_data=(X_test, y_test),\n                 callbacks=[create_model_checkpoint(turkey_model.name)])\n</pre> # Clone model 1 architecture for turkey model and fit the turkey model on the turkey data turkey_model = tf.keras.models.clone_model(model_1) turkey_model._name = \"Turkey_Model\" turkey_model.compile(loss=\"mae\",                      optimizer=tf.keras.optimizers.Adam()) turkey_model.fit(X_train, y_train,                  epochs=100,                  verbose=0,                  validation_data=(X_test, y_test),                  callbacks=[create_model_checkpoint(turkey_model.name)]) <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> <pre>INFO:tensorflow:Assets written to: model_experiments/Turkey_Model/assets\n</pre> Out[\u00a0]: <pre>&lt;keras.callbacks.History at 0x7fdc7a4dd550&gt;</pre> In\u00a0[\u00a0]: Copied! <pre># Evaluate turkey model on test data\nturkey_model.evaluate(X_test, y_test)\n</pre> # Evaluate turkey model on test data turkey_model.evaluate(X_test, y_test) <pre>18/18 [==============================] - 0s 2ms/step - loss: 696.1285\n</pre> Out[\u00a0]: <pre>696.1284790039062</pre> In\u00a0[\u00a0]: Copied! <pre># Load best model and evaluate on test data\nturkey_model = tf.keras.models.load_model(\"model_experiments/Turkey_Model/\")\nturkey_model.evaluate(X_test, y_test)\n</pre> # Load best model and evaluate on test data turkey_model = tf.keras.models.load_model(\"model_experiments/Turkey_Model/\") turkey_model.evaluate(X_test, y_test) <pre>18/18 [==============================] - 0s 2ms/step - loss: 638.3047\n</pre> Out[\u00a0]: <pre>638.3046875</pre> <p>Alright, now let's make some predictions with our model and evaluate them on the test data.</p> In\u00a0[\u00a0]: Copied! <pre># Make predictions with Turkey model\nturkey_preds = make_preds(turkey_model, X_test)\nturkey_preds[:10]\n</pre> # Make predictions with Turkey model turkey_preds = make_preds(turkey_model, X_test) turkey_preds[:10] Out[\u00a0]: <pre>&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=\narray([8858.391, 8803.98 , 9039.575, 8785.937, 8778.044, 8735.638,\n       8684.118, 8558.659, 8461.373, 8542.206], dtype=float32)&gt;</pre> In\u00a0[\u00a0]: Copied! <pre># Evaluate turkey preds\nturkey_results = evaluate_preds(y_true=y_test,\n                                y_pred=turkey_preds)\nturkey_results\n</pre> # Evaluate turkey preds turkey_results = evaluate_preds(y_true=y_test,                                 y_pred=turkey_preds) turkey_results Out[\u00a0]: <pre>{'mae': 17144.766,\n 'mape': 121.58286,\n 'mase': 26.53158,\n 'mse': 615487800.0,\n 'rmse': 23743.305}</pre> <p>And with just one value change, our error metrics go through the roof.</p> <p>To make sure, let's remind ourselves of how <code>model_1</code> went on unmodified Bitcoin data (no turkey problem).</p> In\u00a0[\u00a0]: Copied! <pre>model_1_results\n</pre> model_1_results Out[\u00a0]: <pre>{'mae': 568.95123,\n 'mape': 2.5448983,\n 'mase': 0.9994897,\n 'mse': 1171744.0,\n 'rmse': 1082.4713}</pre> <p>By changing just one value, the <code>turkey_model</code> MAE increases almost 30x over <code>model_1</code>.</p> <p>Finally, we'll visualize the turkey predictions over the test turkey data.</p> In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(10, 7))\n# plot_time_series(timesteps=btc_timesteps_turkey[:split_size], values=btc_price_turkey[:split_size], label=\"Train Data\")\noffset=300\nplot_time_series(timesteps=btc_timesteps_turkey[-len(X_test):], \n                 values=btc_price_turkey[-len(y_test):], \n                 format=\"-\", \n                 label=\"Turkey Test Data\", start=offset)\nplot_time_series(timesteps=btc_timesteps_turkey[-len(X_test):],\n                 values=turkey_preds, \n                 label=\"Turkey Preds\", \n                 start=offset);\n</pre> plt.figure(figsize=(10, 7)) # plot_time_series(timesteps=btc_timesteps_turkey[:split_size], values=btc_price_turkey[:split_size], label=\"Train Data\") offset=300 plot_time_series(timesteps=btc_timesteps_turkey[-len(X_test):],                   values=btc_price_turkey[-len(y_test):],                   format=\"-\",                   label=\"Turkey Test Data\", start=offset) plot_time_series(timesteps=btc_timesteps_turkey[-len(X_test):],                  values=turkey_preds,                   label=\"Turkey Preds\",                   start=offset); <p>Why does this happen?</p> <p>Why does our model fail to capture the turkey problem data point?</p> <p>Think about it like this, just like a turkey who lives 1000 joyful days, based on observation alone has no reason to believe day 1001 won't be as joyful as the last, a model which has been trained on historical data of Bitcoin which has no single event where the price decreased by 100x in a day, has no reason to predict it will in the future.</p> <p>A model cannot predict anything in the future outside of the distribution it was trained on.</p> <p>In turn, highly unlikely price movements (based on historical movements), upward or downward will likely never be part of a forecast.</p> <p>However, as we've seen, despite their unlikeliness, these events can have huuuuuuuuge impacts to the performance of our models.</p> <p>\ud83d\udcd6 Resource: For a great article which discusses Black Swan events and how they often get ignored due to the assumption that historical events come from a certain distribution and that future events will come from the same distribution see Black Swans, Normal Distributions and Supply Chain Risk by Spend Matters.</p> In\u00a0[\u00a0]: Copied! <pre># Compare different model results (w = window, h = horizon, e.g. w=7 means a window size of 7)\nmodel_results = pd.DataFrame({\"naive_model\": naive_results,\n                              \"model_1_dense_w7_h1\": model_1_results,\n                              \"model_2_dense_w30_h1\": model_2_results,\n                              \"model_3_dense_w30_h7\": model_3_results,\n                              \"model_4_CONV1D\": model_4_results,\n                              \"model_5_LSTM\": model_5_results,\n                              \"model_6_multivariate\": model_6_results,\n                              \"model_8_NBEATs\": model_7_results,\n                              \"model_9_ensemble\": ensemble_results,\n                              \"model_10_turkey\": turkey_results}).T\nmodel_results.head(10)\n</pre> # Compare different model results (w = window, h = horizon, e.g. w=7 means a window size of 7) model_results = pd.DataFrame({\"naive_model\": naive_results,                               \"model_1_dense_w7_h1\": model_1_results,                               \"model_2_dense_w30_h1\": model_2_results,                               \"model_3_dense_w30_h7\": model_3_results,                               \"model_4_CONV1D\": model_4_results,                               \"model_5_LSTM\": model_5_results,                               \"model_6_multivariate\": model_6_results,                               \"model_8_NBEATs\": model_7_results,                               \"model_9_ensemble\": ensemble_results,                               \"model_10_turkey\": turkey_results}).T model_results.head(10) Out[\u00a0]: mae mse rmse mape mase naive_model 567.980225 1.147547e+06 1071.236206 2.516525 0.999570 model_1_dense_w7_h1 568.951233 1.171744e+06 1082.471313 2.544898 0.999490 model_2_dense_w30_h1 608.961487 1.281439e+06 1132.006470 2.769339 1.064471 model_3_dense_w30_h7 1237.506348 5.405198e+06 1425.747681 5.558878 2.202074 model_4_CONV1D 570.828308 1.176671e+06 1084.744751 2.559336 1.002787 model_5_LSTM 596.644653 1.273487e+06 1128.488770 2.683845 1.048139 model_6_multivariate 567.587402 1.161688e+06 1077.816528 2.541387 0.997094 model_8_NBEATs 585.499817 1.179492e+06 1086.043945 2.744519 1.028561 model_9_ensemble 567.442322 1.144513e+06 1069.819092 2.584332 0.996839 model_10_turkey 17144.765625 6.154878e+08 23743.304688 121.582863 26.531580 In\u00a0[\u00a0]: Copied! <pre># Sort model results by MAE and plot them\nmodel_results[[\"mae\"]].sort_values(by=\"mae\").plot(figsize=(10, 7), kind=\"bar\");\n</pre> # Sort model results by MAE and plot them model_results[[\"mae\"]].sort_values(by=\"mae\").plot(figsize=(10, 7), kind=\"bar\"); <p>The majority of our deep learning models perform on par or only slightly better than the naive model. And for the turkey model, changing a single data point destroys its performance.</p> <p>\ud83d\udd11 Note: Just because one type of model performs better here doesn't mean it'll perform the best elsewhere (and vice versa, just because one model performs poorly here, doesn't mean it'll perform poorly elsewhere).</p> <p>As I said at the start, this is not financial advice.</p> <p>After what we've gone through, you'll now have some of the skills required to callout BS for any future tutorial or blog post or investment sales guide claiming to have model which is able to predict the futrue.</p> <p>Mark Saroufim's Tweet sums this up nicely (stock market forecasting with a machine learning model is just as reliable as palm reading).</p> <p> Beware the tutorials or trading courses which claim to use some kind of algorithm to beat the market (an open system), they're likely a scam or the creator is very lucky and hasn't yet come across a turkey problem.</p> <p>Don't let these results get you down though, forecasting in a closed system (such as predicting the demand of electricity) often yields quite usable results.</p> <p>If anything, this module teaches anti-knowledge. Knowing that forecasting methods usually don't perform well in open systems.</p> <p>Plus, sometimes not knowing the future is a benefit. A known future is already the past.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#10-milestone-project-3-time-series-forecasting-in-tensorflow-bitpredict","title":"10. Milestone Project 3: Time series forecasting in TensorFlow (BitPredict \ud83d\udcb0\ud83d\udcc8)\u00b6","text":"<p>The goal of this notebook is to get you familiar with working with time series data.</p> <p>We're going to be building a series of models in an attempt to predict the price of Bitcoin.</p> <p>Welcome to Milestone Project 3, BitPredict \ud83d\udcb0\ud83d\udcc8!</p> <p>\ud83d\udd11 Note: \u26a0\ufe0f This is not financial advice, as you'll see time series forecasting for stock market prices is actually quite terrible.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#what-is-a-time-series-problem","title":"What is a time series problem?\u00b6","text":"<p>Time series problems deal with data over time.</p> <p>Such as, the number of staff members in a company over 10-years, sales of computers for the past 5-years, electricity usage for the past 50-years.</p> <p>The timeline can be short (seconds/minutes) or long (years/decades). And the problems you might investigate using can usually be broken down into two categories.</p> <p></p> Problem Type Examples Output Classification Anomaly detection, time series identification (where did this time series come from?) Discrete (a label) Forecasting Predicting stock market prices, forecasting future demand for a product, stocking inventory requirements Continuous (a number) <p>In both cases above, a supervised learning approach is often used. Meaning, you'd have some example data and a label assosciated with that data.</p> <p>For example, in forecasting the price of Bitcoin, your data could be the historical price of Bitcoin for the past month and the label could be today's price (the label can't be tomorrow's price because that's what we'd want to predict).</p> <p>Can you guess what kind of problem BitPredict \ud83d\udcb0\ud83d\udcc8 is?</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#what-were-going-to-cover","title":"What we're going to cover\u00b6","text":"<p>Are you ready?</p> <p>We've got a lot to go through.</p> <ul> <li>Get time series data (the historical price of Bitcoin)<ul> <li>Load in time series data using pandas/Python's CSV module</li> </ul> </li> <li>Format data for a time series problem<ul> <li>Creating training and test sets (the wrong way)</li> <li>Creating training and test sets (the right way)</li> <li>Visualizing time series data</li> <li>Turning time series data into a supervised learning problem (windowing)</li> <li>Preparing univariate and multivariate (more than one variable) data</li> </ul> </li> <li>Evaluating a time series forecasting model</li> <li>Setting up a series of deep learning modelling experiments<ul> <li>Dense (fully-connected) networks</li> <li>Sequence models (LSTM and 1D CNN)</li> <li>Ensembling (combining multiple models together)</li> <li>Multivariate models</li> <li>Replicating the N-BEATS algorithm using TensorFlow layer subclassing</li> </ul> </li> <li>Creating a modelling checkpoint to save the best performing model during training</li> <li>Making predictions (forecasts) with a time series model</li> <li>Creating prediction intervals for time series model forecasts</li> <li>Discussing two different types of uncertainty in machine learning (data uncertainty and model uncertainty)</li> <li>Demonstrating why forecasting in an open system is BS (the turkey problem)</li> </ul>"},{"location":"10_time_series_forecasting_in_tensorflow/#how-you-can-use-this-notebook","title":"How you can use this notebook\u00b6","text":"<p>You can read through the descriptions and the code (it should all run), but there's a better option.</p> <p>Write all of the code yourself.</p> <p>Yes. I'm serious. Create a new notebook, and rewrite each line by yourself. Investigate it, see if you can break it, why does it break?</p> <p>You don't have to write the text descriptions but writing the code yourself is a great way to get hands-on experience.</p> <p>Don't worry if you make mistakes, we all do. The way to get better and make less mistakes is to write more code.</p> <p>\ud83d\udcd6 Resource: Get all of the materials you need for this notebook on the course GitHub.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#check-for-gpu","title":"Check for GPU\u00b6","text":"<p>In order for our deep learning models to run as fast as possible, we'll need access to a GPU.</p> <p>In Google Colab, you can set this up by going to Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU.</p> <p>After selecting GPU, you may have to restart the runtime.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#get-data","title":"Get data\u00b6","text":"<p>To build a time series forecasting model, the first thing we're going to need is data.</p> <p>And since we're trying to predict the price of Bitcoin, we'll need Bitcoin data.</p> <p>Specifically, we're going to get the prices of Bitcoin from 01 October 2013 to 18 May 2021.</p> <p>Why these dates?</p> <p>Because 01 October 2013 is when our data source (Coindesk) started recording the price of Bitcoin and 18 May 2021 is when this notebook was created.</p> <p>If you're going through this notebook at a later date, you'll be able to use what you learn to predict on later dates of Bitcoin, you'll just have to adjust the data source.</p> <p>\ud83d\udcd6 Resource: To get the Bitcoin historical data, I went to the Coindesk page for Bitcoin prices, clicked on \"all\" and then clicked on \"Export data\" and selected \"CSV\".</p> <p>You can find the data we're going to use on GitHub.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#importing-time-series-data-with-pandas","title":"Importing time series data with pandas\u00b6","text":"<p>Now we've got some data to work with, let's import it using pandas so we can visualize it.</p> <p>Because our data is in CSV (comma separated values) format (a very common data format for time series), we'll use the pandas <code>read_csv()</code> function.</p> <p>And because our data has a date component, we'll tell pandas to parse the dates using the <code>parse_dates</code> parameter passing it the name our of the date column (\"Date\").</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#importing-time-series-data-with-pythons-csv-module","title":"Importing time series data with Python's CSV module\u00b6","text":"<p>If your time series data comes in CSV form you don't necessarily have to use pandas.</p> <p>You can use Python's in-built <code>csv</code> module. And if you're working with dates, you might also want to use Python's <code>datetime</code>.</p> <p>Let's see how we can replicate the plot we created before except this time using Python's <code>csv</code> and <code>datetime</code> modules.</p> <p>\ud83d\udcd6 Resource: For a great guide on using Python's <code>csv</code> module, check out Real Python's tutorial on Reading and Writing CSV files in Python.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#format-data-part-1-creatining-train-and-test-sets-for-time-series-data","title":"Format Data Part 1: Creatining train and test sets for time series data\u00b6","text":"<p>Alrighty. What's next?</p> <p>If you guessed preparing our data for a model, you'd be right.</p> <p>What's the most important first step for preparing any machine learning dataset?</p> <p>Scaling?</p> <p>No...</p> <p>Removing outliers?</p> <p>No...</p> <p>How about creating train and test splits?</p> <p>Yes!</p> <p>Usually, you could create a train and test split using a function like Scikit-Learn's outstanding <code>train_test_split()</code> but as we'll see in a moment, this doesn't really cut it for time series data.</p> <p>But before we do create splits, it's worth talking about what kind of data we have.</p> <p>In time series problems, you'll either have univariate or multivariate data.</p> <p>Can you guess what our data is?</p> <ul> <li>Univariate time series data deals with one variable, for example, using the price of Bitcoin to predict the price of Bitcoin.</li> <li>Multivariate time series data deals with more than one variable, for example, predicting electricity demand using the day of week, time of year and number of houses in a region.</li> </ul> <p> Example of univariate and multivariate time series data. Univariate involves using the target to predict the target. Multivariate inolves using the target as well as another time series to predict the target.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#create-train-test-sets-for-time-series-the-wrong-way","title":"Create train &amp; test sets for time series (the wrong way)\u00b6","text":"<p>Okay, we've figured out we're dealing with a univariate time series, so we only have to make a split on one variable (for multivariate time series, you will have to split multiple variables).</p> <p>How about we first see the wrong way for splitting time series data?</p> <p>Let's turn our DataFrame index and column into NumPy arrays.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#create-train-test-sets-for-time-series-the-right-way","title":"Create train &amp; test sets for time series (the right way)\u00b6","text":"<p>Of course, there's no way we can actually access data from the future.</p> <p>But we can engineer our test set to be in the future with respect to the training set.</p> <p>To do this, we can create an abitrary point in time to split our data.</p> <p>Everything before the point in time can be considered the training set and everything after the point in time can be considered the test set.</p> <p> Demonstration of time series split. Rather than a traditionaly random train/test split, it's best to split the time series data sequentially. Meaning, the test data should be data from the future when compared to the training data.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#create-a-plotting-function","title":"Create a plotting function\u00b6","text":"<p>Rather than retyping <code>matplotlib</code> commands to continuously plot data, let's make a plotting function we can reuse later.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#modelling-experiments","title":"Modelling Experiments\u00b6","text":"<p>We can build almost any kind of model for our problem as long as the data inputs and outputs are formatted correctly.</p> <p>However, just because we can build almost any kind of model, doesn't mean it'll perform well/should be used in a production setting.</p> <p>We'll see what this means as we build and evaluate models throughout.</p> <p>Before we discuss what modelling experiments we're going to run, there are two terms you should be familiar with, horizon and window.</p> <ul> <li>horizon = number of timesteps to predict into future</li> <li>window = number of timesteps from past used to predict horizon</li> </ul> <p>For example, if we wanted to predict the price of Bitcoin for tomorrow (1 day in the future) using the previous week's worth of Bitcoin prices (7 days in the past), the horizon would be 1 and the window would be 7.</p> <p>Now, how about those modelling experiments?</p> Model Number Model Type Horizon size Window size Extra data 0 Na\u00efve model (baseline) NA NA NA 1 Dense model 1 7 NA 2 Same as 1 1 30 NA 3 Same as 1 7 30 NA 4 Conv1D 1 7 NA 5 LSTM 1 7 NA 6 Same as 1 (but with multivariate data) 1 7 Block reward size 7 N-BEATs Algorithm 1 7 NA 8 Ensemble (multiple models optimized on different loss functions) 1 7 NA 9 Future prediction model (model to predict future values) 1 7 NA 10 Same as 1 (but with turkey \ud83e\udd83 data introduced) 1 7 NA <p>\ud83d\udd11 Note: To reiterate, as you can see, we can build many types of models for the data we're working with. But that doesn't mean that they'll perform well. Deep learning is a powerful technique but it doesn't always work. And as always, start with a simple model first and then add complexity as needed.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#model-0-naive-forecast-baseline","title":"Model 0: Na\u00efve forecast (baseline)\u00b6","text":"<p>As usual, let's start with a baseline.</p> <p>One of the most common baseline models for time series forecasting, the na\u00efve model (also called the na\u00efve forecast), requires no training at all.</p> <p>That's because all the na\u00efve model does is use the previous timestep value to predict the next timestep value.</p> <p>The formula looks like this:</p> <p>$$\\hat{y}_{t} = y_{t-1}$$</p> <p>In English:</p> <p>The prediction at timestep <code>t</code> (y-hat) is equal to the value at timestep <code>t-1</code> (the previous timestep).</p> <p>Sound simple?</p> <p>Maybe not.</p> <p>In an open system (like a stock market or crypto market), you'll often find beating the na\u00efve forecast with any kind of model is quite hard.</p> <p>\ud83d\udd11 Note: For the sake of this notebook, an open system is a system where inputs and outputs can freely flow, such as a market (stock or crypto). Where as, a closed system the inputs and outputs are contained within the system (like a poker game with your buddies, you know the buy in and you know how much the winner can get). Time series forecasting in open systems is generally quite poor.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#evaluating-a-time-series-model","title":"Evaluating a time series model\u00b6","text":"<p>Time series forecasting often involves predicting a number (in our case, the price of Bitcoin).</p> <p>And what kind of problem is predicting a number?</p> <p>Ten points if you said regression.</p> <p>With this known, we can use regression evaluation metrics to evaluate our time series forecasts.</p> <p>The main thing we will be evaluating is: how do our model's predictions (<code>y_pred</code>) compare against the actual values (<code>y_true</code> or ground truth values)?</p> <p>\ud83d\udcd6 Resource: We're going to be using several metrics to evaluate our different model's time series forecast accuracy. Many of them are sourced and explained mathematically and conceptually in Forecasting: Principles and Practice chapter 5.8, I'd recommend reading through here for a more in-depth overview of what we're going to practice.</p> <p>For all of the following metrics, lower is better (for example an MAE of 0 is better than an MAE 100).</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#scale-dependent-errors","title":"Scale-dependent errors\u00b6","text":"<p>These are metrics which can be used to compare time series values and forecasts that are on the same scale.</p> <p>For example, Bitcoin historical prices in USD veresus Bitcoin forecast values in USD.</p> Metric Details Code MAE (mean absolute error) Easy to interpret (a forecast is X amount different from actual amount). Forecast methods which minimises the MAE will lead to forecasts of the median. <code>tf.keras.metrics.mean_absolute_error()</code> RMSE (root mean square error) Forecasts which minimise the RMSE lead to forecasts of the mean. <code>tf.sqrt(</code><code>tf.keras.metrics.mean_square_error()</code><code>)</code>"},{"location":"10_time_series_forecasting_in_tensorflow/#percentage-errors","title":"Percentage errors\u00b6","text":"<p>Percentage errors do not have units, this means they can be used to compare forecasts across different datasets.</p> Metric Details Code MAPE (mean absolute percentage error) Most commonly used percentage error. May explode (not work) if <code>y=0</code>. <code>tf.keras.metrics.mean_absolute_percentage_error()</code> sMAPE (symmetric mean absolute percentage error) Recommended not to be used by Forecasting: Principles and Practice, though it is used in forecasting competitions. Custom implementation"},{"location":"10_time_series_forecasting_in_tensorflow/#scaled-errors","title":"Scaled errors\u00b6","text":"<p>Scaled errors are an alternative to percentage errors when comparing forecast performance across different time series.</p> Metric Details Code MASE (mean absolute scaled error). MASE equals one for the naive forecast (or very close to one). A forecast which performs better than the na\u00efve should get &lt;1 MASE. See sktime's <code>mase_loss()</code> <p>\ud83e\udd14 Question: There are so many metrics... which one should I pay most attention to? It's going to depend on your problem. However, since its ease of interpretation (you can explain it in a sentence to your grandma), MAE is often a very good place to start.</p> <p>Since we're going to be evaluing a lot of models, let's write a function to help us calculate evaluation metrics on their forecasts.</p> <p>First we'll need TensorFlow.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#other-kinds-of-time-series-forecasting-models-which-can-be-used-for-baselines-and-actual-forecasts","title":"Other kinds of time series forecasting models which can be used for baselines and actual forecasts\u00b6","text":"<p>Since we've got a na\u00efve forecast baseline to work with, it's time we start building models to try and beat it.</p> <p>And because this course is focused on TensorFlow and deep learning, we're going to be using TensorFlow to build deep learning models to try and improve on our na\u00efve forecasting results.</p> <p>That being said, there are many other kinds of models you may want to look into for building baselines/performing forecasts.</p> <p>Some of them may even beat our best performing models in this notebook, however, I'll leave trying them out for extra-curriculum.</p> Model/Library Name Resource Moving average https://machinelearningmastery.com/moving-average-smoothing-for-time-series-forecasting-python/ ARIMA (Autoregression Integrated Moving Average) https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/ sktime (Scikit-Learn for time series) https://github.com/alan-turing-institute/sktime TensorFlow Decision Forests (random forest, gradient boosting trees) https://www.tensorflow.org/decision_forests Facebook Kats (purpose-built forecasting and time series analysis library by Facebook) https://github.com/facebookresearch/Kats LinkedIn Greykite (flexible, intuitive and fast forecasts) https://github.com/linkedin/greykite"},{"location":"10_time_series_forecasting_in_tensorflow/#format-data-part-2-windowing-dataset","title":"Format Data Part 2: Windowing dataset\u00b6","text":"<p>Surely we'd be ready to start building models by now?</p> <p>We're so close! Only one more step (really two) to go.</p> <p>We've got to window our time series.</p> <p>Why do we window?</p> <p>Windowing is a method to turn a time series dataset into supervised learning problem.</p> <p>In other words, we want to use windows of the past to predict the future.</p> <p>For example for a univariate time series, windowing for one week (<code>window=7</code>) to predict the next single value (<code>horizon=1</code>) might look like:</p> <pre><code>Window for one week (univariate time series)\n\n[0, 1, 2, 3, 4, 5, 6] -&gt; [7]\n[1, 2, 3, 4, 5, 6, 7] -&gt; [8]\n[2, 3, 4, 5, 6, 7, 8] -&gt; [9]\n</code></pre> <p>Or for the price of Bitcoin, it'd look like:</p> <pre><code>Window for one week with the target of predicting the next day (Bitcoin prices)\n\n[123.654, 125.455, 108.584, 118.674, 121.338, 120.655, 121.795] -&gt; [123.033]\n[125.455, 108.584, 118.674, 121.338, 120.655, 121.795, 123.033] -&gt; [124.049]\n[108.584, 118.674, 121.338, 120.655, 121.795, 123.033, 124.049] -&gt; [125.961]\n</code></pre> <p> Example of windows and horizons for Bitcoin data. Windowing can be used to turn time series data into a supervised learning problem.</p> <p>Let's build some functions which take in a univariate time series and turn it into windows and horizons of specified sizes.</p> <p>We'll start with the default horizon size of 1 and a window size of 7 (these aren't necessarily the best values to use, I've just picked them).</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#turning-windows-into-training-and-test-sets","title":"Turning windows into training and test sets\u00b6","text":"<p>Look how good those windows look! Almost like the stain glass windows on the Sistine Chapel, well, maybe not that good but still.</p> <p>Time to turn our windows into training and test splits.</p> <p>We could've windowed our existing training and test splits, however, with the nature of windowing (windowing often requires an offset at some point in the data), it usually works better to window the data first, then split it into training and test sets.</p> <p>Let's write a function which takes in full sets of windows and their labels and splits them into train and test splits.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#make-a-modelling-checkpoint","title":"Make a modelling checkpoint\u00b6","text":"<p>We're so close to building models. So so so close.</p> <p>Because our model's performance will fluctuate from experiment to experiment, we'll want to make sure we're comparing apples to apples.</p> <p>What I mean by this is in order for a fair comparison, we want to compare each model's best performance against each model's best performance.</p> <p>For example, if <code>model_1</code> performed incredibly well on epoch 55 but its performance fell off toward epoch 100, we want the version of the model from epoch 55 to compare to other models rather than the version of the model from epoch 100.</p> <p>And the same goes for each of our other models: compare the best against the best.</p> <p>To take of this, we'll implement a <code>ModelCheckpoint</code> callback.</p> <p>The <code>ModelCheckpoint</code> callback will monitor our model's performance during training and save the best model to file by setting <code>save_best_only=True</code>.</p> <p>That way when evaluating our model we could restore its best performing configuration from file.</p> <p>\ud83d\udd11 Note: Because of the size of the dataset (smaller than usual), you'll notice our modelling experiment results fluctuate quite a bit during training (hence the implementation of the <code>ModelCheckpoint</code> callback to save the best model).</p> <p>Because we're going to be running multiple experiments, it makes sense to keep track of them by saving models to file under different names.</p> <p>To do this, we'll write a small function to create a <code>ModelCheckpoint</code> callback which saves a model to specified filename.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#model-1-dense-model-window-7-horizon-1","title":"Model 1: Dense model (window = 7, horizon = 1)\u00b6","text":"<p>Finally!</p> <p>Time to build one of our models.</p> <p>If you think we've been through a fair bit of preprocessing before getting here, you're right.</p> <p>Often, preparing data for a model is one of the largest parts of any machine learning project.</p> <p>And once you've got a good model in place, you'll probably notice far more improvements from manipulating the data (e.g. collecting more, improving the quality) than manipulating the model.</p> <p>We're going to start by keeping it simple, <code>model_1</code> will have:</p> <ul> <li>A single dense layer with 128 hidden units and ReLU (rectified linear unit) activation</li> <li>An output layer with linear activation (or no activation)</li> <li>Adam optimizer and MAE loss function</li> <li>Batch size of 128</li> <li>100 epochs</li> </ul> <p>Why these values?</p> <p>I picked them out of experimentation.</p> <p>A batch size of 32 works pretty well too and we could always train for less epochs but since the model runs so fast (you'll see in a second, it's because the number of samples we have isn't massive) we might as well train for more.</p> <p>\ud83d\udd11 Note: As always, many of the values for machine learning problems are experimental. A reminder that the values you can set yourself in a machine learning algorithm (the hidden units, the batch size, horizon size, window size) are called hyperparameters. And experimenting to find the best values for hyperparameters is called hyperparameter tuning. Where as parameters learned by a model itself (patterns in the data, formally called weights &amp; biases) are referred to as parameters.</p> <p>Let's import TensorFlow and build our first deep learning model for time series.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#making-forecasts-with-a-model-on-the-test-dataset","title":"Making forecasts with a model (on the test dataset)\u00b6","text":"<p>We've trained a model and evaluated the it on the test data, but the project we're working on is called BitPredict \ud83d\udcb0\ud83d\udcc8 so how do you think we could use our model to make predictions?</p> <p>Since we're going to be running more modelling experiments, let's write a function which:</p> <ol> <li>Takes in a trained model (just like <code>model_1</code>)</li> <li>Takes in some input data (just like the data the model was trained on)</li> <li>Passes the input data to the model's <code>predict()</code> method</li> <li>Returns the predictions</li> </ol>"},{"location":"10_time_series_forecasting_in_tensorflow/#model-2-dense-window-30-horizon-1","title":"Model 2: Dense (window = 30, horizon = 1)\u00b6","text":"<p>A na\u00efve model is currently beating our handcrafted deep learning model.</p> <p>We can't let this happen.</p> <p>Let's continue our modelling experiments.</p> <p>We'll keep the previous model architecture but use a window size of 30.</p> <p>In other words, we'll use the previous 30 days of Bitcoin prices to try and predict the next day price.</p> <p> Example of Bitcoin prices windowed for 30 days to predict a horizon of 1.</p> <p>\ud83d\udd11 Note: Recall from before, the window size (how many timesteps to use to fuel a forecast) and the horizon (how many timesteps to predict into the future) are hyperparameters. This means you can tune them to try and find values will result in better performance.</p> <p>We'll start our second modelling experiment by preparing datasets using the functions we created earlier.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#model-3-dense-window-30-horizon-7","title":"Model 3: Dense (window = 30, horizon = 7)\u00b6","text":"<p>Let's try and predict 7 days ahead given the previous 30 days.</p> <p>First, we'll update the <code>HORIZON</code> and <code>WINDOW_SIZE</code> variables and create windowed data.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#make-our-evaluation-function-work-for-larger-horizons","title":"Make our evaluation function work for larger horizons\u00b6","text":"<p>You'll notice the outputs for <code>model_3_results</code> are multi-dimensional.</p> <p>This is because the predictions are getting evaluated across the <code>HORIZON</code> timesteps (7 predictions at a time).</p> <p>To fix this, let's adjust our <code>evaluate_preds()</code> function to work with multiple shapes of data.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#which-of-our-models-is-performing-best-so-far","title":"Which of our models is performing best so far?\u00b6","text":"<p>So far, we've trained 3 models which use the same architecture but use different data inputs.</p> <p>Let's compare them with the na\u00efve model to see which model is performing the best so far.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#model-4-conv1d","title":"Model 4: Conv1D\u00b6","text":"<p>Onto the next modelling experiment!</p> <p>This time, we'll be using a Conv1D model. Because as we saw in the sequence modelling notebook, Conv1D models can be used for seq2seq (sequence to sequence) problems.</p> <p>In our case, the input sequence is the previous 7 days of Bitcoin price data and the output is the next day (in seq2seq terms this is called a many to one problem).</p> <p> Framing Bitcoin forecasting in seq2seq (sequence to sequence) terms. Using a window size of 7 and a horizon of one results in a many to one problem. Using a window size of &gt;1 and a horizon of &gt;1 results in a many to many problem. The diagram comes from Andrei Karpathy's The Unreasonable Effectiveness of Recurrent Neural Networks.</p> <p>Before we build a Conv1D model, let's recreate our datasets.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#model-5-rnn-lstm","title":"Model 5: RNN (LSTM)\u00b6","text":"<p>As you might've guessed, we can also use a recurrent neural network to model our sequential time series data.</p> <p>\ud83d\udcd6 Resource: For more on the different types of recurrent neural networks you can use for sequence problems, see the Recurrent Neural Networks section of notebook 08.</p> <p>Let's reuse the same data we used for the Conv1D model, except this time we'll create an LSTM-cell powered RNN to model our Bitcoin data.</p> <p>Once again, one of the most important steps for the LSTM model will be getting our data into the right shape.</p> <p>The <code>tf.keras.layers.LSTM()</code> layer takes a tensor with <code>[batch, timesteps, feature]</code> dimensions.</p> <p>As mentioned earlier, the <code>batch</code> dimension gets taken care of for us but our data is currently only has the <code>feature</code> dimension (<code>WINDOW_SIZE</code>).</p> <p>To fix this, just like we did with the <code>Conv1D</code> model, we can use a <code>tf.keras.layers.Lambda()</code> layer to adjust the shape of our input tensors to the LSTM layer.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#make-a-multivariate-time-series","title":"Make a multivariate time series\u00b6","text":"<p>So far all of our models have barely kept up with the na\u00efve forecast.</p> <p>And so far all of them have been trained on a single variable (also called univariate time series): the historical price of Bitcoin.</p> <p>If predicting the price of Bitcoin using the price of Bitcoin hasn't worked out very well, maybe giving our model more information may help.</p> <p>More information is a vague term because we could actually feed almost anything to our model(s) and they would still try to find patterns.</p> <p>For example, we could use the historical price of Bitcoin as well as anyone with the name Daniel Bourke Tweeted on that day to predict the future price of Bitcoin.</p> <p>But would this help?</p> <p>Porbably not.</p> <p>What would be better is if we passed our model something related to Bitcoin (again, this is quite vauge, since in an open system like a market, you could argue everything is related).</p> <p>This will be different for almost every time series you work on but in our case, we could try to see if the Bitcoin block reward size adds any predictive power to our model(s).</p> <p>What is the Bitcoin block reward size?</p> <p>The Bitcoin block reward size is the number of Bitcoin someone receives from mining a Bitcoin block.</p> <p>At its inception, the Bitcoin block reward size was 50.</p> <p>But every four years or so, the Bitcoin block reward halves.</p> <p>For example, the block reward size went from 50 (starting January 2009) to 25 on November 28 2012.</p> <p>Let's encode this information into our time series data and see if it helps a model's performance.</p> <p>\ud83d\udd11 Note: Adding an extra feature to our dataset such as the Bitcoin block reward size will take our data from univariate (only the historical price of Bitcoin) to multivariate (the price of Bitcoin as well as the block reward size).</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#making-a-windowed-dataset-with-pandas","title":"Making a windowed dataset with pandas\u00b6","text":"<p>Previously, we used some custom made functions to window our univariate time series.</p> <p>However, since we've just added another variable to our dataset, these functions won't work.</p> <p>Not to worry though. Since our data is in a pandas DataFrame, we can leverage the <code>pandas.DataFrame.shift()</code> method to create a windowed multivariate time series.</p> <p>The <code>shift()</code> method offsets an index by a specified number of periods.</p> <p>Let's see it in action.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#model-6-dense-multivariate-time-series","title":"Model 6: Dense (multivariate time series)\u00b6","text":"<p>To keep things simple, let's the <code>model_1</code> architecture and use it to train and make predictions on our multivariate time series data.</p> <p>By replicating the <code>model_1</code> architecture we'll be able to see whether or not adding the block reward feature improves or detracts from model performance.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#model-7-n-beats-algorithm","title":"Model 7: N-BEATS algorithm\u00b6","text":"<p>Time to step things up a notch.</p> <p>So far we've tried a bunch of smaller models, models with only a couple of layers.</p> <p>But one of the best ways to improve a model's performance is to increase the number of layers in it.</p> <p>That's exactly what the N-BEATS (Neural Basis Expansion Analysis for Interpretable Time Series Forecasting) algorithm does.</p> <p>The N-BEATS algorithm focuses on univariate time series problems and achieved state-of-the-art performance in the winner of the M4 competition (a forecasting competition).</p> <p>For our next modelling experiment we're going to be replicating the generic architecture of the N-BEATS algorithm (see section 3.3 of the N-BEATS paper).</p> <p>We're not going to go through all of the details in the paper, instead we're going to focus on:</p> <ol> <li>Replicating the model architecture in Figure 1 of the N-BEATS paper</li> </ol> <p> N-BEATS algorithm we're going to replicate with TensorFlow with window (input) and horizon (output) annotations.</p> <ol> <li>Using the same hyperparameters as the paper which can be found in Appendix D of the N-BEATS paper</li> </ol> <p>Doing this will give us an opportunity to practice:</p> <ul> <li>Creating a custom layer for the <code>NBeatsBlock</code> by subclassing <code>tf.keras.layers.Layer</code><ul> <li>Creating a custom layer is helpful for when TensorFlow doesn't already have an existing implementation of a layer or if you'd like to make a layer configuration repeat a number of times (e.g. like a stack of N-BEATS blocks)</li> </ul> </li> <li>Implementing a custom architecture using the Functional API</li> <li>Finding a paper related to our problem and seeing how it goes</li> </ul> <p>\ud83d\udd11 Note: As you'll see in the paper, the authors state \u201cN-BEATS is implemented and trained in TensorFlow\u201d, that's what we'll be doing too!</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#building-and-testing-an-n-beats-block-layer","title":"Building and testing an N-BEATS block layer\u00b6","text":"<p>Let's start by building an N-BEATS block layer, we'll write the code first and then discuss what's going on.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#preparing-data-for-the-n-beats-algorithm-using-tfdata","title":"Preparing data for the N-BEATS algorithm using <code>tf.data</code>\u00b6","text":"<p>We've got the basic building block for the N-BEATS architecture ready to go.</p> <p>But before we use it to replicate the entire N-BEATS generic architecture, let's create some data.</p> <p>This time, because we're going to be using a larger model architecture, to ensure our model training runs as fast as possible, we'll setup our datasets using the <code>tf.data</code> API.</p> <p>And because the N-BEATS algorithm is focused on univariate time series, we'll start by making training and test windowed datasets of Bitcoin prices (just as we've done above).</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#setting-up-hyperparameters-for-n-beats-algorithm","title":"Setting up hyperparameters for N-BEATS algorithm\u00b6","text":"<p>Ho ho, would you look at that! Datasets ready, model building block ready, what'd you say we put things together?</p> <p>Good idea.</p> <p>Okay.</p> <p>Let's go.</p> <p>To begin, we'll create variables for each of the hyperparameters we'll be using for our N-BEATS replica.</p> <p>\ud83d\udcd6 Resource: The following hyperparameters are taken from Figure 1 and Table 18/Appendix D of the N-BEATS paper.</p> <p> Table 18 from N-BEATS paper describing the hyperparameters used for the different variants of N-BEATS. We're using N-BEATS-G which stands for the generic version of N-BEATS.</p> <p>\ud83d\udd11 Note: If you see variables in a machine learning example in all caps, such as \"<code>N_EPOCHS = 100</code>\", these variables are often hyperparameters which are used through the example. You'll usually see them instantiated towards the start of an experiment and then used throughout.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#getting-ready-for-residual-connections","title":"Getting ready for residual connections\u00b6","text":"<p>Beautiful! Hyperparameters ready, now before we create the N-BEATS model, there are two layers to go through which play a large roll in the architecture.</p> <p>They're what make N-BEATS double residual stacking (section 3.2 of the N-BEATS paper) possible:</p> <ul> <li><code>tf.keras.layers.subtract(inputs)</code> - subtracts list of input tensors from each other</li> <li><code>tf.keras.layers.add(inputs)</code> - adds list of input tensors to each other</li> </ul> <p>Let's try them out.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#building-compiling-and-fitting-the-n-beats-algorithm","title":"Building, compiling and fitting the N-BEATS algorithm\u00b6","text":"<p>Okay, we've finally got all of the pieces of the puzzle ready for building and training the N-BEATS algorithm.</p> <p>We'll do so by going through the following:</p> <ol> <li>Setup an instance of the N-BEATS block layer using <code>NBeatsBlock</code> (this'll be the initial block used for the network, the rest will be created as part of stacks)</li> <li>Create an input layer for the N-BEATS stack (we'll be using the Keras Functional API for this)</li> <li>Make the initial backcast and forecasts for the model with the layer created in (1)</li> <li>Use a for loop to create stacks of block layers</li> <li>Use the NBeatsBlock class within the for loop created in (4) to create blocks which return backcasts and block-level forecasts</li> <li>Create the double residual stacking using subtract and add layers</li> <li>Put the model inputs and outputs together using <code>tf.keras.Model()</code></li> <li>Compile the model with MAE loss (the paper uses multiple losses but we'll use MAE to keep it inline with our other models) and Adam optimizer with default settings as per section 5.2 of N-BEATS paper)</li> <li>Fit the N-BEATS model for 5000 epochs and since it's fitting for so many epochs, we'll use a couple of callbacks:</li> </ol> <ul> <li><code>tf.keras.callbacks.EarlyStopping()</code> - stop the model from training if it doesn't improve validation loss for 200 epochs and restore the best performing weights using <code>restore_best_weights=True</code> (this'll prevent the model from training for loooongggggg period of time without improvement)</li> <li><code>tf.keras.callbacks.ReduceLROnPlateau()</code> - if the model's validation loss doesn't improve for 100 epochs, reduce the learning rate by 10x to try and help it make incremental improvements (the smaller the learning rate, the smaller updates a model tries to make)</li> </ul> <p>Woah. A bunch of steps. But I'm sure you're up to it.</p> <p>Let's do it!</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#plotting-the-n-beats-architecture-weve-created","title":"Plotting the N-BEATS architecture we've created\u00b6","text":"<p>You know what would be cool?</p> <p>If we could plot the N-BEATS model we've crafted.</p> <p>Well it turns out we can using <code>tensorflow.keras.utils.plot_model()</code>.</p> <p>Let's see what it looks like.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#model-8-creating-an-ensemble-stacking-different-models-together","title":"Model 8: Creating an ensemble (stacking different models together)\u00b6","text":"<p>After all that effort, the N-BEATS algorithm's performance was underwhelming.</p> <p>But again, this is part of the parcel of machine learning. Not everything will work.</p> <p>That's when we refer back to the motto: experiment, experiment, experiment.</p> <p>Our next experiment is creating an ensemble of models.</p> <p>An ensemble involves training and combining multiple different models on the same problem. Ensemble models are often the types of models you'll see winning data science competitions on websites like Kaggle.</p> <p> Example of the power of ensembling. One Daniel model makes a decision with a smart level of 7 but when a Daniel model teams up with multiple different people, together (ensembled) they make a decision with a smart level of 10. The key here is combining the decision power of people with different backgrounds, if you combined multiple Daniel models, you'd end up with an average smart level of 7. Note: smart level is not an actual measurement of decision making, it is for demonstration purposes only.</p> <p>For example, in the N-BEATS paper, they trained an ensemble of models (180 in total, see section 3.4) to achieve the results they did using a combination of:</p> <ul> <li>Different loss functions (sMAPE, MASE and MAPE)</li> <li>Different window sizes (2 x horizon, 3 x horizon, 4 x horizon...)</li> </ul> <p>The benefit of ensembling models is you get the \"decision of the crowd effect\". Rather than relying on a single model's predictions, you can take the average or median of many different models.</p> <p>The keyword being: different.</p> <p>It wouldn't make sense to train the same model 10 times on the same data and then average the predictions.</p> <p>Fortunately, due to their random initialization, even deep learning models with the same architecture can produce different results.</p> <p>What I mean by this is each time you create a deep learning model, it starts with random patterns (weights &amp; biases) and then it adjusts these random patterns to better suit the dataset it's being trained on.</p> <p>However, the process it adjusts these patterns is often a form of guided randomness as well (the SGD optimizer stands for stochastic or random gradient descent).</p> <p>To create our ensemble models we're going to be using a combination of:</p> <ul> <li>Different loss functions (MAE, MSE, MAPE)</li> <li>Randomly initialized models</li> </ul> <p>Essentially, we'll be creating a suite of different models all attempting to model the same data.</p> <p>And hopefully the combined predictive power of each model is better than a single model on its own.</p> <p>Let's find out!</p> <p>We'll start by creating a function to produce a list of different models trained with different loss functions. Each layer in the ensemble models will be initialized with a random normal (Gaussian) distribution using He normal initialization, this'll help estimating the prediction intervals later on.</p> <p>\ud83d\udd11 Note: In your machine leanring experiments, you may have already dealt with examples of ensemble models. Algorithms such as the random forest model are a form of ensemble, it uses a number of randomly created decision trees where each individual tree may perform poorly but when combined gives great results.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#constructing-and-fitting-an-ensemble-of-models-using-different-loss-functions","title":"Constructing and fitting an ensemble of models (using different loss functions)\u00b6","text":""},{"location":"10_time_series_forecasting_in_tensorflow/#making-predictions-with-an-ensemble-model","title":"Making predictions with an ensemble model\u00b6","text":""},{"location":"10_time_series_forecasting_in_tensorflow/#plotting-the-prediction-intervals-uncertainty-estimates-of-our-ensemble","title":"Plotting the prediction intervals (uncertainty estimates) of our ensemble\u00b6","text":"<p>Right now all of our model's (prior to the ensemble model) are predicting single points.</p> <p>Meaning, given a set of <code>WINDOW_SIZE=7</code> values, the model will predict <code>HORIZION=1</code>.</p> <p>But what might be more helpful than a single value?</p> <p>Perhaps a range of values?</p> <p>For example, if a model is predicting the price of Bitcoin to be 50,000USD tomorrow, would it be helpful to know it's predicting the 50,000USD because it's predicting the price to be between 48,000 and 52,000USD? (note: \"$\" has been omitted from the previous sentence due to formatting issues)</p> <p>Knowing the range of values a model is predicting may help you make better decisions for your forecasts.</p> <p>You'd know that although the model is predicting 50,000USD (a point prediction, or single value in time), the value could actually be within the range 48,000USD to 52,000USD (of course, the value could also be outside of this range as well, but we'll get to that later).</p> <p>These kind of prediction ranges are called prediction intervals or uncertainty estimates. And they're often as important as the forecast itself.</p> <p>Why?</p> <p>Because point predictions are almost always going to be wrong. So having a range of values can help with decision making.</p> <p>\ud83d\udcd6 Resource(s):</p> <ul> <li>The steps we're about to take have been inspired by the Machine Learning Mastery blog post Prediction Intervals for Deep Learning Neural Networks. Check out the post for more options to measure uncertainty with neural networks.</li> <li>For an example of uncertainty estimates being used in the wild, I'd also refer to Uber's Engineering Uncertainty Estimation in Neural Networks for Time Series Prediction at Uber blog post.</li> </ul> <p> Example of how uncertainty estimates and predictions intervals can give an understanding of where point predictions (a single number) may not include all of useful information you'd like to know. For example, your model's point prediction for Uber trips on New Years Eve might be 100 (a made up number) but really, the prediction intervals are between 55 and 153 (both made up for the example). In this case, preparing 100 rides might end up being 53 short (it could even be more, like the point prediction, the prediction intervals are also estimates). The image comes from Uber's blog post on uncertainty estimation in neural networks.</p> <p>One way of getting the 95% condfidnece prediction intervals for a deep learning model is the bootstrap method:</p> <ol> <li>Take the predictions from a number of randomly initialized models (we've got this thanks to our ensemble model)</li> <li>Measure the standard deviation of the predictions</li> <li>Multiply standard deviation by 1.96 (assuming the distribution is Gaussian, 95% of observations fall within 1.96 standard deviations of the mean, this is why we initialized our neural networks with a normal distribution)</li> <li>To get the prediction interval upper and lower bounds, add and subtract the value obtained in (3) to the mean/median of the predictions made in (1)</li> </ol>"},{"location":"10_time_series_forecasting_in_tensorflow/#aside-two-types-of-uncertainty-coconut-and-subway","title":"Aside: two types of uncertainty (coconut and subway)\u00b6","text":"<p>Inheritly, you know you cannot predict the future.</p> <p>That doesn't mean trying to isn't valuable.</p> <p>For many things, future predictions are helpful. Such as knowing the bus you're trying to catch to the library leaves at 10:08am. The time 10:08am is a point prediction, if the bus left at a random time every day, how helpful would it be?</p> <p>Just like saying the price of Bitcoin tomorrow will be 50,000USD is a point prediction.</p> <p>However, as we've discussed knowing a prediction interval or uncertainty estimate can be as helpful or even more helpful than a point prediction itself.</p> <p>Uncertainty estimates seek out to qualitatively and quantitatively answer the questions:</p> <ul> <li>What can my model know? (with perfect data, what's possible to learn?)</li> <li>What doesn't my model know? (what can a model never predict?)</li> </ul> <p>There are two types of uncertainty in machine learning you should be aware of:</p> <ul> <li><p>Aleatoric uncertainty - this type of uncertainty cannot be reduced, it is also referred to as \"data\" or \"subway\" uncertainty.</p> <ul> <li>Let's say your train is scheduled to arrive at 10:08am but very rarely does it arrive at exactly 10:08am. You know it's usually a minute or two either side and perhaps up to 10-minutes late if traffic is bad. Even with all the data you could imagine, this level of uncertainty is still going to be present (much of it being noise).</li> <li>When we measured prediction intervals, we were measuring a form of subway uncertainty for Bitcoin price predictions (a little either side of the point prediction).</li> </ul> </li> <li><p>Epistemic uncertainty - this type of uncertainty can be reduced, it is also referred to as \"model\" or \"coconut\" uncertainty, it is very hard to calculate.</p> <ul> <li>The analogy for coconut uncertainty involves whether or not you'd get hit on the head by a coconut when going to a beach.<ul> <li>If you were at a beach with coconuts trees, as you could imagine, this would be very hard to calculate. How often does a coconut fall of a tree? Where are you standing?</li> <li>But you could reduce this uncertainty to zero by going to a beach without coconuts (collect more data about your situation).</li> </ul> </li> <li>Model uncertainty can be reduced by collecting more data samples/building a model to capture different parameters about the data you're modelling.</li> </ul> </li> </ul> <p>The lines between these are blurred (one type of uncertainty can change forms into the other) and they can be confusing at first but are important to keep in mind for any kind of time series prediction.</p> <p>If you ignore the uncertanties, are you really going to get a reliable prediction?</p> <p>Perhaps another example might help.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#uncertainty-in-dating","title":"Uncertainty in dating\u00b6","text":"<p>Let's say you're going on a First Date Feedback Radio Show to help improve your dating skills.</p> <p>Where you go on a blind first date with a girl (feel free to replace girl with your own preference) and the radio hosts record the date and then playback snippets of where you could've improved.</p> <p>And now let's add a twist.</p> <p>Last week your friend went on the same show. They told you about the girl they met and how the conversation went.</p> <p>Because you're now a machine learning engineer, you decide to build a machine learning model to help you with first date conversations.</p> <p>What levels of uncertainty do we have here?</p> <p>From an aleatory uncertainty (data) point of view, no matter how many conversations of first dates you collect, the conversation you end up having will likely be different to the rest (the best conversations have no subject and appear random).</p> <p>From an epistemic uncertainty (model) point of view, if the date is truly blind and both parties don't know who they're seeing until they meet in person, the epistemic uncertainty would be high. Because now you have no idea who the person you're going to meet is nor what you might talk about.</p> <p>However, the level of epistemic uncertainty would be reduced if your friend told about the girl they went on a date with last week on the show and it turns out you're going on a date with the same girl.</p> <p>But even though you know a little bit about the girl, your aleatory uncertainty (or subway uncertainty) is still high because you're not sure where the conversation will go.</p> <p>If you're wondering where above scenario came from, it happened to me this morning. Good timing right?</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#learning-more-on-uncertainty","title":"Learning more on uncertainty\u00b6","text":"<p>The field of quantifying uncertainty estimation in machine learning is a growing area of research.</p> <p>If you'd like to learn more I'd recommend the following.</p> <p>\ud83d\udcd6 Resources: Places to learn more about uncertainty in machine learning/forecasting:</p> <ul> <li>\ud83c\udfa5 MIT 6.S191: Evidential Deep Learning and Uncertainty</li> <li>Uncertainty quantification on Wikipedia</li> <li>Why you should care about the Nate Silver vs. Nassim Taleb Twitter war by Isaac Faber - a great insight into the role of uncertainty in the example of election prediction.</li> <li>3 facts about time series forecasting that surprise experienced machine learning practitioners by Skander Hannachi - fantastic outline of some of the main mistakes people make when building forecasting models, especially forgetting about uncertainty estimates.</li> <li>Engineering Uncertainty Estimation in Neural Networks for Time Series Prediction at Uber - a discussion on techniques Uber used to engineer uncertainty estimates into their time sereis neural networks.</li> </ul>"},{"location":"10_time_series_forecasting_in_tensorflow/#model-9-train-a-model-on-the-full-historical-data-to-make-predictions-into-future","title":"Model 9: Train a model on the full historical data to make predictions into future\u00b6","text":"<p>What would a forecasting model be worth if we didn't use it to predict into the future?</p> <p>It's time we created a model which is able to make future predictions on the price of Bitcoin.</p> <p>To make predictions into the future, we'll train a model on the full dataset and then get to make predictions to some future horizon.</p> <p>Why use the full dataset?</p> <p>Previously, we split our data into training and test sets to evaluate how our model did on pseudo-future data (the test set).</p> <p>But since the goal of a forecasting model is to predict values into the actual-future, we won't be using a test set.</p> <p>\ud83d\udd11 Note: Forecasting models need to be retrained every time a forecast is made. Why? Because if Bitcoin prices are updated daily and you predict the price for tomorrow. Your model is only really valid for one day. When a new price comes out (e.g. the next day), you'll have to retrain your model to incorporate that new price to predict the next forecast.</p> <p>Let's get some data ready.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#make-predictions-on-the-future","title":"Make predictions on the future\u00b6","text":"<p>Let's predict the future and get rich!</p> <p>Well... maybe not.</p> <p>As you've seen so far, our machine learning models have performed quite poorly at predicting the price of Bitcoin (time series forecasting in open systems is typically a game of luck), often worse than the naive forecast.</p> <p>That doesn't mean we can't use our models to try and predict into the future right?</p> <p>To do so, let's start by defining a variable <code>INTO_FUTURE</code> which decides how many timesteps we'd like to predict into the future.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#plot-future-forecasts","title":"Plot future forecasts\u00b6","text":"<p>This is so exciting! Forecasts made!</p> <p>But right now, they're just numbers on a page.</p> <p>Let's bring them to life by adhering to the data explorer's motto: visualize, visualize, visualize!</p> <p>To plot our model's future forecasts against the historical data of Bitcoin, we're going to need a series of future dates (future dates from the final date of where our dataset ends).</p> <p>How about we create a function to return a date range from some specified start date to a specified number of days into the future (<code>INTO_FUTURE</code>).</p> <p>To do so, we'll use a combination of NumPy's <code>datetime64</code> datatype (our Bitcoin dates are already in this datatype) as well as NumPy's <code>timedelta64</code> method which helps to create date ranges.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#model-10-why-forecasting-is-bs-the-turkey-problem","title":"Model 10: Why forecasting is BS (the turkey problem \ud83e\udd83)\u00b6","text":"<p>When creating any kind of forecast, you must keep the turkey problem in mind.</p> <p>The turkey problem is an analogy for when your observational data (your historical data) fails to capture a future event which is catostrophic and could lead you to ruin.</p> <p>The story goes, a turkey lives a good life for 1000 days, being fed every day and taken care of by its owners until the evening before Thanksgiving.</p> <p>Based on the turkey's observational data, it has no reason to believe things shouldn't keep going the way they are.</p> <p>In other words, how could a turkey possibly predict that on day 1001, after 1000 consectutive good days, it was about to have a far from ideal day.</p> <p> Example of the turkey problem. A turkey might live 1000 good days and none of them would be a sign of what's to happen on day 1001. Similar with forecasting, your historical data may not have any indication of a change which is about to come. The graph image is from page 41 of The Black Swan by Nassim Taleb (I added in the turkey graphics).</p> <p>How does this relate to predicting the price of Bitcoin (or the price of any stock or figure in an open market)?</p> <p>You could have the historical data of Bitcoin for its entire existence and build a model which predicts it perfectly.</p> <p>But then one day for some unknown and unpredictable reason, the price of Bitcoin plummets 100x in a single day.</p> <p>Of course, this kind of scenario is unlikely.</p> <p>But that doesn't take away from its significance.</p> <p>Think about it in your own life, how many times have the most significant events happened seemingly out of the blue?</p> <p>As in, you could go to a cafe and run into the love of your life, despite visiting the same cafe for 10-years straight and never running into this person before.</p> <p>The same thing goes for predicting the price of Bitcoin, you could make money for 10-years straight and then lose it all in a single day.</p> <p>It doesn't matter how many times you get paid, it matters the amount you get paid.</p> <p>\ud83d\udcd6 Resource: If you'd like to learn more about the turkey problem, I'd recommend the following:</p> <ul> <li>Explaining both the XIV trade and why forecasting is BS by Nassim Taleb</li> <li>The Black Swan by Nassim Taleb (epsecially Chapter 4 which outlines and discusses the turkey problem)</li> </ul> <p>Let's get specific and see how the turkey problem effects us modelling the historical and future price of Bitcoin.</p> <p>To do so, we're going to manufacture a highly unlikely data point into the historical price of Bitcoin, the price falling 100x in one day.</p> <p>\ud83d\udd11 Note: A very unlikely and unpredictable event such as the price of Bitcoin falling 100x in a single day (note: the adjective \"unlikely\" is based on the historical price changes of Bitcoin) is also referred to a Black Swan event. A Black Swan event is an unknown unknown, you have no way of predicting whether or not it will happen but these kind of events often have a large impact.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#building-a-turkey-model-model-to-predict-on-turkey-data","title":"Building a turkey model (model to predict on turkey data)\u00b6","text":"<p>With our updated data, we only changed 1 value.</p> <p>Let's see how it effects a model.</p> <p>To keep things comparable to previous models, we'll create a <code>turkey_model</code> which is a clone of <code>model_1</code> (same architecture, but different data).</p> <p>That way, when we evaluate the <code>turkey_model</code> we can compare its results to <code>model_1_results</code> and see how much a single data point can influence a model's performance.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#compare-models","title":"Compare Models\u00b6","text":"<p>We've trained a bunch of models.</p> <p>And if anything, we've seen just how poorly machine learning and deep learning models are at forecasting the price of Bitcoin (or any kind of open market value).</p> <p>To highlight this, let's compare the results of all of the modelling experiments we've performed so far.</p>"},{"location":"10_time_series_forecasting_in_tensorflow/#exercises","title":"\ud83d\udee0 Exercises\u00b6","text":"<ol> <li>Does scaling the data help for univariate/multivariate data? (e.g. getting all of the values between 0 &amp; 1)</li> </ol> <ul> <li>Try doing this for a univariate model (e.g. <code>model_1</code>) and a multivariate model (e.g. <code>model_6</code>) and see if it effects model training or evaluation results.</li> </ul> <ol> <li>Get the most up to date data on Bitcoin, train a model &amp; see how it goes (our data goes up to May 18 2021).</li> </ol> <ul> <li>You can download the Bitcoin historical data for free from coindesk.com/price/bitcoin and clicking \"Export Data\" -&gt; \"CSV\".</li> </ul> <ol> <li>For most of our models we used <code>WINDOW_SIZE=7</code>, but is there a better window size?</li> </ol> <ul> <li>Setup a series of experiments to find whether or not there's a better window size.</li> <li>For example, you might train 10 different models with <code>HORIZON=1</code> but with window sizes ranging from 2-12.</li> </ul> <ol> <li>Create a windowed dataset just like the ones we used for <code>model_1</code> using <code>tf.keras.preprocessing.timeseries_dataset_from_array()</code> and retrain <code>model_1</code> using the recreated dataset.</li> <li>For our multivariate modelling experiment, we added the Bitcoin block reward size as an extra feature to make our time series multivariate.</li> </ol> <ul> <li>Are there any other features you think you could add?</li> <li>If so, try it out, how do these affect the model?</li> </ul> <ol> <li>Make prediction intervals for future forecasts. To do so, one way would be to train an ensemble model on all of the data, make future forecasts with it and calculate the prediction intervals of the ensemble just like we did for <code>model_8</code>.</li> <li>For future predictions, try to make a prediction, retrain a model on the predictions, make a prediction, retrain a model, make a prediction, retrain a model, make a prediction (retrain a model each time a new prediction is made). Plot the results, how do they look compared to the future predictions where a model wasn't retrained for every forecast (<code>model_9</code>)?</li> <li>Throughout this notebook, we've only tried algorithms we've handcrafted ourselves. But it's worth seeing how a purpose built forecasting algorithm goes.</li> </ol> <ul> <li>Try out one of the extra algorithms listed in the modelling experiments part such as:<ul> <li>Facebook's Kats library - there are many models in here, remember the machine learning practioner's motto: experiment, experiment, experiment.</li> <li>LinkedIn's Greykite library</li> </ul> </li> </ul>"},{"location":"10_time_series_forecasting_in_tensorflow/#extra-curriculum","title":"\ud83d\udcd6 Extra-curriculum\u00b6","text":"<p>We've only really scratched the surface with time series forecasting and time series modelling in general. But the good news is, you've got plenty of hands-on coding experience with it already.</p> <p>If you'd like to dig deeper in to the world of time series, I'd recommend the following:</p> <ul> <li>Forecasting: Principles and Practice is an outstanding online textbook which discusses at length many of the most important concepts in time series forecasting. I'd especially recommend reading at least Chapter 1 in full.<ul> <li>I'd definitely recommend at least checking out chapter 1 as well as the chapter on forecasting accuracy measures.</li> </ul> </li> <li>\ud83c\udfa5 Introduction to machine learning and time series by Markus Loning goes through different time series problems and how to approach them. It focuses on using the <code>sktime</code> library (Scikit-Learn for time series), though the principles are applicable elsewhere.</li> <li>Why you should care about the Nate Silver vs. Nassim Taleb Twitter war by Isaac Faber is an outstanding discussion insight into the role of uncertainty in the example of election prediction.</li> <li>TensorFlow time series tutorial - A tutorial on using TensorFlow to forecast weather time series data with TensorFlow.</li> <li>\ud83d\udcd5 The Black Swan by Nassim Nicholas Taleb - Nassim Taleb was a pit trader (a trader who trades on their own behalf) for 25 years, this book compiles many of the lessons he learned from first-hand experience. It changed my whole perspective on our ability to predict.</li> <li>3 facts about time series forecasting that surprise experienced machine learning practitioners by Skander Hannachi, Ph.D - time series data is different to other kinds of data, if you've worked on other kinds of machine learning problems before, getting into time series might require some adjustments, Hannachi outlines 3 of the most common.</li> <li>\ud83c\udfa5 World-class lectures by Jordan Kern, watching these will take you from 0 to 1 with time series problems:<ul> <li>Time Series Analysis - how to analyse time series data.</li> <li>Time Series Modelling - different techniques for modelling time series data (many of which aren't deep learning).</li> </ul> </li> </ul>"},{"location":"11_passing_the_tensorflow_developer_certification_exam/","title":"Preparing for the TensorFlow Developer Certification (archive)","text":"<p>Note: As of 1 May 2024, the TensorFlow Developer Certification is no longer available for purchase. After being in contact with the TensorFlow Certification team, they stated they were closing the program with no official next steps (see email below).</p> <p>With this in mind, the details on this page are for archive reasons only. The materials in the course are still valid to learn TensorFlow/Deep Learning in general. The TensorFlow Developer Certificate was always an optional extension to the course.</p> <p>See #645 on GitHub for more.</p> <p></p> <p>After going through the Zero to Mastery TensorFlow for Deep Learning course, you might be interested in taking the TensorFlow Developer Certification exam.</p> <p>If so, these steps will help you.</p> <ul> <li>\ud83d\udcd6 Resource: Get the slides for this section</li> </ul>"},{"location":"11_passing_the_tensorflow_developer_certification_exam/#preface","title":"Preface","text":"<p>I took and passed the TensorFlow Developer Certification exam myself shortly after it came out. After which, I wrote an article and made a YouTube video on how I did it and how you can too.</p> <p>Many of the course materials as well as the document you're reading now, were built with the following two resources in mind:</p> <ul> <li>\ud83d\udcc4 Read: How I got TensorFlow Developer Certified (and how you can too)</li> <li>\ud83c\udfa5 Watch: How I passed the TensorFlow Developer Certification exam (and how you can too)</li> </ul> <p> My TensorFlow Developer Certificate (delivered after passing the exam).</p>"},{"location":"11_passing_the_tensorflow_developer_certification_exam/#what-is-the-tensorflow-developer-certification","title":"What is the TensorFlow Developer Certification?","text":"<p>The TensorFlow Developer Certification, as you might\u2019ve guessed, is a way to showcase your ability to use TensorFlow.</p> <p>More specifically, your ability to use TensorFlow (the Python version) to build deep learning models for a range of tasks such as regression, computer vision classification (finding patterns in images), natural language processing (finding patterns in text) and time series forecasting (predicting future trends given a range of past events).</p>"},{"location":"11_passing_the_tensorflow_developer_certification_exam/#why-the-tensorflow-developer-certification","title":"Why the TensorFlow Developer Certification?","text":"<p>My first reason was fun. I wanted to give myself a little challenge to work towards and a reason to read a new book I\u2019d purchased (Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow 2.0).</p> <p>But two other valid reasons are:</p> <ol> <li>To acquire the foundational skills required to build machine learning powered applications (you've already done this with the rest of the course)</li> <li>Showcasing your skill competency to a future employer*</li> <li>Get added to the TensorFlow Certificate Network</li> </ol> <p> Once you pass the TensorFlow Developer Certification, you get added to the TensorFlow Certificate Network, a resource people from around the world can use to find TensorFlow Certified Developers.</p> <p>*You can do this in a multitude of ways. For example, creating a GitHub repo where you share your code projects and a blog where you write about the things you've learned/worked on.</p>"},{"location":"11_passing_the_tensorflow_developer_certification_exam/#certificates-are-nice-to-have-not-need-to-have","title":"Certificates are nice to have not need to have","text":"<p>I got asked whether the certification is necessary in a\u00a0livestream Q&amp;A about this course.</p> <p>It\u2019s not.</p> <p>In the tech field, no certification is\u00a0needed. If you have skills and demonstrate those skills (through a blog, projects of your own, a nice-looking GitHub), that is a certification in itself.</p> <p>A certification is only one form of proof of skill.</p> <p>Rather than an official certification, I\u2019m a big fan of starting the job before you have it.</p> <p>For example, if you have some ideal role you\u2019d like to work for a company as. Say, a machine learning engineer. Use your research skills to figure out what a machine learning engineer would do day to day and then start doing those things.</p> <p>Use courses and certifications as foundational knowledge then use your own projects to build specific knowledge (knowledge that can\u2019t be taught).</p> <p></p> <p>Do certificates guarantee a job? Tweet by Daniel Bourke.</p> <p>With that being said if you did want to go for the certification, how would you do it?</p>"},{"location":"11_passing_the_tensorflow_developer_certification_exam/#how-to-prepare-your-brain-for-the-tensorflow-developer-certification","title":"How to prepare (your brain) for the TensorFlow Developer Certification","text":"<p>First and foremost, you should have experience writing plenty of TensorFlow code.</p> <p>After all, since certification is proof of skill, there's no point in going for certification if you don't have some sort of skill at using TensorFlow (and deep learning in general).</p> <p>If you've gone through the Zero to Mastery TensorFlow for Deep Learning course, coded along with the videos, done the exercises, you've got plenty of skill to take on the exam.</p> <p>To prepare, go through the TensorFlow Developer Certificate Candidate Handbook. Use this as your ground truth for the exam.</p> <p>It's a well-written document so I'm not going to repeat anything from within it here, rather suggest some actions which you might want to take.</p>"},{"location":"11_passing_the_tensorflow_developer_certification_exam/#the-skills-checklist","title":"The Skills Checklist","text":"<p>Going through the handbook, you'll come across a section named the Skills checklist. It is what it says it is.</p> <p>Of course, I'm not going to tell what's actually on the exam. But looking at the skills checklist, you'll find sections on:</p> <ol> <li>TensorFlow Developer Skills (TensorFlow fundamentals)</li> <li>Building and training neural networks using TensorFlow 2.x</li> <li>Image classification</li> <li>Natural language processing (NLP)</li> <li>Time series, sequences and predictions</li> </ol> <p>If there are five sections, can you guess how many questions will be on the exam?</p> <p>Each of the questions requires you to submit a trained model in <code>.h5</code> format.</p> <p>If you've been through the course materials, you know how to do this.</p> <p>The model you submit is graded on how well it performs. Don't overthink this. If you build a fairly well-performing model, chances are it'll pass. If you think your model needs to improve its performance, go through the steps covered in the Improving a model section of the course. </p> <p>You've got 5-hours during the exam to build and train models on the datasets provided. The models will not take long to train (even on CPU). This is plenty of time.</p> <p>\ud83d\udee0 Exercises</p> <ol> <li>Read through the TensorFlow Developer Certification Candidate Handbook.</li> <li>Go through the Skills checklist section of the TensorFlow Developer Certification Candidate Handbook and create a notebook which covers all of the skills required, write code for each of these (this notebook can be used as a point of reference during the exam).</li> </ol> <p> Example of mapping the Skills checklist section of the TensorFlow Developer Certification Candidate handbook to a notebook.</p>"},{"location":"11_passing_the_tensorflow_developer_certification_exam/#how-to-prepare-your-computer-for-the-tensorflow-developer-certification","title":"How to prepare (your computer) for the TensorFlow Developer Certification","text":"<p>Got TensorFlow skills? </p> <p>Been through the TensorFlow Developer Certification Candidate Handbook? </p> <p>Decided you're going to take on the exam?</p> <p>Now it's time to set your computer up.</p> <p>The exam takes place in PyCharm (a Python integrated developer environment or IDE). If you've never used PyCharm before, not to worry, you can get started using the PyCharm quick start tutorial. Plus, being able to get setup in a new development environment is part of being a skilled developer.</p> <p>But wait, PyCharm has a lot going on, how do I know to set it up for the TensorFlow Developer Certification Exam?</p> <p>There's a guide for that!</p> <p>The TensorFlow team have written a guide (similar to the handbook above) on how to set up your environment to take TensorFlow Developer Certification Exam.</p> <p>Again, I'm not going to repeat what's mentioned in the document too much because it's another well-written guide (plus, if things change over time, new versions etc, best to adhere to the guide).</p> <p>Reading through this as well as following each of the tests it suggests will ensure your computer is ready to go.</p> <p>\ud83d\udee0 Exercises</p> <ol> <li>Go through the PyCharm quick start tutorials to make sure you're familiar with PyCharm (the exam uses PyCharm, you can download the free version).</li> <li>Read through and follow the suggested steps in the setting up for the TensorFlow Developer Certificate Exam guide.</li> <li>After going through (2), go into PyCharm and make sure you can train a model in TensorFlow. The model and dataset in the example <code>image_classification_test.py</code> script on GitHub should be enough. If you can train and save the model in under 5-10 minutes, your computer will be powerful enough to train the models in the exam.<ul> <li>Make sure you've got experience running models locally in PyCharm before taking the exam. Google Colab (what we used through the course) is a little different to PyCharm.</li> </ul> </li> </ol> <p> Before taking the exam make sure you can run TensorFlow code on your local machine in PyCharm. If the example <code>image_class_test.py</code> script can run completely in under 5-10 minutes on your local machine, your local machine can handle the exam (if not, you can use Google Colab to train, save and download models to submit for the exam).</p>"},{"location":"11_passing_the_tensorflow_developer_certification_exam/#troubleshooting-tidbits","title":"Troubleshooting tidbits","text":"<p>If you've been through the Zero to Mastery TensorFlow for Deep Learning course, you've had plenty of experience troubleshooting different models. </p> <p>But for reference, here are some of the main issues you might run into (inside and outside of the exam):</p> <ul> <li>Input and output shapes \u2014 print these out if you're stuck.</li> <li>Input and output datatypes \u2014 TensorFlow usually prefers float32.</li> <li>Output activation functions \u2014 for classification: <code>sigmoid</code> vs <code>softmax</code>, which one should you use?</li> <li>Loss functions \u2014 for classification <code>sparse_categorical_crossentropy</code> vs <code>categorical_crossentropy</code>, which one should you use?</li> <li>Ways to improve a model \u2014 if your model isn't performing as well as it should, what can you do?</li> </ul>"},{"location":"11_passing_the_tensorflow_developer_certification_exam/#questions","title":"Questions","text":"<p>Can I use external resources (Google, Stack Overflow, TensorFlow documentation, previous code) during the exam?</p> <p>Yes. The exam is open book. You will have access to all of the resources you would usually have access to whilst writing TensorFlow code outside of the exam.</p> <p>Do I need a GPU?</p> <p>No. The models built in the exam aren't extremely large. So you can train them on CPU. I'd advise trying out to see if you can run one of the example models and if you can train in under 5-10 minutes (without GPU), you can proceed with the exam. If you need access to a GPU, you can always train a model on Google Colab and download it in <code>.h5</code> and then submit it during the exam.</p> <p>Can I just do the courses, read the book(s) and practice myself, do I really need the certificate?</p> <p>Of course. At the end of the day, skills are what you should be after, not certificates. Certificates are nice to haves not need to haves.</p> <p>If you say certificates aren\u2019t needed, why\u2019d you get it?</p> <p>I like having a challenge to work towards. Setting a date for myself, as in, \u201cI\u2019m taking the exam on June 3rd\u201d, gave me no choice but to study.</p>"},{"location":"11_passing_the_tensorflow_developer_certification_exam/#extra-curriculum","title":"Extra-curriculum","text":"<p>If you'd like some extra materials to go through to further your skills with TensorFlow and deep learning in general or to prepare more for the exam, I'd highly recommend the following:</p> <ul> <li>TensorFlow in Practice Specialization on Coursera</li> <li>Hands-On Machine Learning with Scikit-Learn, Keras &amp; TensorFlow 2nd Edition</li> <li>MIT Introduction to Deep Learning</li> </ul>"}]}